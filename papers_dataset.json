[
    {
        "context": "# KEYWORDS:\n\nRamp reversal memory, TaS2, metal insulator phase transition, non-volatile memory, correlated electron systems, transition metal dichalcogenides.# The Ramp Reversal Memory (RRM)\n\nThe ramp reversal memory (RRM) is a recently discovered non-volatile memory effect, observed in a number of thin film transition metal oxides (TMOs) with temperature driven metal to insulator transitions (MIT).[1] The system can be made to remember one or more temperatures using a simple heating and cooling protocol. There is a well-established heuristic model for the effect, but to date there is still no accepted first-principal explanation. A major question which we address is \u2013 how general is the RRM? I.e., does it appear in systems other than TMOs? Herein, we demonstrate an RRM effect in a different material system, a bulk crystal of the transition metal dichalcogenide 1T-TaS2.[2]\n\nThe RRM has been reported for three thin films of correlated TMOs: VO, V2O3 and NdNiO3,[1,3\u20135] all of which exhibit a first order temperature driven MIT, with a hysteresis between the heating and cooling curves. Their temperature driven MIT is coupled to a structural transition (or deformation, in the case of NdNiO3).[6\u201311] Other properties these systems have that are presumed necessary for the appearance of the memory effect are:\n\n1. The systems exhibit a spatially-phase-separated state during the phase transition, where both insulating and metallic domains co-exist.[12\u201314]\n2. The transition temperature can be affected by various local properties, such as strain, doping and oxygen vacancies.[8,11,15\u201321]\n\nThe RRM was observed as a resistance change in the resistance vs temperature (R-T) curve (during heating) following a simple heating and cooling protocol. A detailed exemplification of the effect is provided in the supplementary material (SM) section S1[SM] and further detailed in previous publications.[1,3\u20135] In a nutshell, beginning from the low-temperature insulating phase, the system is heated to some temperature in the spatially phase-separated state, referred to as the reversal temperature \u2013 TR. Then the system is cooled again, producing a so-called reversal loop (RL). In the following heating curve, which is the memory reading measurement, the R-T exhibits a higher resistance than in the non-perturbed measurement, and the change is the largest near TR.\n\nThe change is attributed to local increases in the transition temperature of the MIT, as will be elaborated below. Thus, to analyze the change, one can look at the temperature dependent shift in the transition temperature, \u0394T, which in essence is the temperature difference between two points on the R-T curves of the perturbed and unperturbed measurement with the same resistance value, see SM section S1 and Ref[4,5] for details. It is also possible to analyze the temperature dependent normalized change in resistance, \u0394R/R. When plotting \u0394R/R or \u0394T as a function of temperature, a peak appears corresponding to the reversal temperature (TR) \u2013 this is the manifestation of the RRM.# R-T Heating Measurement and RRM Effect\n\nSince heating to the fully metallic state results in erasing of the memory, the following R-T heating measurement returns to the unperturbed R-T curve.# Heuristic Model of RRM\n\nA heuristic model can capture all properties of the RRM. It postulates that if the temperature ramp reversal (from heating to cooling) is performed while the system is in the spatially phase-separated state, then the local transition temperature of the phase boundaries (between the insulating and metallic coexisting domains) increases. We refer to the phase boundaries with increased MIT transition temperature as scars. Thus, during the following resistance measurement there is a delay in the advancement of the transition, which appears as an averaged temperature delay in the transition. Hence, the measurement of \u0394T is a meaningful estimation to the magnitude of the RRM effect.[4,5] Finally, when the temperature is increased above TR, the previous scars are \u201chealed\u201d, and the memory is erased. The underlying mechanism responsible for this TC shift is still not substantiated and it may alter between different systems. Mechanisms including local strain[5] and local oxygen vacancy motion[3] were suggested. Based on this model, the RRM should potentially appear in other systems with similar characteristics.# Additional Properties of RRM\n\nAdditional properties of the RRM which are important to mention in the context of this research are:\n\n1. The RRM appears only on the heating curve and not on the cooling curve.\n2. The magnitude of the \u0394T measurement increases when more reversal loops are performed (prior to measurement).\n3. The effect is nonvolatile, and if the system is not heated above the reversal temperature the memory is retained.\n4. It is possible to write a few memories simultaneously, as long as a previous memory is not erased by heating the sample above its reversal temperature.# Demonstration of RRM Effect in 1T-TaS2\n\nIn this paper, we demonstrate that the RRM effect is also present in the correlated layered material 1T-TaS2. 1T-TaS2 is a Van der Waals transition metal dichalcogenide, with each layer composed of a plane of hexagonally arranged Ta atoms sandwiched by two S layers and the central Ta atoms are arranged in an octahedral structure. It undergoes several temperature-driven first order charge density wave (CDW) phase transitions, which include MIT transitions of different magnitude.[22,23]# R-T Measurement of 1T-TaS2\n\nIn Figure 1a we plot an R-T of the crystal we measured (see SM S2 for details on synthesis[SM]). Upon cooling, 1T-TaS2 transitions from an incommensurate CDW (I-phase) below 550K to a nearly commensurate CDW (N-phase) at 350K, and finally to a commensurate CDW (C-phase) below 180K, which is accompanied by a metal insulator transition with a resistance change of nearly one order of magnitude. The heating process reveals a slightly different...# Results\n\nMacroscopic millimeter-sized high-quality single crystals of 1T-TaS2 were grown using the chemical vapor transport method. [40,41] Figure 1a shows the R-T measurements of one such crystal and Figure 1b shows the R-T are well repeated when performing additional R-T loops. The R-T measurements were performed in our home-made cryostat, see SM S3[54] for an image of the device and the experimental section for measurement details.\n\nFigure 2 demonstrates the memory writing protocol of the RRM effect for a 1T-TaS2 bulk sample. The RRM writing and measurement protocol for reversal temperatures on the heating curve (like those used in the study of RRM in correlated oxides [1,4]) are plotted in Figure 2a-c, and the measurements of the RRM on the cooling curve are presented in Figure 2d-g. We plot the R-T curves in Figure 2a alongside the corresponding temperature vs time plot in Figure 2b (time on the y-axis). The \u2018heating\u2019 measurement starts at low temperatures (i.e., the full C-phase, 140K), ramps up to the full T-phase at 240K, and then returns to the initial low temperature, forming a major loop (ML). This establishes a baseline curve for comparison with subsequent measurements (green and orange curves). Next, the sample is heated to a specific \u201creversal temperature\u201d (TR) of 219K, i.e., an intermediate temperature in the phase-coexistence stage (marked by a vertical gray dashed line in Figures 2a-c), and then cooled back to 140K, creating a reversal loop (RL).# Memory Effects in 1T-TaS2\n\nPerformed 5 consecutive RLs, ending at 140K. Finally, another ML is performed by heating the sample to 240K and cooling back to 140K, constituting the memory reading stage (red curve).[1] Additional MLs are then performed to evaluate system stability and verify (in case there is a memory) that the memory is erased and the baseline is recovered (black and purple curves). Memory is assessed by comparing the ML heating curve before and after the RLs (red and orange curves). When the memory effect is present, the curves differ significantly near TR. However, as depicted by the red curve in Figure 1c, there is no clear modification in the resistance, and the R-T curve following the RL is similar to other ML heating curves. So, there is no signature of the RRM in the heating curve.\n\nA similar protocol modified to apply to the cooling curve is shown in Figure 2d,e, where now the RLs are performed by cooling from the T-CDW phase at 240K down to a reversal temperature. The protocol begins with a ML (from 240K to 140K) followed by four RLs (from 240K to 190.3K) and concludes with additional MLs. Now, when comparing the R-T curves of the ML before the RLs and after the RLs (red curve and orange curves, accordingly, plotted in Figures 2d and 2f) a striking difference is observed \u2013 the MIT seems to be shifted to lower temperatures. However, in the following ML (after heating to 240K and cooling again) the baseline R-T is recovered (black curve in Figure 2f). I.e., the RLs resulted in a memory effect that is erased after cooling the 1T-TaS2 to low enough temperatures. This is one of the main features of the RRM effect, which was not previously reported for RLs on the cooling curve. The transition in 1T-TaS2 from the N-phase to the C-phase can also be observed as a decrease in the magnetic susceptibility as a function of temperature, showing a hysteresis similar to that observed in the transport measurements. We attempted to measure the RRM effect in the cooling curve of the susceptibility as well. Somewhat surprising, we could not observe a memory effect in the susceptibility measurements, see SM S5 for further details.[54] Measurements of the 1T-TaS2 crystal were performed for different temperature ramp rates and for additional 1T-TaS2 crystals from different batches (prepared at different times), all showing similar results, see SM section S4 for representative measurements.[54]\n\nWe emphasize that R-T behavior of the RLs is qualitatively different than that observed when measuring minor loop effects in systems with hysteresis.[42] In such systems, if the extremum temperatures are not outside the hysteretic region one would expect the phase of the system in consecutive minor loops to be driven toward its final state, i.e., toward higher resistance.# Memory Properties in 1T-TaS2\n\nFollowing, we demonstrate the memory properties observed in 1T-TaS2 and compare them to the RRM properties reported previously in thin film TMOs.[1,4,5] Two different procedures to analyze the RRM[1,4] were previously presented: (1) calculate the normalized resistance difference (\u0394R/R) vs. temperature of the ML following the RLs, and (2) calculate the temperature shift (\u0394T) required to return to the original resistance at each temperature and plot it vs. temperature, see Figure 2f where both \u0394T and \u0394R are marked. The results of these analyses are plotted in Figure 2g, where a clear peak appears in both cases (solid orange curve for the \u0394R/R analysis and solid blue curve for the \u0394T analysis). The peak corresponds to the reversal temperature. Comparing analyses of the next ML (dashed orange and blue curves, following the same color coding, Figure 2g) shows the peaks have disappeared, demonstrating erasure of the memory. These features are similar to those reported previously for systems with RRM. There is, however, an interesting difference between them (in addition to the aforementioned difference that now the RLs are on the cooling curve): The magnitude of the temperature shift, \u0394T, is above 1.5K, while the highest value measured previously was 0.6 K[5] and is usually around 0.15K.[4,5] We note there is an observable dip around 195K in both analyses that appears in most measurements with no relation to the reversal temperature.# The magnitude of the RRM effect\n\nThe magnitude of the RRM effect was shown to be related to the measurement of \u0394T, while the \u0394R/R measurement is proportional to the magnitude of the resistance change across the MIT,[5] so the \u0394T analysis has a physical interpretation. Below we show various features of the RRM using the \u0394T analysis, all features were observed also in \u0394R/R measurements.# Figure 3a\n\nFigure 3a demonstrates that the position of the \u0394T peak follows the reversal temperature, as is the case for systems with RRM (the measurement protocol is identical to that presented in Figure 2e). Three different TRs were measured, and the \u0394T vs T analyses are plotted, where the reversal temperature (marked by the perpendicular lines) coincides with the corresponding peak position.# Figure 3b\n\nFigure 3b presents the \u0394T magnitude for different number of reversal loops (TR=191.58K), for 3, 7 and 13 RLs. The measurement protocol is detailed in SM S5.[54] The peak position remains the same while the peak magnitude increases with the number of RLs and seems to saturate when the number of loops increases. This is similar behavior to the reported behavior in RRM systems [1,4].# Figure 3c\n\nIn Figure 3c we show the ability to write memories simultaneously, and their dependence on the order they are written. The measurement protocol is detailed in SM S5.[54] In one case we perform 3 RLs to TR = 185.6K, followed by 3 additional RLs to a higher TR = 191K, and only then measure the memory effect (via a ML). The result is plotted as the blue curve in Figure 3c, where 2 peaks are discerned. In the second case, the order is reversed, i.e., we first perform RLs to the higher TR =191K and then 189.7K.# Memory Effects in RRM Systems\n\nThe lower TR = 185.6K. In this case (cyan curve in Figure 3c) the \u0394T plot shows a single peak, corresponding to the effect of the second and lower TR. The perpendicular lines mark the TRs. This outcome indicates that (i) it is possible to write more than one memory, and (ii) reaching temperatures below the reversal temperature erases the memory set at higher TRs. This is one of the principal properties of RRM systems.# Volatility of the Memory Effect\n\nNext, we focus on volatility of the memory effect in various scenarios. We measured the time dependence of dwelling at high temperatures after writing a memory. The system was stabilized at 280K, once for a 3-hour dwell and another time for an 18-hour dwell. The results and the measurement protocol are presented in Figures 4a and 4b, showing that the memory is not volatile on these time scales.# RRM in Oxides\n\nWhen we studied the RRM in oxides, cooling to very low temperatures did not erase the effect. However, in 1T-TaS2 the memory is on the cooling curve, so it is reasonable that at high enough temperatures the memory will be erased. We performed the memory writing protocol using different maximum temperatures (Tmax) of both the MLs and RLs, the results are presented in Figure 5.# Figure 5 Summary\n\nFigure 5. RRM measurements of a TR=187.3K with different Tmax. TR is marked. (a) \u0394T vs T plot of the measurements. (b) The \u0394T peak as a function of Tmax of the measurement. The \u0394T peaks in b and the \u0394T plots in a have the same color coding.# 280K\n\nAll measurements exhibit similar \u0394T magnitude, around 1.2K (Note that also the peak position is the same, 187.3K). Starting at Tmax = 285K, the \u0394T peak begins to decrease, and the memory disappears around ~300K. This result cannot be compared to other RRM systems and is unique (at this point) to 1T-TaS2. We will return to this result in the discussion.# Figure 6\n\nRRM measurement of TR = 187.3K, with and without dwell at TR. (a) \u0394T vs T of the measurements. Blue, with three-hours dwell at TR, red, without a dwell. Solid lines mark the \u0394T peaks of the measurements, with the same color coding. A shift of 3.2K between the \u0394T peaks of the measurements is marked. (b) Resistance vs temperature of the measurements. In-set: A zoom-in plot of the R vs T curve, focusing on the TR region. A shift of 3.2K between the TR of the measurements is marked, corresponding to the shift in (a). (c) Resistance vs time of the measurements. It can be observed that during the whole three-hours dwell, the resistance of the sample continues to increase. The detailed protocol can be found in SM S7.The delay cannot be attributed to some stabilization time between the sample\u2019s temperature and the temperature sensor.# Memory Effect in Thick Flakes of Exfoliated 1T-TaS2\n\nWe attempted to measure the memory effect in thick flakes of exfoliated 1T-TaS2. A light microscope image of a 47nm thick flake is shown in Figure 7e, see SM S9 for information on device preparation.[54] We started with a memory writing protocol in the cooling curve, as illustrated in Figure 7a,b. Here 10 RLs were conducted, with a TR of 159K and MLs with a range of 100-240K. This measurement reveals a number of interesting outcomes. First, we see that the transition in the cooling branch is steplike, while in the heating curve the transition occurs more gradually, similar to the bulk crystal. Second, the MIT transition temperature in the cooling curve is different for each cooling cycle and the hysteresis is extremely wide. Flakes with wide hysteresis were previously reported.[46,47] We could not find previous reports showing these large variations.# Discussion\n\nBefore we delve into the discussion of the RRM properties in macroscopic 1T-TaS2, we highlight features related to the ramp-reversal experiments with the thick flakes. We observed a very sharp step-like transition in the cooling curve, and that the switching temperature varies widely with each cooling-cycle, between 144K and 169K. The heating curve, in contrast, is more smooth and does not fluctuate as much between heating cycles. This could be a result of strain from the substrate, and some stochasticity in the switching process for cooling, while there is not much strain when heating. We are not aware of this feature addressed in previous reports where flakes of 1T-TaS2 were measured. As far as the RRM is concerned, we could not measure it in the flakes. This is due to the sharp MIT that changes with temperature, making it impossible to attain a phase coexistence state of the metal and insulator, which is a crucial ingredient for memory.# Measurements and Observations\n\nMeasurements are above 250K. Unlike previous reports of hidden states in 1T-TaS2, where the metastable states were achieved by exciting the system out of equilibrium, here it occurs through thermal equilibrium, requiring only heating and cooling of the sample. The suggested explanation for the scar formation is general and heuristic in nature, requiring development of a theoretical microscopic explanation. Solving this issue will require further studies with different measurement techniques, such as local probe measurements of the RRM.# Comparison of RRM in 1T-TaS2 and TMOs\n\nAn additional difference between the RRM in 1T-TaS2 and TMOs, is the magnitude of the \u0394T peak. The \u0394T peak magnitude measured in previous systems was ~0.15K in VO2 on c-cut and r-cut sapphire, 0.2K in NdNiO3, and a maximum value of \u0394T~0.6K was observed for VO2 deposited atop SiO2. Here, we measured a \u0394T peak surpassing 2.5K (for a few RLs). Note that this value indicates how much the transition temperature is shifted due to the memory effect, and a 2.5K shift is a substantial effect.# Factors Affecting the Magnitude of the Peak\n\nWe identify two parameters that can affect the magnitude of the peak. The first one is that the local temperature shift at the phase boundaries is larger in TaS2 during the \u2018writing\u2019 stage, resulting in a shift in the measured R-T curves. It was shown for VO2 thin films that if they are less strained by the substrate, i.e., when the film is not lattice matched, then during the ramp reversal, the phase-boundary distortions (scars) are more free to reorganize. This resulted in more stable scars, that require larger energy to be released, i.e., a large \u0394T and a larger RRM effect. Here, 1T-TaS2 is a 3-dimensional macroscopic Van der Waals layered material. It is reasonable that flakes at the phase boundary of the phase-separated state could slip relative to one another, resulting in high-energy phase-boundary scars.\n\nA second source is related to how the local properties of 1T-TaS2 change spatially. According to the RRM heuristic model the memory develops in the phase boundaries of the phase-separated domains (here between the T- and C-phases). If the transition temperatures of nearby domains are correlated, i.e. the spatial change of TC is slow, then the \u0394T peak will be smaller than if the neighboring domains are not correlated and may have very different switching temperatures. The reasoning is that if the domains are less correlated then more T-phase islands will appear, resulting in more phase-boundaries during the reversal, and then more areas where the transition temperature changes, leading to a larger effect.# Numerical Simulations\n\nTo demonstrate this, we performed numerical simulations where we include a \u201ccorrelation length\u201d parameter. The model simulates the nucleation and growth of the C-phase in the T-phase.# Numerical simulation of the RRM effect in 1T-TaS# Figure 8\n\nAs the system is cooled. As the correlation length increases, the Tc of more distant sites will be correlated. The local Tc and the correlation length are controlled by randomly assigning Tcs from a Gaussian distribution to a specific percentage of sites, and then smoothly changing the Tc between these sites. The correlation length is then defined as the average distance between uncorrelated sites, so that 1/1 indicates adjacent sites are uncorrelated and as 1 \u2192 0 means the correlation length \u2192 \u221e and all sites are correlated. See Figure 8b for an example of a Tc map for a realization with 3.16 sites. In each realization of the simulation, the system follows a standard RRM protocol and calculates the \u0394T plot between the \u201creading\u201d curve and \u201cbase\u201d curve. For more details on the numerical simulation see SM S10.[54]\n\n|Temperature (K)|Homon memory C-CDW|Homon memory T-CDW|\n|---|---|---|\n|175| | |\n|180| | |\n|185| | |\n|190| | |\n|195| | |\n|200|200| |\n|205| | |\n\n(a) Schematic of the RRM memory writing stages, different CDW phases, and resistance as a function of temperature for a bulk TaS. (b) Tc distribution map for 3.16. (c) \u0394T vs temperature calculation for TR = 189K plotted as a function of different values, appearing color coded in the legend. (d) \u0394T vs temperature simulation for different TR with 1.3. (e) \u0394T vs temperature simulation for different TR with 10. The local Tc shift in d. and e. is 1.5K.# Results for simulations with different\n\nare plotted in Figure 8c. The \u0394T vs temperature curves of an RRM protocol for TR = 189K are shown, the correlation length is marked in the legend, ranging from 22.4 sites, to short correlation length 1.1, see legend. The local TC shift for the sites at the phase-boundaries during the reversal were set to 1.5K. As the correlation length increases the peak increases, starting from 0.4 and growing toward the local Tc shift of 1.5. In addition, the width of the \u0394T also grows. This demonstrates our claim that the measured \u0394T will be larger as the correlation length becomes smaller.\n\nWe attain additional insight from the simulations in Figure 8d and 8e, where we simulate the \u0394T vs temperature for different reversal temperatures (see legends) and two different correlation lengths, Figure 8d for 1.3 sites and 8e for 10 sites (the local Tc shift is 1.5K). In both cases the \u0394T peak value is larger at the center of the transition, where many sites are switching so there are more scars, and smaller toward the beginning and end of the transition where there are fewer scars. However, there are notable differences. First, the magnitude and the width of the \u0394T peak is larger for the shorter for all reversal temperatures, as explained above and shown in Figure 8c for a single TR. Second, for short there is a wide range of reversal temperature, between 185K and 195K, that have a similar \u0394T magnitude, while for longer correlation length the peak magnitude changes with each reversal temperature.# Considering the experimental results\n\nwhich show that the \u0394T is larger and wider in TaS2 as compared to TMOs, and that the magnitude is similar for different reversal temperatures (compare Figure 3a with reference [5]), we hypothesize that the correlation length in TaS2 is shorter than in TMOs. This is plausible since the TMO were thin granular films, where each grain can be a nucleation site, and the composition of adjacent grain should, in general, have similar characteristics of oxygen vacancies, strains, etc., resulting in a similar transition temperature that changes only in grains further away. Note that the correlation length should be normalized to the average size of the domains that are switching of course. The TaS2, is a single crystal, so the domains with different transition temperatures are probably larger and could have much less correlations between them.# Conclusion\n\nIn summary, we have measured the presence of the RRM in bulk 1T-TaS\u2082, demonstrating the generality of the ramp reversal effect. While 1T-TaS\u2082 has the main RRM properties as previously demonstrated in TMO systems, a notable difference is that the effect exists in the cooling branch, and not in the heating branch. We show the mechanism of the RRM is related to the phase coexistence between T- and C- CDW phases, explaining why it can only be observed on the cooling branch. However, a microscopic mechanism for memory writing in this system is still lacking. We hope this study will encourage further experimental and theoretical research into this intriguing phenomenon. Importantly, the discovery of RRM in an additional system so different from TMOs emphasizes the universality of this memory effect. Our results suggest that RRM could be a more general phenomenon in correlated materials with a phase transition occurring through spatial phase separation. Meaning, it should exist in systems with ferromagnetic, ferroelectric and other types of phase transitions, where the ramp-reversal of a driving force can lead to emergent memory phenomena in these correlated systems.# Resistance vs Temperature measurements\n\nSilver paste was used to connect wires to the 1T-TaS\u2082 bulk sample. The sample was cooled by our home-made cryostat with a temperature range of 77K to 370K. The resistance was measured by a Keithley 2400, with current source of 4mA. The temperature was controlled by a Lakeshore 335 and was ramped with 5 kelvin/min. Both 2 and 4 probe measurements were performed. We compared the two and prove that the 2 probe measurements are reliable, see SM S3 for more information.[54]# Flakes fabrication and measurements\n\n1T-TaS\u2082 flakes were fabricated by using an exfoliating technique. 1T-TaS\u2082 bulk was pressed to a tape, and then the tape was pressed onto a SiO\u2082 300nm/Si substrate, leaving flakes on the surface. Then, Au contacts were added by a photolithography mask planned for each flake. The flakes were measured by the same setup measurement of the bulk, with a voltage source of 1mV.",
        "context_id": 0,
        "question": "At what temperature does the MIT occur in 1T-TaS2?",
        "answer": [
            "190 K"
        ],
        "context_length": 27044
    },
    {
        "context": "# 1 Introduction\n\nMany complex real-world problems can naturally be formulated as multi-agent systems\u2014e.g. managing traffic (Zhang et al., 2019), controlling fleets of ride-sharing vehicles (Sykora et al., 2020) or a network of trains (Mohanty et al., 2020), optimising electricity grid usage (Khattar and Jin, 2022), and improving dynamic packet routing in satellite communication (Lozano-Cuadra et al., 2024). Improving on solutions to such problems is an important endeavour, because of the potentially immense societal benefits that they offer. Multi-Agent Reinforce-\n\n1 https://instadeepai.github.io/og-marl/\n\n\u00a92024 Formanek et al.# Multi-Agent Reinforcement Learning (MARL)\n\nMARL is a promising avenue to finding solutions to such problems, but the field faces a host of hurdles which must first be overcome. One key difficulty is the access to accurate and efficient simulators, for online experience generation and exploration. To learn robust policies, extensive interactions with an environment are usually required (Yu, 2018), which makes simulator efficiency of paramount importance. Yet, for real-world applicability, the fidelity between the simulator and reality must also be maintained. Unfortunately, this balance of achieving high throughput in an online simulator while maintaining realistic dynamics is difficult, and practitioners must often rely on more basic environments with simplifying assumptions. The situation is particularly challenging when there are many agents interacting in complex ways, as is the case in MARL.\n\nWhat typically does exist in such systems as those described above is the ability to capture large amounts of useful data. Across a range of complex control scenarios, even in situations where many agents are acting and the physical dynamics are not well understood (i.e. where designing a bespoke simulator would be very challenging), it may be straightforward to record data during operation. This opportunity is what offline RL leverages, by bridging the gap between RL and supervised learning. In the offline domain, the aim is to develop algorithms that use large, existing datasets of sequential decision-making transitions (whether recorded from the real-world, or created in simulation, or a mixture thereof) to learn optimal control policies, which can later be deployed online (Levine et al., 2020). The offline paradigm promises to help unlock the full potential of RL when applied to the real-world, where success has thus far been limited (Dulac-Arnold et al., 2021; Rafael Figueiredo Prudencio, 2024). In the multi-agent setting, algorithms are designed to learn a joint policy from a static dataset of previously collected multi-agent transitions, generated by a set of interacting behaviour policies.\n\nSingle-agent offline RL has enjoyed relatively widespread research attention and success (Prudencio et al., 2023). Core to such developments, the community has benefited greatly from standardised and publicly available datasets\u2014as found in libraries such as D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020). Yet, in the multi-agent case, such offerings are limited both in number and in quality. In fact, we argue in this paper that work in offline MARL has disproportionately focused on algorithmic innovation, and neglected the role of data almost entirely. It is common practice for authors to generate their own datasets for their experiments, almost as an after-thought, while little effort has been made to understand how the quality and content of these multi-agent datasets affect training dynamics and final performance. Such insights have been immensely valuable in the single-agent setting (Schweighofer et al., 2022), and it is known that multi-agent systems face additional complexities (Tilbury et al., 2024), where similar insights would be particularly useful.\n\nImportantly, it should be clear that the end-goal for offline MARL is for systems to be deployed in the real world: real datasets, yielding real policies, useful in real applications. Yet, the path there is long and winding. To make progress, we focus in this paper on a simplified context: cooperative scenarios (where existing online MARL research is most mature, and many real-world examples exist), with data recorded from simulators. Solving these simpler problems does not magically solve the more complex ones, but we start here as a necessary first step towards deploying MARL in the wild. Notice that this journey parallels the one that computer vision has undertaken, as an example of a more established.# Current State of Datasets for O!ine MARL\n\nField of machine learning\u2014before starting to tackle the enormously challenging task of, say, end-to-end learning from raw pixels for a self-driving car in the real-world (e.g. Bojarski et al. (2016)), it was important to first tackle simpler problems, like handwritten digit recognition (LeCun et al., 1989). Focusing on fundamental datasets like MNIST (LeCun et al., 2010) has been integral to the progress in computer vision. We make similar efforts for offline MARL here.\n\nOur paper is structured in the following way. We start surveying the current state of the field in Section 2, by studying how authors have, until now, been treating data in their research\u2014with the evidence showing a general lack of care in the way data is considered. We build on this finding in Section 3 to show why this carelessness is problematic. Through four clear examples, we show how the specifics of the data has a significant impact in the learned performance of algorithms, something which has previously been overlooked. We respond in Section 4 with three contributions: firstly, a clear guideline on how data should be treated in offline MARL going forward; secondly, a standardized set of datasets, comprising over 80 environment-scenario-quality combinations, with a well-documented and accessible API, and an easy mechanism to extend this repository; and finally, useful tooling for researchers to understand the nature of their datasets, as an initial effort to promote data awareness in offline MARL.# 2 The Current State of Datasets for O!ine MARL\n\nOffline MARL remains a relatively nascent field, with only a handful of papers released on the topic to date (see Table 1), but progress is accelerating. We want to understand how authors have been handling the data component of their research in the work done thus far by trying to answer the question: what is the state of the field, with respect to data itself? To do so, we present a comprehensive survey of work in empirical offline MARL, from leading peer-reviewed academic venues, to assess (1) how their data were generated and (2) what information about their datasets was provided. Though a simple assessment, we find these two axes already particularly telling in what they reveal. Table 1 summarises our findings.\n\nFrom Table 1, we firstly notice that the majority of papers assessed generated their own datasets. Each paper also creates these datasets in different ways\u2014using a wide variety of underlying online algorithms to learn policies for the generated trajectories. The dataset labels themselves also vary across papers, with no consistent naming convention. Information about the dataset properties is also sparse. To measure a given dataset\u2019s \u2018quality\u2019, the mean episode return of trajectories is often reported, but even this metric is not always given. The return distribution is mostly ignored, except for the occasional proxy of reporting the standard deviation. Essentially, there is little information presented on the contents and diversity of the experience in a given dataset. Yet these aspects of a dataset are crucial to the resulting performance of offline learning. In the single-agent literature, it has been shown that dataset properties have a marked impact on results (Schweighofer et al., 2022). In the multi-agent context, other complex aspects of coordination (Tilbury et al., 2024; Barde et al., 2024) make the contents and characteristics of datasets even more important to understand.\n\nWe observe here that the field of offline MARL has struggled to find common ground to benchmark proposed algorithms. Even accepting that many authors generate their own datasets for their papers, there has been carelessness in reporting information about such.# 3 Why Dataset Characteristics Matter\n\nClaims of algorithmic improvement become moot if a common basis of data is missing, since a dataset is one of the control variables in empirical offline MARL experiments which can impact performance significantly. We illustrate this point by giving four examples that progressively show how algorithmic results are tightly coupled with data. These examples are not intended to make sweeping claims about the field, but should rather serve as a series of \u201cproof by existence\u201d demonstrations cautioning researchers of how peculiarities of datasets may be influencing their experimental findings. When reflecting on this evidence, it becomes clear that overlooking data is problematic for the field, and that ultimately, there is a serious need for a shift in current research practices.# Dataset Mean\n\nWe begin our illustration with likely the most intuitive example: what happens to final performance when the average return of the dataset changes? We construct four distinct datasets with increasing means on three scenarios (5m_vs_6m, 3s5z_vs_3s6z and 2s3z) from the SMACv1 environment (Samvelyan et al., 2019), by subsampling episodes from OG-MARL datasets (Formanek et al., 2023a). For each dataset, we fix the standard deviation at approximately 2.0 as calculated over 2000 episodes. We then train two offline MARL algorithms, IQL+CQL (Formanek et al., 2024) and MAICQ (Yang et al., 2021), on the individual datasets and report the final evaluation episode return, averaged across 10 random seeds.# Figure 2\n\nTo demonstrate the surprising effect that the standard deviation (std) can have on the performance of an offline MARL experiment we generate 5 datasets that each had the same mean but differing std. We then train two offline MARL algorithms, IQL+CQL and MAICQ, on the data and report the final performance. We repeat the experiment across three different SMAC scenarios, two different algorithms, 10 random seeds and an evaluation batch size of 32. We then aggregate the results as per Gorsane et al. (2022).# (b) Dataset mean and standard deviation.\n\n|Median|IQM|Mean|\n|---|---|---|\n|12.0|13.5|15.0|# (c) The aggregated results across scenarios (5m_vs_6m, 3s5z_vs_3s6z and 2s3z), algorithms (IQL+CQL and MAICQ) and random seeds are given for each dataset. The median, interquartile mean (IQM), and mean are all given with bootstrap confidence intervals (Agarwal et al., 2021).\n\nWe see that the aggregated final performance of IQL+CQL and MAICQ is positively correlated with the mean episode return of the dataset used for training. As the average quality of the data improves, so does the offline learning from this dataset\u2014an intuitive result. This relationship is often used in the single-agent literature, where the final performance is reported as a percentage of the mean return in the dataset (Agarwal et al., 2019; Gulcehre et al., 2020), but doing so is rare in the multi-agent domain. In fact, the simple metric of the dataset mean is sometimes omitted entirely, as shown in Table 1.# Dataset Spread.\n\nAnother important property of a dataset in offline RL is the diversity of the experience (Agarwal et al., 2019). In the single-agent setting, it is well-understood that many offline RL algorithms benefit from more diverse datasets (Schweighofer et al., 2022). Usually, this benefit arises because diverse experience leads to better coverage of the state and action spaces, resulting in fewer out-of-distribution actions, which are known to cause issues when training offline (Fujimoto et al., 2019). However, the effect of diversity in the multi-agent setting is less well understood and should not be taken for granted. For example, Tilbury et al. (2024) demonstrate a more complex relationship between dataset.# Figure 3\n\nWe generate two datasets on 2-Agent Halfcheetah each with very similar episode return means and standard deviations, but distinct data distributions. We then train MADDPG+CQL on each dataset and report its performance over 1 million training steps. We repeat the experiment over 10 random seeds.# (a) Histogram of the episode returns for two datasets with similar means and standard deviations.\n\n|Episode return|Count|\n|---|---|\n|0|3688|\n|2000|3665|\n|4000|2885|\n|6000|2866|# (b) Mean and standard deviation.\n\ndiversity and final performance in the offline MARL setting than might be initially assumed. In the literature, the standard deviation (std) of the data\u2019s episode returns is sometimes reported as a proxy for diversity.# (c) Training curves.\n\nWe thus continue our illustration with another example: what happens when solely the std of the returns in a dataset changes? We now construct five distinct datasets, each with the same mean, but with increasing spread around that mean, keeping the number of included episodes constant. We create these datasets by subsampling from OG-MARL. Once again, we look at the three SMAC scenarios 5m_vs_6m, 3s5z_vs_3s6z and 2s3z. Figure 2 shows the respective histograms of the datasets, and the corresponding aggregated results across two algorithms (IQL+CQL and MAICQ) and 10 random seeds.\n\nIn this situation, we begin to see a more complex relationship emerge. Rather than a simple linear relationship between diversity and performance, optimal results are found at intermediate levels of std. This result is less intuitive than before (where a higher mean return simply meant higher performance), and begins to hint at the impact of multi-agent dynamics. It is worth reiterating that very few papers report the std (or some other proxy for diversity) of their datasets\u2014just three of the thirteen in Table 1 had done so.# Dataset Distribution.\n\nOur illustration continues with a question that builds on the previous two: controlling for equal mean and std, can two different dataset distributions yield different results? We construct two more datasets subsampled from OG-MARL with this property. We look at the 2halfcheetah scenario from MAMuJoCo (Peng et al., 2021).\n\n2. This setup is reminiscent of Anscombe\u2019s Quartet (Anscombe, 1973), comprising four datasets that have almost identical summary statistics, yet look markedly different when visualised.# (b) Means and standard deviations of the episode returns in subsampled datasets.\n\n|Dataset|Mean|Stddev|# Traj|# Trans|\n|---|---|---|---|---|\n|CFCQL|12.05|4.36|4992|140073|\n|OG-MARL|12.05|4.36|4992|134985|# (c) Training curves\n\nFigure 4: We use two subsampled datasets of the 5m_vs_6m scenario from SMACv1, with almost identical distributions, but from two different sources (Formanek et al., 2023a; Shao et al., 2023) (the Medium quality in both cases). We then train IQL+CQL on each dataset and report its final performance. We repeat the experiment over 10 random seeds.\n\nWe can see here that an offline algorithm, when trained on two datasets with nearly identical summary statistics, can yield significantly different final performances. In essence, these metrics of a dataset only paint a limited view of the underlying distributions, yet these distributions may have a notable impact on the results. Here, we note that even those authors from Table 1 who have reported the mean and std of their datasets, have nonetheless omitted a visualization of their datasets for further understanding. Therefore, authors might be missing key insights on the characteristics of the data they are using in their work.# Dataset Coverage.\n\nWe conclude our illustration with possibly the most illuminating example of the subtleties of dataset characteristics and their effect on performance. We ask: can we have a situation where the return distributions are very similar, such that the summary statistics are similar and the histograms are closely aligned, yet yield significantly different results for the same offline algorithm?\n\nFor this example, we look at two publicly available datasets, from OG-MARL (Formanek et al., 2023a) and CFCQL (Shao et al., 2023) respectively. We consider the 5m_vs_6m scenario from SMACv1 (Samvelyan et al., 2019), and take the Medium quality dataset from each source. To further control the experiment, we subsample the original datasets to around 140k transitions, matching the distributions of episode returns in the process. The result is not only equal mean and std but also nearly identical, visually indistinguishable histograms, which we show in Figure 4a and detail in Table 4b. We train IQL+CQL on both datasets, across 10 random seeds. The results are illustrated in Figure 4c.# Table 2:\n\nThe Joint State-Action Coverage (Joint-SACo) scores for two datasets on the 5m_vs_6m SMAC task, one from OG-MARL and the other from CFCQL (Shao et al., 2023). A lower score means there is less Joint State-Action coverage in the dataset, i.e. there are more repeated transitions in the dataset.\n\n|Dataset|Quality|Joint-SACo score|\n|---|---|---|\n|CFCQL|Medium|0.10|\n|OG-MARL|Medium|0.83|\n\nWe see here a curious outcome\u2014despite the return distributions being essentially the same, the achieved algorithm performance is significantly different. The difference cannot be explained solely by the statistics and distribution of the episode returns. Evidently, there are other significant differences between the datasets which escape the reach of our current lens. We note that episode return is itself a summary of a trajectory, and is an abstraction of the actual experience in the dataset.\n\nTo better understand what is happening here, we extend to the multi-agent setting the approach from Schweighofer et al. (2022) to measure dataset diversity. Specifically, we extend their State-Action Coverage (SACo) metric to a version that operates on the joint state and action space of agents in MARL. We define this metric in the same way as SACo: the ratio between the number of unique state-action pairs and the total number of state-action pairs in a dataset. In the multi-agent case, the difference is that the action refers to the joint action of all agents in the system. We use this Joint-SACo metric on the 5m_vs_6m datasets from Figure 4a, with the results given in Table 2.\n\nWhereas the datasets from this example have almost identical return distributions, we see that they have very different values for state-action coverage. In fact, from Table 2 we observe that 90% of the data from Shao et al. (2023) are repetitions of previously seen state-action pairs, compared to just 17% in the data from Formanek et al. (2023a). This finding illuminates how complexities in multi-agent dynamics fail to be encapsulated solely in episode return values, and why we must be particularly careful.# Takeaways.\n\nMuch of the multi-agent literature uses datasets that are self-generated. Furthermore, there is often carelessness in the amount of effort put towards reproducibility and comparison. Datasets are often labelled with qualitative descriptors such as \u201cGood\u201d, \u201cmedium\u201d or \u201cbad\u201d, descriptors which ought to have bearing on some property of the dataset. However, the most obvious property, episode return mean, is only sometimes reported, and rarely is episode return std reported. Our four progressive examples hope to illustrate the following key point. The mean, std, and distribution of episode returns are significantly important when learning from a multi-agent dataset, with a notable impact on learned performance. But they are not enough, and even when controlling for them, significant differences may still occur. Crucially, the outcomes from these examples do not hold in every scenario, but that is exactly the point: these peculiar dynamics can arise when not controlling for the dataset used, and what we witness can conceivably be leveraged by authors to erroneously present algorithmic advancements as more performant than what in reality is truly the case.\n\n9What is our main message, then? Ultimately, we see an urgent need for authors to pay attention to data in offline MARL research. At the very least, if authors are generating their own data for their experiments, they must provide easy access to this data for future work, hosted in perpetuity, with ample documentation of the generation procedure, the contents, and a reasonable quantitative description including the episode return distributions, the state-action coverage, and so on. In addition to such efforts, there is evidently a growing need for the standardisation of datasets in offline MARL. In the absence of a common foundation, it is impossible for researchers to speak the same language, introducing doubt into the veracity of algorithmic developments. We also see the groundwork of a fruitful new avenue of multi-agent research\u2014how does the characteristics of a dataset impact multi-agent learning, with all the associated complex dynamics that inevitably arise?# 4 Putting Data at the Centre of Offline MARL\n\nHaving illustrated how a lack of consideration for the impact of data in offline MARL is problematic, we take a step towards alleviating some of the issues we have identified in the previous section. By making three data-driven contributions to the community, we hope to bring data closer to the centre of research in the field. Our first contribution is a set of clear guidelines for how researchers should approach generating datasets in offline MARL. If there is an important reason for authors to generate their own datasets, there should at least be sound principles to follow. Secondly, we significantly enhance the standardisation of data in the field by converting over 80 datasets from prior works into a consistent format (Toledo et al., 2023), which has an emphasis on speed, ease-of-use, clear documentation, and integration into existing frameworks. We upload these datasets to Hugging Face for reliable access in perpetuity. We recommend using existing OG-MARL (Formanek et al., 2023a) datasets for future research and encourage any new datasets generated by the community to be added to the repository following the standards and formats outlined there. That said, we still converted all the datasets we could get access to from prior work for the sake of continuity and the possibility of comparing with those works. Finally, we present an ever-growing repository of open-source tools that can be used to access, analyse, and edit these standardised datasets, for future research.# 4.1 Dataset guidelines\n\nThe gold standard solution is to standardise all of the existing datasets (which we attempt through OG-MARL) and to use a shared methodology when generating novel datasets. When generating a new dataset, there are certainly basic guiding principles that ought to be followed to ensure good scientific practice. We outline such guidelines in the blue box below.# 4.2 Standardising existing datasets\n\nThe single-agent offline RL community have benefited significantly from the widespread adoption of common datasets such as RL Unplugged (Gulcehre et al., 2020) and D4RL (Fu et al., 2020). The offline MARL field could similarly benefit from the adoption of a common set of benchmark datasets.\n\n3. https://huggingface.co/datasets/InstaDeepAI/og-marl/# Guidelines for generating new datasets for offline MARL research# 1. Is a new dataset really necessary?\n\n- Is there an existing dataset in the field you could use instead?\n- If a new dataset is required for your research, make sure you document why and how exactly your dataset is different.# 2. Have you documented all of the relevant information regarding how you generated your data?\n\n- Document which environment you used and how people can access the environment. Make sure to be explicit about the version of the environment used. This is to ensure proper version control for comparisons as later environment versions might be released in the future (in some cases from authors different than the original creators).\n- Document relevant high-level environment properties e.g. number of agents, action size, observation size, sparse/dense reward and so on.\n- Document how you generated the dataset. For example, which online MARL algorithm collected the experience and how did you sub-sample the data?# 3. Have you included a quantitative analysis of the composition of your data?\n\n- Report the following summary statistics for your datasets: episode returns min, mean, max and standard deviation, as well as the number of episodes and transitions in your dataset.\n- Include plots of the episode return distribution e.g. histograms or violin plots.\n- Include a measure of action-space coverage for your dataset e.g. Joint-SACo.# 4. Can other researchers access your datasets? And will they still be able to in a year from now?\n\n- Make a download link easily accessible. Ensure the link will not expire and that downloads are successful from different regions of the world.\n- Consider adding your datasets to a community-driven datasets repository such as OG-MARL (Formanek et al., 2023a).\n- Use a dataset format that is widely adopted in the field. Or include sufficient documentation on how to load and use your dataset.\n- Include a dataset licence.\n\nAlthough there are multiple possible starting points for standardisation, we recommend OG-MARL (Formanek et al., 2023a). OG-MARL is solely focused on providing standardised datasets to the community, rather than hosting datasets that exist only as a by-product of algorithmic research. These datasets have already been cited in multiple works (Formanek et al., 2023b; Zhu et al., 2023; Yuan et al., 2023; Formanek et al., 2024; Tilbury et al., 2024; Putla et al., 2024; Ruhdorfer et al., 2024; Jing et al., 2024). The repository is ever-growing and extendable by the community, and supports a wide range of environments.\n\nWe standardise the format of these datasets to the open-source and industry-supported utility of Vault (Toledo et al., 2023), because of its focus on speed, clear documentation, and easy accessibility. For future offline MARL research, we strongly recommend using OG-MARL datasets in the Vault format. However, to enable comparisons to past works in the literature, we have additionally converted datasets from other authors into the Vault.# 4.3 Dataset analysis tools\n\nOur final contribution towards improving data awareness in offline MARL is a set of tools which can be used to download, subsample, combine, and analyse datasets. These tools, which live in OG-MARL, can be used on any dataset which conforms to the Vault API. The tools are accompanied by a demonstrative notebook 4, which explains how to use them.# Simplified loading of datasets\n\n- Support for downloading all 88 Vault datasets from OG-MARL.\n- Support for downloading a Vault from a user-specified URL.# Dataset analysis tools\n\n- Dataset structure: describe_structure prints the pytree structure of each dataset in the Vault, and gives the number of transitions and trajectories in each dataset.\n- Episode returns: describe_episode_returns plots for each dataset in the Vault the histogram and violin plot of its episode returns, and outputs a table containing the episode return mean, standard deviation, minimum, and maximum.\n- Coverage properties: describe_coverage produces a log-log plot of count frequencies of unique state-action pairs, as well as the Joint-SACo value for each dataset.\n- Summary: descriptive_summary plots episode return histograms, and outputs a table containing episode return mean, standard deviation, minimum and maximum, Joint-SACo, number of transitions and number of trajectories in each dataset.# Tools to subsample and combine Vaults\n\n- Subsample a Vault to within one trajectory\u2019s length of a specified number of transitions.\n- Combine a list of datasets into one larger dataset.\n- Subsample two datasets to have near-identical episode return distributions.\n- Subsample a dataset to have a specific episode return distribution.\n\nprovides enough understanding of the Vault and OG-MARL systems to be able to work on custom tools and workflows. Our set of utilities are outlined below.\n\nWe provide a demonstration of our tools using the 2s3z scenario from SMACv1 (Samvelyan et al., 2019), with the dataset from OG-MARL (Formanek et al., 2023a). We focus on dataset analysis (where we give insights into dataset composition), as well as subsampling and combining tools (which can be used for a variety of reasons: to make datasets smaller so that they use less memory, to create datasets for ablations, and to combine datasets when more training data is required).# Analysis\n\nOur analysis tools cover all requirements stipulated in the analysis section of our dataset generation guidelines. We provide four high-level functions to generate various insights for a user-specified selection of datasets in the Vault format. Calling descriptive_summary with a provided Vault will generate a summary such as the one illustrated by Figure 5, with both tabular and histogram information returned. Users can further access episode return violin plots by calling describe_episode_returns, detailed structural information about the Vault by calling describe_structure, and state-action count information by calling describe_coverage.\n\nFrom these outputs, we can now analyse the dataset and notice interesting insights. For example, we can see in Table 5 that the Poor dataset contains far fewer trajectories than the Good and Medium datasets, despite containing a similar number of transitions. On average, the episodes in the Poor dataset contain more than 100 transitions, which means that the episodes usually roll out to their full length. We also notice that there is not much of a\n\n13# (a) Tabular values returned from the descriptive_summary function\n\n|Dataset|Mean|Stddev|Min|Max|# Transitions|# Trajectories|Joint-SACo|\n|---|---|---|---|---|---|---|---|\n|Good|18.32|2.95|0.00|21.62|995 829|18 616|0.98|\n|Medium|12.57|3.14|0.00|21.30|996 256|18 605|0.98|\n|Poor|6.88|2.06|0.00|13.61|996 418|9 942|0.96|# (b) Histograms returned from the descriptive_summary function\n\nFigure 5: The results returned when calling descriptive_summary on the 2s3z Vault from OG-MARL (Formanek et al., 2023a)\n\ndi\"erence between the Joint-SACo of the datasets, though each dataset\u2019s coverage is diverse, with at most 4% being repeated pairs in each case. The information gained here can at a glance help a researcher understand a dataset better.# Subsampling and combining datasets\n\nOur tools also allow researchers to stitch and slice datasets. While it is important to keep datasets standardised, experiments may require datasets which do not yet exist; there may be constrained memory requirements, new experiments, or ablations requiring new datasets. Rather than expecting researchers to create entirely new datasets for such experiments, which might introduce inconsistencies between datasets available in the field, we give researchers the tools to flexibly subsample and combine existing datasets according to the properties that their experiments require. Figure 6 illustrates an assortment of such examples\u2014showing how datasets can be subsampled or combined, and showing how desired distributions can be systematically created. We also provide a notebook in the OG-MARL open-source repository that demonstrates how to subsample a dataset according to a user-specified episode return histogram. Our hope is that even though our subsampling tools allow researchers to manipulate data more easily, our analysis tools make it easy enough to uncover dataset discrepancies that researchers should be dissuaded from changing datasets without disclosing their amendments.\n\nOur work opens up an easy-to-use pipeline for data use, generation and understanding in o#ine MARL. Researchers can now easily expose the contents of datasets for flexible and well-informed access to the material on which their algorithms train. We hope the impact of our work is better understanding not only of datasets in o#ine MARL, but also of the environments we use and how a data collecting policy interacts with these.\n\n5. https://github.com/instadeepai/og-marl/blob/main/examples/dataset_subsampling_demo.ipynbFigure 6: Illustrative dataset subsampling examples, using the 2s3z Vault.# 5 Conclusion\n\nIn this paper, we have surveyed the literature and found that data is largely neglected in the data-driven field of offline MARL. We have shown why paying attention to data in offline MARL research is crucial, through simple yet illuminating examples. We contribute to the community: a guideline for generating multi-agent datasets; a standardised repository of over 80 environment-scenario-quality combinations, with a well-documented and accessible API; and useful tooling to aid the understanding of these datasets. In conjunction, these efforts aim to catalyse progress by aligning the field towards scientific rigour. In doing so, along with other standardisation efforts\u2014e.g. standardised baselines (Formanek et al., 2024)\u2014we feel that the discipline is ripe with opportunity to solve hard problems. We encourage researchers to adopt and extend our offerings, working collectively to push the field forward. By working from a strong foundation together, significant breakthroughs can be made.",
        "context_id": 1,
        "question": "Which library provides standardized and publicly available single-agent offline RL datasets as mentioned in the Introduction?",
        "answer": [
            "D4RL"
        ],
        "context_length": 33477
    },
    {
        "context": "# Current Research on Secure Robot Learning Framework\n\nTo protect robots\u2019 performance from deterioration or even destruction, the study of designing robotic systems with high assurance is a hot topic, attracting researchers from the control and computer science communities.\n\nThis study intends to establish a secure robot learning framework for cyber attack scheduling and countermeasures. It analyses how an adversary with limited attack energy can construct an optimal false data injection attack scheme to disrupt a robot\u2019s tracking performance and proposes a secure control algorithm against such adversaries. Finally, we hope our paper will open the way to future research on this hot security challenge. The source code for our learning-based controller and open-source environment for reinforcement learning research on Agilicious can be found in https://github.com/SamySam0/BaBee.# 1.1 Motivations\n\nIn the control community, none of the existing methods adequately address optimal attack and countermeasure design problems on underactuated nonlinear complex systems. The method in [8] focused on typical control problems without attack, while the method in [9] focused on state estimation problems and cannot be applied to the secure control problem. We draw inspiration from [10], which deals with ground vehicles of greater linearity. However, under attacks, the secure control problem for underactuated dynamical systems cannot be solved by applying the methods in [10].# 1.2 Objectives and Contributions\n\nIn this paper, we develop an autonomous control system, called a nominal controller, and two reinforcement learning-based methods to solve the optimal false data injection attack and countermeasure design problems for underactuated nonlinear systems. To this end, our study provides:\n\n- (a) a nominal controller for a robot under no attacks. Such a controller is capable of flying and stabilising the system.\n- (b) a learning-based malicious adversary employing false data injection attacks is constructed to perturb the control signals sent by the nominal controller and deteriorate its tracking performance with minimal cost.\n- (c) a learning-based countermeasure is developed to mitigate attacks with minimum control cost and recover the tracking performance.disturbed by the attacker. All the above algorithms are provided and described following the proximal policy optimisation approach.\n\nWe base our work on unmanned aerial vehicles, particularly on X-shaped quadrotors. As a study case, we deploy our framework on the Agilicious quadrotor, a state-of-the-art quadcopter for autonomous flying deployed in 2023 by the University of Zurich. In fact, our team was the first in the United Kingdom to deploy this quadrotor and implement reinforcement learning on its platform. Therefore, on top of the proposed adversary attack and defence countermeasure frameworks, our work introduces (d) a comprehensive breakdown of Agilicious quadrotors, including software designs and hardware alternatives, (e) a detailed reinforcement-learning framework to train autonomous controllers on Agilicious-based agent, and (f) a new open-source environment that builds upon PyFlyt for future reinforcement learning research on Agilicious platforms. These aim to promote easy reproducibility with minimal engineering overhead for future research on this promising quadrotor. As a little caviar, our team pre-designed various tasks, including hovering, obstacle avoidance, and trajectory tracking, to allow for straightforward experimental work on this framework.\n\nThe three learning-based controllers are evaluated against their respective objectives through a series of experiments. This includes experimenting in simulation to obtain metrical results, and testing their ability to be deployed in real-world scenarios. Results are compared to the state-of-the-art when they exist in order to provide the reader with a clear comparison to successful works.# 1.3 Report Structure\n\nThe rest of this paper is organised as follows. Chapter 2 introduces background knowledge in quadrotor dynamics, cyber-attacks and countermeasures, and deep reinforcement learning. Chapter 2 Section 2.4 describes and refers the reader to related works. Chapter 3 provides the experimental setup for our experiments. This includes our simulation and the assembly of the physical quadrotors used, i.e. Agilicious and Crazyflie, with detailed information on their hardware components and software stacks. Chapter 4 introduces our objectives and formulates the problems to be solved. Then Subsection 4.2 presents the design of our autonomous control system, Subsection 4.3 gives the learning-based false data injection attack algorithm, and Subsection 4.4 the secure robot learning framework for mitigating attacks. Chapter 5 provides simulation and# 2.1.2 Thrust and Moments\n\nThe quadrotor\u2019s attitude and position can be controlled by altering the speeds of its four motors to control thrust and rotations. The following forces and moments can be performed on the quadrotor: the thrust generated by spinning rotors, the pitching and rolling moments resulting from the speed difference between the four rotors, the gravity, gyroscopic effects, and yawing moments. However, the gyroscopic effect can be ignored as it is only noticeable in lightweight quadrotor constructions. Additionally, the yawing moments arise from imbalances in the rotational speeds of the four rotors, which can be countered by having two rotors spinning in opposite directions.\n\nTherefore, the propellers are grouped into pairs, with two diametrically opposed motors in each group, easily identifiable by their rotation direction. Namely, we group:\n\n- the front and rear propellers, rotating counterclockwise; and\n- the left and right propellers, rotating clockwise.\n\nBy varying the rotational speeds of each motor, we can control the motion in the six degrees of freedom including forward and backward movements, lateral movement,# 2.2 Motion Control of Quadrotors\n\nVertical motion, roll motion, pitch motion, and yaw motion. However, we can directly influence only four of those six motions.\n\nVertical\nYaw\nPitch\nRoll\nFigure 2.2: Simplified illustrations of Altitude (vertical), Yaw, Pitch and Roll motions. The front propeller is highlighted in green for convenience.\n\nDepending on the rotational speed of each propeller, it is possible to identify the four basic movements of the quadrotor illustrated in Figure 2.2. Those are:\n\n1. (a) The roll motion (\u2203), being when left and right motors do not rotate with equal speed. Rolling increases when the left motor spins faster than the right one.\n\n\u2203 = l \u00b7 (F2 \u2192 F4 )\n\n= l \u00b7 k \u00b7 (w22 \u2192 w42) (2.1)\n2. (b) The pitch motion (#) when the front and rear motors do not rotate with equal speed. Pitching increases when the front motor spins faster than the rear one.# = l \u00b7 (F1 \u2192 F3 )\n\n= l \u00b7 k \u00b7 (w12 \u2192 w32) (2.2)\n3. (c) The yaw motion (%) when the two motor groups do not sum to the exact same rotational speed. Yawing increases when the left and right motors spin faster than the front and rear ones.\n\n% = b \u00b7 (w42 + w22 \u2192 w12 \u2192 w32) (2.3)\n4. (d) The torque and thrust caused by each rotor act particularly in the body\u2019s z-direction. Accordingly, the vertical motion (Fz) is the net propulsive force in the z-direction given by the sum of forces produced by each motor,\n\nFz = F1 + F3 + F2 + F4 \u2192 mg\n\n= k \u00b7 (w12 + w32 + w22 + w42) \u2192 mg (2.4)\n\nwhere k and b are the lift and drag constants defined empirically, mg is the force of# 2.1.4 Differential Flatness\n\nAlthough the previous section shows that the quadrotor\u2019s orientation and position change with respect to its thrust and angular velocities, it does not prove that any trajectory in the space of flat outputs will be dynamically feasible for the quadrotor. Specifically, it does not show that controlling thrust and angular velocities guarantees that it can take any desired trajectory in 3D space. To prove that, we must show that the quadrotor system is differentially flat; that is, the states and inputs can be written as algebraic functions of the flat outputs and their time-derivatives. This proof is way beyond the scope of our work but can be read in [13].\n\n18# The proof shows that the extended dynamical model of a quadrotor subject to rotor drag with four controlled inputs is differentially flat.\n\nIn fact, the quadrotor states *[!, \u220bb , \u2200, (] and inputs [F z , ( w ] can be written as algebraic functions of four selected flat outputs and a finite number of their derivatives. They chose the flat outputs to be the quadrotor\u2019s position ! and its heading %*, and proved that the commanded orientations and collective thrust are functions of the flat outputs.\n\nThis concludes on the fact that commanded orientations and collective thrust are not only directly influencing the quadrotor\u2019s position and orientation with respect to the inertial frame, but can also represent any realistic trajectory in 3D space.# 2.2 Quadrotor Attacks and Countermeasures\n\nIn this section, we define cyber-attacking in the context of quadrotor tracking disruption and, more specifically, introduce false data injection as a man-in-the-middle approach. Additionally, we describe countermeasures as a way to mitigate malicious attacks and preserve the quadrotor\u2019s tracking performance.\n\nIn the context of quadrotor tracking and control, we refer to cyber-attacking as a malicious attempt to compromise the availability and integrity of transmitted information. Nonetheless, it is essential to mention that system faults and attacks differ fundamentally. That is, while faults manifest from a system\u2019s unintended error, attacks are concealed and designed elaborately *[14\u201316]*. Such disruptions can be particularly concerning for quadrotors as they mainly rely on real-time data for stable flight and navigation.\n\nOne specific form of cyber-attack is the false data injection, where incorrect or misleading data are inserted into the quadrotor\u2019s data streams. In our study, the method used to execute false data injections is the man-in-the-middle approach, where an attacker intercepts and alters the communications between the quadrotor and its control system. This involves altering actuator signals and sending disrupted control commands to the flight control system. Such attacks may result in minor disruptions or even complete loss of control when no countermeasure is executed, posing significant risks to safety and task integrity.\n\nTwo types of strategies are typically employed to effectively address and counter# 2.3 Reinforcement Learning\n\nThis section introduces reinforcement learning as a mathematical solution to autonomous quadrotor control, optimal false data injections and countermeasure design. Moreover, it explains how deep reinforcement learning can handle the high-dimensional state spaces and complex environments required to deal with three-dimensional quadrotor controls. Finally, it goes in-depth with the Proximal Policy Optimization algorithm implemented to train our three controllers.\n\nReinforcement learning is a subset of Machine Learning where an intelligent system, referred to as an agent, learns through trial and error by interacting in an unknown environment and receiving feedback on its actions [17]. The mechanism is the following: In any given state of an environment, an agent uses its policy to choose which action to take based on the observed state, and receives a reward from the environment for doing so. The reward tells the agent \u201chow good\u201d his action was. This process continues with the agent repeatedly taking actions and receiving rewards until the environment terminates or the agent reaches a final state. Reinforcement learning uses the rewards obtained through training episodes to update the agent\u2019s policy and progress toward the optimal policy in an environment. Essentially, reinforcement learning allows an agent with no prior information about the environment to learn a strategy about how to interact with the environment and maximize accumulated rewards. An illustration of the interaction between an agent and its environment is displayed in 2.4.\n\nThere exist two main challenges with reinforcement learning. These are (1) to write a qualitative reward function to receive accurate feedback from the agent\u2019s actions, and (2) to design an environment that accurately describes the environment in which the agent will act when deployed. Indeed, this latter point is at the heart of the Simulation-to-Real-World transfer challenge.# 2.3.1 Markov Decision Processes\n\nFormally, reinforcement learning can be described as Markov decision processes (MDP). We represent these with the tuple (S, A, P, R), where S is the state space, A is the action space, P is the transition probability matrix from a state-action pair at time t onto a distribution of possible states at time t + 1, R(s, a, st+1) is the immediate reward function, and \u03b3 \u2208 [0, 1) is the discount factor for future rewards, where lower values place more emphasis on immediate rewards.# Policies\n\nIn general, the policy \u03c0* is a mapping from states to a probability distribution over actions: \u03c0*: S \u2192 p(A = a|S). In our case, the Markov decision processes are episodic, meaning that the state is reset after each episode of length T. Therefore, the sequence of states, actions and rewards in an episode constitutes a policy rollout. Every rollout of a policy accumulates rewards from the environment, resulting in the return:\n\nR = r0 + \u03b3r1 + \u03b32r2 + ... + \u03b3T-1rT.\n\nThe goal of reinforcement learning is to find an optimal policy \u03c0* which maximizes the expected return from all states:\n\n\u03c0* = argmax E[R|\u03c0].\n\nTherefore, the agent must deal with long-range time dependencies, as the consequences of an action often only materialize after many transitions in the environment. This is known as the temporal credit assignment problem [18].# 2.3.3 Deep Reinforcement Learning\n\nDeep reinforcement learning combines reinforcement learning and deep neural networks by using deep neural networks as function approximators to help agents learn how to achieve their goals. Where tabular learning previously struggled to represent such problems, this approach supports high-dimensional and continuous state spaces, as well as complex environments. Figure 2.5 shows how deep learning and reinforcement learning can be used together.\n\n|Reward|Agent|DNN|policy|\n|---|---|---|---|\n|State|Take action|Observe state| |\n\nFigure 2.5: Simplified illustration of an end-to-end deep reinforcement learning framework. Neural network architectures may vary according to the specific task.\n\nIn recent works, deep neural networks have been used to approximate policy functions, resulting in superhuman performances at tasks such as board games [21], computer vision [22] and robotics [23]. For instance, Figure 2.6 showcases Dactyl, OpenAI\u2019s robotic hand capable of dexterous manipulations.\n\nThe learning process for deep learning algorithms is the following: During training, the agent iteratively observes a state and selects an action based on the neural network\u2019s predictions. The neural network aims at approximating the policy and predicting the future outcomes of a given action. According to the chosen algorithm, the action may# 2.3.4 Proximal Policy Optimization Algorithm (PPO)\n\nAlthough numerous reinforcement learning algorithms exist (Figure 2.7), the selection of a method depends entirely on the specific working task. In this section, we describe the Proximal Policy Optimization algorithm (PPO) [26] as a deep learning algorithm widely used in quadrotor control [27, 28] due to its fast and easy implementation and improved training stability.# 2.14 PPO Algorithm\n\nAdditionally, it can operate in continuous high-dimensional state and action spaces without an exponential increase in complexity. For instance, PPO has become OpenAI\u2019s default reinforcement learning algorithm due to its ease of use and good performance.\n\nPPO is composed of an actor \u03c0(\u00b7|s) which outputs a probability distribution for the next action given the state at timestamp t, and a critic V(s) which estimates the expected cumulative reward from that state. Since both the actor and critic take the states as input, we can save computation by sharing the neural network architecture between the policy and the value functions. Therefore, [26] proves that we must use a loss function that combines the policy surrogate and a value function error term. Furthermore, as suggested in [29], this objective can be further augmented by adding an entropy bonus to ensure sufficient exploration. Combining these terms, we obtain the following objective maximized at each iteration:\n\nLt CLIP+VF+S(\u03c0) = Et[Lt CLIP(\u03c0) + c1 LVF(\u03c0) + c2 S[\u03c0old(s)]]\n\nwhere c1 and c2 are coefficients which balance the importance of the accuracy of the critic against the exploration capabilities of the policy, S denotes an entropy bonus, and LVF is a squared-error loss:\n\nLVF(s) = (V\u03c0(s) - Vtarg)2\n\nAs suggested by Stable Baselines' implementation, we will use c1 = 0.5 and c2 = 0.05.# Clip term\n\nThe clip term Lt CLIP(\u03c0) maximizes the probability of actions, given by the actor \u03c0(\u00b7|st), that resulted in an advantage. Additionally, it tries to improve the policy\u2019s training stability by avoiding large policy updates between training epochs. The motivation behind the latter is based on empirical analysis which proved that smaller policy updates during training were more likely to converge to an optimal solution [30]. The clip term is defined as:\n\nLCLIP(\u03c0) = Et[min(r(\u03c0) At, clip(r(\u03c0), 1 - \u03b5, 1 + \u03b5) At)]\n\n1 https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3\n\n25# Current Policy and Ratio Function\n\nwith r(#) = \u2217 #old (a, s), \u2217# (a, s)t t t (2.16)\n\nA\u02c6t = \u2192V (s) + rt + )rt+1 + \u00b7 \u00b7 \u00b7 + )T \u2192t+1 r T \u21921 + ) T \u2192t V (s T ),t\n\nwhere the ratio function r(#) tells how likely we are to take action at in the current policy as opposed to the previous policy. In order to stabilise the training, we do not allow for drastic changes in policy and, therefore, take the minimum of the new policy and its clipped version between [1 \u2192 ,, 1 + ,]. In their paper, [26] recommend using , = 0.2 so that the ratio can only vary from 0.8 to 1.2.\n\nThe advantage \u02c6t measures how wrong the critic was for the given state s. It does this by running the policy for T timesteps and measuring the difference between the value at the initial state V (s) and the true cumulative reward obtained from following policy \u2217# over the next T steps.# Value-function Term\n\nTo have a good estimate of the advantage, we need a critic which can accurately predict the value of a given state. The value-function term accounts for learning such a function with a simple mean square error loss between its predicted expected reward and the observed cumulative reward (V# (s) \u2192V t targ) 2 computed as\n\nLt V F = MSE (rt + )rt+1 + . . . + ) T \u2192t+1 r T \u21921 +V (s T ),V (s)) = \u21d2 At \u21d22. t \u02c6 2 (2.17)# Algorithm\n\nAn implementation of the proximal policy optimization algorithm that uses fixed-length trajectory segments is given in Algorithm 1. In every iteration, each of N parallel actors collects T timesteps of data. Then, we construct the surrogate loss on these NT timesteps of data and optimize it with Adam [31] gradient descent for K epochs.# Algorithm 1 Proximal Policy Optimization Algorithm\n\n1. for iteration = 1, 2, . . . do\n2. for actor = 1, 2, . . . , N do\n3. Run policy \u2217old in environment for T timesteps\n4. Compute advantage estimates \u02c6 1 , . . . , AA \u02c6 T\n5. end for\n6. Optimize surrogate L wrt #, with K epochs and minibatch size M \u21d1 NT\n7. #old \u2243 #\n8. end for# 2.4 Related Works\n\nAutonomous quadrotor control systems have been investigated considerably over the past few years. Initiating from mathematical models such as PD [32] or MPC [33], recent studies have tried to train learning-based controllers using reinforcement learning [8, 24, 25]. As a result, although mathematical models provide the safest and most predictable behaviour for autonomous flying, learning-based controllers have demonstrated strong abilities to solve non-linear control problems that control theory could not solve.\n\nOn that account, although learning-based controllers may have limited flying abilities, combining them with mathematical models could indeed solve significant complex problems that occur during quadrotor flying operations. That is notably the case with malicious attacks, which typically can be categorised into four distinct types: the denial of service (DoS) attack, the false data injection attack, the replay attack, and the zero-dynamics attack. For the reader\u2019s convenience, all are described in [34]. Although securing robotic systems in the presence of malicious attacks is a new challenge, a few results have already been reported. For example, [35] showed how to capture and control a UAV by spoofing GPS data. Regarding stealthy attacks on ground vehicle sensors, [14] proposed a robust control system that can estimate system states while under attack by utilising redundant sensor measurements.\n\nFollowing is a description of the state of research in this area. In [36], Guo, P. et al. designed an attack detection scheme based on the non-linear dynamics of a mobile robot to warn the system in case of actuator attacks. In their work, the proposed scheme was successfully verified on two types of robots with various attack scenarios. Furthermore, [37] proposed a defence scheme that switches distributed control.# Current Research on Cyber-Physical Systems Security\n\nBetween multiple robots to avoid actuator DoS attacks and falsified data injections. However, these two solutions do not directly preserve tracking performance but instead suggest ways to detect or improve the robustness of robotic systems. Over the past two decades, alternative secure algorithms have emerged from schemes previously developed for cyber-physical systems, based on control-theoretical approaches. Some examples include a switching observer-based estimate scheme [38], a linear quadratic secure controller [39], and a learning-based secure tracking control algorithm [40].\n\nIn addition, false data injection attacks have been widely investigated due to their stealthy characteristic. As a result, many systems have been proposed, including attack detection schemes [41], secure state estimate algorithms [42, 43], and resilient controllers [44]. Unfortunately, these programs either cannot be applied to quadrotor systems or do not commit to preserving a confident level of stability in agile settings. From an attacker\u2019s perspective, researchers have investigated how to construct effective attack sequences to deteriorate system performance, such as DoS attack scheduling [45] and false data injection attack scheduling [46], based on which can be designed more effective countermeasures.\n\nIn general, although progress has been made in securing cyber-physical systems, the results mentioned above rely on exact system knowledge, which may not be obtained easily. Using reinforcement learning may allow us to design systems policies without using such knowledge. In fact, a few reinforcement learning algorithms have been proposed [30, 47, 48] to this end, based on which learning-based autonomous control with stability guaranteed [8] have been developed. More recently, considering the complexity and scale of cyber-physical systems, some attempts have been made at utilising deep reinforcement learning to solve security problems. Although not directly designed for unmanned aerial vehicles, encouraging results have been demonstrated on problems such as anomaly detection, secure control, attack detection schemes and other security-related applications [49].\n\nSimilarly, adversaries may also use deep reinforcement learning approaches to construct attack schemes [50]. However, how to design an optimal attack scheme and optimal countermeasure for quadrotors under false data injection attacks is still a hot challenge awaiting some solutions. This study provides a solution employing deep reinforcement learning and opens the way for some future research.\n\n28# 3.1 Quadrotor Hardware Assembling\n\nThis section introduces the physical quadrotors used in our experiments and the reasons behind our choices. Furthermore, it reports previous results obtained on these models in the context of tracking performance and reinforcement-learning-based control.# 3.1.1 Agilicious\n\nAgilicious is a co-designed hardware and software framework tailored to autonomous and agile quadrotor flight. It was deployed by the University of Zurich in 2023 as an open-source and open-hardware quadrotor supporting both model-based and neural-network\u2013based controllers. It provides high thrust-to-weight and torque-to-inertia ratios for improved agility, and GPU-accelerated compute hardware for efficient neural network inference. Our customised Agilicious framework is described in Figure 3.1.# Motivation\n\nIn order to build an agile quadrotor and experiment with autonomous tracking tasks, the quadrotor should meet the following pair of design requirements. It must (1) carry the computing hardware needed for autonomous operation and (2) be capable of agile flight. To meet the first requirement on computing resources needed for true autonomy, a quadrotor should carry sufficient computing capacity to run estimation, planning, and control algorithms concurrently; hence requiring heavy hardware. However, the physical model must deliver adequate thrust-to-weight and torque-to-inertia ratios to permit agile flight. The thrust-to-weight ratio can often be enhanced using more powerful# Onboard Compute Capability\n\n|Framework|Open-source|Onboard Computer|CPU (higher better)|mark|GPU|Maximum speed|Thrust to weight|\n|---|---|---|---|---|---|---|---|\n|PX4 [52]|\u21ad|\u2701|-|\u2701|-|-|-|\n|Paparazzi [53]|\u21ad|\u2701|-|\u2701|-|-|-|\n|DJI [54]|\u2701|\u2701|-|\u2701|140km/h|4.43| |\n|Skydio [55]|\u2701|\u2701|-|\u2701|58km/h|-| |\n|Parrot [56]|\u2701|\u2701|-|\u2701|55km/h|-| |\n|Crazyflie [57]|\u21ad|\u2701|-|\u2701|-|2.26| |\n|FLA-Quad [58]|\u21ad|\u21ad|3.383|\u2701|-|2.38| |\n|GRASP-Quad [59]|\u2701|\u21ad|625|\u2701|-|1.80| |\n|MIT-Quad [60]|\u2701|\u21ad|1.343|\u21ad|-|2.33| |\n|ASL-Flight [61]|\u21ad|\u21ad|3.383|\u2701|-|2.32| |\n|RPG-Quad [62]|\u21ad|\u21ad|633|\u2701|-|4.00| |\n|MRS UAV [63]|\u21ad|\u21ad|8.8846|\u2701|-|2.50| |\n|Agilicious [51]|\u21ad|\u21ad|1.343|\u21ad|131km/h|5.00| |\n\nFigure 3.2: A comparison from Agilicious work on different available consumer and research platforms with respect to available onboard compute capability and agility. The sequence of experience to obtain the above results is described in [51]. The PX4 [52] and the Paparazzi [53] are rather low-level autopilot frameworks without high-level computation capability. The open-source frameworks FLA [58], ASL [61], and MRS [63] have relatively large weight and low agility. The DJI [54], Skydio [55], and Parrot [56] are closed-source commercial products that are not intended for research purposes. The Crazyflie [57] does not allow for sufficient onboard compute or sensing, while the MIT [60] and GRASP [59] platforms are also not open-source. Instead, the Agilicious [51] framework provides agile flight performance, onboard GPU-accelerated compute capability, as well as open-source and open-hardware availability.# Components\n\nThe hardware system of the quadrotor aims to carefully manage the trade-off between onboard computing capability and platform agility. We describe below the components used to build the flight and compute hardware, as well as why we decided to use them. A detailed description of the hardware system and inter-components communications is displayed in Figure 3.3.# High-level Controller\n\nThe high-level controller is responsible for providing the necessary computing power to run the full flight stack, including estimation, planning, optimization-based control and neural network inference. Therefore, it must provide strong computing power while preserving limited weight onboard. It combines two components:\n\n1. A Carrier Board which serves as an interface between the computing unit and the various peripherals and connectors (such as USB, Ethernet, sensors...) through serial ports;\n2. A Compute Unit which provides the necessary computing power onboard and is directly embedded onto the Carrier Board. For the latter, although we recommend using the Nvidia Jetson TX2 as it possesses the best performance-to-size ratio, it is hardly available to order. Therefore, we decided to use the Nvidia Xavier NX as it performs almost as good as the former and supports accelerated inference from the Nvidia CUDA general-purpose GPU architecture. As for the carrier board, we used the A203 Carrier Board, which is compatible with the Nvidia Xavier NX and offers the minimal configuration required to communicate with the low-level controller, i.e., two UART ports.# Low-level Controller\n\nThe low-level controller provides real-time low-latency interfacing and control. It is made up of two components:\n\n1. The first is a Flight Controller (FC), which is a microcontroller equipped with various sensors (gyroscope, accelerometer and magnetometer) responsible for processing inputs from the high-level controller and sending control commands to the ESC, as well as getting feedback from the ESC and sharing them with the high-level controller over the digital bi-directional DShot protocol.\n2. The second is the Electronic Speed Controller (ESC). It is the interface between the Flight Controller and the quadrotor\u2019s motors. Each motor on the quadrotor is connected to the ESC, which controls the speed and direction of the motor based on commands received from the Flight Controller by adjusting the power supplied to the motors. The ESC sends feedback to the Flight Controller, such as rotor speed, IMU, battery voltage and flight mode information over a 1 MBaud serial bus at# 500 Hz\n\nFor our Flight Controller and ESC, we used the SpeedyBee F7 V3 FC with BetaFlight Firmware and the SpeedyBee BL32 50A 4-in-1 ESC, respectively.# Flight Controller\n\nMotor\n\nLiPO Battery\n\nXavier NX\n\nESC\n\nWIFIKey\n\nFigure 3.3: Top view of the hardware system describing the communications between components.\n\n(a) The Nvidia Xavier NX (Compute Unit) is embedded at the back of the A203 Carrier Board and connected through a 260-pin Connector. The Compute Unit runs all calculations on board from the A203 Carrier Board, supplied by a LiPO battery. To access a router or network, the A203 Carrier Board is also equipped with a Wifi Key on its USB1 port.\n\n(b) The Carrier Board is connected to the Flight Controller to communicate by sending control commands through its USB0-RX (transmitting) port and receiving feedback on its UART1-RXD (receiving) port. In order to use USB0 as a transmitting port, we utilise a USB-to-TTL cable (note that the USB-to-TTL cable inverts RX and TX; therefore, we use RX instead of TX on USB0 for transmission).\n\n(c) The Flight Controller is connected to the Carrier Board to communicate by sending feedback on its T3 (transmitting) port and receiving control commands on its R2 (receiving) port. It is also connected to the ESC through an 8-pin connector (on Bi-directional protocol) to send the processed Control Commands to the ESC and receive the root feedback from the latter.\n\n(d) The ESC receives processed control commands and varies the speed and direction of the motor based on these commands. ESC\u2019s power is also provided on board by the LiPO battery.\n\n33# Flight Hardware\n\nTo maximize the agility of the quadrotor, it must be designed as lightweight and small as possible [64] while still being able to accommodate the Xavier NX compute unit. Therefore, we selected the following off-the-shelf drone components, summarised in Table 3.1. The Armattan Chameleon 6-inch frame is used as a base plate since it is one of the smallest frames with enough space for computing hardware. Its carbon fibre makes it durable and lightweight (86g).\n\nFor propulsion, four 5.1-inch three-bladed propellers are used with fast-spinning DC motors rated at a high maximum power of 750W. The chosen motor-propeller combination achieves a continuous static thrust of 4 \u21d3 9.5N on the quadrotor and consumes about 400W of power per motor. To match the high power demand of the motors, a lithium-polymer battery with 2000 mAh and a rating of 120C is used. Therefore, the total peak current of 110A is well within the limit of the battery.\n\n|Component|Product|Specification|\n|---|---|---|\n|Carrier Board|A203 Carrier Board|87mm x 52mm x 26mm, 100 g|\n|Compute Unit|Nvidia Jetson Xavier NX|70mm x 45mm, 80 g|\n|Flight Controller (FC)|SpeedyBee F7 V3 FC|41 x 38 x 8.1mm, 9 g, BetaFlight firmware|\n|Motor Controller (ESC)|SpeedyBee BL32 50A 4-in-1|45.6 x 40 x 8.8mm, 12 g, 4x 50A|\n|Frame|Armattan Chameleon 6 inch|4 mm carbon fiber, 86 g|\n|Motor|TMotor F40 Pro V|23x6 mm stator, 2150 kV, 750 W, 4x 34 g|\n|Propeller|T5147 POPO Racing Tri-blade|5.1 inch length and 4.8 inch pitch, 4x 4.4 g|\n|Battery|Tattoo R-Line v3.0 2000|4! 3.7 V, 2000 mAh, 217 g|\n\nTable 3.1: Overview of the components of the flight hardware design.# Sensors\n\nTo arbitrarily navigate into an environment, quadrotors need some way to measure their absolute or relative locations and orientations. For that, many odometry solutions exist to estimate state changes over time. The most famous positional sensing techniques are GPS for outdoor localisation and Motion Capture systems for indoor and controlled environments. Alternatives exist with visual sensing methods, such as camera-based visual-inertial odometry, but we decided not to use that one in order to match our current research flow. In our work, we utilise Motion Capture with eight.# 3.4 Odometry Estimation Using Motion Capture\n\nCameras as a means to obtain odometry estimates of the quadrotor\u2019s position (in m) and linear velocity (in m/s) expressed in the inertial frame, as well as orientation (in \u00b0) and body rates (in \u00b0/s) in the body frame. First, we placed some reflective markers on the quadrotor\u2019s body. An example of a suitable configuration is displayed in Figure 3.4. It is essential to follow the requirements below to obtain optimal results:\n\n- Visibility: Markers should be distributed such that most of them are visible by the motion capture cameras no matter what the quadrotor\u2019s orientation is.\n- Avoid Occlusion: Markers should be placed such that they are less likely to be occluded by the drone\u2019s body or arms during flight. Solutions imply placing some markers on stalks or elevated positions.\n- Orientation Tracking: To track orientation, markers should not be placed in a uniform pattern. We must use an asymmetrical arrangement to help the motion capture system distinguish the drone\u2019s orientation efficiently.\n\nFigure 3.4: Example of a suitable reflective markers configuration on a Crazyflie quadrotor. The four reflective markers are circled in red.\n\nFurthermore, the motion capture system must be calibrated so that each camera understands the geometry, position and orientation it captures in space and infers a reference state with respect to the world. After calibration, the motion capture system can accurately estimate the odometry of a quadrotor in the inertial frame.\n\nFinally, the motion capture system sends these estimates to a Base Computer. In turn, the Base Computer publishes them in real-time on a ROS topic accessible by the onboard Compute Unit, which will utilise them to calculate control commands. Figure 3.5 provides an illustration depicting the entire system in action.# Base Computer\n\nFigure 3.5: Entire motion capture system illustrating how the Onboard Compute Unit can obtain the quadrotor\u2019s state estimates, shared by the Base Computer and derived through motion capture. The motion capture cameras send individual tracking data to the Base Computer. The latter calculates and publishes odometry estimates to the appropriate ROS topic hosted on the router. The quadrotor\u2019s Onboard Computer Unit subscribes to the ROS topic on the router\u2019s IP to read odometry estimates in real time and produce control commands.# Pilot\n\nAgilicious uses modular software components unified in a pipeline and orchestrated by a control logic: the pilot. The modules consist of an estimator, a sampler, a controller, and a bridge. All are working together to complete a given task. These modules are executed sequentially (illustrated in Figure 3.1) within a forward pass of the pipeline, corresponding to one control cycle. The pilot contains the main logic needed for flight operation and handling of the individual modules. At its core, it loads and configures the software modules and runs the control loop.# Pipeline\n\nA pipeline is a distinct configuration of a defined module sequence, for example, an estimator followed by a controller and a bridge. These pipeline configurations can be switched at any time. In our work, we experiment on two different pipelines: (1) The first uses an intermediate sampler, which, given estimator states,# 1. Introduction\n\ngenerates a subset of points along the desired trajectory to pass to a state-of-the-art MPC controller. The second pipeline directly exploits our trained neural network as the controller without the need for a sampler.# 2. Estimator\n\nThe first module in the pipeline is an estimator, which provides a time-stamped state estimate for the subsequent software modules in the control cycle. A state estimate is a set x = [p, q, \u220b, (, a, \u2212, j, s, b ( , ba , f d , f ] with quadrotor\u2019s position p, orientation unit quaternion q, velocity \u220b, body rate (, linear a and angular \u2212 accelerations, jerk j, snap s, gyroscope and accelerometer bias b ( and b a, and desired and actual single rotor thrusts f d and f. We use an Extended Kalman Filter (EKF) as our feed-through estimator, which fuses the data from the Inertial Measurement Unit (IMU) with the state estimates recovered from the motion capture system to provide a more accurate and reliable estimate of the quadrotor\u2019s state. In parallel, because sensors operate at different frequencies, the estimator synchronizes these signals to provide a consistent final state estimate.# 3. Sampler (Optional)\n\nWhen utilising a mathematical control system such as a PD or MPC, the controller module needs to be provided with a subset of trajectory points that encode the desired progress along it. The sampler is responsible for providing that sequence. In our experiment on an MPC-based controller, we use a position-based sampling scheme that selects trajectory progress based on the current position of the quadrotor and its target location. Note that such a sampler is unnecessary when using our neural-network-based controller since it takes raw state estimates as inputs to directly predict control commands as expected body rates and collective thrusts.# 4. Controller\n\nThe next module in the chain is the controller. It outputs control commands given state estimates (or given a set of trajectory points if a sampler is used). In our experiment, as an initial approach, we utilise a state-of-the-art MPC that uses the full non-linear model of the platform to generate body rate controls. In the context of reinforcement learning, we use our neural network as a controller, which, given state estimates, outputs desired body rates and collective thrusts [\u2203\u02d9, #, %, F]. Therefore, this latter approach does not need a sampler to generate intermediate trajectory points.\n\n37# Bridge\n\nThe final module of our pipeline is a bridge. It serves as an interface between the hardware and software layers to send the control commands to the low-level controller. We decided to use a bridge provided by Agilicious, which communicates via an SBUS protocol. As a standard in the quadrotor community, this protocol further promotes reproducibility. Additionally, it allows for interfacing with BetaFlight\u2019s flight controllers, as required for our system.# 3.1.2 Crazyflie\n\nBeing eight times smaller and twenty times lighter than Agilicious, Crazyflies are a series of small quadrotors developed by Bitcraze AB, promoting lightweight design and versatility. Additionally, thanks to their compact and accurate architecture, Crazyflies represent good baselines for quick real-world deployment post-training. However, their design comes to the cost of lower onboard computing capacity and poor agility.# Motivation\n\nIn some of our experiments, we chose Crazyflie as an alternative to Agilicious. This is done to deploy our trained controllers to real-world scenarios without the security and safety concerns that Agilicious raises.# Components\n\nAs indicated above, the hardware of a Crazyflie is much lighter than that of Agilicious. For convenience, an illustration of its hardware is depicted in Figure 3.6. Crazyflie has two microprocessors: (a) the STM32F4 handles the firmware stack, including low-level and high-level controllers; and (b) the NRF51822, which handles all the radio communications. Due to limited computation capacities onboard, calculations are done off-board from the base computer, with control commands transmitted through radio signals to the Crazyflie. Finally, the system is alimented by a LiPO battery directly connected to the body\u2019s bolt. Reflective markers are also attached to the quadrotor\u2019s body to obtain state estimates in the same way as with Agilicious (Figure 3.5).# Pilot\n\nRegarding the pilot, Crazyflie uses the same pipeline as Agilicious, except for a few details omitted here for simplicity, as they have no impact on the understanding or reproducibility of our work.# 3.2 Simulation and Environment\n\nIn this section, we introduce the methodology used to set up our environment and describe its potential to support future reinforcement learning research on Agilicious quadrotors.\n\nThroughout our investigations, we developed a custom environment based on the open-source PyFlyt simulator [65]. Although most related works were conducted on the famous pybullet-drone [66] environment, our strategy involved modifying PyFlyt to accommodate the specific physical properties of the Agilicious quadrotor. The main reason why PyFlyt was chosen over alternatives such as pybullet-drone, MuJoCo [67], AirSim [68] and Flightmare [69] is that it offers greater adaptability and customisation options. This allows us to modify the environment in aspects such as state and action.\n\nFigure 3.6: Illustration of the hardware system of a Crazyflie quadrotor and its communication with the base computer.# Current Research on Reinforcement Learning Environments\n\nspaces, physical properties, kinematics, and reward functions. Additionally, the environment proposed by Agilicious, named Argviz, does not support reinforcement learning integration. Therefore, this study is the first to propose an open-source environment that builds upon PyFlyt for future reinforcement learning research on Agilicious models. To that end, we have pre-designed various tasks, including hovering, obstacle avoidance and trajectory tracking, to allow future deployments without extensive engineering overhead.# PyBullet\n\nAs its physics engine, PyFlyt utilises PyBullet, a free open-source simulator used in numerous reinforcement learning research over the past decade [70\u201372]. It provides a fast real-time simulation built in C++ and thus supports realistic collisions and aerodynamics at a level other physics simulators may lack. The relative performance of PyBullet compared to other popular simulators has been demonstrated in prior works from Korber et al. [73].# OpenAI Gym\n\nIn addition to its adaptability, PyFlyt incorporates the OpenAI Gym interface, providing a user-friendly and training-efficient experience by allowing CUDA-based GPU usage. Moreover, this allows simultaneous and parallel execution of environments, resulting in fast and parallelised training. As a result, this is estimated to save us about half the training time and 20% of the estimated energy consumption.\n\n40# 4.1 Preliminaries and Problem Formulation\n\nOur study mainly focuses on designing a secure learning framework for a quadrotor under malicious false-data injection attacks. The blueprint of our framework is described in Figure 4.1, where we introduce the three layers developed in this paper. The first is a nominal controller responsible for controlling the quadrotor in total autonomy. The second is an attacker, which monitors the sensor data transmitted on the communication network and learns optimal injection attacks to insert into the control signals of the nominal controller. The third is a learning-based secure control algorithm designed to mitigate attacks by adjusting control signals within the secure control layer.\n\n|Actuator|Robot|Sensor|\n|---|---|---|\n|Nominal control layer|Nominal control layer|Nominal control layer|\n|Learning-based attack layer|Learning-based attack layer|Learning-based attack layer|\n|Learning-based secure control layer|Learning-based secure control layer|Learning-based secure control layer|\n\nFigure 4.1: Our proposed secure framework which includes (a) a nominal controller layer, (b) an optimal attack layer, and (c) a learning-based secure control layer. All three layers are designed using deep reinforcement learning.\n\n41We use an X-shaped quadrotor (displayed in Figure 4.2) to describe our design process and the effectiveness of the proposed secure framework. Details on our quadrotor, including hardware, software stack and communication layers, are discussed in 3.1. Specifically, we design a nominal controller trained using reinforcement learning to move the quadrotor towards a given location and hover. Furthermore, to address the secure control problem, we design both a malicious attacker and a mitigating defender using equivalent reinforcement learning approaches.\n\nFigure 4.2: Agilicious: the main quadrotor used in this work.# 4.1.1 Kinematic Model of our Quadrotor\n\nThe motion of our quadrotor is defined as [x, y, z, \u2203, #, %], where (x, y, z) represents the position of the quadrotor in the inertial frame, and (\u2203, #, %) its orientation in the body frame. Its kinematic model is described in subsection 2.1.3 as\n\n[ \u02d9, \u02d9, \u02d9, \u2203x y z \u02d9, #, %],\u02d9 \u02d9 (4.1)\n\nwhich implies the collective thrust of the quadrotor v and its body rates [\u2203\u02d9, #, %].\u02d9 \u02d9 We assume that the velocity v satisfies vmax < v < vmin with vmax and vmin the maximal and minimal velocities, respectively. For simplicity, as we do not aim to do acrobatic flying, the orientations of the quadrotor in its body frame are constrained to \u2203 \u2191]\u2192 3 \u2217, 3 \u2217[, # \u2191] \u2192 3 \u2217, 3 \u2217[, and % \u2191] \u2192 3 \u2217, 3 \u2217[.\n\nFor later calculations, we describe the virtual quadrotor generating the desired trajectories with\n\nx\u02d9r , \u02d9r , \u02d9 r ,\u02d9r , #y z \u2203 \u02d9 r , %r ,\u02d9 (4.2)\n\nand which follows the kinematic model described in 2.1.# 4.1.2 Attack Model\n\nAs illustrated in Figure 4.3, a malicious adversary interrupts control commands sent from the nominal controller to the quadrotor\u2019s actuators via the communication network and injects false data to disrupt the quadrotor\u2019s trajectory. Under such attacks, the control commands are described as follows:\n\n|v(k)|=|va (k)|+|vb (k)|\n|---|---|---|---|---|\n|\u2203\u02d9(k)|=|\u2203a (k)|+|\u2203\u02d9b (k)|\n|\u02d9(k)|=|#a (k)|+|#\u02d9b (k)|\n|%\u02d9(k)|=|%a (k)|+|%\u02d9b (k)|\n\nwhere va (k), \u02d9a (k), \u02d9a (k) and \u02d9a (k) are the false data attacks designed in section 4.3; and vb (k), \u02d9b (k), \u02d9b (k) and \u02d9b (k) are control signals sent by the nominal controller designed in 4.2. We assume that the following knowledge is known to our adversary:\n\n- Adversaries inject malicious attacks without violating the physical constraints of the quadrotor, by which adversaries can both save attack energy and guarantee the effectiveness of attacks to some degree.\n- Adversaries can access the communication network through the man-in-the-middle cyber-attack, for which a blueprint is given in Figure 4.3. That is, the adversaries can intercept the communication network between the actuators and the controller and secretly inject false data.\n\nFigure 4.3: Illustration of a man-in-the-middle attack. An adversary acts secretly as a middleman. It intercepts the signals sent from the controller to the quadrotor\u2019s actuators, modifies the commands, and sends them back to the actuators.# 4.1.4 Problem Formulation\n\nAs shown in the above examples and described in the literature review of section 2.2, quadrotors\u2019 performance can be compromised or even destroyed by cyber threats. Our work focuses on solving the following three problems:\n\n- How to create a reinforcement-learning-based control system capable of flying a quadrotor in complete autonomy;\n- How to learn optimal false data injection attacks to deteriorate a quadrotor\u2019s performance and disturb its trajectory;\n- How to design a secure control algorithm for quadrotor systems to mitigate false data injection attacks and recover flying abilities.\n\nThe three problems are defined mathematically below.# 4.2 Control System for an Autonomous Quadrotor\n\nIn this section, we learn an optimal control system to fly our quadrotor in total autonomy. While one can design such a controller following the control theory, e.g. utilising a PD or MPC controller, we decide to take a reinforcement learning approach to solve Problem 1. By taking this approach, we aim to contribute to current research on learning-based controllers and provide comparative resources against mathematical systems.# 4.2.1 Environment for a Quadrotor under Nominal Control\n\nAs defined in subsection 2.3.1, a reinforcement learning setup includes an agent and an environment, which interact to improve the agent\u2019s capabilities. The environment here is defined by a Markov decision process as a tuple (S, A, P, R, ) where S is the state space, A is the action space, P is the transition probability matrix from a state-action pair at time t onto a distribution of possible states at time t + 1, R is the immediate reward function, and ) \u2191 [0, 1) is the discount factor. Given that the quadrotor is controlled by the nominal controller, we describe the MDP as:\n\ns(k + 1) \u2198 P (s(k + 1)|s(k), ub (k)), (4.12)\n\nmeaning the transition probability from s(k) to s(k + 1) under the nominal controller ub(k), where s = [x(k), y(k), z(k), \u2203(k), #(k), %(k)] with s \u2191 S, and ub(k) \u2191 A.\n\nMore details on the environment. We describe below the MDP defining our nominal environment in more details, including vectorial and constraint definitions.\n\n- The action space A = [vb, \u02d9b, # \u2203 \u02d9b, \u02d9b] with v \u2191 [0, 3.5] and \u02d9, #% \u2203 \u02d9, \u02d9 \u2191 [\u2192 3 % \u2217, 3 \u2217];\n- The state space S = [\u2203\u02d9, #, %, \u2203, #, %, \u02d9, \u02d9, \u02d9, x, y, z, \u2203prev, \u02d9prev, %\u02d9x, %\u02d9y, %\u02d9z] where \u2203\u02d9prev, \u02d9prev, %prev are previous actions, and xh, yh, zh are hover point coordinates;\n- The reward function R is defined as R(k) = k \u2192 XT(k)QbX(k) \u2192 (T(k)Lb((k) according to Problem 1, where Qb \u2196 0 and Lb \u2196 0 are weighting matrices.# 4.2.2 Objectives in the Learning of a Nominal Controller\n\nThrough reinforcement learning, we want to minimize the action-value function Qb(s(k), ub(k)), where \u2217b denotes the nominal policy to learn which finds an optimal function that maps a state and an action to the expected future reward of taking that action. If we can obtain such a function which ensures that the expected future reward is accurate to the true outcome, we can greedily select an action at time t according to \u2217b and guarantee the best outcome in the future. The expression of Qb(s(k), ub(k)) is computed as:\n\nQb(s(k), ub(k)) = )Es(k+1)[Vb(s(k + 1))] + R(k), (4.13)\n\nwhere )Es(k+1)[Vb(s(k + 1))] is the expected cumulative reward from state s(k + 1), given that action ub(k) was taken on state s(k) according to the probability distribution.# Remark 2\n\nThe hover position varies with each training episode to prevent the nominal controller from becoming overly specialized in navigating to a unique hover position in space, i.e. overfitting to its reward. This strategy ensures that the controller actually learns how to \u201cfly\u201d towards a given point instead of learning to converge to a static point through the experimentation of arbitrary sequences of actions during training. Furthermore, its spawn (or initial) position is altered at each training episode to guarantee the controller\u2019s ability to fly arbitrarily in three-dimensional space.Based on the definition of \u00af (k), the attacker\u2019s objective is to maximize the action-R value function Q \u2217a ( \u00af(k), u a (k)), where \u2217 a denotes the attack policy to be learned. The definitions of Q \u2217 a ( \u00af(k), u a (k)) and V\u2217 a ( \u00af(k)) are given below, but explanations are omitted due to similarity with the previous definitions 4.13 and 4.14.\n\nQ \u2217a ( \u00af(k), u a (k)) = )E \u00af(k+1) [V\u2217 a ( \u00af(k + 1))] + Rs\n\nV\u2217a ( \u00af(k)) =&ua (ks &)[\u2217a (u a (k)| \u00af(k)) s&1)Ps \u00afk+1|k \u21d3 ( R\u00af (k) + )V\u2217a ( \u00af(k + 1)))]s\n\nIf we can find a solution to the following optimization problem (4.20), Problem 2 can be solved:\n\n\u2217a \u2194= arg maxQ\u2217 a ( \u00af(k), ua (k)), \u2217 a (4.20)\n\nwhere \u2217a \u2194is the optimal attack policy, and u a (k) samples from this optimal policy.# 4.3.3 Learning-based False Data Injection Algorithm\n\nWe employ reinforcement learning following the proximal policy optimization algorithm to learn the optimal attack policy \u2217a\u2194 and value-function V\u2217a \u2194. We maintain a shared neural network architecture for both the policy and value-function to learn, and train it by minimizing the same loss as the nominal controller:\n\nL t CLIP+VF+S(#) = Et[L t CLIP(#) \u2192 c1 LVF(#) + c2 S[\u2217# old (s)]], \u02c6 (4.21)\n\nexplained in subsection 2.3.4. Note that S still guarantees sufficient exploration. The parameters of our policy network are trained using Algorithm 3, proposed below. Once the network has been successfully trained, false data injection attacks sampled from policy \u2217a can be applied to deteriorate the quadrotor\u2019s tracking performances.\n\nRemark 3. Similarly to Remark 2, variations are introduced in the desired hover point [xh , yh , zh ] \u2191 S\u00af and initial quadrotor\u2019s position. However here, this is made to learn optimal false data injections over many different trajectories.\n\n52# Algorithm 3 Learning-based false data injection attack algorithm\n\n1. for iteration = 1,2,... do\n2. Initialize replay memory buffer Ma\n3. for actor = 1,2,...,N do\n4. for each data collection step of T total timesteps do\n5. Sample ub (k) and u a (k) from the nominal controller\u2019s policy \u2217b and attack policy \u2217 aold (\u00b7| \u00af) respectively\n6. u(k) \u2243 u b (k) + u a (k)\n7. Update the memory Ma \u2243 Ma \u2199 < \u00af, u a (k), r,V\u2217 aold ( \u00af), \u2217a old (at | \u00af) >st\n8. Take combined action u(k)\n9. if episode terminated then\n10. Change hovering point coordinates [xh , yh , zh ] and quadrotor\u2019s initial position to prevent overfitting\n11. end if\n12. end for\n13. Compute advantage estimates \u02c61 , . . . , AA \u02c6 T\n14. end for\n15. Optimize surrogate L CLIP+VF+S wrt \u2217 a , with K epochs and minibatch size M \u21d1 NT from Ma\n16. \u2217aold \u2243 \u2217a\n17. end for# 4.4 Learning-based Countermeasure\n\nFrom an adversary\u2019s perspective, a quadrotor\u2019s tracking performance can be deteriorated by injecting falsified commands generated using Algorithm 3. In this section, we provide a solution to Problem 3, i.e., a learning-based secure control algorithm to stabilize the quadrotor and mitigate the malicious attacks. Moreover, the proximal policy optimization algorithm is modified to account for the attacker\u2019s injections on top of the nominal control commands.# 4.4.1 Markov Decision Process of a Secured Quadrotor\n\nSimilarly to the attacker\u2019s environment described in Section 4.3.1, the Markov decision process of a quadrotor with a secure controller is a tuple where \u02dc is the state space, AS is the action space, P\u02dc is the transition probability matrix, \u02dc R function, and \u02dc \u2191 [0, 1) is the discount factor. Carefully note the tilde instead of the bar over these elements. This time, the action space is modified to \u02dc = [vd , \u02d9d , \u02d9d , \u02d9d ] with #%# 4.4.3 Learning-based Secure Control Algorithm\n\nThe proximal policy optimization algorithm is again employed to learn the optimal countermeasure as a policy \u2217d\u2194 and value-function V\u2217d \u2194. We maintain a shared neural network architecture for both the policy and value-function to learn, and train it by minimizing the loss in Eq. 4.21. According to the objectives defined above, Algorithm 4 is derived to learn the optimal secure control policy \u2217d\u2194 from which secure control commands can be sampled to recover the quadrotor\u2019s tracking performance.# 5.1 Training and Hyperparameter Tuning\n\nThis section describes the training setup used throughout our experiments, introduces the hyperparameter tuning process, and discusses the training results for all three of our controllers.# 5.1.1 Training Setup\n\nThe training of our three agents, i.e. the nominal, attacker and secure controllers, was made in their distinct environments described in 4.2.1, 4.3.1 and 4.4.1, respectively. The training process is performed in accelerated environments capped to the GPU\u2019s capacity and is integrated into a Linux Centos virtual machine1. The machine provides a dedication of 64GB RAM, two 2.60GHz 12-core processors (Intel Xeon E5-2690 v3) and a Tesla V100 GPU.\n\nOur overall training framework involves five parallel environments, each assigned to an actor, for a total of N = 5 parallel actors. In each episode, a hovering point is generated within the range xh \u2191 [\u21921, 1], yh \u2191 [\u21921, 1], zh \u2191 [0, 2] with a uniform probability distribution, where one unit of distance in the environment is proportional to one meter in the real world. That allows the agents to learn by exploring a vast three-dimensional space and to not overfit to a single trajectory or hovering point, as described in Remark 2. Furthermore, at each episode, the quadrotor is spawned at a random position taken from the same ranges as above so that its optimal path relative to the hovering.\n\n1 Machine provided by the Computational Shared Facility (https://ri.itservices.manchester.ac.uk/csf3/) of the University of Manchester.# 5.1.2 Hyperparameter Tuning\n\nTo optimize the training and performance of our agents, the following hyperparameters are tuned: The policy architecture F\u2217, learning rate +, batch size M \u21d1 NT, number of epochs K, discount factor ), and number of timesteps per iteration T. A grid of parameter values was created from background experience and inspiration from the successful literature [76, 77]. The values considered for the nominal controller are the following:\n\n- T \u2191 {5120, 10240}\n- F\u2217 \u2191 {(64, 64), (128, 64), (128, 128)}\n- + \u2191 {1 \u21d3 10\u21924, 3 \u21d3 10\u21924}\n- M \u2191 {128, 256}\n- K = 10\n- ) \u2191 {0.85, 0.99}\n\nThe policy architecture of the attacker and secure controllers are reduced to F\u2217 \u2191 {(64, 64), (128, 64)} based on the assumption that their task has improved linearity over autonomous control. As a result, we obtain 48 and 32 hyperparameter combinations, respectively. The number of iterations was decided through Early Stopping based on the average reward obtained by evaluating the policy over twenty episodes every 45 iterations. This means that training was terminated when the agent would not improve anymore or when convergence to a local maximum occurred (e.g., if the quadrotor is unable to stabilize when dealing with the nominal controller or unable to recover when dealing with the defender).# Training of the nominal controller\n\nAs we can observe from Figure 5.1, models with ) = 0.99 are, on average, much more performant than those with ) = 0.85. In fact, while the latter group receives constant# 5.2 Learning Rate and Model Architectures\n\nThe same holds for the learning rate *\u03b1 in Subfigure 5.2c, where \u03b1 = 3 \u21d3 10\u21924 converges much faster than \u03b1 = 1 \u21d3 10\u21924. By increasing the learning rate, we define the step size, i.e. how much we update our model based on its loss at each iteration. The risk with a high learning rate is the exploding gradient effect [78], where the learning rate is too large, causing the parameters to overshoot the minimum of the loss function and potentially diverge to infinity. However, \u03b1 = 3 \u21d3 10\u21924 seems to preserve a proper balance between fast convergence and stability in learning. It should also be noted that, as described in section 2.3.4, the proximal policy optimization algorithm enhances stability in the learning by constraining the policy updates. Nonetheless, by choosing \u03b1 = 3 \u21d3 10 \u21924, we can reduce our training time and energy to about 9 \u21d3 105 training steps, compared to 16 \u21d3 105 with \u03b1 = 3 \u21d3 10\u21924*.\n\nRegarding model architectures, Subfigure 5.2b shows that the proposed three architectures yield similar results, with their convergence speed ordered with respect to their size. However, bigger models are more expensive to train as they require updating more weights at each iteration. Therefore, although we prioritize model performance, in case two models yield very close or equal results, we prioritize the one with lower training time and energy requirement.\n\n|Policy architecture (F*)|[64, 64]|[128, 64]|[128, 128]|\n|---|---|---|---|\n|Convergence speed to training cost ratio|3.3:1.00 (3.3)|4.6:1.07 (4.3)|5.5:1.12 (4.9)|\n\nTable 5.1: Convergence speed to training cost ratio of different policy architectures (F*). Convergence speed is decided based on the total number of timesteps executed and training cost based on the total GPU usage for training.\n\nGiven the convergence speed to training cost ratios calculated in Table 5.1, we can discard the architecture *F* = [64, 64]*. Although that one requires less computation, its ratio to convergence speed is under-performing compared to alternatives. On the other hand, the other two architectures can be further considered as they possess significantly greater ratios.\n\nFinally, we decided not to make any direct conclusion on the effect of the batch sizes (M) as their results are instead very close and such a parameter does not seriously affect the total GPU usage.\n\n59# Performance Comparison Between Models with T=5120 and T=10240# Training of the attack controller\n\nTo select the hyperparameters of our attack controller, we further constrain the space of parameters we search through based on the assumption that optimal false data injection attacks have greater linearity than autonomous control. Therefore, because a bigger policy network only benefits from the greater non-linearity it can provide, F\u2217 = [128, 128] may not be an appropriate option to consider. Furthermore, as the results of the nominal controller suggest, ) = 0.85 does not fit our environment due to its sensitivity and complex nature. As a result, the following parameter spaces are adjusted: F\u2217 = {(64, 64), (128, 64)} and ) = 0.99.# Figure 5.4:\n\nFollowing the same process as in Figure 5.1, we display the performance comparison between groups of attacker models with (a) steps per iterations T = 5120 and T = 10240, (b) model architectures F\u2217 = [64, 64] and F\u2217 = [128, 64], (c) learning rates + = 1 \u21d3 10\u21924 and + = 3 \u21d3 10 \u21924, and (d) batch sizes M = 128 and M = 256.We might notice slightly faster convergence for the bigger architecture, this does not affect the final model as they converge to the same average reward with very little overhead. Consequently, the smaller architecture F\u2217 = [64, 64] may provide equal performance with better training costs.\n\nFinally, although the learning rate + = 3\u21d310\u21924 in Subfigure 5.4c performed slightly better than + = 1 \u21d3 10\u21924, we may find it interesting to look at the result of the remaining four hyperparameter sets, which include variations in M \u2191 {128, 256} and + \u2191 {+ = 1 \u21d3 10\u21924, + = 3 \u21d3 10\u21924}.# Training of the secure controller\n\nFor our secure controller, we search through the same hyperparameter space as the one used by the attack controller. Figure 5.6 compares the effect between these parameters based on the same model groups.# Figure 5.6\n\nFollowing the same process as in Figure 5.1, we display the performance comparison between groups of defender models with (a) steps per iterations T = 5120 and T = 10240, (b) model architectures F\u2217 = [64, 64] and F\u2217 = [128, 64], (c) learning rates + = 1 \u21d3 10\u21924 and + = 3 \u21d3 10 \u21924, and (d) batch sizes M = 128 and M = 256.\n\nAs we can observe in Subfigures 5.6a and 5.6c, both the number of timesteps per iteration (T) and the learning rate (+) exhibited a strong influence on model performance. In fact, these discrepancies follow the ones obtained on the attack controller but with amplified effects on the average reward.\n\nMoreover, Subfigures 5.6d and 5.6b show that the batch size (M) and model architectures (F\u2217) further impact training performance. However, given the results from the four hyperparameter sets on M and F\u2217 displayed in Figure 5.7, M = 128 may indeed be better than M = 256. Surprisingly, however, F\u2217 = [64, 64] reveals better individual performance compared to the overall analysis from 5.6b.\n\n64# Conclusion on hyperparameter selection\n\nThe final hyperparameters chosen for our three agents are summarized in Table 5.2.\n\nWe notice that the exact same hyperparameters came up on top for the attack and secure controllers. Furthermore, we note that the common choices between the nominal and the other controllers generally refer to hyperparameters that are affected by the environment, i.e. timesteps per iterations T, batch size M and discount factor ). On the other hand, the ones varying are solely based on the task and its reward, i.e. the policy architecture F* and learning rate a.\n\nThe entire training process took about 28 hours on the hardware configuration provided above. As an indication, this represents an average of 15 minutes per model.|Controller|Timesteps per iteration (T)|Architecture of policy (F\u2217)|Learning rate (+)|Batch size (M)|Discount factor (\u03b3)|\n|---|---|---|---|---|---|\n|Nominal|5120|[128, 64]|3 \u21d3 10\u21924 \u21924|256|0.99|\n|Attack|5120|[64, 64]|1 \u21d3 10\u21924|128|0.99|\n|Defence|5120|[64, 64]|1 \u21d3 10|128|0.99|\n\nTable 5.2: Final hyperparameters selected for each of the three controllers.# 5.2 Evaluation in Simulation\n\nIn this section, we provide the simulation results on the autonomous control system (i.e. the nominal controller) and validate the proposed optimal false data injection attack scheme and secure countermeasure. In our experiment, the quadrotor is initialised at coordinates (0.0, 0.0, 0.5) and tasked with reaching six different hover points. The six hover points are set up such that the experiment covers a wide trajectory space. Throughout this section, we define hover point coordinates as Pr = (x, y, zr).# 5.2.1 Tracking Control of a Quadrotor using the Nominal Controller designed in Algorithm 2\n\nFigure 5.8 shows the tracking performance of the nominal controller designed in Algorithm 2. As we can observe from these simulations, the learning-based controller can successfully command the quadrotor to reach all six hover points by employing close-to-optimal trajectories. Additionally, the controller could stabilise the quadrotor upon arrival at the hovering location. Although some small fluctuations occur during stabilisation, this is an expected behaviour when using reinforcement-learning-based controllers. In fact, these results are aligned with the state-of-the-art in [76].\n\nSubfigures 5.8a and 5.8e show the ability of the nominal controller to fly the quadrotor over wide-amplitude areas and reach high-altitude locations through short optimal paths. Moreover, the trajectories suggest that the controller makes good use of dynamics. In fact, such trajectories resemble some that would be taken by professional drone pilots. Finally, because of the broad trajectories, we could expect difficulties when stabilising upon reaching the target position. However, the controller was able to stabilise the quadrotor without the need for any adaptation time.\n\nSubfigures 5.8d and 5.8f exhibit the quadrotor\u2019s ability to reach low-altitude locations through wide-amplitude trajectories. As with the above examples, the quadrotor could take short optimal paths towards its target location and stabilise upon arrival.# Figure 5.8: Trajectories of the quadrotor under the nominal controller, towards each of the six hovering points. Each experiment has a maximum duration of ten seconds, i.e. the expected duration if the quadrotor does not crash.\n\nis worth noticing that the nominal controller is doing a great job maintaining a stable altitude throughout its flights.\n\nFinally, Subfigures 5.8b and 5.8c aim to further illustrate the quadrotor\u2019s capability to lift and hover. Please note that figure 5.8b is zoomed in for the reader\u2019s convenience. Subfigure 5.8b tasks the quadrotor with instantly hovering upon the episode\u2019s start. This represents a challenging task as the quadrotor needs to instantly catch the dynamics and adapt its thrust to its initial altitude without much time for observation. However, we can observe that it could answer this challenge by immediately stabilising and hovering. Furthermore, Subfigure 5.8c displays the quadrotor trajectory when tasked to lift and hover. While some non-optimal controllers may take an arbitrary path towards this location, our controller was able to lift reasonably straight to quickly reach the hover point and stabilise.\n\n67# 5.2.3 Tracking Control of a Quadrotor under the Secure Controller learned using Algorithm 4\n\nIn this subsection, we ran the same experiments as above. However, we added the secure controller to show that the tracking performance of an attacked quadrotor could be recovered by using the countermeasure designed in Algorithm 4.\n\nThe simulation results providing tracking performance of the quadrotor under secure countermeasures are displayed in Figure 5.13. Subfigures 5.13b and 5.13c are zoomed in for convenience.\n\n|(a) Pr = (0.85, 0.90, 1.7)|(b) Pr = (0.0, 0.0, 0.5)|(c) Pr = (0.0, 0.0, 1.2)|\n|---|---|---|\n|(d) Pr = (0.7, 0.85, 0.7)|(e) Pr = (0.0, \u21921.0, 1.5)|(f) Pr = (\u21921.0, \u21921.0, 0.5)|\n\nFigure 5.13: Trajectories of the quadrotor under optimal attacks and secure countermeasures, towards each of the six hovering points. Each experiment has a maximum duration of ten seconds, i.e. the expected duration if the quadrotor does not crash.\n\nThese results demonstrate that the quadrotor\u2019s tracking performance under the secure controller is recovered in all six experiments. Specifically, the controller is able to provide countermeasures for the two types of attack proposed by the malicious attacker, i.e., motor failure and maximal boosting.\n\n72# 5.3 Deployment in Real World\n\nIn this section, we deployed our three learning-based controllers on physical quadrotors, i.e., on real hardware. The aim is to provide real-life settings and observe the performance of our controllers under non-ideal conditions.# 5.3.1 Agilicious Agent\n\nAs demonstrated in [79], learning-based controllers are less predictable than mathematical models. In addition, as described in section 3.1.1, Agilicious is an extremely agile and powerful quadrotor with a top speed of 131km/h. Consequently, deploying learning-based controllers on such a quadrotor in real-life conditions requires adequate settings to avoid damaging equipment and harming surrounding people. Therefore, we decided to minimise the potential risks by taking the following approach.\n\nWe deployed our learning-based controllers within the planning trajectory module and used a mathematical model (MPC) for control. As suggested in [79], using the learning-based controller within planning provides a good indication of model performance while limiting deployment risks. By doing so, we can preview the quadrotor\u2019s trajectory before launching and, thus, fly Agilicious in our laboratory with enhanced predictability and guaranteed safety. The results of this experiment are displayed in Figure 5.15.\n\nAs we can observe from Figures 5.15a and 5.16a, the quadrotor under nominal planning control and no attack can successfully fly in real-world settings. On the other hand, Figures 5.15b and 5.16b demonstrate the behaviour of the same quadrotor but under optimal false data injection and secure countermeasure. These results show the# Drone trajectory\n\n|Start Point|End Point|\n|---|---|\n|1.5|1.4|\n|1.3|1.2|\n|1.1|1.0|\n|0.75|0.5|\n|0.25|0.0|\n|0.25|0.5|\n\n(a) Without attack\n\n(b) With optimal attack and countermeasure\n\nFigure 5.15: Trajectories of the Agilicious quadrotor with learning-based controllers for planning and MPC for control, without (a) and with (b) optimal attack and secure countermeasure.we can see that the quadrotor recovers gradually over time to reach the target location with satisfactory tracking performance, similar to those in simulation (Section 5.2.3).\n\nIt is important to note that, although this experiment tries to further approximate the conditions of flying Agilicious in the real world under learning-based autonomy, we cannot guarantee that such results are entirely accurate to real-world outcomes. In fact, this problem is part of a hot challenge called Simulation-to-Real-World transfer, where researchers explain that simulation conditions can only approximate those from the real world with small but unavoidable error margins.# 5.3.2 Crazyflie Agent\n\nMoreover, as described in section 3.1.2, Cazyflies are less hazardous for deployment in non-adapted environments. Therefore, we deployed our learning-based controllers on a Crazyflie quadrotor in real life to compare with previous planning-limited controls.\n\nTo this end, our learning-based controller is directly integrated within the control module of the quadrotor. That is, no mathematical model is used anymore, and the learning-based controller is no longer limited to planning. This allows to accurately reproduce real-life settings and accounts for non-ideal conditions such as sensor measurement incertitude, motion capture margins, hardware measurement errors, etc.\n\n|Expected trajectory|Expected trajectory|\n|---|---|\n|Drone trajectory|Drone trajectory|\n|Start Point|Start Point|\n|End Point|End Point|\n|1.4|1.4|\n|1.3|1.3|\n|1.2|1.2|\n|1.1|11|\n|1.0|1.0|\n|0.75|0.75|\n|500|500|\n|250|250|\n|0.00|0.00|\n|250|250|\n|500|500|\n|0.75|0.75|\n|~0.'888838870|~0,9888,88835|\n\nFigure 5.17: Trajectories of the Crazyflie quadrotor with learning-based controls, without (a) and with (b) optimal attack and secure countermeasure.\n\n76# (b) With optimal attack and countermeasure\n\nFigure 5.18: Tracking error of the Crazyflie quadrotor with learning-based controls, without (a) and with (b) optimal attack and secure countermeasure. Each experiment is run twenty times and tracking errors are averaged to provide unbiased results. The maximum duration of an experiment is fourteen seconds, i.e. the expected duration if the quadrotor does not crash.\n\nThe experimental setup is given in Figure 3.5 and studies the same scenario as in the Agilicious deployment described above. Furthermore, the experiments were conducted in a closed environment to reduce gusting and light reflection for the motion capture system.\n\nThe results of Crazyflie\u2019s deployment are displayed in Figures 5.17 and 5.18 as snapshots of the quadrotor\u2019s trajectory in real life and tracking error wrt time. As we can observe, the results are similar to what was obtained on Agilicious in section 5.3.1. Indeed, although this experiment evaluates the trained controller in autonomous and real-world settings, the results obtained confirm the capabilities of our controllers to transfer into real-world conditions without limitations on the control module.\n\n77# 5.4.1 Outcomes in Simulation\n\nThe experimental results obtained in subsection 5.2.1 demonstrated the ability of the nominal controller designed in Algorithm 2 to optimally fly a quadrotor towards a desired location and hover with minimal fluctuations upon arrival. More specifically, we proved the nominal controller to be efficient in flying over wide amplitudes, high altitudes (a,e) and low altitudes (d,f), as well as lifting (c) and hovering (b). In addition, the learning-based controller could make good use of its dynamics, with trajectories resembling some that professional drone pilots would take.\n\nFurthermore, subsection 5.2.2 revealed the effect of injecting false data into the quadrotor\u2019s actuators. Specifically, Figure 5.12 showed that the optimal false data injections designed in Algorithm 3 outperform random attacks at tracking performance deterioration. Figure 5.10 proved the effectiveness of our malicious attack algorithm to disrupt the quadrotor\u2019s trajectories and crash the system in all provided experiments, with two major types of attacks: motor failure and maximal boosting.\n\nFinally, subsection 5.2.3 demonstrated the secure countermeasure\u2019s effectiveness in recovering the quadrotor\u2019s tracking performance to complete its trajectory. All provided experiments, which previously failed under attacks, were then able to complete successfully thanks to the secure countermeasure designed in Algorithm 4. Furthermore, our secure controller also proved to be efficient under random attacks in all experiments.# 5.4.2 Outcomes in Real World\n\nExperimental results from subsection 5.3 illustrated all three controllers in action within the planning module of the Agilicious quadrotor. Although we were unable to use them within the control module for safety reasons, pairing them with an MPC suggested that all three controllers were indeed effective in close-to-real-world conditions (Figures 5.15 and 5.16), with results aligned with the simulation outcomes.\n\nMoreover, we successfully deployed our three controllers in real-world settings on the Crazyflie quadrotor, providing a safer approach to real-world deployment. The results obtained were once again aligned with our simulation outcomes and confirmed the capabilities of our controllers to transfer into real-world conditions without limitations on the control module (Figures 5.17 and 5.18).# Conclusion\n\nThis paper offered a thorough investigation into the control systems of autonomous quadrotors, with a particular focus on enhancing robustness against cyber-physical attacks, specifically false data injections. After providing evidence of the necessity of designing secure control schemes for autonomous quadrotor systems, we applied deep reinforcement learning techniques to enhance the security of such systems against cyber threats. This paper has two critical improvements.\n\nThe first is a proposed nominal controller capable of reaching a target location through optimal trajectories and stabilising the quadrotor with minimal fluctuations upon arrival. Our experimental analysis proved the effectiveness of our nominal controller in doing so with great use of its dynamics over diverse types of trajectories, both in simulation and real-world scenarios.\n\nThe second lies in the design of an attack and a secure controller. Although some secure algorithms have been proposed in the literature, most cannot be applied to underactuated nonlinear complex systems, i.e., quadrotors, or do not commit to preserving a confident level of stability in agile settings. Consequently, the effectiveness of existing results does not apply to quadrotors in the way this paper suggests. By taking a deep reinforcement learning approach, an optimal false data injection attack was established to outperform random data injection methods and deteriorate the tracking performance of a quadrotor under nominal controls. Furthermore, it proposed a secure countermeasure framework following the same approach, under which the performance of an attacked quadrotor can be recovered immediately.# The effectiveness of our proposed solutions\n\nThe effectiveness of our proposed solutions were evaluated in simulation and real-world scenarios over a wide range of experiments. These revealed a significant enhancement in the quadrotor\u2019s robustness to cyber-attacks. By integrating the learning-based secure controller, we ensured the continuous stability and safety of the quadrotor, even when subjected to malicious cyber-attacks. The results highlighted the quadrotor\u2019s capability to adapt to different attack scenarios, thus validating our methodology\u2019s effectiveness in real-time threat mitigation.\n\nFurthermore, our work leveraged the capabilities of the Agilicious quadrotor, a state-of-the-art platform which provides the best size-to-computing-power ratio for agile and autonomous flights. As the first team in the United Kingdom to deploy this quadrotor and implement reinforcement learning on its platform, our work introduced a comprehensive breakdown of this quadrotor, including software designs and hardware alternatives. Additionally, this paper provided a detailed reinforcement-learning framework to train autonomous controllers on Agilicious-based agents to support future research on this quadrotor. Finally, we introduced a new open-source environment that builds upon PyFlyt for future research on Agilicious platforms. These contributions promote easy reproducibility with minimal engineering overhead for future works.# 6.1 Limitations\n\nDespite providing significant implications, our investigations encountered several development and deployment limitations.\n\nOne significant limitation encountered was the inability to deploy the Agilicious quadrotor with controllers integrated outside its planning module. This state-of-the-art quadrotor for autonomous systems required specific deployment conditions that were not available in our laboratory settings. Facilities equipped with more advanced systems, such as bird nets and other specialised infrastructures, might offer the necessary environment for safe deployment. As a result, we were restricted in our ability to fully explore and validate the practical aspects of our proposed solutions under more realistic conditions.\n\nAnother critical limitation was the exploration of hyperparameters within our controllers. As we observed during our tuning process, hyperparameters indeed played a# 6.2 Future Works\n\nBuilding on the outcomes and insights gained from this study, we proposed several promising directions for future research that could significantly enhance the cyber-security of autonomous quadrotor systems.\n\nFirst, the defender proposed in this paper must be activated upon detecting an attack. Thus, its effectiveness is contingent on the preliminary identification of an attack through a third-party framework. This detection could be achieved using anomaly detection methods or more straightforward mathematical strategies, as indicated in [41]. Nevertheless, implementing a continuously active defence system could eliminate the need for such initial detection, offering a more robust solution against cyber threats. This approach would, however, necessitate that the secure controller learns to refrain from modifying initial control commands when the quadrotor is not under attack.\n\nFurthermore, the stability analysis conducted in our experiments is entirely empirical, lacking a mathematical demonstration of the system\u2019s stability. Although most current deep reinforcement learning approaches operate on empirical bases, introducing a mathematical proof could affirm stability from a theoretical standpoint. This would confirm that deep reinforcement learning methods are both practically and theoretically sound solutions for addressing cyber threats in quadrotor systems. For reference, [10] provided such an analysis on a two-dimensional system.\n\nLooking forward, this research lays a solid foundation for further exploration into the security of autonomous systems. The optimal false data injection algorithm developed in this study provides a baseline for future research on exposing significant vulnerabilities in quadrotor systems and developing alternative secure controllers. Moreover, future studies could expand upon this work by exploring additional types of cyber-attacks, integrating multi-agent systems, or applying the methodologies developed to different classes of autonomous vehicles.# Conclusion\n\nIn conclusion, this study not only achieved its stated objectives but also significantly advanced the field of autonomous system security through the innovative application of deep reinforcement learning to the secure problem of underactuated nonlinear complex systems. The implications of this project extend beyond academic inquiry, offering practical solutions to some of the most pressing challenges in robotic security.\n\n82",
        "context_id": 2,
        "question": "What platform recorded a 225% increase in malicious attacks on autonomous systems?",
        "answer": [
            "Upstream"
        ],
        "context_length": 84111
    },
    {
        "context": "# Motivation\n\nA review of Deep Learning applications for cryospheric studies, including, e.g. glaciers, ice sheets, permafrost, and snow, is provided by Liu (2021). Their review provides a broad perspective on the field, focusing on a few selected studies. Here, we instead focus on a single application area, i.e. glacier mapping, which is rapidly growing, thereby developing maturity from a DL perspective.\n\nTo our knowledge, this is the first study that provides an overview of the glacier mapping literature based on DL. Here, we describe some of the significant studies published so far and summarize which data sources have been used, highlighting that glacier mapping is, to a large extent, a multi-modal task.# Structure\n\nIn Section 2, we provide an introduction for glacier mapping: we explain why this is an essential problem in cryospheric studies, then discuss the benefits of relying on DL and finally provide an overview of the associated challenges. Section 3 is dedicated to the data modalities used in the studies included in our literature review (Section 4), which covers two major topics in glacier mapping, detecting i) the full extent of a glacier (Section 4.1) and ii) calving fronts (Section 4.2). Next, we provide a discussion in Section 5 followed by a summary (Section 6). Additionally, in Section 7 we include a list of resources (mainly databases) that could be exploited with DL.# Glacier Mapping\n\nTo study various properties of glaciers, e.g. area, hypsometry, we first need to know where they are located. Various regional glacier inventories have been produced, leading to the first global glacier inventory, the RGI (RGI 1.0 Consortium, 2012) that was initially derived from Global Land Ice Measurements from Space (GLIMS), which is a multi-temporal glacier database. The RGI was initially developed as part of the Fifth Assessment Report of the Intergovernmental Panel on Climate Change (Stocker et al., 2013) and was designed to be a snapshot of all the glaciers in the world at the beginning of the 21st century. There are, however, significant variations among the subregions, as RGI is a compilation of regional inventories from various sources, most of which are based on satellite imagery from 1999\u20132010. In the latest version (RGI 7.0 Consortium, 2023), the RGI contains close to 275,000 glaciers, with a minimum area of 0.01 km2, covering a total surface of \u2192 707,000 km2, with an estimated \u00b1 5% error. Using RGI, we# Deep Learning\n\nPrevious studies have shown that DL can provide a significant performance improvement compared to classical approaches for a wide range of EO tasks, including for instance, land-cover classification, vegetation parameters estimation (e.g. height, biomass) and precipitation down-scaling or now-casting (Yuan et al., 2020). Consequently, DL was also adopted in cryospheric studies, e.g. for sea-ice concentration forecasting (Andersson et al., 2021), glacier evolution modelling (Bolibar et al., 2022; Jouvet et al., 2022), modelling the ice thickness of Antarctica (Leong et al., 2020), estimating the mass balance of ice sheet and its various components (Roda Husman et al., 2024; Van Der Meer et al., 2023) and detecting blue ice in Antarctica (Tollenaar et al., 2024), to name a few examples.\n\nA particular area of cryospheric sciences where DL is rapidly evolving is the domain of glacier mapping. First, given the large number of glaciers, fully automating the task of mapping glacier outlines is of high interest. Historically, semi-automatic methods were used for glacier mapping, which usually consist of thresholding band ratios, e.g. Red / Short-Wave InfraRed (SWIR), as exemplified in Figure 1, or the Normalized Difference Snow Index (NDSI). This thresholding step is usually followed by manual corrections, especially for debris-covered glacier parts or those subject to shadowing (Paul et al., 2002, 2020). Similarly, rough estimates of calving fronts can be obtained from thresholding reflectance or backscatter values, as the ocean will appear much darker than ice and snow in imagery. However, such approaches are easily confounded by the presence of sea ice (Liu et al., 2004). A particular difficulty of these semi-automatic methods relates to the fact that, in many cases, thresholds need to be calibrated to each different scene, e.g. to account for varying atmospheric conditions or to capture shadowing effects. From this perspective, DL has the added value of potentially exploiting local features and automatically finding the appropriate threshold.\n\nSecond, even though DL models have associated errors, one can assume that these errors are systematic in space and time (assuming an in-distribution testing scenario). In contrast, the quality of the manual glacier outlines can significantly vary depending on the individuals ('experts') performing the glacier labelling (Paul et al., 2013). To reduce these errors related to manual labelling in newer inventories, a preparation phase is usually implemented to ensure a consistent and homogeneous quality among the human annotators (Linsbauer et al., 2021). Third, DL can learn from multiple data sources simultaneously, which is particularly helpful for more complex classification cases. Such an example is classifying debris-covered glaciers (Xie et al., 2020) that are visually very difficult to distinguish from their (non-glaciated) surroundings, which classical band-ratio methods can only (partly) circumvent through time-consuming and error-prone manual corrections (Paul et al., 2020).\n\nDespite the enormous potential of DL to be applied in the field of glacier mapping, some substantial challenges exist depending on the scientific question of interest. First, despite continuous improvements, the glacier inventories needed for training DL architectures suffer from considerable uncertainties. Although DL can still deal with \"noisy\" labels to a certain extent and still learn the underlying patterns (Arpit et al., 2017; Zlateski et al., 2018), these uncertainties certainly affect the performance metrics used when evaluating the quality of the predictions, making it more challenging to compare different methods. Additionally, the uncertainties in the inventories also vary from one region to another, thus potentially introducing biases when training global models. Moreover, in many cases, it is difficult (and often even entirely impossible) to ensure a perfect temporal match between the (optical) input data and the glacier inventory, which adds to the uncertainties. Lastly, glacier mapping remains a challenging task even for experts, especially for debris-covered glaciers where the interpretation can be subjective, sometimes leading to errors in the order of 10%-20% for small glaciers (Paul et al., 2020). These limitations should be accounted for when developing adequate DL frameworks for glacier mapping, and associated uncertainties in DL model predictions should always be quantified (Maslov et al., 2024).Figure 1: Band-ratio method. Example of the Band-ratio method being applied to Vadret da Misaun, a glacier in Switzerland.# 3 Data Modalities\n\nThis section summarises the data modalities exploited by various DL models from the literature, surveyed in Section 4, or used by experts when building glacier inventories.# Optical (multi-spectral) imagery\n\nOptical data2 is by far the most commonly used type of observation for glacier mapping, primarily because in many circumstances (e.g. for clean-ice glaciers), optical data can already support glacier delineation on its own, without the need of additional data sources. The importance of optical data relative to some other modalities (i.e. Synthetic Aperture Radar (SAR) backscatter intensities, Interferometric Synthetic Aperture Radar (InSAR) coherence, DEM, thermal imaging) was empirically evaluated using ablation studies in the works of Peng et al. (2023) and Maslov et al. (2024). Optical data alone helps building very accurate glacier outlines, especially for debris-free glaciers where the classical band-ratio method or NDSI thresholding yields robust results. The band-ratio method (e.g. Red / SWIR) can separate the very low spectral reflectance of ice and snow in the shortwave infrared versus the high reflectance in the visible spectrum (Paul et al., 2013, 2015). From a DL perspective, optical data offers many advantages: i) numerous data sources available, including many with open-access policies, e.g. Sentinel-2 or Landsat-9 (or older), ii) most of the\n\nNote that we here mainly refer to medium resolution imagery (10 to 30 m Ground Sample Distance (GSD)).# Digital Elevation Model\n\nThe surface elevation is usually provided as an additional input for glacier mapping efforts since it helps the models to extract further information related to topography, e.g. the glacier flow direction, which can be a valuable source of information, especially for calving glaciers. Additionally, using the elevation information, the DL models can \"understand\" where the glacier terminus is, which can be covered by debris as opposed to the accumulation areas.\n\nHowever, obtaining a large-scale DEM from satellite data requires specialized sensors and processing techniques, with additional challenges in mountain regions. For instance, two standard techniques are stereo photogrammetry and InSAR. If the former is affected by cloud and snow coverage or topographic shadowing (Hugonnet et al., 2021), the latter is challenged by the steep terrain and can yield significant ice-penetration biases (Berthier et al., 2023; Dehecq et al., 2016). Studies that rely on DEM for glacier mapping often need to account for a mismatch in timing between the DEM and the considered visual imagery. Despite this limitation, DEM information can still provide helpful information.# Figure 2: Spatial resolution comparison\n\nThe effect of spatial resolution on visual products is here illustrated for Rottalgletscher, a glacier in Switzerland (46.52\u00b0 N, 7.95\u00b0 E). We use the RGB bands from the Landsat-8 acquisition on 22/08/2018 (image courtesy of the U.S. Geological Survey), from Sentinel-2 on 20/08/2018 (courtesy of EU Copernicus program) and the VHR aerial image from swisstopo (2024b) (flight year = 2018). As resolution increases, an increasing level of detail can be observed, particularly pronounced for the debris-covered and shadowed glacier parts. The right panels provide a zoomed-in view of the glacier tongue, illustrating, among others, how the crevasses, a distinct feature of glaciers, become visible with increasing spatial resolution.\n\nAbout the glacier topography and its surroundings. A few standard and openly available DEM choices are Copernicus GLO-30 DEM (Cop30DEM), Shuttle Radar Topography Mission (SRTM) DEM or its improved version - NASADEM, and ALOS World 3D - 30m (AW3D30). Figure 4 displays a DEM at two different spatial resolutions, i.e. 30m and 0.5m, for the tongue of the glacier from Figure 2. While both DEMs roughly capture the valley in which the glacier flows, crevasses become partially visible in the VHR, which can help to better identify the debris-covered parts. This comparison again suggests that spatial resolution could affect the models\u2019 performance.\n\nWhen at least a pair of DEMs is available, it is possible to derive a surface elevation change map. For instance, a negative elevation change rate was observed over the last two decades (2000-2019) for almost all the glaciers in the world (Hugonnet et al., 2021). As a result, DEMs differences can help capture the glacier extent by contrasting it to the surrounding topography where typically no change is expected. This information can supplement image classification approaches, especially for debris-covered glaciers, as these are difficult to classify using optical data alone. However, the availability of global products is somewhat limited: the only global product that is based on the same data source (i.e. ASTER) from Hugonnet et al. (2021) comes at 100 m GSD. Moreover, for short timescales, high-precision DEMs are needed, both vertically and horizontally, to distinguish between potentially small glacier surface changes and off-glacier.# Figure 4: DEMs at two different spatial resolutions\n\nThe figure displays the tongue of the Rottalgletscher glacier in Switzerland (46.52\u2192 N, 7.94 \u2192 E) (see also Figure 2). The DEMs are extracted from the swissALTI 3D DEM (swisstopo, 2024a) and the Copernicus GLO-30 DEM, and are here displayed with a superimposed shaded relief. While both DEMs capture the shape of the valley the glacier flows in, only the DEM with a sub-meter resolution captures smaller scale features such as crevasses.# Figure 5: Differencing of DEMs\n\nHere, we illustrate the role of resolution in DEM differencing for an entirely debris-covered glacier, Glatscher da Sut Fuina, in Switzerland (46.535\u00b0 N, 9.473\u00b0 E). While both DEM differences allow identifying the location of the glacier, the VHR version (central panel), based on two swissALTI3D DEMs (swisstopo, 2024a), is significantly more accurate. Note that for this VHR DEM differencing, the two DEMs were not co-registered before differentiation, which would allow for some artefacts to be removed. The DEM differencing map based on the lower-resolution DEMs (Hugonnet et al., 2021) (lower panel) is only able to roughly capture the location of the glacier, with some potential outliers (e.g., the significantly positive pixels). This figure illustrates that when using DEM differencing to detect glaciers, the role of the spatial resolution becomes important for relatively small glaciers.# 4 Literature Overview\n\nThis section briefly reviews some of the key studies that employed DL models for automatic glacier delineation. Note that this is not an exhaustive review of all existing methods but rather a short overview of some of the most relevant and innovative works, emphasizing the particular challenges that have been addressed and shedding light on the obstacles that will require further research. Additionally, we highlight the various data sources used in each work to illustrate the importance of relying on multi-sensor approaches when mapping glaciers through DL. Given the significant differences in input data sources, labels, and/or considered regions, the evaluation scores mentioned throughout this section should not be overinterpreted or used to compare the performance of the various methods.\n\nThe section is structured as follows. First, we describe the works on glacier extent mapping in Section 4.1, with three sub-categories: i) standard methods, i.e. those that focus in general on glacier extent mapping with the primary goal of automatizing the process, ii) studies that perform glacier mapping on multiple acquisitions to quantify temporal glacier area changes and, iii) studies that map the extent of rock glaciers, which we treat as a separate category given their significant differences compared to typical glaciers, thereby usually requiring more specialized methodologies. The various studies and corresponding methods are then summarized in Tables 1 to 3, respectively. The second part covers the works on calving front detection, which are then summarized in Table 4. Note that most of the paragraphs cover a single study, with some exceptions where follow-ups are also included.# 4.1.1 Glacier Extent Mapping - Standard Methods\n\nThe methods included in the following paragraphs treat the glacier extent mapping problem as a single-image segmentation task and propose various modifications to existing DL architectures, usually with the goal of improving the performance for the debris-covered parts of the glacier which remain hard to detect. One common characteristic of these methods is their data fusion capability: as highlighted in Table 1, most works use at least two data sources, with optical data being the main one.# GlacierNet (Xie et al., 2020)\n\nThe first work that employs a DL architecture is GlacierNet (Xie et al., 2020). Whereas previous studies that used ML relied on classical approaches, e.g. support vector machine (SVM), random forest (RF) or shallow networks like multi-layer perceptrons (MLPs) (Khan et al., 2020; Zhang et al., 2019b), GlacierNet proposes a Convolutional Neural Network (CNN) architecture built upon the fully convolutional model SegNet (Badrinarayanan et al., 2017). The main purpose of the work was to address the challenge of detecting debris-covered glaciers. It was trained using all eleven Landsat 8 bands and the AW3D30 DEM, from which additional features were derived, i.e. slope angle, slope-azimuth divergence index (SADI), profile curvature, tangential curvature and unsphericity curvature. The study focuses on two sub-regions from HMA with different glacier conditions and properties: Nepal Himalaya and the central Karakoram in Pakistan. The glacier boundaries used for training were obtained from GLIMS (Raup et al., 2007) and further modified i) by improving the termini delineations, which can be at a different position due to the time mismatch between the imagery and the inventory, and ii) by removing the snow-covered accumulation zones, as it is indistinguishable from the surrounding snow-covered terrain. Once the model is trained and inferences are made, a post-processing step is applied to improve the predictions by i) connected-region size thresholding, ii) gap-filling, iii) improving the predictions over lake-contact termini based on the Normalized Difference Water Index (NDWI). Using only the imagery and the DEM as inputs, the method achieves 80.9% Intersection Over Union (IOU) and 89.5% F1 on the testing regions, which further increase to 84.1% and 91.4%, respectively, when including all the DEM-derived features.\n\nIn a different study focused on the central Karakoram region, Xie et al. (2021) compare GlacierNet (Xie et al., 2020) against five different CNN-based segmentation models. Instead of GLIMS, the authors used for the ablation areas the more-accurate contours from the Glacier Area Mapping for Discharge in Asian Mountains (GAMDAM) dataset (Nuimura et al., 2015). The chosen baseline methods include three versions of U-Net (Ronneberger et al., 2015): Mobile-UNet (Jing et al., 2022) i.e. U-Net with a MobileNetV2 backbone (Sandler et al., 2018), Res-UNet i.e. U-Net with a ResNet34 backbone (He et al., 2016), and R2U-Net (Alom et al., 2018) i.e. U-Net with recurrent convolutional layers. The last two are FCDenseNet (J\u00e9gou et al., 2017) and DeepLabv3+ (Chen et al., 2018) with an Xception backbone (Chollet, 2017). They found that DeepLabv3+ performs the best, with an 86.2% IOU. However, GlacierNet gives the second-highest score, only 0.2% lower, but at a smaller computational cost.# GlacierNet2 (Xie et al., 2022)\n\nIn a follow-up study, Xie et al. (2022) proposed an improvement of GlacierNet by i) using multiple models to improve the predictions over the glacier termini, ii) including also the snow-covered accumulation zones, which were previously discarded. GlacierNet2 can be considered as a two-member ensemble model as it combines the previous GlacierNet with the predictions from a DeepLabv3+ model (Chen et al., 2018) with an Xception backbone (Chollet, 2017). Each sub-model is trained independently, then their weights are frozen, and lastly, a final 1x1 convolutional layer is trained to fuse their predictions. However, the predictions from GlacierNet alone are still kept and post-processed in parallel with those from the fused GlacierNet-DeepLabv3+ model. Adding to the post-processed steps proposed in the previous work (i.e. gap-filling and region-size thresholding), the authors implement an additional step where the final predictions of the two sub-components are compared at the termini and disagreements are addressed through a k-Nearest Neighbor (KNN) classifier. Also, a particular post-processing pipeline is applied to the accumulation areas for the snow-covered pixels detected using the NDSI. An ablation study shows that the final model reaches an 88.4% IOU and a 93.8% F1 score, improving by 1-2% the baselines, i.e. GlacierNet, DeepLabv3+ and their combination, when evaluated on ablation-zone mapping.# Tian et al. (2022)\n\nAn improved U-Net (Ronneberger et al., 2015) architecture is used by Tian et al. (2022) to segment glaciers in the Pamir Plateau. The original architecture is improved by incorporating the channel attention module from Roy et al. (2018), the so-called channel squeeze and excitation block. On top of this, the authors also used a conditional-random field (CRF) method as post-processing to refine the results. For training, they used optical data from Landsat-8 and the SRTM DEM with the labels based on GLIMS (Raup et al., 2007). These were manually corrected to account for a temporal gap between the imagery and the labels during which glacier changes occurred. The model achieved an F1 score of 89.5% after adding the attention mechanism, further improved to 89.8% with the CRF-based refinement, performing better than the original U-Net, which obtained 88.9%. They also tested the GlacierNet model (Xie et al., 2020), which obtained an F1 score of only 84.9%.# Chu et al. (2022b)\n\nSimilar to the previous study of Tian et al. (2022), in this work, Chu et al. (2022b) also incorporate an attention mechanism, i.e. convolutional block attention module (CBAM) (Woo et al., 2018). They build upon the DeepLabv3+ architecture (Chen et al., 2018) with a ResNet-34 backbone (He et al., 2016). Additional improvements were obtained using test-time augmentation and depth-wise separable convolution at the end of the segmentation (Chollet, 2017). They used high-resolution (8m) multi-spectral data (R, G, B, Near-InfraRed (NIR)) from the Gaofen-6 satellite, which also provides a panchromatic band with 2m resolution. For training, they manually delineated glaciers from the Tanggula, Kunlun and the Qilian Mountains. Finally, they compared their results based on the model\u2019s predictions with existing regional inventories, showing a more accurate extraction of the debris-free glaciers. Their model achieves an F1 score of 98.54%. They compared to multiple baselines, the best performing one being the original DeepLabv3+ model, which achieves already an F1 score of 98.3%, followed by U-Net (Ronneberger et al., 2015) with a ResNet-18 backbone (He et al., 2016) which obtains an F1 score of 97.8%.# Peng et al. (2023)\n\nThe transformer-based architecture builds upon Swin-Unet (Cao et al., 2023), with a series of improvements from various other works. The decoder was coupled with locally-grouped self-attention and global sub-sampled attention modules from Twins-SVT (Chu et al., 2021), together with the conditional position encoding from Chu et al. (2022a). The decoder uses Local-Global CNN Blocks, ending with a Feature Refinement Head as the segmentation head, both from Unet-Former (Wang et al., 2022). The study focuses on the Qilian Mountains, using the inventory from Li et al. (2020). As input data, the following were used: five optical bands (Sentinel-2), i.e. R, G, B, NIR and SWIR (B11), from which three indices were obtained i.e. Normalized Difference Vegetation Index (NDVI), NDWI and NDSI; two SAR backscatter intensity images (VV and VH polarizations), using the Level-1 Ground Range Detected from Sentinel-1; and a DEM using the 8m High Mountain Asia Digital Elevation Model (HMADEM) (Shean, 2017) as the main source, complemented with the SRTM one for the regions not covered by the former. The final model achieves an F1 score of 84.3%, followed by the Swin transformer (Liu et al., 2021) with 82.9% and the model from the previous study of Chu et al. (2022b), based on DeepLabv3+ (Chen et al., 2018), with 82.2%. A detailed ablation study on various input features groups shows that optical bands are the most important but also highlights the benefit of combining multi-source datasets.# Thomas et al. (2023)\n\nThis work focuses on mapping debris-covered glaciers from three HMA sub-regions, i.e. Hunza (Karakoram), Manaslu (Central Himalayas) and the Khumbu (Central Himalayas). The methodology is similar to the one from a previous study (discussed in Section 4.1.3), focused on mapping rock glaciers (Robson et al., 2020).# GlaViTU (Maslov et al., 2023)\n\nFollowing the advances in Computer Vision research, an architecture based on a VisionTransformer (ViT) (Dosovitskiy et al., 2020) is used. The authors propose a hybrid CNN-transformer model, called Glacier-VisionTransformer-U-Net or GlaViTU, by combining SEgmentation TRansformer (SETR) (Zheng et al., 2021) with a ResNet-backbone U-Net (He et al., 2016; Ronneberger et al., 2015). The method is compared to three baselines. First, TransUNet (Chen et al., 2021), i.e. a different type of U-Net that uses a hybrid ResNet-50 (He et al., 2016) + ViT (Dosovitskiy et al., 2020) encoder. The second, ResU-Net, is the U-Net with a classical ResNet backbone, also used in the previously mentioned study of Xie et al. (2021). The third baseline is a SETR-B/16 model with progressive upsampling decoder (Zheng et al., 2021). For a fair comparison, all the baselines are modified by adding a data-fusion CNN-based head as in the proposed architecture. This block was proposed to better fuse the three input data modalities, i.e. multi-spectral optical data from Landsat 7, 8 and Sentinel-2, \u03c9 0 -calibrated amplitude images from Envisat and Sentinel-1, and DEMs (Cop30DEM and AW3D30). The dataset is much larger compared to previous studies, and it covers six regions worldwide: the European Alps, High-Mountain Asia, Indonesia, New Zealand, the Southern Andes and Scandinavia. On average, over these six regions, GlaViTU achieves an 87.5% IOU, followed by TransU-Net and ResU-Net with 86.3% and 85.2%, respectively.\n\nIn a more recent study, Maslov et al. (2024) extend the previous dataset towards a global one. Compared to the previous studies, the dataset is much larger, covering most of the glacier regions outside the two ice sheets. With around 19,000 glaciers, it covers ca. 7% of the total glaciated area. The training labels are mainly based on GLIMS (Raup et al., 2007) and RGI (Pfeffer et al., 2014), but for 8 out of the 23 sub-regions, local inventories are used. In addition to the data sources used in their previous work, i.e. optical, SAR and DEM, the authors also investigate whether adding thermal data when available from Landsat-8 helps but did not find it effective. Moreover, in this work, InSAR coherence images are also considered (when available) instead of the amplitude images, improving performance. The original GlaViTU model is further improved by updating the data fusion block, which is now equipped with feature weighting and includes squeeze-and-excitation blocks. The DeepLabv3+ model (Chen et al., 2018), with a ResNeSt-10 backbone (Zhang et al., 2022), is used as a baseline, after adding the same data fusion block as for GlaViTU. The average IOU over all the regions is 89.4%, almost 2% higher than the baseline, which obtains 87.7%. Instead of training a single global model, Maslov et al. (2024) also investigate four additional training strategies, i.e. regional training, finetuning and location encoding (by using either the region or the coordinate), showing that on average, the regional and the fine-tuned models perform the best, with almost 1% better than the global model. Lastly, it was investigated for the first time whether uncertainty estimation techniques, namely Monte Carlo dropout (Gal et al., 2016) and temperature scaling (Guo et al., 2017), can be used to get further insights into the predictions. They concluded that temperature scaling alone could already help to extract confidence intervals for the predictions and qualitatively found that the model is more uncertain on the debris-covered segments or those under the shadow, thus illustrating the practicality of the uncertainty estimates. So far, this study represents the most comprehensive dataset for glacier mapping using DL which can facilitate further methodological developments by directly using the processed data and the same validation procedure.# 4.1.2 Area Change Estimation\n\nSince the area of a glacier is a crucial indicator of its health, tracking its evolution over time is an important application of glacier mapping methods. Traditionally, this is done by manually re-creating glacier inventories after a certain period, usually decades, such that long-term impacts of climate change can be observed (see Section 2 where we provide examples). In principle, once a DL model is trained (as presented in the previous section), it can be applied to delineate glaciers at different points, with the predicted results being used to analyze the temporal changes. However, a few\n\n3 Note that sometimes changes to the original architecture are made, see Section 4.1.1.\n\n4 Often features are further derived e.g. NDVI from optical or slope from DEM.\n\n5 We only indicate the main Regions of Interest (ROIs) from where the training data was extracted. Still, the coverage can vary significantly, e.g. from a few image scenes to almost complete coverage.\n\n6 This refers to the processed training data, not the raw one. The latter is usually openly available from the specified source.\n\n7 In case the study has an output product (e.g. an inventory).\n\n8 In both studies \u03c9 0 -calibrated amplitude images are used. In the second study, InSAR coherence images are also included when available.\n\n9 used only in the second study\n\n10 In this work, the original GlaViTU model from Maslov et al. (2023) is slightly modified, see Section 4.1.1# Challenges in Detecting Glacier Changes\n\nChallenges arise in practice when detecting glacier changes from outlines derived from a DL framework, requiring additional assumptions. To list a few:\n\n- To extract an area change rate for each individual glacier, the same ice divides have to be used, as usually the methods only classify a pixel as being part of the glacier or not, without any knowledge about what a glacier is as an independent object. This can be problematic when tributaries are becoming disconnected from the main glacier.\n- In general, to increase the signal-to-noise ratio, there should be a relatively long period between the considered outlines. However, most models are trained on imagery from a single source, e.g., Landsat-8, usually constrained by the static inventory that provides the labels, e.g., the RGI. If the resulting model is then applied on data from different sensors to increase the temporal coverage, we potentially need to deal with generalization issues.\n- Even if the data comes from the same sensor, temporal generalization issues can still occur. One such situation is when the imagery has different characteristics compared to the one used for training, e.g. different cloud coverage or seasonal snow conditions. See Figure 6 as an example.\n- Increasing debris-coverage in a warming climate (Compagno et al., 2022; Tielidze et al., 2020) can also affect the temporal generalization. Since automatic methods are still significantly more affected by errors on debris-covered glacier parts than clean ice, the change in the debris coverage percentage can introduce biases in the results. An example is provided in Figure 7.\n- Once a model is trained, we ideally want to employ it on the entire ROI to cover all the glaciers. However, this requires applying the model also on the data on which it was trained. Given the risk of memorization, especially when training with noisy labels (Arpit et al., 2017), we can expect that the model errors over time are not independent, thus breaking the temporal generalization assumption.\n\nTo summarize, additional challenges arise when relying on DL for glacier area change quantification. These challenges perhaps explain why there are only a relatively small number of DL works that explicitly focus on glacier changes. A few works that attempted to identify glacier changes are presented in the following three paragraphs and summarized in Table 2.# GlacierCoverNet (Roberts-Pierel et al., 2022)\n\nThe availability of long time-series data from the Landsat program offers some opportunities for glacier studies. Roberts-Pierel et al. (2022) exploit the data starting from 1985 up to 2020 covering Alaska to study how glacier surface area evolved over time, an important indicator of the glaciers\u2019 health. After building a temporal mosaic to account for clouds and seasonal snow, they obtained 18 biannual image composites, each with full spatial coverage of the selected region. Based on this, they extract five features: NDSI, NDVI, Normalized Burn Ratio (NBR) and Tasseled Cap Brightness & Wetness (Kauth et al., 1976). Additionally, a DEM is used as input, together with three features derived from it: curvature and aspect intensity (North and South), i.e. a method of scaling the cosine of aspect by the sine of slope (Kirchner et al., 2014). The proposed model is based on the FSPNet architecture (Zhao et al., 2017) with a ResNeSt-101 backbone (Zhang et al., 2022). The glacier outlines from RGI 6.0 were used for training. Another significant difference compared to the previous studies is that the model is trained to predict separately whether a pixel is no glacier, supra-glacial debris or debris-free glacier. This was achieved by developing a method for identifying the debris pixels and then assuming that those within the RGI outlines are debris-covered glacier pixels. Once the model was trained using the image composites as close as possible to the RGI dates, it was applied on the entire time-series, thus producing 18 glacier inventories for Alaska. These show a significant area loss: the total glaciated area decreased by 8425 km2 (i.e. -13%), with sizeable sub-regional variability. The relatively fine temporal resolution also allows the inspection of the glacier area changes through time, showing a much stronger shrinkage over the last 15 years (2005-2020) compared to the earlier period. Lastly, the distinction between debris and clean ice allows studying the debris evolution over time, revealing an increase of around 64% over 1985-2020, also with significant sub-regional variability.# Rajat et al. (2022)\n\nA similar investigation but on a much smaller scale shows that the Himachal glaciers retreated significantly, from an estimated total area of around 4,021 km2 in 1994 to only 2199 km2 in 2021, resulting in an# Figure 6: Debris-covered glacier with fresh snow\n\nThe effect of fresh snow when mapping a fully debris-covered glacier is here shown for Glatscher da Sut Fuina, in Switzerland (46.535\u00b0 N, 9.473\u00b0 E) (same glacier as in Figure 5). The two aerial images from swisstopo (2024b) capture an important issue that can affect automatic glacier extent mapping & area change analysis, i.e. predicting the presence of a glacier using, as a proxy, the superimposed fresh snow, hardly present outside the glacier surface (upper panel, where the snow only remains over the debris-covered glacier, which is colder than the non-glaciated surroundings). Since automatic methods, including DL-based ones, are usually challenged in the case of debris cover, in this example, one would probably underestimate the glacier surface in 2022 (without snow) and thus overestimate the shrinkage rate. This figure illustrates that choosing imagery with similar climatic conditions, ideally without any snow, should be a priority, which is especially important for glacier area change analyses. See also Figure 7.# Annual Retreat Rate\n\nAnnual retreat rate of \u2192 68 km2 (\u2192 1.68%). To obtain these results, a U-Net was trained on manually annotated glaciers by visualizing Landsat-8-based NDSI and a DEM from USGS, with additional derived features. The model, a U-Net (Ronneberger et al., 2015), is trained using a subset of four bands and then applied on four different years, i.e. 1994, 2001, 2011 and 2021, using input data from Landsat 4/5/8.# Diaconu et al. (2023)\n\nThis is another study that makes use of DL for investigating glacier area changes, focused on a different region, the European Alps (RGI-11). The input data consists of a subset of five bands (R, G, B, NIR, SWIR-B12) from Sentinel-2 and a DEM. A major advantage of this study is the relatively good quality of the labels: they are based on a new inventory (Paul et al., 2020) and estimated to be of better quality than the previous RGI one. It is based on Sentinel-2 data from (mainly) 2015, ensuring a perfect match between the satellite data used for training and the glacier outlines. The model used is a U-Net (Ronneberger et al., 2015) with a ResNet34 backbone (He et al., 2016). To avoid using inferences made on the training data, which can lead to biases, five models are trained using a regional cross-validation scheme. Once the models are trained, they are applied on the most recent data, i.e. 2023, which was a strong melt year according to Glacier Monitoring in Switzerland (GLAMOS) (GLAMOS, 2023). This offers, however, ideal conditions for glacier mapping as it reduces the chances of seasonal snow, which can cause many false positives.\n\nBased on the models\u2019 predictions, the areas are estimated both at the inventory time and in 2023, which are then compared to estimate change rates for each individual glacier. To increase the signal-to-noise ratio, the authors make two important assumptions: i) the glaciers do not grow in the given period, supported by geodetic mass-balance studies (Hugonnet et al., 2021) and ii) the models make systematic errors. The second assumption, if true, implies that the segments missed by the models, e.g. debris-covered or under shadow, do not significantly affect the estimated change rates. The regional cross-validation scheme helps support this assumption since it decreases the chances that models would perform differently at the inventory time compared to 2023. To further reduce the impact of the models\u2019 errors on the area change rates, an outlier filtering scheme was used to drop the glaciers for which the model performs poorly. After this step, individual estimates for around 1,300 glaciers are provided, representing 87% of the glacierized area in the region. Regionally, the estimate is around -1.8% loss per year, which illustrates the high sensitivity of the glaciers in this region to climate change. A glacier-level analysis further shows significant inter-glacier variability.# Figure 7\n\nA glacier with increasing debris coverage. For many debris-covered glaciers, there is a tendency for debris cover to increase over time, as illustrated here for the Tambogletscher, a glacier in Switzerland (46.504\u00b0 N, 9.291\u00b0 E). The two aerial images from swisstopo (2024b) show that initially, in 2019, the glacier was only partially covered by debris (upper panel), while three years later (lower panel), it has become almost completely debris-covered. Such transitions will affect automatic glacier extent mapping & area change analysis since automatic methods, including DL-based ones, usually face important challenges related to detecting the presence of debris. In the example presented here, many detection methods will likely underestimate the glacier area in 2022 and thus overestimate the glacier shrinkage rate. This figure suggests that capturing the uncertainties in the methods becomes critical to avoid significant biases in the estimates. See also Figure 6.\n\n|Publication (model name)|model architecture|data modality (source)|ROI|data output|\n|---|---|---|---|---|\n|Roberts-Pierel et al. (2022)|FSPNet (Zhao et al., 2017)|\u2022 optical (Landsat 4/5/7/8) \u2022 DEM (USGS-3DEP)|Alaska|-/-/O|\n|Rajat et al. (2022)|U-Net (Ronneberger et al., 2015)|\u2022 optical (Landsat 4/5/8)|HMA: Himachal|-|\n|Diaconu et al. (2023)|U-Net (Ronneberger et al., 2015)|\u2022 optical (Sentinel-2) \u2022 DEM (NASADEM)|European Alps (RGI-11)|C/D/-|\n\nTable 2: Summary of DL-based studies focused on glacier area change analysis. For details, see Section 4.1.2 or the corresponding publications. The full links are also provided in Section 7.\n\n11 Note that sometimes changes to the original architecture are made, see Section 4.1.1.\n\n12 Often features are further derived e.g. NDVI from optical or slope from DEM.\n\n13 We only indicate the main ROIs from where the training data was extracted. Still, the coverage can vary significantly, e.g. from a few image scenes to almost complete coverage.\n\n14 This refers to the processed training data, not the raw one. The latter is usually openly available from the specified source.\n\n15 In case the study has an output product (e.g. an inventory).# 4.1.3 Rock Glaciers Mapping\n\nA special class of glaciers is the so-called rock glaciers. These are essentially a mixture of frozen debris and ice16. As opposed to typical glaciers, which, by definition, are flowing, rock glaciers can be both active and non-active. They are important for cryosphere studies as they can indicate the permafrost distribution in the region. Rock glaciers are also affected by climate change but are more resilient due to the insulated effect of the rocky material and the active layer (Robson et al., 2020). Detecting this type of glaciers from optical data is even more difficult as, by definition, they are covered by debris. Consequently, the studies on this area usually have a multi-modal approach, e.g., by including SAR coherence that can capture small deformations that could occur over time (at least for the active ones). An example of a rock glacier is provided in Figure 8. In the following paragraphs, we describe three major studies focused on rock glaciers and then summarized in Table 3.# Robson et al. (2020)\n\nRock glaciers pose even more challenges to automated methods compared to glaciers that are only partially covered by debris due to the spectral similarity between them and the surrounding material. Robson et al. (2020) propose to use a CNN model combined with OBIA to further improve the predictions using additional morphological and spatial characteristics. Given the difficulty of the task, various data sources are used: optical data from Sentinel-2, InSAR coherence data from Sentinel-1 and a Pl\u00e9iades DEM (processed by the authors). The study focuses on the La Laguna and Poiqu catchments, in the Andes, and the central Himalayas, respectively. The ground truth labels are obtained from a dataset by Schaffer and Macdonell for the La Laguna catchment and the one from Bolch et al. (2020) for Poiqu. The authors used a relatively small DL model, a five-layers CNN trained with eleven input features: five optical bands (R, G, B, NIR, SWIR) + three derived indices (NDVI, Modified Normalized Difference Water Index (MNDWI)(Xu, 2006), Soil Adjusted Vegetation Index (SAVI) (Alba et al., 2012)), InSAR coherence, DEM + derived curvature. With the full pipeline, i.e. CNN + OBIA, they automatically mapped 108 of the 120 glaciers considered in the validation set. The authors also investigated, for a small sub-region of the Poiqu catchment, whether using higher resolution optical data and the corresponding DEM from Pl\u00e9iades (2m) boosts the performance and found a 9% increase in precision but only 1% increase in recall.# Hu et al. (2023)\n\nIf there are already many inventories for typical glaciers, including RGI (Pfeffer et al., 2014), which has global coverage, rock glaciers are yet to be identified in some regions. The Western Kunlun Mountains represent\n\n16 For a more technical definition, see RGIK (2023).# 4.1 Rock Glaciers Mapping\n\none such case. Hu et al. (2023) built an inventory for the active rock glaciers in the region using InSAR from ALOS and VHR images from Google Earth. Using this data, a DeepLabv3+ model (Chen et al., 2018) with an Xception backbone (Chollet, 2017) was trained on Sentinel-2 (only RGB) and applied to the entire region to further identify glaciers that were previously missed, increasing the initial number of 290 glaciers to 413. The outlines produced by the model had however to be manually inspected and corrected, therefore further research is necessary towards a fully-automatic pipeline.\n\nSun et al. (2024) For the first time, a regional-scale inventory for rock glaciers was built by Sun et al. (2024). The final benchmark dataset contains 44,273 glaciers covering \u2192 6000 km2 (\u03bc = 0.14 km2). This was achieved by initially compiling multiple existing inventories, not only from the Tibetan Plateau but also from other regions, into a large dataset containing 4085 rock glaciers. This was then used to train a DL model using Planet Basemaps. The model architecture is the same as in Hu et al. (2023), i.e. a DeepLabv3+ model (Chen et al., 2018) with an Xception backbone (Chollet, 2017). The initial predictions of the DL model were investigated and manually corrected following a strict guideline (RGIK, 2023), with the effort of seven mappers and two independent reviewers. At this stage, high-resolution Google Earth images and ESRI basemaps were also utilized. When comparing the initially DL-produced polygons with the final ones revised by experts, a 63% F1 score was obtained (precision = 55%, recall = 73%).\n\n|Publication|(model name)|model architecture|data modality (source)|ROI|data output|\n|---|---|---|---|---|---|\n|Robson et al. (2020)|custom (5-layers CNN)|\u2022 optical (Sentinel-2, Pl\u00e9iades)|\u2022 InSAR coherence (Sentinel-1)|La Laguna catchment (Andes)|-|\n|Hu et al. (2023)|DeepLabv3+ (Chen et al., 2018)|\u2022 optical (Sentinel-2)| |HMA: Western Kunlun|C/-/O|\n|Sun et al. (2024)|DeepLabv3+ (Chen et al., 2018)|\u2022 optical (Planet Basemaps)| |HMA: Tibetan Plateau|-/-/O|\n\nTable 3: Summary of DL-based studies focused on rock glaciers mapping. For details, see Section 4.1.3 or the corresponding publications. The full links are also provided in Section 7.# 4.2 Calving Front Detection\n\nFor marine-terminating glaciers, changes in the calving front are an essential indicator of the underlying glacial dynamics, with shifts in the calving front hinting at melt processes or surge events. As the mass loss of the major ice sheets in Antarctica and Greenland is projected to be a major contributor to global sea level rise (Calvin et al., 2023), understanding and monitoring these developments is paramount.\n\nVarious DL approaches have been suggested in recent years to automatically monitor calving fronts for the Earth\u2019s ice sheets. Generally, these methods use either optical or SAR imagery as their primary source of imagery, with some methods taking auxiliary data as additional inputs, such as elevation data. The following is an overview of relevant existing works on DL for calving front detection:\n\nBaumhoer et al. (2019) Seeing the potential of DL methods for automated detection of ice sheet calving fronts in Antarctica, Baumhoer et al. (2019) adapted the U-Net model (Ronneberger et al., 2015) for this task.# Mohajerani et al. (2019)\n\nIn a first case study for Greenland glaciers, Mohajerani et al. (2019) trained a U-Net model (Ronneberger et al., 2015) to detect calving fronts for three glaciers in Landsat imagery. For Landsat 5, the green band was extracted, while the panchromatic band was used for Landsat 7 and 8. Calving fronts were annotated manually for each image. The studied regions cover the Helheim, Sverdrup, Kangerlussuaq, and Jakobshavn glaciers. The imagery is segmented into two classes, namely a \u201cbackground\u201d class and a \u201ccalving front\u201d class, following an edge-detection approach rather than a zonal segmentation approach. The final calving front prediction is then derived via a path-finding algorithm and re-projection to the original geographical coordinates.# Zhang et al. (2019a)\n\nSeeing the value of SAR data for calving front detection, Zhang et al. (2019a) manually delineated coastline positions for the Jakobshavn Isbrae glacier in Greenland using imagery from the TerraSAR-X mission. As a segmentation model, they train a U-Net model (Ronneberger et al., 2015) to segment the imagery into a \u201cglacier\u201d and an \u201cocean\u201d class. The calving front is then extracted by tracing the boundary between these two classes in a post-processing step.# CALFIN (Cheng et al., 2021)\n\nCheng et al. (2021) built a large-scale dataset of calving front observations in Greenland by manually annotating Landsat data from 1972 to 2019, covering 66 glaciers. Similar to Mohajerani et al. (2019), single-channel data is used (in this case, from the near-infrared band). However, the imagery is pre-processed into three channels by applying contrast normalization algorithms and stacking the results. Using this dataset, a DeepLabv3+ model (Chen et al., 2018) with an Xception backbone (Chollet, 2017) is then trained to both segment the imagery into ocean and land classes, as well as directly mark the calving front. In a post-processing step, the calving front is then extracted by constructing a minimum-spanning tree and finding the longest path within this tree.# HED-UNet (Heidler et al., 2022)\n\nSeeing the advantages of both segmentation and edge detection approaches for the task of calving front detection, Heidler et al. (2022) combined these two tasks in a single model. This dual training enhances model predictions near the calving front, which tend to be imprecise and blurry for segmentation-based models. The developed model takes inspiration from both the U-Net (Ronneberger et al., 2015) and the HED edge detector (Xie et al., 2015). The dataset from Baumhoer et al. (2019) is used as training and evaluation data. However, the DEM input channel is identified as a potential confounder in this study, which can cause the model to overfit to this static data product and ignore the actual SAR imagery. The authors, therefore, advocate against using elevation data as a direct input feature for the DL model and instead suggest only incorporating this data in a separate post-processing step. IceLines (Baumhoer et al., 2023) is a continuously updated data product for the entire Arctic derived using this model.# Loebel et al. (2022)\n\nSeeing that previous studies for calving front extraction in Greenland mostly relied on single-channel imagery, Loebel et al. (2022) studied the benefits of incorporating various additional data modalities into the training process. Using Landsat-8 imagery as the main imagery source, they manually delineate calving fronts for 23 glaciers in Greenland glaciers and two glaciers on the Antarctic Peninsula. The used DL model is a deepened version of the U-Net (Ronneberger et al., 2015) model, including two additional down- and up-sampling stages to allow for a larger spatial context. Starting with panchromatic imagery as a baseline input modality, the authors test various configurations by adding multi-spectral channels, statistical texture information, and topography data from the BedMachine Greenland v3 dataset (Morlighem et al., 2017). The largest gains in model accuracy are observed when multi-spectral information.# Gourmelon et al. (2022)\n\nIn an effort to make the task of calving front detection more approachable for researchers from the field of computer vision, Gourmelon et al. (2022) build CaFFe, a \u201cmachine-learning ready\u201d dataset of calving front positions in Greenland, Antarctica, and Alaska. The imagery used comes from various SAR sensors, namely ERS-1/2, RADARSAT 1, Envisat, ALOS, TerraSAR-X, TanDEM-X, and Sentinel-1. As ground truth annotations, the authors provide both calving front masks for edge-oriented approaches and zone labels that segment the imagery into the classes \u201cocean\u201d, \u201crock\u201d, \u201cglacier\u201d, and \u201cNA\u201d. As a baseline model for future comparisons, the authors train an adapted U-Net model (Ronneberger et al., 2015), which is augmented by an atrous spatial pyramid pooling layer (Chen et al., 2018) in the bottleneck of the network. Notably, this study quantifies the effect of the seasons and image resolution on the prediction accuracy, suggesting that summer images are to be preferred over winter images and that higher resolution can help with prediction accuracy in some cases.# Periyasamy et al. (2022)\n\nSeeing the wide-spread use of the U-Net model (Ronneberger et al., 2015) for calving front detection, Periyasamy et al. (2022) conducted a systematic study to better understand the influence of specific hyperparameters for this task and give recommendations on how to tune models for calving front detection. Using an earlier version of the CaFFe dataset (Gourmelon et al., 2022), they optimize various components of the training process, such as data preprocessing, data augmentation, loss function, bottleneck, normalization layers and dropout layers. They observe optimal performance when using adaptive histogram equalization.# COBRA (Heidler et al., 2023)\n\nNearly all existing studies employing DL for calving front detection are trained to provide dense predictions, either in the form of semantic segmentation or edge detection. Heidler et al. (2023) take another approach: by adopting the idea of deep active contours (Peng et al., 2020), they introduce a model that directly predicts a contour line, parameterized by a sequence of vertices in the image space. In this way, the model is encouraged to focus on the actual calving front during training, and the predictions can be used without any post-processing steps. This model is trained on the CALFIN dataset (Cheng et al., 2021), and has been applied for a large-scale study of calving front dynamics in Svalbard (Li et al., 2024).# Zhang et al. (2023)\n\nTo leverage the increasing amount of openly available remote sensing data, Zhang et al. (2023) developed an automated pipeline for calving front extraction using both SAR (from Sentinel-1) and optical data (Landsat 5,7,8 and Sentinel-2). For training labels, the authors make use of TermPicks (Goliber et al., 2022), a large-scale dataset with manually digitized calving fronts. To quantify the uncertainties in the model (a DeepLabv3+ (Chen et al., 2018)), Monte Carlo dropout (Gal et al., 2016) is employed, combined with a temporal ensemble (i.e. combining multiple predictions for the same date). The high-temporal resolution of their results allows them to capture the seasonal variability, with the final product, AutoTerm, covering 295 outlet glaciers in Greenland and 278,239 calving fronts.# AMD-HookNet (Wu et al., 2023)\n\nA common observation in calving front detection research is the need for large spatial context windows. Naive solutions to addressing this requirement, such as training on larger image patches and increasing the size of convolutional filters, are computationally inefficient. Setting out to address this issue in a more elegant manner, Wu et al. (2023) introduce AMD-HookNet, a deep neural network designed to operate on two versions of a satellite scene at different spatial resolutions. By interlocking two U-Net (Ronneberger et al., 2015) branches with attention layers, more general information with a wide spatial context can be applied to improve the predictions of the high-resolution branch. This model is trained on the CaFFe dataset introduced by Gourmelon et al. (2022). Evaluations show that this dual-resolution approach can indeed improve prediction accuracy considerably.# HookFormer (Wu et al., 2024)\n\nRecently, vision transformers (Dosovitskiy et al., 2020) have become the tool of choice for many computer vision tasks. Following the ideas introduced in the previous study, Wu et al. (2024) combined a Swin Transformer model (Liu et al., 2021) with the HookNet approach, where a high-resolution and a low-resolution branch are interleaved. The resulting model is again trained on the CaFFe dataset Gourmelon et al. (2022). Compared to the previous evaluations, this transformer-based model outperforms the previous CNN-based models.# Table 4: Summary of DL-based studies focused on calving-front detection.\n\n|Publication|model architecture|data modality (source)|ROI|data output|\n|---|---|---|---|---|\n|Baumhoer et al. (2019)|U-Net (Ronneberger et al., 2015)|SAR (Sentinel-1)|Antarctica|-|\n|Heidler et al. (2022)|custom; based on HED (Xie et al., 2015) & U-Net (HED-UNet) (Ronneberger et al., 2015)|DEM (TanDEM-X)|(8 sites)|C/-/O|\n|Mohajerani et al. (2019)|U-Net (Ronneberger et al., 2015)|optical (Landsat 5,7,8)|Greenland|(4 glaciers) C/D/-|\n|Zhang et al. (2019a)|U-Net (Ronneberger et al., 2015)|SAR (TerraSAR-X)|Jakobshavn Isbrae (Greenland)|-/-/O|\n|Zhang et al. (2021)|DeepLabv3+ (Chen et al., 2018)|optical (Landsat-8, Sentinel-2)|Greenland|(3 glaciers) C/D/O|\n|Cheng et al. (2021) (CALFIN)|DeepLabv3+ (Chen et al., 2018)|CALFIN dataset (Cheng et al., 2021)|Greenland|C/D/O|\n|Heidler et al. (2023) (COBRA)|custom; based on Deep Snake (Peng et al., 2020)|optical (Landsat 5,7,8)|Greenland (66 glaciers)|C/\u2191/O|\n|Loebel et al. (2022)|U-Net (Ronneberger et al., 2015)|bed topography (Bed-Machine (Morlighem et al., 2017))|Greenland (23 glaciers) & Antarctica (2 glaciers)|C/D/-|\n|Periyasamy et al. (2022)|U-Net (Ronneberger et al., 2015)|earlier version of CaFFe (see next line)|-|-|\n|Gourmelon et al. (2022)|U-Net (Ronneberger et al., 2015)|CaFFe dataset (Gourmelon et al., 2022)|6 glaciers|C/D/O|\n|Wu et al. (2023) (AMD-HookNet)|custom; two branch U-Net (Ronneberger et al., 2015)|SAR (ENVISAT, ESR 1&2, Sentinel-1, TerraSAR-X, TanDEM-X, ALOS, RADARSAT-1)|Antarctica, Greenland, and Alaska|-/-/\u2191|\n|Wu et al. (2024) (HookFormer)|custom; two branch Swin Transformer (Liu et al., 2021)|SAR (Sentinel-1)|Greenland|-/-/\u2191|\n|Zhang et al. (2023)|DeepLabv3+ (Chen et al., 2018)|optical (Sentinel-2, Landsat-5,7,8)|(295 glaciers)|C/-/O|\n\nFor details, see Section 4.2 or the corresponding publications. The full links are also provided in Section 7.\n\n22 Note that sometimes changes to the original architecture are made, see Section 4.2.\n\n23 Indicates whether a \u2018benchmark\u2018 dataset is used.\n\n24 Often features are further derived e.g. NDVI from optical or slope from DEM.\n\n25 We only indicate the main ROIs from where the training data was extracted. Still, the coverage can vary significantly, e.g. from a few image scenes to almost complete coverage. We provide the number of glaciers, where possible, but this should only be taken as a rough estimate as it depends on how glacier systems are separated into individual glaciers.\n\n26 This refers to the processed training data, not to the raw one. The latter is usually openly available from the specified source. \u2192 means see the data link from the previous publication.\n\n27 In case the study has an output product (e.g. an inventory).# 5 Discussion\n\nIn this work, we provided an overview of glacier mapping with DL, with a first part focusing on mapping (delineating) the full glacier extent (Section 4.1) and a second part detailing the automatic extraction of calving fronts (Section 4.2). Although the two fields evolved relatively independently, the utilized methodologies are generally similar, with many works relying on fully convolutional segmentation models that are well established in the DL community. Based on the data sources that these studies use, we can conclude that glacier mapping heavily relies on data fusion, as most of the methods make use of at least two different data modalities, e.g. for glacier extent mapping, usually, at least an optical and a DEM are combined. This multimodal characteristic indicates the advantage of using DL for glacier mapping tasks, as it can automatically learn to extract useful information from each modality. We note, however, that most of the glacier mapping studies simply concatenate the input bands coming from each source, which may be sub-optimal (see, e.g. Li et al. (2022) for a review on data fusion with DL, focused on remote sensing applications).\n\nSome of the limitations we identified in the considered studies (Section 4) include:\n\n- Some studies do not clarify how the training-test data split was performed and whether an additional validation set was used for hyper-parameter tuning. Determining this information is particularly challenging in cases where the source code is missing.\n- Most of the studies that propose new methodologies (e.g. improvements for DL architectures or pre/post-processing pipelines) usually rely on different datasets, thus making it difficult to compare the added value of the proposed improvements with respect to other studies and existing methods. Additionally, a detailed ablation study (i.e. a systematic analysis where components of a model are successively removed) is often missing to empirically justify some of the methodological contributions.\n- Most studies do not investigate how sensitive the proposed methods are to random initialization or the training-validation split.\n- There is a risk of inconsistent/biased results in the studies that apply the same single model on the entire ROI since this implies including the results based on the data on which the model was trained.\n\nBased on these limitations and other more general aspects, we provide a few recommendations in the next section that could be adopted in future studies. We then briefly discuss in Section 5.2 another promising area of glacier-related research where DL also starts playing a significant role, i.e. glacier mass balance and evolution modelling. We close with an outlook in Section 5.3.# used point mass balances (vs. glacier-wide mass balance in works of Bolibar et al. (2020b, 2022)) through various techniques, i.e. random forest (RF), GBR, support vector machine, and MLPs. Diaconu et al. (2024) made use of the Open Global Glacier Model (Maussion et al., 2019) to reconstruct annual MBs and use the resulting dataset to systematically study various uncertainty estimation methods for ML models (e.g. the ensemble method), analysing their impact on the quality of the predictions.# Recently, DL efforts have also been focused on modelling the ice dynamical processes within glaciers. Jouvet et al. (2022) created the Instructed Glacier Model (IGM), which employs a convolutional neural network architecture to emulate the behaviour of computationally expensive ice flow models that are based on solving Navier-Stokes (NS) equations (typically referred to as \u2019Full-Stokes\u2019 in glaciology). Harnessing its extreme gain in computational costs (about three orders of magnitude compared to original NS calculations), IGM was then used to invert for various key components in glacier evolution modelling, such as ice thickness (Cook et al., 2023; Jouvet, 2023). In a recent update, IGM was retrained to not only reproduce \u2019expensive\u2019 simulations performed with NS but also to include more physical constraints (Jouvet et al., 2023). Other noteworthy recent advances for ice flow modelling through DL include the use of universal differential equations (UDEs) to model glacier flow (Bolibar et al., 2023). In the latter work, by combining an ice flow model with differential equations with an embedded neural network, a parameter (i.e. the creep component of the ice flow) can be automatically learnt from data as a nonlinear function. This approach combines the advantages of mathematical models (e.g. interpretable, incorporates domain knowledge) with those of ML (e.g. flexible, data-driven), which can also serve as a framework for discovering new empirical laws for glacier processes.# To conclude, DL applied for glacier mass balance and evolution modelling is a young but rapidly evolving field that is likely to substantially change future models for glacier projections. Like glacier mapping through DL, this field will largely benefit from new data becoming available.# We see many promising directions in the field of automatic glacier mapping based on DL. First, we expect (and encourage) the development of large-scale benchmark datasets that will accelerate the methodological developments. Second, we recommend using time-series input data, as this can potentially mitigate the errors caused by (partial) occlusions in single images, e.g. by clouds. Alternatively, single-image model approaches can be used on multiple acquisitions, from which a (temporal) prediction ensemble can be built. Third, we suggest exploring more sophisticated data fusion techniques instead of simply concatenating the input bands from all the sources, as in most current studies. We also expect a rapid development of DL-based methods for glacier evolution modelling, briefly discussed here in Section 5.2. These developments will largely profit from the increasing number of (global) products in the field, e.g. Hugonnet et al. (2021), with many other promising datasets currently being finalized (Dussaillant et al., 2024; Zemp et al., 2024). Additionally, given the complexity of glacier-climate interactions, we advocate for developing methods that can make use of expert knowledge, e.g. by including physical constraints in the models, with the recent work by Bolibar et al. (2023) as an example.# In this work, we provided an overview of the glacier mapping literature based on DL, highlighting the methodological contributions, the regions on which each study is focused and the data modalities used for training. We divided the studies into two major sub-fields in glacier mapping, which evolved in parallel: i) glacier extent mapping and ii) calving front detection. Both types of classification problems are usually tackled using multi-source datasets, showing the benefit of using DL for automatically extracting the relevant features from each modality. We then provide a compact summary of the available resources (data and source codes) to facilitate further experimentation.",
        "context_id": 3,
        "question": "What is the minimum area of glaciers listed in the RGI 7.0?",
        "answer": [
            "0.01 km2"
        ],
        "context_length": 65565
    },
    {
        "context": "# 1. Introduction\n\nUnderstanding traffic interactions and the way they affect future vehicle trajectories is inherently complex [1]. In mixed traffic environments, where human-driven and automated vehicles coexist, this complexity is amplified, requiring precise interaction representation and behavior modeling for reliable motion prediction [2, 3, 4]. These scenarios often present dynamic interaction topologies with underlying relations, with interaction patterns as well topologies continuously evolving depending on the surrounding context as exemplified by lane change maneuvers.\n\n\u21e4Corresponding authors: Yang Zhou (yangzhou295@tamu.edu), Haotian Shi (hshi84@wisc.edu);\n\nPreprint submitted to Elsevier September 19, 2024# Vehicle Interactions and Driving Behaviors\n\nThese relationships play a crucial role in guiding each vehicle\u2019s decision-making processes. Additionally, each vehicle can display multiple possible modalities in driving intentions and behaviors, including both longitudinal (e.g., acceleration and braking) and lateral (e.g., lane-changing and lane-keeping) maneuvers. Furthermore, collective behaviors, arising from interactions within a group of vehicles and encompassing both cooperative and competitive behaviors, further complicate the understanding of these interactions. Figure 1 describes the interaction among multi-vehicles and the corresponding multi-modal driving behaviors. Therefore, it is necessary to model the interaction and multi-modality and reason the interaction relation to accurately capture interactions and forecast their future behaviors.# Figure 1: Major challenges\n\n- (a) Vehicle interaction\n- (b) Behavior multi-modality\n- (c) Interaction relational reasoning# (b) Multi-modality of driving behaviors\n\nLeft lane change\n\nLane keeping\n\nRight lane change# (c) Interaction relation between vehicles and behaviors\n\n|vehicletarget|agent|\n|---|---|\n|surrounding vehicle|behavior node|\n|interaction|interaction graph|\n|predicted trajectory|historical marking|\n|potential conflict| |\n\nE\u21b5orts have been made to address the challenges of vehicle interactions and driving behavior multi-modality. Three primary approaches have been developed: social operation methods, attention-driven methods, and graph-based techniques. Social operations use pooling mechanisms to generate socially acceptable trajectories by capturing the influence of surrounding agents. Attention-driven approaches use attention mechanisms to dynamically weigh neighboring agents\u2019 information. Graph-based methods leverage graph structures to model non-Euclidean spatial dependencies, effectively handling varying interaction topologies and predicting dynamic interactions. These complex interactions create uncertainty, complicating the accurate forecasting of a single future trajectory with high confidence due to varying driving behaviors in identical situations, driven by individual driver characteristics and psychological factors. Addressing the multi-modality of driving behaviors often involves introducing latent variables, categorized into those with explicit semantics and those without. Models with explicit semantics use latent variables to clearly represent driving intentions, identifying.# Specific Maneuvers and Behaviors for Multi-Modal Trajectory Predictions\n\nModels without explicit semantics employ generative deep learning techniques, such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), to produce diverse trajectories by adding noise to encoded features. While these models generate a wide range of possible trajectories, they often struggle with issues related to interpretability and identifying the most effective strategies for sampling from the generated trajectories.\n\nDespite the advances in modeling vehicle interactions and probabilistically forecasting multi-modal future trajectories, the inherent complexity of social dynamics in traffic systems continues to present significant challenges, with limitations arising from the complex nature of agent interactions in two key aspects:# 1. Focus on Pair-Wise Interactions\n\nPrevailing methods primarily focus on pair-wise interactions rather than group-wise interactions. In multi-vehicle systems, dynamic interactions among vehicles often exhibit cooperative and competitive behaviors, which have been rarely explored. Effectively capturing the collective influence of vehicle groups is vital for understanding complex social dynamics, especially in scenarios where vehicles engage in multi-modal behaviors such as lane changes, acceleration, and deceleration. In such contexts, vehicles need to make decisions based on the actions and intentions of several surrounding agents simultaneously, further complicating the task of accurate behavior prediction.\n\nFor example, competitive behaviors, such as overtaking and lane-changing, can lead to conflicting objectives between vehicles. In the lane changing scenario as depicted in Figure 2, vehicle 1 attempts a right lane change (R) while vehicle 3 maintains lane-keeping (K) and vehicle 4 performs a left lane change (L). Conversely, cooperative interactions, such as car-following, are illustrated by vehicles 1, 3, and 4 forming a platoon, all maintaining lane-keeping while potentially responding to vehicle 2\u2019s left lane change. These examples highlight the need for models that can effectively capture the complexities of both individual and group dynamics in multi-modal traffic scenarios.# 2. Limitations of Traditional Graph-Based Methods\n\nWhile traditional graph-based methods excel at capturing pair-wise relationships, they are limited in representing the more intricate group-wise interactions. This limitation arises because graph-based approaches model interactions between pairs of agents with fixed topologies, making it difficult to capture the simultaneous influence of multiple agents on each other\u2019s behaviors. Unlike standard graphs, where each edge connects only two nodes, hypergraphs are capable of connecting an entire group of nodes within a shared context through a single hyperedge.\n\nAs a result, a hypergraph, which is a generalization of a graph where hyperedges can connect multiple nodes to represent the higher-order relationships, offers a more accurate reflection of the group-wise interactions and allows for a more accurate representation of the collective influence of multiple agents in varying traffic conditions. This adaptability is essential for addressing the stochastic nature of human behavior, as hypergraph models can more effectively manage the uncertainty and variability inherent in the interactions resulting from collective behaviors, thereby facilitating more socially inspired automated driving.# Proposed Method\n\nTo address the aforementioned challenges, this paper presents a novel hypergraph-based method for multi-modal trajectories prediction with relational reasoning. The proposed framework contains two parts: Graph-based Interaction-aware Anticipative Feasible Future Estimator (GIRAFFE) and Relational Hypergraph Interaction-informed Neural motion generator (RHINO). GIRAFFE enables multi-agent, multi-modal motion prediction of preliminary multi-vehicle trajectories, based on while RHINO framework, which utilizes an innovative Agent-Behavior Hypergraph to capture.# Merging\n\nFigure 2: Pair-wise interaction and group-wise interaction in different scenarios.\n\nLeveraging GroupNet [41] as its backbone, RHINO learns a multi-scale hypergraph topology in a data-driven manner to model group-wise interactions. Through neural message passing across the hypergraph, this approach integrates interaction representation learning and relational reasoning, enhancing the social dynamics of automated driving. Furthermore, a Conditional Variational Autoencoder (CVAE) framework is employed to generate diverse trajectory predictions by sampling from hidden...# Key Contributions\n\nTo summarize, the key contributions of this work are as follows:\n\n1. The framework adopts multi-scale hypergraphs to represent group-wise interactions among different modalities of driving behavior and the corresponding motion states of multiple agents in a flexible manner.\n2. This framework incorporates interaction representation learning and relational reasoning to generate motions that are plausible and concurrently in a probabilistic manner depicted by the learned posterior distribution.\n\nThe remainder of this paper is structured as follows: Section 2 outlines the problem statement of this research. Section 3 introduces the methodology. Section 4 details the experimental setup and analysis of the results obtained. Section 5 concludes this paper.# 2.1. Problem Definition\n\nThe vehicle trajectory prediction in the dynamic realm of multi-vehicle interaction context of multi-lane highways involves determining the future movements of a target vehicle based on historical data and multi-modal predictions of its own state and the states of surrounding vehicles. This domain addresses two primary challenges: (i) multi-agent multi-modal trajectory prediction and (ii) prediction-guided motion generation after reasoning.\n\nThe objective of trajectory prediction is to estimate the future trajectories of the target vehicle and its surrounding vehicles, given their historical states. The historical states, spanning a time horizon [1, . . . , T], are represented as <math>X_{1:T} = \\{X_1, X_2, \\ldots, X_T\\} \\in \\mathbb{R}^{T \\times N \\times C_1}</math>, where N denotes the number of vehicles and <math>C_1</math> denotes the number of features, including longitudinal and lateral positions and velocities. Each historical state <math>X_t = \\{x_{ti} | \\forall i \\in [1, N], \\forall t \\in [1, T]\\} \\in \\mathbb{R}^{C_1}</math> at time step t captures these details for each vehicle i. Notably, the superscript refers to vehicle indices with i = 1 representing the target vehicle, and the subscript to time steps, with <math>C_1 = 4</math> for the input data.\n\n<math>RF^{\\times N \\times M \\times \\text{Pred}(\\cdot)}</math> provides preliminary predictions of multi-modal trajectory candidates <math>X_{M+1:T+F}</math>. The prediction model <math>H_{C2}</math> for all the N vehicles over the future time horizon [T + 1, . . . , T + F] with M modes of driving behaviors. This model takes historical data <math>X_{1:T}</math> as the input and outputs <math>m_i | \\forall m</math> future longitudinal and lateral positions, where <math>C_2 = 2</math>. The forecasted states <math>X_{f}^{\\hat{M}} = \\{x_f | \\forall i \\in [1, N], \\forall f \\in [1, F]\\}</math> aim to estimate each vehicle\u2019s future trajectory for each behavior mode m at time step <math>T + f</math>. This is mathematically formulated as:\n\n<math>X_{M+1:T+F}^{\\hat{T}} = H_{\\text{Pred}}(X_{1:T})</math> (1)\n\nBased on that, the motion generation model <math>H_{\\text{Gen}}(\\cdot)</math> is further developed to generate plausible trajectories considering the implicit group-wise interactions, using both historical states <math>X_{1:T}</math> and preliminary multi-modal future trajectory candidates <math>X_{M+1:T+F}</math> provides K plausible trajectory <math>Y_{K+1:T+F}</math> time steps. Each generated state <math>\\hat{T}_{K+f}, \\ldots, \\hat{T}_{K+F} \\in \\mathbb{R}^{[1, K]}, \\forall i \\in [1, N], \\forall f</math> is given as input. The generation model <math>Y^{\\hat{T}} = \\{\\hat{k}_{i+f} | \\forall k y_{T} \\in \\mathbb{R}^{[1, F]}\\} \\in \\mathbb{R}^{F \\times N \\times K \\times C_2}</math> for all the N vehicles for the next F.RC2 represents the k-th generated longitudinal and lateral positions of the i-th vehicle at time step T + f. The formulation for this generation problem is:\n\nYK +1:T +F \u02c6T= HGen(X1:T, XM +1:T +F)\u02c6T# 3. Methodology\n\nGiven the aforementioned problem, we first develop a customized framework architecture. Then, the vital components are further elaborated.# 3.1. Framework Architecture\n\nThe proposed framework adopts an integrated architecture, as shown in Figure 3, which involves two major components:\n\n- GIRAFFE: Graph-based Interaction-aware Anticipative Feasible Future Estimator, which leverages graph representations to capture pair-wise interactions during both the historical and future time horizons, providing preliminary multi-modal trajectories prediction candidates for vehicles.\n- RHINO: Relational Hypergraph Interaction-informed Neural motion generator, which utilizes multi-scale hypergraph representations to model group-wise interactions and reason the interaction relations among the multi-modal behaviors. Built upon the preliminary multi-modal trajectories by GIRAFFE, RHINO will further generate plausible future trajectories for all vehicles in a probabilistic manner.\n\nThe subsequent sections will provide an in-depth explanation of the two principal frameworks.# 3.2. GIRAFFE: Graph-based Motion Predictor\n\nIn our context, a graph representation G is adopted by modeling N vehicles as nodes V 2 N\u21e5CRN and the pair-wise interaction as the edges E 2 R|N\u21e5N|. Further, the feature matrix X 2 R containing vehicle states (i.e., longitudinal and lateral position and speed) and the adjacency matrix A 2 RN\u21e5N describing the interactions among nodes are further utilized to describe the graph. By that, we can define an Agent Graph as:# Definition 1 (Agent Graph)\n\nLet Ga be a graph representing the motion states and interaction of N agents, with each agent represented as a node. Ga is expressed as:\n\nGa = (Va, Ea; Xa, Aa)\n\nwhere Va 2 RN denotes the node set, Ea 2 R|N\u21e5N| denotes the edge set, Xa 2 RN\u21e5C represents the feature tensor, Aa 2 RN\u21e5N indicates the adjacency matrix.\n\nTo better represent the interaction and relations of the predicted multi-agent multi-modal trajectory candidates with graphs, we expand each agent node to multiple nodes of the number of behavior modes based on our previous work [25], which further renders an Agent-Behavior Graph.# Motion Generation Framework Architecture\n\n|Input:|Intermediate Output:|Output:|\n|---|---|---|\n|Historical Trajectory|Prediction Model|Preliminary Multi-modal Trajectory Generation|\n|XI:T|XY+1T+F|Yf+1t+F|\n|HPred: Xi:t + XY+1:T+F|HGen: Xit XY+1T+F|Y#+IT+F|\n\nGIRAFFE: Graph-based Interaction-aware Multi-agent Multi-modal Motion Prediction\n\nRHINO: Hypergraph-based Interaction Relational Reasoning for Motion Generation\n\nFuture\n\n|XIt|Interaction Encoder|Multi-modal Derader|\n|---|---|---|\n|Xut|Hypergraph Relational Encoder|Posterior Distribution|\n| |Motion Generator| |\n| |eatner| |\n\nHistorical Hypergraph Relational Encoder# Definition 2 (Agent-Behavior Graph)\n\nLet Gb be a graph representation of the multi-modal motion states of N agents, with each of M behavior modes for each agent represented as a node. Gb is expressed as:\n\nGb = (Vb, Eb; Xb, Ab)\n\nwhere Vb \u2208 R|MN| denotes the node set, Eb \u2208 R|MN\u00d7MN| denotes the edge set, Xb \u2208 R|MN|\u00d7C represents the feature tensor, Ab \u2208 R|MN|\u00d7|MN| indicates the adjacency matrix.\n\nThe transition from an Agent Graph Ga to an Agent-Behavior Graph Gb is by an expansion function Fexpand(\u00b7) as:# 8\n\nGb = Fexpand (Ga),\n\nXb = [[Xia,mN M (3)\n\nAb = [[nAi j \u2326 \u21e4mn N N a | 8m, n 2 {1, . . . , M}o\n\nAs shown in Figure 4, in this process, each agent node via 2 Va in the Agent Graph is expanded into M behavior-specific nodes vib 2 Vb, corresponding to the M potential behavioral modes of the vehicle. These newly generated behavior nodes, which may exhibit significant interdependencies, are interconnected through edges ei jb, forming a more complex interaction structure. Consequently, the adjacency matrix Ab is extended to accommodate the expanded node set, resulting in increased dimensionality. Here, \u21e4 is a behavior correlation matrix, and each element \u21e4mn in the matrix represents the correlation between behavior mode m of one agent and behavior mode n of another agent. The feature tensor Xb of the behavior nodes encodes the possible motion states under each behavioral mode, capturing the multi-modal nature of vehicle behavior.# Definition 2\n\nAgent-Behavior Graph\n\nAgent Graph\n\nAdjacency Matrix\n\n(Va,E\";X\" A\") R Agent-Behavior Graph Adjacency Matrix RIMNIxIMNI\n\nFigure 4: Definitions of graphs\n\nBased on that, a deep neural network is designed to capture interactions between the target vehicle and surrounding vehicles by Ga, and to represent the output, which consists of predicted multi-agent multi-modal trajectories, by Gb, as illustrated in Figure 5. Three key modules of GIRAFFE HPred(\u00b7) are introduced below:# Interaction Encoder\n\nThe Interaction Encoder utilizes a Di\u21b5usion Graph Convolution Network (DGCN) architecture to encode the dynamic graph embeddings, as described in Appendix B. The DGCN captures the bidirectional dependencies among vehicles by applying di\u21b5usion convolutions, which consider both forward and reverse processes to model the influence of surrounding.# GIRAFFE: Graph-based Interaction aware Multi-agent Multi-modal Motion Prediction\n\n|Input:|Output:|\n|---|---|\n|Historical States Xr \u20ac RNxTxz|Predicted multi-modal Trajectory RY+l:T+F RNx3xFx2|# Interaction Encoder\n\nIntention Predictor\n\n|Historical|states|Future|Hidden states|\n|---|---|---|---|\n|Historical DGCN Module| |DGCN Module| |# Intention Predictor\n\n| | | | |Predicted|intentions|\n|---|---|---|---|---|---|\n| | |1|0|3|1|# Multi-modal Decoder\n\n|Whid|Hdec|GRU|Predicted multi-modal trajectory|\n|---|---|---|---|\n|1| | |RT+1T+F|\n\nFigure 5: GIRAFFE Framework.\n\nvehicles and the target vehicle\u2019s impact on them [43, 44]. This encoder adopts a DGCNH (\u00b7) to generate graph embeddings for historical of and a DGCNF (\u00b7) to generate future embeddings, merging them into a comprehensive representation that spans the entire time window of interest.\n\nH\u02dcT = DGCNH (XT) (4)\n\nH\u02dcF = DGCNF (XT) (5)\n\nH\u02dc = [ H\u02dcT , H\u02dcF ] (6)# Intention Predictor\n\nThe Intention Predictor addresses the classification of future driving intentions, both laterally and longitudinally. Using the encoded graph representation, two MLP layers reduce the dimensions and encode the features into a latent space. The LatMLP layers with softmax activation then classify the lateral intentions over the future time horizon. These predictions help in understanding the potential maneuvers the vehicle might take, such as lane changes or speed adjustments.\n\nHIP = MLP(MLP( H))\n\nmlat = softmax(LatMLP( HIP))# Multi-modal Decoder\n\nFinally, the Multi-modal Decoder fuses the predicted intentions of multiple agents with the latent space to produce multiple future trajectory distributions for each agent. This decoder uses a trainable weight matrix to combine features from distinct historical and future time steps, emphasizing the importance of sequential motion patterns. The GRU-based decoder ensures temporal continuity in the predicted trajectories, mapping the fused features to a bivariate Gaussian distribution representing the future vehicle positions. This approach allows the model to generate probabilistic predictions for multiple agents.\n\nWhid = softmax(Wmap \u2326 M\u02c6)\n\nHdec = Whid \u00b7 H\u02dc\n\nXM +1:T +F \u02c6T= MLP(GRU(MLP(Hdec, M\u02c6)))# 3.3. RHINO: Hypergraph-based Motion Generator\n\nUnlike traditional graph representations, which are confined to pair-wise relationships, hypergraphs offer a more sophisticated and comprehensive framework for representing group-wise interactions. By connecting multiple vehicles that exhibit strong correlations through hyperedges, hypergraphs enable a more robust analysis and optimization of the complex network of interactions. The concept of an Agent Hypergraph to represent agents and their group-wise interactions is introduced as follows:# Definition 3 (Agent Hypergraph)\n\nLet Hb be a hypergraph representation of the motion states of N agents, with each agent represented as a node. The hypergraph Ha is expressed as:\n\nHa = (Va, Ua; Xa, Ha)\n\nwhere Va \u2208 RNa denotes the node set, Ua \u2208 RL denotes the edge set, Xa \u2208 RN\u00d7C represents the feature tensor, H \u2208 RN\u00d7L indicates the incidence matrix, where Hi j indicates whether node vi is part of the hyperedge uj.\n\nTo convert an Agent Graph Ga into an Agent Hypergraph Ha, we introduce a transformation function Ftransform(\u00b7). This function enables the shift from a pairwise interaction framework to a higher-order interaction model represented by the hypergraph. Formally, the transformation is expressed as:# Agent-Behavior Graph\n\n|Agent Graph|Adjacency Matrix|\n|---|---|\n|(V\",E4;X\",A\")|R|# Agent Hypergraph\n\n|Agent Hypergraph|Incidence Matrix|\n|---|---|\n|H\"|RNXL|# Agent-Behavior Hypergraph\n\n|Agent-Behavior Hypergraph|Incidence Matrix|\n|---|---|\n|(Ve| |\n\nFigure 6: Definitions of graphs and hypergraphs\n\nVa = [{vi a}N\n\nUa = [nuj\n\nHa = Ftrans form (Ga),\n\nXa = [XiaN\n\nIn this transformation, the node set Va and the feature tensor Xa remain consistent between the graph and the hypergraph representations. However, the primary modification occurs in the edge formulation. The transformation replaces the pairwise edges of the original Agent Graph with hyperedges that can connect multiple nodes simultaneously. This redefinition of edges as hyperedges within the hypergraph Ha allows for the modeling of group-wise interactions, where a single hyperedge u 2 Ua can link more than two nodes, capturing higher-order relationships among agents. The incidence matrix Ha is updated to reflect this change, where each entry Hai j# Agent-Behavior Hypergraph\n\nindicates whether agent vi participates in hyperedge u.j. As a result, Ha can represent multi-agent interactions that involve multiple agents simultaneously, providing a richer and more flexible structure for modeling the dynamics of the system.\n\nTo enhance the understanding of complex group-wise interactions in multi-agent systems, it is essential to extend the traditional Agent Hypergraph model to account for the diverse behavioral modes of each agent. This is achieved by decomposing each agent node in the Agent Hypergraph Ha into multiple behavior-specific nodes, which correspond to the different modes of behavior each agent can exhibit. The result of this decomposition is the Agent-Behavior Hypergraph, denoted as Hb.# Definition 4 (Agent-Behavior Hypergraph)\n\nLet Hb be a hypergraph representation of the multi-modal motion states of N agents, with each of M behavior modes for each agent represented as a node. The hypergraph Hb is expressed as:\n\nHb = (Vb, Ub; Xb, Hb)\n\nwhere Vb \u2208 \u211d|MN| denotes the node set, Ub \u2208 \u211dL denotes the edge set, Xb \u2208 \u211d|MN|\u00d7C represents the feature tensor, Hb \u2208 \u211d|MN|\u00d7L indicates the incidence matrix, where Hij indicates whether node vi is part of the hyperedge u.j.\n\nTo formally describe the process of transitioning from an Agent Hypergraph Ha to an Agent-Behavior Hypergraph Hb, the expansion function Fexpand(\u00b7) is applied. This function decomposes each agent node into multiple behavior-specific nodes and updates the hyperedge structure accordingly. The behavior-specific nodes correspond to the different behavior modes, while the hyperedges represent the higher-order interactions among the behavior modes of different agents.\n\nLet:\n\n|Vb|=|[[nvi,moN M|\n|---|---|---|\n|Ub|=|[[[nuj,mn | uaj LM M, 0 and \u2217mn, 0|\n|Hb|=|Fexpand(Ha)|\n|Xb|=|Xia,m|\n\nThe node set Vb expands each agent vi into multiple behavior-specific nodes vib,m, where each m represents a different behavioral mode of the agent. The hyperedges Ub are formed between the behavior-specific nodes based on the group-wise interactions present in the original hypergraph, with the correlation between behavior modes captured by \u2217mn. The feature tensor Xb captures the state of each behavior node, inheriting the feature data from the original hypergraph. The incidence matrix Hb records whether a behavior-specific node vib,m is part of a hyperedge ubj,mn.This extended framework, hyperedges can represent these aforementioned complex group-wise interactions by connecting behaviors of multiple vehicles that are influenced simultaneously by a shared context, as illustrated in Figure 6. Therefore, an Agent-Behavior Hypergraph is defined to model the multi-agent, multi-modal system for reasoning about group-wise interaction relations.\n\nIn addition to the expansion process, the transformation function Ftrans form(\u00b7) converts an Agent-Behavior Graph Gb into an Agent-Behavior Hypergraph Hb. This transformation replaces the pairwise edges of the graph with hyperedges that capture higher-order interactions between behavior modes across multiple agents. The transformation is guided by the adjacency matrix Ab of the original graph and the behavior-mode correlation matrix \u21e4mn. The transformation function Ftrans form(\u00b7) is expressed as:\n\nHb = Ftrans form(Gb)\n\nThis structure allows for the connection of behavior nodes across different agents, enabling the representation of interactions among diverse behaviors of multiple agents in a shared context. Hence, the Agent-Behavior Hypergraph not only captures the individual behavior of each agent but also models how these behaviors interact and influence one another within a multi-agent system.# 3.3.1. RHINO Framework Architecture\n\nThe core of RHINO is to learn a multi-scale Agent-Behavior Hypergraph, where nodes represent the behaviors of agents and hyperedges capture their group-wise interactions. This hypergraph is then used to learn agent and interaction embeddings to better understand the underlying interaction relations. We also incorporate a basic multi-agent trajectory generation system based on the CVAE framework to handle the stochasticity of each agent\u2019s potential behaviors and motion states, generating plausible trajectories for each vehicle.\n\nThus, as illustrated in Figure 7, RHINO comprises the following modules:\n\n- Hypergraph Relational Encoder, which transforms both the original historical states and predicted multi-agent multi-modal trajectories into hypergraphs, modeling and reasoning the underlying relation between the vehicles.\n- Posterior Distribution Learner, which captures the posterior distribution of the future trajectory given the historical states and the predicted multi-modal future motion states of all the vehicles in the vehicle group.# Historical Hypergraph Relational Encoder\n\nMotion Generator, which decodes the embeddings by concurrently reconstructing the historical states and generating the future trajectories.# 3.3.2. Hypergraph Relational Encoder\n\nWe employ two Hypergraph Relational Encoder modules: a Historical Hypergraph Relational Encoder for handling historical states and a Future Hypergraph Relational Encoder for predicted multi-agent multi-modal trajectories from GIRAFFE. For the Historical Hypergraph Relational Encoder, the input historical states XT form an Agent Hypergraph HTa. For the Future Hypergraph Relational Encoder, the predicted multi-agent multi-modal trajectories X\u02c6T +1:T +F form an Agent-Behavior Hypergraph HFb, where each agent node is expanded into three lateral behavior nodes with corresponding predicted future states. Both modules share the same structure regardless of the input hypergraph types.# Multi-scale Hypergraph Topology Inference\n\nTo comprehensively model group-wise interactions in the hypergraphs at multiple scales, we infer a multi-scale hypergraph to reflect interactions in groups with various sizes. Let H = {H(0), H(1), \u00b7 \u00b7 \u00b7 , H(S)} be a multi-scale hypergraph, and V = {v1, v2, \u00b7 \u00b7 \u00b7 , vN(s) = {u1}} be a set of nodes. As shown in Figure 8, at any scale s, H(s) = (V, U(s)) representing group-wise relations with J hyperedges. A = (V, U(s)) has a hyperedge set U, uJ larger s indicates a larger scale of agent groups, while H(0) = (V, U(0)) models the finest pair-wise agent connections. The topology of each H(s) is represented as an incidence matrix H(s).\n\nTo understand and quantify the dynamic interactions between agents within a given system, we adopt trajectory embedding to distill the motion states of agents into a compact and informative representation. To infer a multi-scale hypergraph, we construct hyperedges by grouping agents that have highly correlated trajectories, whose correlations could be measured by mapping the.# Figure 8: Hypergraph encoder.\n\nTrajectories as a high-dimensional feature vector. For the i-th agent in the system, the trajectory embedding is denoted as q. This embedding is a function of the agent\u2019s motion states, defined over a temporal window extending from time 1 to time T. The embedding function Xi, denoted as fq, which is a trainable MLP, is responsible for transforming the motion states into a vector qi where d is the dimensionality of the embedded space. Mathematically, the trajectory embedding is represented as:\n\nqi = f(Xi) (15)\n\nThe affinity between agents is represented by an affinity matrix A \u2208 \u211dN\u00d7N, which contains the pairwise relational weights between all agents. The affinity matrix is defined as:\n\nA = {Aij | \u2200i, j = 1, ..., N} (16)\n\nEach element Aij is computed as the correlation between the trajectory embeddings of the i-th and j-th agents. The correlation is the normalized dot product of the two trajectory embeddings, expressed as:\n\nAij = \\frac{q\u22a4qj}{||q||2 ||qj||2}}  (17)\n\nHere, k\u00b7k2 denotes the L2 norm. The relational weight Aij measures the strength of association between the trajectories of the i-th and j-th agents, capturing the degree to which their behaviors.# Hypergraph Neural Message Passing\n\nare correlated. This enables the assessment of interaction patterns and can uncover underlying social or physical laws governing agent dynamics.\n\nThe formulation of a hypergraph necessitates the strategic formation of hyperedges that reflect the complex interaction between the nodes in the system. At the outset, the 0-th scale hypergraph H(0) is considered, where the construction is based on pair-wise connections. Each node establishes a link with another node that has the highest affinity score with it.\n\nAs the complexity of the system is scaled up, beginning at scale s = 1, the methodology shifts towards group-wise connections. This shift is based on the intuition that agents within a particular group should display strong mutual correlations, suggesting a propensity for concerted action. To implement this, a sequence of increasing group sizes {J(s)}s=1S is established. For every node, denoted by v, the objective is to discern a group of agents that are highly correlated, ultimately leading to s groups or hyperedges at each scale s. The hyperedge associated with a node vi at a given scale s is indicated by ui(s). The determination of the most correlated agents is framed as an optimization problem, aiming to link these agents into a hyperedge that accounts for group dynamics:\n\nui(s) = arg maxA,\n\n1\n\ns.t. ||A|| = J(s); vi \u2208 A; i = 1, . . . , N\n\nThe culmination of this hierarchical structuring is a multi-scale hypergraph, encapsulated by the set {H(s) \u2208 RN\u00d7N}s=1S, where each scale s embodies a distinct layer of abstraction in the representation of node relationships within the hypergraph.# Hypergraph Neural Message Passing\n\nIn order to discern the patterns of agent motion states from the inferred multi-scale hypergraph, we have tailored a multi-scale hypergraph neural message passing technique to construct the hypergraph topology. This method iteratively acquires the embeddings of vehicles and the corresponding interactions through node-to-hyperedge and hyperedge-to-node processes, as depicted in Figure 9.\n\nThe node-to-hyperedge mapping aggregates agent embeddings to generate interaction embeddings. Initially, for any given scale, the initial embedding for the ith agent, vi = qi \u2208 Rd. Each node vj is associated with a hyperedge ui, given that vj is an element of ui. This mapping facilitates the definition of the hyperedge interaction embedding. The hyperedge interaction embedding for a hyperedge ui is defined as a function of the embeddings of the nodes contained within it, modulated by the neural interaction strength ri and categorized through coefficients ci,l. The per-category function Fl models the interaction process for each category, which is crucial for capturing the nuances of different interaction types. Each Fl is a trainable MLP, responsible for processing the aggregated node embeddings within the context of a specific interaction category. The mathematical formulation is:\n\nui = ri \u2211l=1L \u2211j Bvj \u2208 ui ci,l Fl(Bvj)# Streneth\n\n|J(2)|FC:|\n|---|---|\n|Scale 2|ARent|\n\nFigure 9: Hypergraph encoder.\n\nThe neural interaction strength ri encapsulates the intensity of the interaction within the hyperedge and is obtained through a trainable model F, applied to a collective embedding zi with a sigmoid function as Eq.(21). This collective embedding zi is represented as the weighted sum of the individual node embeddings within the hyperedge, signifying the aggregated information of agents in a group as Eq. (22). The weight wj for each node is determined by a trainable MLP Fw as Eq. (23).\n\nri = (F(z))Xwjivjr (21)\n\nzi = \u03a3(v \u2208 uij) 0 B Bvj,X 1 C (22)\n\nwj = Fw(B B vm2uivm) (23)\n\nThe neural interaction category coefficient ci represents the model\u2019s reasoning about which type of the interaction is likely for hyperedge u, where j ci,l denotes the probability of the l-th neural interaction category within L possible categories. These coefficients are computed using a softmax function applied to the output of another trainable MLP F, which is further adjusted by an i.i.d. sample Gumbel distribution g as described in Appendix C, which adds some random noise and a temperature parameter \u03c4 which controls the smoothness of the probability distribution [45]:# 3.3.3. Posterior Distribution Learner\n\nIn our study, we incorporated multi-scale hypergraph embeddings into a multi-agent trajectory generation system using the CVAE framework [46] to address the stochastic nature of each agent\u2019s behavior, as shown in Figure 10. Here, we denote the historical trajectories as XT and denote the predicted future trajectories XT +1:T +F as XF. Let log p(XF |XT) denote the log-likelihood of predicted future trajectories XF given historical trajectories XT. The corresponding Evidence Lower Bound (ELBO) is defined as follows:\n\nX1:T as XT, and\n\nF(z) + g!ci\n\nci = softmax\n\nThese components, including neural interaction strength, interaction category coefficients, and per-category functions, provide a comprehensive mechanism for reasoning over complex, higher-order relationships, allowing the model to adapt its understanding of how agents collectively behave in diverse scenarios.\n\nThe process of hyperedge-to-node mapping is a pivotal step that allows for the update and refinement of agent embeddings within the hypergraph framework. Each hyperedge uj is mapped back onto its constituent nodes v, assuming every vi is included in uj. The primary objective of this phase is to update the embedding of an agent. This is achieved through the function F, which is a trainable MLP. The updated agent embedding \u02dci v is the result of the function applied to the concatenation of the agent\u2019s current embedding and the sum of the embeddings of all hyperedges that the agent is part of. Formally, the update rule for the agent embedding is represented as:\n\n\u02dci v = F(v, B6B6v, B6, B6, B6, B6, B6, @4 i u 2Uiuj)\n\nwhere Ui = {uj | vi 2 u} j denotes the set of hyperedges associated with the i-th node v, and [ \u00b7, \u00b7 ] symbolize the operation of embedding concatenation. This operation fuses the individual node embedding with the collective information conveyed by the associated hyperedges. This amalgamation is crucial as it encapsulates the influence exerted by the interactions within the hyperedges onto the individual agent.\n\nThe Hypergraph Relational Encoder applies the node-to-hyperedge and hyperedge-to-node phases iteratively, allowing agent embeddings to be refined and enriched as relationships within hyperedges evolve. Upon the completion of these iterations, the output is constructed as the concatenation of the agent embeddings across all scales. The final agent embedding matrix V\u02dca is composed of the embeddings of all agents, where each agent embedding vi is a concatenation of the embeddings from all scales, expressed as:\n\nVa \u02dc = [\u02dci v(0), \u02dci (1), . . . , \u02dci (S)]\n\nwhere v\u02dci = [\u02dci v, v]# Figure 10: Posterior Distribution Learner.\n\nlog p(XF | XT) = Eq(Z | XF, XT) log p(XF | Z, XT)\nKL(q(Z | XF, XT) || p(Z | XT)),\n\nwhere Z \u2208 \u211dN\u00d7dz represents the latent codes corresponding to all agents;\np(Z | XT) is the conditional prior of Z, modeled as a Gaussian distribution.\nKL represents the Kullback\u2013Leibler divergence function. In this framework,\nq(Z | XF, XT) is implemented through an encoding process for embedding learning,\nand p(XF | Z, XT) is realized via a decoding process that forecasts the future trajectories XF.\n\nThus, the goal of the Posterior Distribution Learner is to derive the Gaussian parameters for the approximate posterior distribution.\nThis involves computing the mean \u03bcq and the variance q based on the final output embeddings Va and the target embeddings VT.\nThese parameters are generated through two separate trainable MLPs, F\u03bc and F, respectively.\nThe latent code Z, representing possible trajectories, is then sampled from a Gaussian distribution parameterized by these means and variances.\nThe final output embeddings Vp are a concatenation of the latent code Z, the final output embeddings Va, and the target embeddings VT.\n\nThe equations governing these processes are as follows:\n\n\u03bcq = F\u03bc(F Va, VT a)\n\nq = F(F Va, VT a)\n\nZ \u223c N(\u03bcq, Diag(q2))\n\nVp = [Z, Va]T\n\nIn these notations, \u03bcq and q represent the mean and variance of the approximated posterior distribution.\nF\u03bc and F are the trainable MLPs that produce these parameters.\nZ denotes the latent code of possible trajectories, and Vp stands for the output embeddings, which fuses the latent code and the historical embeddings.\n\n19# 3.3.4. Motion Generator\n\nThe Motion Generator\u2019s objective is dual: to predict future trajectories and to reconstruct past\ntrajectories from the given embeddings. The decoder accomplishes this by applying successive\nprocessing blocks, each contributing a residual that refines the trajectory estimates, as shown in\nFigure 11. The first processing block, FRes1, takes the output embeddings V and the target past\ntrajectory XT to generate initial estimates of the future and reconstructed past trajectories X\u02c6F,1 and\nX\u02c6T,1 respectively.\n\nX\u02c6F,1, X\u02c6T,1 = FRes1(\u02dcp, XT ) V (33)\n\nSubsequently, the second block, FRes2, refines these estimates by considering the output embeddings and the residual of the past trajectory, which is the difference between the target past trajectory and the initial reconstructed past trajectory XT - X\u02c6T,1. This results in the second set of\nresiduals X\u02c6F,2 and X\u02c6T,2:\n\nX\u02c6F,2, X\u02c6T,2 = XRes2(\u02dcp, XT - X\u02c6T,1) V (34)\n\nBoth FRes1 and FRes2 are composed of a GRU encoder for sequence encoding and two MLPs\nserving as the output header. The final predicted future trajectory Y\u02c6F and the reconstructed past\ntrajectory X\u02c6T are obtained by summing the respective residuals from both processing blocks:\n\nY\u02c6F = X\u02c6F,1 + X\u02c6F,2 (35)\n\nX\u02c6T = X\u02c6T,1 + X\u02c6T,2 (36)\n\nThis approach enables the model to iteratively refine its predictions and reconstructions, leveraging the capability of deep learning models to capture complex patterns in the data through a series of non-linear transformations.# 4.1. Data Preparations\n\nThis research leverages two open-source datasets for the purpose of model training and validation: the Next Generation Simulation (NGSIM) dataset [47],[48] and the HighD dataset [49]. The NGSIM dataset provides a comprehensive collection of vehicle trajectory data, capturing activity from the eastbound I-80 in the San Francisco Bay area and the southbound US 101 in Los Angeles. This dataset encapsulates real-world highway scenarios through overhead camera recordings at a sampling rate of 10Hz. The HighD dataset originates from aerial drone recordings executed at a 25 Hz frequency between 2017 and 2018 in the vicinity of Cologne, Germany. Spanning approximately 420 meters of bidirectional roadways, it records the movements of approximately 110,000 vehicles, encompassing both cars and trucks, traversing an aggregate distance of 45,000 km. After data pre-processing, the NGSIM dataset encompasses 662 thousand rows of data, capturing 1,380 individual trajectories, while the HighD dataset comprises 1.09 million data entries, including 3,913 individual trajectories. For the purpose of training and evaluation of the model, the partition of the data allocates 70% to the training set and 30% to the test set. For the temporal parameters of the model, we adopt T = 30 frames to represent the historical horizon and F = 50 frames to signify the prediction horizon.# 4.2. Training and Evaluation Metrics\n\nTraining loss of GIRAFFE. The training loss function for GIRAFFE is a summation of three terms:\n\nLPRED = Lpred + Lint + Lfut (37)\n\nThe first component, Lpred, is the mean squared error (MSE) between the fused predicted trajectory and the ground truth future trajectory:\n\nLpred = k Y\u0302F - YF k22 (38)\n\nThe second component Lint represents the negative log-likelihood (NLL) of the predicted driving intentions, treating it as a classification task:\n\nLint = NLL( M\u0302; M) = \u03a3m\u2208M log P( \u02c6|XT)m (39)\n\nThe third component Lfut is the loss associated with the inference of the future-guided graph feature matrix, which is an intermediate output:\n\nLfut = k H\u0302F - HF k22 (40)# Training loss of RHINO\n\nThe training loss function for RHINO is also a summation of three components:\n\nLGEN = Lelbo + Lrecon + Lvar (41)\n\nThe first component, Lelbo, corresponds to the ELBO loss [46] commonly used in variational autoencoders. It consists of a reconstruction loss term and a regularization term based on the Kullback-Leibler divergence between the learned distribution and a prior distribution:\n\nLelbo = &alpha; k Y\u02c6F - YF k2 + KL&theta;N (\u03bcq, Diag(q22) N (0, I)) (42)\n\nThe second component, Lrecon, represents the Historical Trajectory Reconstruction loss, which measures how accurately the reconstructed historical trajectories match the true historical data:\n\nLrecon = k X\u02c6T - XT k22 (43)\n\nThe final component, Lvar, is the Variety loss, inspired by Social-GAN [21]. This loss encourages diversity in the predicted future trajectories by minimizing the error across multiple sampled future trajectories:\n\nLvar = mink k Y(k) - YF k2k  Y\u02c6F (44)# Table 1: Hyperparameter Settings\n\n|Parameter|Value|Parameter|Value|\n|---|---|---|---|\n|T|30|decaying factor|0.6|\n|F|50|&alpha;|1|\n|neuron # of MLPs|128| |0.8|\n|learning rate|0.001| |0.5|# Evaluation metrics\n\nTo ascertain the predictive accuracy of the model, we employ the Root Mean Square Error (RMSE) as the evaluative criterion. This metric quantitatively measures the deviation between the predicted position, expressed as (Ylf,lat, Ylf,lon), and the ground truth position, indicated by (Ylf,lat, Ylf,lon) for all time steps within the predictive horizon [T + 1, T + F].\n\nRMS E = 1 / LF &sum;l=1L &sum;f=T + 1T + F ( (Y\u02c6lf,lat - Ylf,lat)2 + (Y\u02c6lf,lon - Ylf,lon)2 ) (45)\n\nwhere the superscript l denotes the l-th test sample from the aggregate test sample set with length L.# 4.3. Results of Trajectory Generation\n\nThe experimental results for trajectory generation of the K trajectories using the HighD dataset are presented in Figure 12. As can be found that, RHINO demonstrates strong generative capabilities, effectively producing plausible motion in a dynamic interactive traffic environment. To provide a more quantitative analysis, trajectory generation inaccuracies are illustrated in Figure 13. The generated longitudinal and lateral trajectories, along with the error box plots and heatmaps, are displayed. The box plot reveals that errors in both axes increase with the prediction time step by the nature of error propagation. However, the errors remain within an acceptable range, indicating decent model performance, which demonstrates high precision in trajectory generation. Notably, the model maintains a lower error margin for shorter prediction horizons, which is critical for short-term planning and reactive maneuvers in dynamic traffic environments.\n\n|Longitudinal Position (m)|Longitudinal Position (m)|Longitudinal Position (m)|Longitudinal Position (m)|\n|---|---|\n|0|E|0|]|\n|1|]|5| |\n|0|Longitudinal Position (m)|0| |\n|Longitudinal Position (m)|Historical Trajectory|Generated Trajectory|Historical Trajectory|\n|Ground Truth - Sun|Predicted Trajectory - Sum|Ground Truth - Tar|Best Prediction|\n\nFigure 12: Trajectory generation results in highway scenarios.\n\nThe experiment focused on predicting vehicle trajectories within mixed traffic environments on highways by employing hypergraph inference to model group-based interactions. Through the application of hypergraph models at varying scales s = 2, 3, 5, the experiment captured the evolution of multi-vehicle interactions across both historical and future horizons. The figures depict these dynamics through three distinct columns: the first column presents vehicle trajectories.# Figure 13: Longitudinal and lateral trajectory generation error analysis.\n\n|Longitudinal Position (m)|Time (frame)|Time (frame)|\n|---|---|---|\n|0|0|4.5|\n|1|1|4|\n\nand the corresponding hyperedges, visualized as polygons that encapsulate groups of interacting vehicles. The second column illustrates the affinity matrix, where both rows and columns represent vehicles, and the strength of their relationships is indicated by the matrix values. The third column shows the incidence matrix, detailing the relationship between nodes and hyperedges, with each column representing a hyperedge and each vehicle\u2019s involvement in that hyperedge marked by a 1 in the corresponding row.\n\nThe hypergraph-based approach is particularly effective in modeling complex, higher-order interactions that are beyond the scope of traditional pairwise models. By forming hyperedges that encompass multiple vehicles, the model captures the collective influence that a group\u2019s behavior exerts on an individual vehicle. For instance, in the scenario shown in Figure 14, at scale s = 5, when the target vehicle TAR initiates a lane change, the hypergraph reflects the interaction not only with a single neighboring vehicle but also with multiple surrounding vehicles, such as following vehicle F, preceding vehicle P, and preceding vehicle RP in the right lane. More examples are illustrated in Appendix A. This capability to model group-wise interactions across different scales is 24.# Historical States Affinity Matrix\n\n|540|1250|1|\n|---|---|---|\n|iuU| | |\n|Tofol| | |\n|Longitudinal Position (m)|240|75.0-|\n|1| | |\n|Mn -| | |\n|Longitudinal Pain (m)|250 -|3|\n|Longitudinal Facitan (r)|Hvpenuze| |\n|Future States|540| |\n|Interaction Hypergraph Inference|Affinity Matrix|Incidence Matrix|\n|1| |E|\n|#50,|ee|~up|\n|IAKT URI|No|4|\n|~inn 76.0-4.0-100.0 20 40 6.0 8.0| |eapa|\n|Lateral Position (m)| | |\n|Historical surrounding| | |\n|Ground Truth Founding|1| |\n|Fnuicedcuruunding| | |\n|Historical Target| | |\n|Ground Truth Target|URI F UFRF|TAE|\n|4e|Node| |\n\nFigure 14: Trajectory generation with hypergraph inference.\n\nEssential for accurately predicting vehicle trajectories in congested highway environments, where the actions of one vehicle can trigger ripple effects that influence an entire group. The hypergraph\u2019s dynamic formation of hyperedges ensures that predicted trajectories remain adaptable and responsive to broader traffic conditions.# 4.4. Comparisons and Ablation Study\n\nTo evaluate the privileges of our proposed method, the state of art methods (i.e., Social-LSTM (S-LSTM) [20], Convolutional Social-LSTM (CS-LSTM) [31], Planning-informed prediction (PiP) [50], Graph-based Interaction-aware Trajectory Prediction (GRIP) [26], Spatial-temporal dynamic attention network (STDAN) [22]) are compared.\n\nThe compared results presented in Table 2 and Figure 15. As can be found that, the proposed framework demonstrates good performance with respect to the RMSE across a prediction horizon of 50 frames when compared with existing baseline models. It exhibits a reduced loss in comparison to C-LSTM, CS-LSTM, PiP, and GRIP. These outcomes suggest that the proposed model.\n\n25# Effectively captures salient features pertinent to long-term predictions.\n\nIn summary, the proposed framework outperforms baseline models on the HighD dataset and delivers commendable performance on the NGSIM dataset.\n\nSince the RHINO adopts the GIRAFFE, we further compare the trajectory generation capability of RHINO with our previous work [25] and its enhanced version GIRAFFE. Both RHINO model and the enhanced GIRAFFE model consistently outperform the baseline models, demonstrating superior performance in various metrics. This suggests that our proposed approaches effectively address the limitations present in prevailing models by robustly capturing complex interactions.# Table 2: Prediction Error Obtained by Different Models in RMSE (m)\n\n|Dataset|Horizon (Frame)|S-LSTM|CS-LSTM|PiP|GRIP|STDAN|GIRAFFE|RHINO|\n|---|---|---|---|---|---|---|---|---|\n|NGSIM|10|0.65|0.61|0.55|0.37|0.42|0.38|0.32|\n| |20|1.31|1.27|1.18|0.86|1.01|0.89|0.78|\n| |30|2.16|2.08|1.94|1.45|1.69|1.45|1.34|\n| |40|3.25|3.10|2.88|2.21|2.56|2.46|2.17|\n| |50|4.55|4.37|4.04|3.16|3.67|3.24|2.97|\n|HighD|10|0.22|0.22|0.17|0.29|0.19|0.19|0.19|\n| |20|0.62|0.61|0.52|0.68|0.27|0.42|0.26|\n| |30|1.27|1.24|1.05|1.17|0.48|0.81|0.42|\n| |40|2.15|2.10|1.76|1.88|0.91|1.13|0.65|\n| |50|3.41|3.27|2.63|2.76|1.66|1.56|0.89|# Figure 15: Prediction error obtained by different models in RMSE on NGSIM dataset (left) and HighD dataset (right).\n\nAblation study is conducted to provide more insights into the performance of our RHINO model, especially the impact of different components on the prediction performance by disabling the.# Table 3: Ablation Test Results of RHINO in RMSE (m)\n\n|Horizon (Frame)|RHINO w/o HG|RHINO w/o MM|RHINO w/o PDL|RHINO|\n|---|---|---|---|---|\n|10|0.21|0.22|0.24|0.19|\n|20|0.31|0.37|0.42|0.26|\n|30|0.68|0.73|0.80|0.42|\n|40|0.97|1.06|1.18|0.65|\n|50|1.25|1.34|1.57|0.89|# Ablation Test Results in RMSE\n\n1.5 RHINO w/o HG\n\nRHINO w/o MM\n\nRHINO w/o PDL\n\nRHINO\n\nE 10 _\n\nd 0.5\n\n0.0 10 Future Time Horizon (Frame) 5020 30 40# Figure 16: Ablation Study of RHINO.\n\ncorresponding component from the entire RHINO. In particular, we consider the following four variants:\n\n- RHINO w/o HG (hypergraph) variant does not use the multi-scale hypergraphs representation but only adopts the pair-wise connected graph representations in the Hypergraph Relational Encoder.\n- RHINO w/o MM (multi-modal) variant does not adopt the multi-agent multi-modal trajectory prediction results and only use the single predicted future states for each agent as the input of the RHINO.\n- RHINO w/o PDL (posterior distribution learner) variant skips the Posterior Distribution Learner and directly input the graph embedding into the Motion Generator.\n\nAn investigation into the effects of model design variations, as presented in Table 3 and Figure 16. The removal of various components from RHINO invariably leads to performance degradation to varying degrees. Compared to the full RHINO, omitting the multi-scale hypergraphs results in\n\n27# 5. Conclusions\n\nIn this study, we proposed a hypergraph enabled multi-modal probabilistic motion prediction framework with reasonings. This framework consists of two main components: GIRAFFE and RHINO. GIRAFFE focuses on predicting the interactive vehicular trajectories considering modalities. Based on that, RHINO, leveraging the flexibility and strengths on modeling the group-wise interactions, facilitate relational reasoning among vehicles and multi-modalities to render plausible vehicles trajectories. The framework extends traditional interaction models by introducing an agent-behavior hypergraph. This approach better aligns with traffic physics while being grounded in the mathematical rigor of hypergraph theory. Further, the approach employs representation learning to enable explicit interaction relational reasoning. This involves considering future relations and interactions and learning the posterior distribution to handle the stochasticity of behavior for each vehicle. As a result, the framework excels in capturing high-dimensional, group-wise interactions across various behavioral modalities.\n\nThe framework is tested using the NGSIM and HighD datasets. The results show that the proposed framework effectively models the interactions among groups of vehicles and their corresponding multi-modal behaviors. Comparative studies demonstrate that the framework outperforms prevailing algorithms in prediction accuracy. To further validate the effectiveness of each component, ablation studies were conducted, revealing that the full model performs best.\n\nSeveral potential extensions of the framework include incorporating road geometries, vehicle types, and real-time weather data to improve trajectory prediction. By integrating weather information from sources like the OpenWeather API, the system could adjust predictions based on conditions such as temperature, wind, and precipitation, enhancing safety and route optimization [51]. Additional enhancements, like traffic signal integration, V2V and V2I communication, and human driver intent, could further improve accuracy and reliability in dynamic urban environments, minimizing disruptions and fostering safer, more informed autonomous driving.",
        "context_id": 4,
        "question": "What figure describes the interaction among multi-vehicles and the corresponding multi-modal driving behaviors in the Introduction?",
        "answer": [
            "Figure 1"
        ],
        "context_length": 53351
    },
    {
        "context": "# I. INTRODUCTION\n\nAdvances in quantum information science (QIS) have provided new perspectives on quantum many-body systems. Investigations of entanglement have shed further light onto the structure and dynamics of matter (in nuclear physics, see e.g. [1\u201338]), and, in turn, have been guiding the development of improved methods for describing quantum many-body systems based on entanglement optimization and/or truncation. One example is tensor network techniques which can describe classes of weakly-entangled states efficiently with classical computers. These includes, for example, the density matrix renormalization group (DMRG) [39], matrix product states (MPS) [40\u201343], projected entangled pair states (PEPS) [44] and the multi-scale entanglement renormalization ansatz (MERA) [45, 46]. DMRG and variants thereof have been adapted to various kinds of nuclear systems for which a natural weakly-entangled bipartition can be established [12, 18, 47\u201358]. Tensor-networks states, and other methods related to entanglement re-organization have also been shown to be beneficial in the context of quantum simulations with noisy intermediate-scale quantum (NISQ) computers [59\u201373].\n\nEntanglement, however, is not the only necessary ingredient to establish whether a quantum state can be efficiently simulated classically, or necessitates the use of a quantum computer. The degree of non-stabilizerness, or \"magic\", of a quantum state is also required to establish such distinction. Magic quantifies the deviation of a state from stabilizer states, which are a class of quantum states that can be prepared using Clifford operations alone. Stabilizer states can be highly entangled, and yet, as encapsulated by the Gottesman-Knill theorem, can be efficiently prepared and simulated with a classical computer [74]. As such, both entanglement and magic are required to assess the computational complexity and quantum resource requirements for simulating physical systems, and to design an optimal classical/quantum workflow of the simulations. The exponentially-scaling classical resources required to prepare a given quantum state are directly related to the number of T-gates required to prepare the state on a digital quantum computer, and hence related to the non-zero magic of the state. As nuclei, described by non-relativistic nucleons, involve a finite number of particles, the concept of asymptotic scaling with systems size, as is used to define complexity classes of problems, is replaced by the scaling of resources required to achieve a given level of precision in target observables, e.g., Ref. [75]. Indirectly, this includes the scaling of the active space and the fidelity of the Hamiltonian, when compared with that emerging from quantum chromodynamics.\n\nAs with entanglement, measures to quantify the magic in a quantum state have been developed, such as, for example, the minimum relative entropy of magic [76] quantifying the minimum distance between a quantum state and the nearest stabilizer state. Other measures such as, for example, the mana [76] and thauma [77], the robustness of magic [78], stabilizer extent [79], stabilizer norm [80] and stabilizer nullity [81], are related to the minimum number of stabilizer states required to expand.\n\n1 A number of properties of nuclei can be comfortably computed with a certain level of precision using classical computing resources alone. However, systematically reducing uncertainties in these properties will become increasingly demanding, and eventually beyond the capabilities of classical computing.# 2\n\nThe quantum state of interest (stabilizer rank), and give an estimate of the computational complexity of Clifford simulations of a quantum circuit. More easily accessible measures of magic have been introduced, in particular, the stabilizer R\u00e9nyi entropies (SREs) [82] and the Bell magic [83], which have been shown to be measurable in quantum computing experiments [83\u201385], and efficiently calculable for MPS [86\u201389].\n\nWhile the entanglement structures of various types of physical systems have been extensively examined, the magic properties of quantum many-body systems are much less known. Investigations of magic in the Ising and Heisenberg models [84, 87, 90\u201392], in two-dimensional lattice gauge theories [93], and in potential simulations of quantum gravity [94], have been performed as examples.\n\nTowards building an understanding of the quantum resources required to simulate nuclear systems, we have recently investigated the magic-power of the nucleon-nucleon and nucleon-hyperon S-matrices. We found that magic and entanglement do not always have the same behaviour in these scattering processes. In particular, certain scattering states were found to exhibit large entanglement and low or zero magic in specific energy regions. Such differences in the behaviours of entanglement and magic have also been investigated in different contexts. For example, Ref. [91] showed that, in the one-dimensional Heisenberg model described by MPS, magic typically saturates faster than entanglement with respect to increase of the bound dimension. In random quantum circuits including measurements, it has been demonstrated that scaling of entanglement and magic with subsystem size undergo phase transitions at different sampling densities, p [95, 96]. Further, it has been demonstrated that the quantum phase transition in the XYZ Heisenberg chain can be signaled by measures of magic, while being undetected by entanglement measures [92].\n\nIn this work, as another step towards a more general characterization of the quantum complexity of nuclear systems, we investigate and compare magic and entanglement patterns in the ground states of nuclei. Specifically, we calculate magic and entanglement measures in light (p-shell) and mid-mass (sd-shell) nuclei, described within the framework of the well-established interacting nuclear shell model [97]. We compute SREs to measure the magic content of the wave functions, and the n-tangles to estimate detailed multipartite entanglement. Such n-tangles are well-suited measures to characterize shell-model wave functions which include refined multi-nucleon correlations in restricted model spaces. While the n-tangles can be calculated exactly for the present model-space sizes, magic, which involves the evaluation of an exponentially-large number of Pauli strings, can only be calculated in an approximate way in typical sd-shell nuclei. Motivated by the work in Ref. [93], which employed Markov Chain Monte Carlo (MCMC) techniques to sample the Pauli strings and calculate the SREs in 2D lattice gauge theories, we have performed an extensive and detailed suite of MCMC evaluations of Pauli strings for each nucleus, and have developed a new MCMC algorithm, which we call PSIZe-MCMC, to accelerate sampling Pauli string expectation values within wave function exhibiting collectivity, such as many sd-shell nuclei.\n\nNuclei exhibit an impressive range of shape deformations. Even light nuclei can be significantly deformed in their ground and excited states, induced by the two-nucleon tensor force and higher. This is particularly pronounced in sd-shell nuclei, peaking in the vicinity of Mg. While the deformation parameters become smaller with increasing neutron number for a given proton number Z, the systems may enter a region of shape-co-existence on their way to instability. An example of this behavior can be found in the Mg isotopes, where 24 Mg is maximally deformed in the isotopic chain, and with \u03c9 decreasing with increasing neutron number. 28 Mg is suspected to be in the region of shape co-existence on the road from normal (shell model) state ordering to inverted orderings around 32 Mg (see, for example, Ref. [98]), brought about by a closing gap to the f p-shell. This evolution is expected to arise from significant multi-particle correlations, both classical and quantum, in the nuclear wavefunctions. We take steps toward systematically quantifying the quantum correlations in these nuclei.\n\nThe manuscript is organized as follows: In section II we provide a brief reminder of the shell-model description of nuclear wave functions, and in section III we describe the mapping of these wave functions to qubits, which will serve for the subsequent calculations of entanglement and magic measures. In section IV A we describe and present calculations of multipartite entanglement in p- and sd-shell nuclei via computations of n-tangles. In section V we briefly review the stabilizer formalism leading to the definition of SREs as a measure of magic, and present results for p- and sd-shell nuclei obtained via exact or MCMC computations using the newly-introduced PZIe-MCMC algorithm. In section VI we examine, in more detail, the behaviour of entanglement and magic in comparison with deformation of nuclei of the Ne and Mg chains. Finally, conclusions and outlook are presented in section VII.# II. RELEVANT ASPECTS OF THE SPHERICAL SHELL-MODEL DESCRIPTION OF NUCLEI\n\nThe interacting shell model [97] is an active-space configuration-interaction method that has been successfully used for decades in the description of the structure of nuclei. Based on the large energy gaps between major single-particle (harmonic oscillator) shells, this framework assumes that only nucleons around the Fermi level are active and interact within a valence shell, while nucleons below a major shell gap can be considered frozen. The single-particle space is thus divided into three parts: the inert fully-occupied core of orbitals, the active partially-filled valence space, and the space of re-# 5\n\nprojections). Also due to particle-number conservation, n-tangles with odd values of n vanish. Thus, the 4-tangle, which represents a two-body operator, is the lowest-order tangle capturing many-body entanglement.# FIG. 4.\n\nDistributions of Pauli-string expectation values \u2191\u02c6 y \u03b5(n)\u2193 \u2194 \u2191!|\u02c6 y \u03b5(i 1 )\u2197 ... \u2197 \u02c6 y \u03b5(i n )|!\u2193 with |\u2191\u02c6 y \u03b5(n)\u2193| \u2198 10 \u21914 for n = 2, 4, 6, 8 in Be isotopes. The pure proton (\u03d1), pure neutron (\u03d6) and mixed proton-neutron (\u03d1\u03d6) Pauli strings are shown separately. Bin widths of 0.05 were used.\n\npeaked around zero. In 8 Be, we note a large contribution of proton-neutron 8-tangles, which is related to 4-body proton-neutron entanglement, and thus could signal the two-\u03d1 structure displayed by this nucleus. We note, however, that n-tangles with n \u21d0 4 in general do not provide a fully irreducible measure of multipartite entanglement, and thus may encompass contributions from lower-body tangles [138]. In 8 Be we observe that 2-tangles vanish and thus the 4-tangles capture genuine 4-orbital entanglement. The 8-tangles, however, may contain contributions from products of 4-tangles. It is clear that the increase in proton entanglement with growing N that was seen in Fig. 3 is due to a few contributions of large magnitudes. On the other hand, the distribution of proton-neutron entanglement is drastically different across the chain, as it displays a large number of small contributions, thus presenting a more collective behaviour.# FIG. 6. Values of the proton-neutron, pure proton, and pure \u03c9(n),# as defined in Eq. (9).\n\nscribed above. It is seen that around N = Z, the entanglement is largely shared between proton and neutron orbitals and collectively distributed among many components, while same-isospin entanglement dominates in nuclei away from N = Z, and is characterised by fewer but larger components.\n\nThe 8-tangles present a rather different behaviour, as we observe a strong proton-neutron component around N = Z, which may again signal \u03d1-particle correlations. Interestingly, such large proton-neutron 8-tangle component persists in even-even isotopes all the way to 28 Ne, a nucleus at the boundary of the island of inversion, predicted to exhibit possible shape coexistence [141\u2013143]. In comparison, pure proton and pure neutron components are almost negligible along the chain.# FIG. 5. Network representation of the 4-tangles in 8\u219112 Be.\n\nThe nodes represent the single-particle orbitals and are labeled as in Fig. 1. The values of the edges, representing the entanglement, are determined as in Eq. (10) and are indicated by both the darkness and the thickness of the lines. Orbitals that are shown closer together are also more entangled. The plots have been generated with NetworkX [139].\n\nInterestingly, the summation of proton-neutron 4-tangles, \u03f1(4), appears to be rather homogeneous and weaker compared to the higher n-tangles, which present more distinct structures. The 6-tangles, \u03f1(6), related to \u03b5\u03d1 3-body entanglement, present large contributions in a few odd-mass nuclei, while the 8-tangles, \u03f1(8), largely dominate in a region around even-even nuclei around the center of the shell.\n\nAs an example, we examine the Ne chain in more detail. The 4-tangles for these isotopes present a similar behaviour to those in the Be chain: the two-body proton-neutron entanglement is strongest around N = Z, while the pure proton component dominates in isotopes with neutron excess. This is again understood as adding neutrons redistributes the protons over the orbitals, via proton-neutron interaction. The details of these 4-tangles can be seen in Fig. 7, in the same network form as described.\n\nIn 20 Ne, the entanglement is mainly distributed among the d5/2 and s1/2 proton and neutron orbitals. As neutrons are added to the system, the neutron d3/2 becomes more occupied and entangled with the rest. In 28 Ne, the proton-neutron 8-tangles are largely contained within the proton d5/2 and neutron d3/2. Neon isotopes with odd neutron numbers appear to have low proton-neutron 8-orbital entanglement.# 9\n\nwavefunctions. Starting from a general expansion of the density matrix of an arbitrary state |!\u2192:\n\n\u21bc\u02c6 = |!\u2192 \u2197!| = dP \u2191 G nQ\u2197!| P1 \u02c6 |!\u2192 P \u02c6 = 1 c P P ,\u02c6\n\n\u2193 \u2197!| P\u02c6 |!\u2192 and \u02dc n Q where c P G is the subgroup of G n Q in Eq. (12) with phases \u03c6 = +1, the authors of Ref. [82] showed that the quantity# FIG. 9.\n\nThe chart of the measures of magic of the p-shell and sd-shell nuclei computed from their active-space nuclear shell-model wavefunctions calculated using the BIGSTICK code. The solid-gray lines denote the limits of stability, while the dashed-gray lines denote the closed shells in the spherical nuclear shell model. The numerical values used to generate these figures can be found in Table VIII- Table XIV in App. D. While it is the experimentally determined dripline that is displayed [149], our results are obtained from an isospin-symmetric nuclear interaction without Coulomb, and hence a meaningful comparison would involve modifications that include a shift toward neutron excess due to the Coulomb interaction.# FIG. 10.\n\nHistograms of the (non-zero) Pauli-string expectation values \u2191!| P\u02c6 |!\u2193 obtained from the BIGSTICK shell-model wavefunctions for 6Be and 8Be. A bin width of 0.05 has been used.# VI. COMPARISONS\n\nIt is interesting to compare the behavior of the quantum information with the shape parameters in an isotopic chain. As specific examples, we examine the behavior of M2, the summed proton-neutron n = 2, 4, 6-tangles \u03f1 \u03b5\u03d1 and the 32Mg, calculated from Hartree-Fock-Bogoliubov \u03c9 deformation parameter for 18Ne - 30Ne and 20Mg - generator coordinate method of Ref. [150], provided in Ref. [151]. We selected the proton-neutron part of the n-tangles as proton-neutron correlations are usually considered to be related to deformation. For convenience, we have normalized each quantity to its maximum value, as shown in Fig. 11. In the Mg isotope chain, while \u03c9 drops to zero for 28Mg, a nucleus in the shape coexistence region at the boundary of the island of inversion, the magic and n-tangles remain significant, before becoming small (magic) or vanishing n-tangles at the closed neutron shell. The same behaviour is exhibited by the Ne chain. This suggests a connection between the deformation of a nucleus and the classical resources required to compute its ground state wavefunction using a spherical basis. As the required classical computing resources scale with the exponential of the magic [152, 153], our results suggest that they scale exponentially with the (non-trivial) \u201cshape-complexity\u201d of the nucleus, something that is not captured.# VII. CONCLUSIONS\n\nAdvances in quantum information science are transforming our understanding of quantum many-body systems, and are providing new techniques and algorithms for predicting their properties and dynamics that are out of reach of experiment and of classical computing alone. This new technology provides opportunities to further improve our understanding of nuclei and nuclear reactions. Further, the now anticipated fault-tolerant quantum computers and processing units should provide computational capabilities that were impractical to consider seriously just a few years ago.\n\nNuclei are particularly interesting self-bound systems of two species of fermions (protons and neutrons) with strong short-range central and tensor two-body forces, strong three-body forces, and long-range electromagnetic interactions (neglecting the weak interactions). Beyond electromagnetism, the two species of fermions are nearly identical, but because of fine-tunings in the Standard Model, the small differences in quark masses are significantly amplified, for instance furnishing a two-nucleon system near unitarity. Combined, these features give rise to remarkable structures and complexities of nuclei, including of light nuclei.\n\nIn the context of computational complexity, the non-stabilizerness of a quantum state, encapsulated by the measures of magic, determines the quantum resources that are required to prepare the state, beyond the classical resources. Entanglement alone is insufficient to define a need for quantum resources, as some entangled states are accessible via a classical gate set, as encapsulated in the Gottesman-Knill theorem [152] and codified by Aaronson and Gottesman [153]5. It is the combination of non-stabilizerness with large-scale multi-partite entanglement that drives the need for quantum computing resources to prepare and manipulate a quantum state.\n\nIn this work, we have examined the quantum complexity in light and mid-mass nuclei, focusing on the entanglement structure and magic of the active nucleons in the p-shell and sd-shells of the spherical nuclear shell model. We have found, unsurprisingly, that the known complexity of these nuclei, including collective effects such as shape deformation and shape co-existence, which present challenges for the spherical shell model, are reflected in measures of multi-nucleon entanglement and magic. In deformed nuclei and isotopes on the path to instability, the higher-body entanglement, including collective proton-neutron entanglement, is prominent, as are the measures of magic. The relatively large values of these quantities persist for isotopes beyond those that are deformed.\n\nImplicit in our studies is the computation of matrix elements of strings of Pauli operators in the ground-state wavefunctions of p-shell and sd-shells nuclei that are mapped to qubits, with each qubit defining the occupancy of a single-particle shell model state. Calculations in the p-shell and in sd-shell nuclei with only one specie present are performed exactly, while for the typical sd-shell nuclei, the measures of magic are evaluated using an extensive suite of detailed MCMC evaluations. To accelerate the convergence of these evaluations in deformed nuclei, we introduced the PSIZe-MCMC algorithm, where the d matrix elements of Pauli strings of I, Z operators are evaluated exactly, while the remaining matrix elements are evaluated using MCMC.\n\nFrom a theoretical perspective, there is a path to be pursued in which transformations among the basis states and Hamiltonian are identified that reduce the magic and multi-partite entanglement in the ground state wavefunctions. Repeating our calculations using a deformed/collective basis is expected to yield results with less quantum complexity. This is along the lines of work we and others have pursued in entanglement re-\n\nExamples of further advances can be found in, e.g., Refs. [154\u2013156].",
        "context_id": 5,
        "question": "What algorithm was developed to accelerate sampling Pauli string expectation values in sd-shell nuclei?",
        "answer": [
            "PSIZe-MCMC"
        ],
        "context_length": 20196
    },
    {
        "context": "# 1. Introduction \u2013 Current state of health data protection in medical devices\n\nFrom a regulatory standpoint, few, if any, fields are as densely regulated as healthcare[1]. It comprises an array of distinct sectors, from drugs to medical procedures. In all cases, highly sensitive personal data, such as health-related data is required and generated. Among the different subfields within digital healthcare and innovation, the current work focuses on medical devices.\n\nA medical device is broadly defined as any instrument, apparatus, implement, machine, appliance, implant, reagent for in vitro use, software, material, or other similar or related article, intended to be used, alone or in combination, for human beings, for one or more of the specific medical purposes[2]. These purposes include the diagnosis, prevention, monitoring, treatment, or alleviation of disease; the diagnosis, monitoring, treatment, alleviation of, or compensation for an injury or disability; the investigation, replacement, modification, or support of the anatomy or a physiological process.\n\nThe processing of health-related information is foundational in healthcare[3], demanding rigorous alignment with the fundamental right to privacy, as enshrined in international treaties under Article 8 of the European Convention on Human Rights (ECHR), in effect since 1953. This right is further embodied within the European Union\u2019s (EU) data protection framework, essentially through Regulation (EU) 2016/679, the General Data Protection Regulation (GDPR), which concretises this right resulting from Article 8 of the Charter of Fundamental Rights of the European Union (CFR), proclaimed on December 2000. The GDPR, being a regulation, warrants conjugation with additional field-specific regulations. In the case of medical devices, the GDPR should be applied in# Data Protection Impact Assessment (DPIA) for Medical Devices\n\nTandem with Regulation (EU) 2017/745 on Medical Devices (MDR) to balance the individuals\u2019 rights and the safety of the devices. The MDR clarifies that data protection laws need to be applied when medical devices process personal data, under Recital 47 and articles 62(4)(h), 72(3), 92(4), 110(1)\u2013(2). Therefore, if a medical device regulated by the MDR processes personal data, it also falls under the GDPR [4].\n\nImportantly, the increased technological development of medical devices results in a steady increase of devices with digital features capable of processing health data. In this regard, according to GDPR Article 35 (1) and Recital 84, where a type of processing, in particular using new technologies, is \u201clikely to result in a high risk to the rights and freedoms of natural persons, the controller should be responsible for the carrying-out of a data protection impact assessment to assess, in particular, the origin, nature, particularity and severity of that risk\u201d, which crystalizes the desired outcomes of the process known as Privacy Impact Assessment (PIA), also officially defined as Data Protection Impact Assessment (DPIA). Although mandatory, at least for the scenarios depicted in the GDPR, the same regulation allows, under Article 35, for DPIA processes to be flexible in their formal organization and structure, provided they meet the regulatory requirements. This results in DPIAs being essentially structure and format-agnostic, provided they cover said requirements. However, such neutrality results in a lack of a systematic methodology to assess security, safety and privacy risks associated with the use of medical devices, with putative consequences to patient health [5].\n\nTo bridge these gaps, this paper proposes a unified approach to conducting DPIAs centred on medical devices with data processing capabilities, presenting a methodology to conduct DPIAs to assist law, technology, and health practitioners in assuring data security, personal data protection and privacy. The suggestion is to address the challenge by bringing together hard law such as the GDPR and MDR, with well-defined.# International Standards and Legal Frameworks\n\nInternational standards \u2013 soft law \u2013 such as those emanating from the International Standard Organization (ISO) and the International Electrotechnical Commission (IEC).\n\nNotwithstanding the importance of taking into account the prospective EU Legal Framework for the cybersecurity of medical devices and its relationships, including Data Laws (GDPR, Data Governance Act, European Health Data Space and Data Act), Medical Device Laws (MDR), AI Laws (AI Act) and Cybersecurity Laws (NIS Directive and Cybersecurity Act)[6] \u2013 for the sake of focus and space economy - this paper builds solely upon the GDPR, MDR and some selected ISOs/IECs can help build robust DPIAs in the context of medical devices.# 2. Methods\n\nThe methodology of this paper is rooted in a multidisciplinary literature review, combining legal analysis with a review of related technical standards. The core legal frameworks examined include the GDPR and MDR (Table 1), with a specific focus on their intersection in regulating medical devices that process personal health data. The GDPR\u2019s Article 35, which mandates DPIAs for high-risk data processing activities, is closely analysed in the context of medical devices. In parallel, the MDR\u2019s Recitals 47 and Articles 62(4)(h), 72(3), and 110 are explored to understand the obligations imposed on medical device manufacturers regarding safety and data protection.\n\nThe review also considers international standards, particularly those issued by the ISO and IEC (Table 1), which provide essential technical guidance to complement the GDPR and MDR. These standards, such as ISO/IEC 29134, ISO/IEC 27701 and ISO/IEC 27002, serve as tools to implement in concretum data protection by design and default, as required under article 25 GDPR.# Table 1 \u2013 Legal and technical sources used in this work \u2013 divided between non-binding (soft law) and binding rules (hard law).\n\n|Source|Type|Description|\n|---|---|---|\n|GDPR|Hard law|General Data Protection Regulation (EU 2016/679)|\n|MDR|Hard law|Medical Device Regulation (EU 2017/745)|\n|ISO/IEC 27001:2022|Soft law|Information security, cybersecurity and privacy protection \u2014 Information security management systems \u2014 Requirements|\n|ISO/IEC 27002|Soft law|Cybersecurity controls|\n|ISO/IEC 27701:2019|Soft law|Security techniques \u2014 Extension to ISO/IEC 27001 and 27002 for privacy information management \u2014 Requirements and guidelines|\n|ISO/IEC 29134:2023|Soft law|Information technology \u2014 Security techniques \u2014 Guidelines for privacy impact assessment|\n|ISO/IEC 29151:2017|Soft law|Information technology \u2014 Security techniques \u2014 Code of practice for personally identifiable information protection|\n|IEC 62304:2006|Soft law|Medical device software \u2014 Software life cycle processes|# 3.1. An overview of medical devices architecture and their regulation\n\nAs we mentioned earlier, a medical device is broadly defined as any instrument, apparatus, implement, machine, appliance, implant, reagent for in vitro use, software, material, or other similar or related article, intended to be used, alone or in combination,# Medical Device Classification\n\nfor human beings, for one or more of the specific medical purposes [1]. All these purposes can be found in detail in the relevant legislation, namely the MDR (Article 2) and the In Vitro Diagnostic Medical Devices Regulation (IVDR, EU Regulation 2017/746). An important distinction is that, while medicines and drugs attain their preventive, therapeutic or diagnostic by pharmacological means, medical devices do not [7], although there are some mixed-type medical devices that release drugs or medicines [8], which are outside the scope of this work.\n\nFinally, it should be mentioned that the thousands of medical devices in existence are, as per the MDR, classified according to their risk profile, varying from class I to class III. This medical device categorization is based on the risk they pose to patients and users. The classification determines the level of regulatory scrutiny they must undergo before entering the market.# Class I Medical Devices (Low Risk)\n\nClass I MD (Low Risk) include low-risk devices that do not come into contact with vital organs or systems. They are typically non-invasive and may include items used temporarily or for simple monitoring, such as bandages, examination gloves or band-aids. Class I devices are often exempt from premarket notifications, owing to their limited technology integration and minimal risk to patients. Such criteria take into account factors such as the duration of use, invasiveness, and the criticality of the device\u2019s function.# Class II Medical Devices (Moderate to High Risk)\n\nClass II (further divided in class IIa and IIb, moderate to high risk) comprise devices that are generally more complex than class I devices and may be invasive or involve prolonged use. Here, we find devices such as surgical needles, dental fillings, and hearing aids (class IIa), and also infusion pumps, ventilators, and certain diagnostic imaging equipment (class IIb).# Class III Medical Devices (High Risk)\n\nFinally, class III MD (high risk) are often life-supporting or life-sustaining and present a high potential risk to human life in case of failure. They include MD such as# 3.2 Data protection challenges associated with the use of medical devices\n\nNotwithstanding the current use of thousands of medical devices, most of which still devoid of data processing capabilities, the field witnessed a growing trend of increasingly sophisticated products [9], often integrating advanced technology to provide critical health insights and therapeutic benefits. However, such advancements bring challenges, particularly regarding data protection. Indeed, these devices process sensitive personal data which includes main operations such as collection, storage, transfer or erasure, as per Article 4 (2) GDPR.\n\nManufacturers must engage and implement measures to ensure data protection falls into several categories, such as technical and organizational measures. Likewise, dataProtection is to be assured both by default and also by design (GDPR, Article 25), frequently requiring technical and organizational measures. Such measures are typically included in the DPIA, which not only maps the data processing activities involved in the use of the device, but also the identified risks and mitigation controls.\n\nFailure to conduct a DPIA when required \u2013 before data processing \u2013 or to adequately address the risks identified, can result in significant fundamental rights violations, commercial damages with civil and criminal liability compensation, fines and penalties under the GDPR and national member states related laws. Under GDPR, these can include fines of up to \u20ac10 million or 2% of the global annual turnover, whichever is higher (GDPR, Article 83[4][a]). Indeed, there have been some decisions from European Courts regarding violation of personal data processing [10], including health data [11].# 3.3 Proposed risk assessment framework for DPIAs\n\nAs we mentioned earlier, there is no mandated structure or template for a DPIA. However, the former Article 29 Working Party on Data Protection \u2013 now the European Data Protection Board (EDPB), has produced guidelines, still valid, that stipulate the key information necessary for a DPIA, as well as its corresponding minimum requirements [12], as depicted in Table 2. Although the open structure of a DPIA allows it to be adjusted to particular characteristics and needs of distinct technical fields, in the specific case of medical devices, which are subjected to numerous and tight regulations, such liberty often results in markedly different DPIA structures across the field. To this end, ISO/IEC standards can help organizations achieve greater consistency and reliability in their privacy risk assessments. This approach not only aligns with global best practices for privacy management but also facilitates compliance with various regulatory requirements across jurisdictions. Leveraging these standards ensures that privacy considerations aresystematically integrated into the design and operation of medical devices, thereby enhancing the protection of personal data and reducing the risk of regulatory breaches and associated liabilities.\n\nAnother advantage of using such standards is that they can be updated and keep up with technological developments more easily than the legislative process can normally produce normative content. For example, the ISO/IEC 29134:2017 standard was updated to ISO/IEC 29134:2023 [13] just six years after it came into force, due to the rapid growth in the field of security techniques it addresses.# Table 2 \u2013 Major steps to be observed in a DPIA, corresponding scope and objectives, and suggested standards.\n\n|STEPS|SCOPE/OBJECTIVES|STANDARDS|\n|---|---|---|\n|Processing Description|Nature, scope, context and purposes of the processing considered; Personal data, recipients, and storage duration recorded;|GDPR, Recital 90 ISO/IEC 29134:2023 [13]|\n| |Functional description of the processing operation provided;|GDPR, Article 35 (7)(a) IEC 62304:2006 [14]|\n| |Assets (hardware, software, networks, people, paper, etc.) identified;|GDPR, Article 35 (8)|\n|Necessity & Proportionality|Measures to comply with the Regulation determined;|GDPR, Article 35(7) (b/d)|\n| |Specified, explicit, and legitimate purpose(s); The lawfulness of processing;|GDPR, Recital 90 ISO/IEC 27701:2019 [15] ISO/IEC 29151:2017 [16]|\n| |Adequate, relevant, and limited data Limited storage duration|GDPR, Article 5(1) (b/c/e) GDPR, Article 6|\n|Data Subject Rights|Information provided to data subjects;|ISO/IEC 29134:2023 [13] GDPR, Articles 12-17|\n| |Right of access and data portability;|ISO/IEC 29151:2017 [16] GDPR, Articles 19-20|\n| |Right to rectification and erasure;|ISO/IEC 27002:2022 [17]|\n| |Right to object and restrict processing;|ISO/IEC 27701:2019 [15]|\n|Processor Relations|Relationships with processors;|ISO/IEC 29134:2023 [13]|\n| |Safeguards for international transfers;| |# Risk Management\n\nRisks to rights and freedoms of data subjects managed;\n\n|Regulation|Description|Standard|\n|---|---|---|\n|GDPR, Article 25|Origin, nature, particularity, and severity of risks assessed;|ISO/IEC 27001:2013 [18]|\n|GDPR, Article 32|Threats leading to risks identified|ISO/IEC 29151:2017 [16]|\n|GDPR, Article 35(7)|Risk sources considered;|IEC 62304:2006 [14]|\n|(c/d)|Potential impacts identified|ISO/IEC 29134:2023 [13]|\n|GDPR, Recital 84|Likelihood and severity of risks estimated;| |\n|GDPR, Recital 90|Measures to treat risks determined;| |\n|Interested parties|Advice from the Data Protection Officer (DPO);| |\n|GDPR, Article 35(2)|Views of data subjects or their representatives sought where appropriate;|ISO/IEC 29134:2023 [13]|\n|GDPR, Article 35(9)|Document the DPIA process, including all identified risks and the measures implemented to address them.| |\n|Document and approve the DPIA|The DPIA might need to be subject of Data Protection Officer (DPO) advice and, in residual high-risk cases, submitted to the prior consultation with Data Protection Authority (DPA).|ISO/IEC 29134:2023 [13]|# Monitor and Review\n\nDPIAs should be considered living documents, subject to review and updating as processing activities or risks change. Regular monitoring ensures that the measures remain effective.\n\nFor example, the initial step of a DPIA, in which processing operations are described, is pivoted in GDPR Article 35 as a starting point. Such statute, however, can benefit from the concomitant use of ISO/IEC 29134:2023 [13], which outlines the necessary components for describing data processing activities. This includes detailing the nature, scope, context, and purposes of processing. The standard emphasizes the importance of clearly identifying and documenting the data types being processed, the stakeholders involved (e.g., data controllers, processors, and data subjects), and the flow of data within the system. In the specific case of medical devices that include software components, IEC 62304:2006 [14] provides a framework for the software life cycle processes specific to medical device software. In the context of describing the processing operation, this technical document guides the definition of the software system, including.# Data Processing Impact Assessment (DPIA)\n\nits intended purpose, functions, and how it interacts with other systems and data flows.\n\nThis is critical for accurately describing the data processing activities of the software component within a medical device.# Step 2: Assessing Necessity and Proportionality\n\nAnother step, assessing necessity and proportionality in data processing activities, is largely based on the dispositions of GDPR articles 5, 6 and 35. They can, however, benefit from the concomitant use of ISO/IEC 27701:2019 [15], which guides on implementing data minimization and purpose limitation, pivotal resources to assessing necessity. The standard encourages organizations to only collect and process data that is directly necessary for the specified purposes, helping align with the GDPR's requirement to evaluate necessity. By implementing these controls, organizations can ensure that data processing is not excessive and is strictly limited to what is necessary to achieve the intended purpose. Further contributions can be drawn from ISO 29151:2017 [16], which provides guidelines for documenting and justifying the need for processing activities.# Step 3: Identifying and Assessing Risks\n\nFor step 3 of a DPIA, which involves identifying and assessing risks, GDPR Article 35 provides the foundation by mandating that organizations evaluate the potential impacts of data processing on the rights and freedoms of individuals. This step is crucial for understanding the privacy risks associated with processing activities and is further enhanced by the guidance offered in ISO/IEC 29134:2023 [13]. The standard elaborates on methods to identify privacy risks, emphasizing a systematic approach to evaluating both the likelihood and severity of risks. It includes recommendations for conducting risk assessments that consider factors such as unauthorized access, data breaches, and potential discrimination, thus ensuring a comprehensive evaluation of privacy impacts.\n\nMoreover, when dealing with medical devices that incorporate software, ISO/IEC 29151:2017 [16] is particularly relevant as it provides a detailed set of controls for protecting personally identifiable information (PII). This includes specific measures for# Assessing the Risks to PII\n\nAssessing the risks to PII, focusing on how data is collected, stored, and transmitted. The standard emphasizes not only the identification of risks but also the need to document these risks clearly, including the contexts in which they arise and their potential impacts on individuals. This detailed documentation is essential for substantiating the risk assessments required under GDPR and for aligning with industry best practices in data protection and cybersecurity.# Step 4: Identification of Measures to Mitigate Risks\n\nOnce risks have been identified and mapped, step 4 of the DPIA focuses on the identification of measures to mitigate said risks. GDPR Articles 25 and 32 provide a legal framework for implementing appropriate technical and organizational measures to ensure data security and minimize risks. This includes adopting strategies such as encryption, pseudonymization, access controls, and regular audits. However, leveraging ISO/IEC 29134:2023 [13] alongside these legal requirements can further refine this process by offering specific guidelines on risk mitigation strategies tailored to the context of data protection impact assessments.# Additional Insights for Medical Devices\n\nFor medical devices that involve software components, IEC 62304:2006 [14] and ISO/IEC 29151:2017 [16] provide additional critical insights. IEC 62304:2006 outlines software development processes that include safety classification and risk management, emphasizing the integration of risk control measures directly into the software life cycle. This ensures that software-related risks, including those associated with data processing, are mitigated from the design phase onwards. ISO/IEC 29151:2017 complements this by providing specific controls focused on protecting PII, such as implementing robust access control mechanisms, ensuring data accuracy, and maintaining data integrity.# Step 5: Documentation and Approval of the DPIA\n\nThe subsequent step of the DPIA process, step 5, involves the documentation and approval of the DPIA, ensuring that all identified risks and the corresponding mitigation measures are thoroughly recorded and reviewed. GDPR Articles 35 and 36 outline the...# Requirements for Documenting the DPIA Process\n\nand, in certain high-risk cases, submitting the DPIA to the relevant Data Protection Authority (DPA) for consultation.\n\nThis documentation serves as evidence of compliance and helps to demonstrate that the organization has taken appropriate steps to address data protection risks. ISO/IEC 29134:2023 [13] provides a structured approach for documenting DPIAs, including recommendations for the format and content of the assessment report. The standard emphasizes the importance of clear and comprehensive documentation of each stage of the DPIA, including descriptions of the processing activities, identified risks, mitigation measures, and the decision-making process. This ensures that the DPIA is not only a tool for risk management but also a transparent record that can be reviewed and audited.\n\nFor medical devices with software components, IEC 62304:2006 [14] and ISO/IEC 29151:2017 [16] further reinforce the importance of thorough documentation. IEC 62304:2006 specifies the need for maintaining detailed records throughout the software development life cycle, including risk management activities and decisions regarding software safety. ISO/IEC 29151:2017 supports these requirements by providing guidelines on documenting privacy controls and their effectiveness, ensuring that all measures to protect PII are clearly articulated and justified. Additionally, ISO/IEC 27701:2019 [15] extends the documentation requirements to include privacy-specific considerations within the context of a Privacy Information Management System (PIMS). It underscores the need for clear approval processes and, where necessary, consultation with internal and external stakeholders, including Data Protection Officers (DPOs) and data protection authorities (DPA).# Step 6 of the DPIA Process\n\ninvolves monitoring and reviewing the DPIA on an ongoing basis to ensure that the data protection measures remain effective and relevant as processing activities or associated risks evolve. GDPR Article 35(11) emphasizes that# DPIA Review and Update\n\nDPIAs should be dynamic documents that are regularly reviewed and updated, especially when there are changes in the nature, scope, context, or purposes of the processing. This ensures that the DPIA continues to address current risks and remains aligned with the principles of data protection by design and by default. ISO/IEC 29134:2023 [13] supports this ongoing review by recommending regular assessments of the DPIA\u2019s effectiveness and the adequacy of implemented controls. It guides establishing review schedules and criteria, such as changes in technology, new types of data processing, or the emergence of new threats. This ensures that DPIAs are not static but evolve in response to the changing risk landscape.# Importance of Continuous Monitoring\n\nFor medical devices with software components, IEC 62304:2006 [14] and ISO/IEC 29151:2017 [16] further highlight the importance of continuous monitoring and updating of risk management processes. IEC 62304:2006 mandates a continuous software maintenance plan that includes procedures for ongoing risk management throughout the software\u2019s life cycle. This includes monitoring for new vulnerabilities, updating software as needed, and reassessing the associated risks, which is crucial for devices that handle sensitive health data.# Framework of a PIMS\n\nAdditionally, ISO/IEC 27701:2019 [15] extends the concept of ongoing monitoring and review within the framework of a PIMS. It recommends regular audits and continuous improvement cycles to ensure that privacy controls remain effective and compliant with evolving legal and regulatory requirements. This approach is particularly important for medical devices, where the rapid pace of technological innovation can introduce new privacy and security challenges.# Data Protection by Design and by Default\n\nUbiquitously and omnipresently throughout the process, DPIA involves ensuring data protection by design and by default, a principle firmly rooted in GDPR Article 25. This article requires that data protection measures are embedded into the design of processing activities from the outset, ensuring that personal data is protected throughout the data lifecycle. ISO/IEC 29134:2023 supports this requirement by emphasizing the...# Incorporating Privacy Controls in Development\n\nNeed to incorporate privacy controls early in the development process, specifically recommending that privacy considerations be integrated into the architecture and design phases of the data processing system. For medical devices with software components, ISO/IEC 27701:2019 and ISO/IEC 29151:2017 provide additional frameworks that are crucial for embedding privacy by design and by default.# ISO/IEC 27701 and ISO/IEC 29151\n\nISO/IEC 27701 extends the ISO/IEC 27001 and 27002 standards to include privacy-specific controls, guiding organizations on how to implement and maintain a PIMS that aligns with the principles of data protection by design. This involves the adoption of privacy-enhancing technologies and practices, such as pseudonymization and anonymization, to minimize data exposure and mitigate risks.\n\nAdditionally, ISO/IEC 29151:2017 offers detailed controls for ensuring that data protection measures are consistently applied across all processing activities, including the use of default settings that favour privacy. These standards stress the importance of designing systems that limit data collection, processing, and retention to what is necessary for the specified purposes, thereby operationalizing the GDPR\u2019s requirements for data protection by default.# Integration of Standards\n\nFor medical device software, integrating these standards helps ensure that privacy is not an afterthought but a foundational element of the system's design and operation, thereby enhancing compliance and safeguarding personal data throughout the device's lifecycle.\n\nBy integrating these standards, organizations can ensure that their DPIAs are living documents that are continuously updated and refined, maintaining a proactive approach to data protection and ensuring compliance with both GDPR and best practices in the industry.# 4. Conclusion\n\nThis paper explores the intricate interplay between hard law, such as the GDPR and MDR, and soft law standards, particularly those from ISO and IEC, in the context of conducting DPIA for digital medical devices. The analysis underscores that while GDPR provides a robust legal framework for data protection, its broad and sector-agnostic nature necessitates the support of specific standards that can address the unique challenges posed by medical devices, especially those with software components.\n\nIt is argued that standards such as ISO/IEC 29134:2023, ISO/IEC 27701:2019, and ISO/IEC 29151:2017 within the DPIA process enhance the practical implementation of privacy by design and by default. Therefore, it is argued that these standards offer detailed guidance that complements the hard law under GDPR\u2019s requirements, providing a structured approach to risk assessment, risk mitigation, and the integration of privacy controls throughout the lifecycle of medical devices. Moreover, standards such as IEC 62304:2006 play a critical role in defining the software development processes and risk management specific to medical device software, ensuring that safety and privacy considerations are embedded from the outset.\n\nUltimately, this paper advocates for a unified, law-based and standards-oriented approach to conducting DPIAs for medical devices, highlighting the need for continuous adaptation and alignment of legal and technical measures. By leveraging both hard law and sector-specific standards, stakeholders can ensure that privacy considerations are systematically integrated into the design and operation of medical devices, thereby safeguarding personal data and fostering trust in digital health innovations. This integrated approach not only enhances compliance but also positions organizations to better navigate the complex regulatory landscape and address the emerging challenges of data protection in healthcare.# The convergence of hard law and soft law\n\nThe convergence of hard law and soft law is not merely complementary but essential for achieving effective data protection in the rapidly evolving landscape of digital health technologies. As the paper demonstrates, the symbiotic relationship between these legal and technical frameworks enables a more responsive and objective approach to data protection that is capable of keeping pace with technological advancements. This is particularly crucial in the medical devices sector, where the balance between innovation, security and privacy must be expertly maintained.# Conclusion\n\nIn conclusion, the complex interplay between data protection, product safety, and privacy in the realm of medical devices, demands a harmonized regulatory approach. By integrating the GDPR, MDR, and ISO/IEC standards, a comprehensive framework for DPIAs can be established, ensuring that privacy by design and default principles are effectively implemented. This multidisciplinary strategy not only mitigates risks but also bridges the gap between legal requirements and the practical realities faced by manufacturers and regulators. Ultimately, this convergence fosters both innovation and compliance, safeguarding patient data and promoting trust in medical device technologies.",
        "context_id": 6,
        "question": "What article of the European Convention on Human Rights is the right to privacy enshrined in?",
        "answer": [
            "Article 8"
        ],
        "context_length": 30294
    },
    {
        "context": "# Educators and Technology in Education\n\nEducators are more than workers within educational systems; they are stewards of educational systems. Their job is not only to deliver lessons, grade assignments, provide feedback, and meet with parents; they must also analyze student performance data, identify patterns that inform targeted interventions and personalized learning plans, continuously develop the curriculum, set ambitious learning goals and use up-to-date pedagogical theory to adapt instructional strategies, act as advocates for educational policies that promote inclusivity and equity, and much more (Reich, 2020). Most educators deeply care about the learning and wellbeing of their students and colleagues. Given the chance, they will do whatever they can to make improvements to these ends. In this role as architects of change, educators deal with conflicting definitions of success, multiple stakeholders, complex causal relationships, ambiguous data, and intricate human factors. Amid all this, most educators and the educational systems around them are strained to the capacity of what their time, training, and budgets allow.\n\nThe problem is not merely that they must perform demanding tasks, but more so that they must constantly implement improvements and interventions amid the complex challenges of the organizations in which they work. These challenges can be especially difficult in implementation of related education technology, which is continuously developing at sometimes rapid pace (Ross, 2020). Whether the context is an individual classroom, a school district, or a postsecondary institution, implementing beneficial human-technology partnerships requires attending to the needs and constraints of these classrooms, districts, institutions, and so forth as organizations and engaging in this work as a partnership with educators. This chapter lays out the principles and processes of developing successful educator-technology partnerships including key considerations for each step and an example protocol for engaging in this endeavor.\n\nNew technologies are often proposed as solutions to improve educational systems (or even \u201crevolutionize\u201d education itself), but attempts to impose technological solutions often fail spectacularly (Daniel, 2012; Cuban, 2003). In 2013, the Los Angeles Unified School district excitedly budgeted over $1 billion to provide every student, teacher, and administrator with an iPad without first considering what needs these devices were supposed to meet. Adding technology for technology\u2019s sake and attempting to make these devices the centerpiece ofday-to-day operations was not only unnecessary, but outright disruptive to students\u2019 learning. Without a discernible need for the new devices, many teachers coped with the disruption by simply not using them (Lin, 2015; Lapowski, 2015). Even when new technology is designed to meet educational needs, technology developers do not always consider the constraints present in the environments where it is meant to be used. In 2005, driven by the belief that access to technology could empower low-income communities around the globe, a team of influential innovators, educators, and technologists hatched an ambitious (now infamous) plan to provide One Laptop Per Child everywhere (Kraemer et al., 2009). To their credit, they performed small-scale pilot tests of the supposedly rugged, low-cost laptops they had developed. These pilots revealed the need to revise the initial idea behind the project due to lack of sufficient tech support and infrastructure in the target communities. Rather than collaborating with members of these communities to revise their idea, leaders of the project marketed the idea to Western philanthropists by telling heartwarming tales of children who used the laptops to learn to read. In the end, simply building libraries in the target communities would likely have done more good than designing and distributing special rugged laptops.\n\nAlthough certain burdensome tasks performed by educators (tedious grading, feedback, communication, bookkeeping, etc.) are reasonable opportunities for technological solutions, technology developers must change the way they define these practical problems faced by educators. Too often, technology developers erroneously suppose they are solving well-structured problems. A well-structured problem is one with a clear definition of success, unambiguous data, and simple causal relationships (Simon, 1973). It seems straightforward enough to reason that if giving detailed personal feedback to students is too time consuming, an AI chatbot that automates this task solves the problem. The key point of failure is not the capabilities of the technology itself, but the process of incorporating that technology into complex educational systems (Reich, 2020; Ames, 2019; Dancy & Henderson, 2010; Allen & Kizilcec, 2023; Wieman, 2017; Damshroder et al., 2022; Cuban, 2003). This should not be too surprising to anyone who has observed the challenges of incorporating new technology, or any new work practices, in their own workplace.\n\nEven when the technology works as promised and the problem it is intended to solve is a well-structured one, incorporating that solution into an educational organization is a remarkably ill-structured problem (Dancy & Henderson, 2010; Damshroder et al., 2022). Ill-structured problems have many paths to various solutions and conflicting definitions of success (Simon, 1973). The distinction between well-structured and ill-structured problems is, interestingly enough, a significant concept in the fields related to machine learning as well as human learning. The term \u201cill-structured problem\u201d is credited to Herbert Simon (Simon, 1973), who is highly respected in pedagogy and computer science communities alike. Most readers will not need to review the works of Herbert Simon to understand how to overcome the challenges of making improvements in educational organizations. You probably already know how to solve, and have experience solving ill-structured problems within the scope of your profession. Think of a time when you were solving a problem, and it was not clear initially what problem you were even solving. One with multiple possible paths to various solutions that may impact multiple people in differing ways. How did you approach this problem compared to one with a clear path# Principles for Developing Successful Educator-Technology Partnerships\n\nIll-structured problems demand a progression of first deeply exploring the nature of the problem followed by iteratively testing possible solutions. In the case of implementing new human-technology partnerships, there is an additional end phase of solidifying that new partnership for the long-term. Though each phase of implementing a new technology has a unique focus, and each setting has unique needs and constraints, a common set of core principles is essential throughout. From the very beginning, the principle of thorough inquiry grounds the project in a full understanding of local needs, the principle of teamwork and communication connects the project to the source of that understanding, and the principle of iterative revision initiates the project\u2019s adaptation to the unique local context. As the plan advances to small-scale testing, these same three principles provide resilience to setbacks, and allow the project to capitalize on successes. Even once the final version of the intervention is being used at full-scale, it is not fully implemented until it is subject to the ordinary ongoing revision practices of the organization (Moulin et al., 2020). All three principles are mutually complementary. Inquiry and collaboration are means to the end of making necessary revisions.# Strategies for Problem Solving\n\nYou probably used some of the following strategies: You collaborated with multiple people, including those who would be impacted by the outcome, and those who may have insight into possible solutions. You set flexible timelines, maintained active channels of communication, and carefully monitored progress along the way. You made a plan that balances and prioritizes the most important needs and constraints of the people involved. In short, you used these strategies because you knew to anticipate revision. When working on a well-structured problem without the need to anticipate revision, you simply work to build a solution; but when working on an ill-structured problem, you use these strategies to build a team, build a theory, and build a solution.\n\nAll of these strategies are strongly supported by evidence from Implementation Science, which is the scientific study of incorporating new technology into organizations (Damschroder et al., 2022; Moulin et al., 2020). The core principles and processes of Implementation Science provide a framework to approach these problems more like an expert designer who spends time and resources to fully understand the problem, develops and tests a predictive model for solving the problem, and iteratively adapts to new complications (Perry et al., 2019; Moulin et al., 2019). After deeply exploring stakeholders\u2019 needs and multiple solutions to meet those needs, this implementation process provides resilience to challenges and uncertainties that will arise by using incremental testing to expose unforeseen challenges, collaborating with local leaders to adapt accordingly, and assimilating the new intervention into the regular operations of the organization. The framework of these processes provide the opportunity to engage in expert-like design thinking, but is not so formulaic to allow for a perfunctory approach. Successful educator-technology partnerships require thoughtful application of principles of thorough inquiry, teamwork and communication, and iterative revision.# Thorough Inquiry\n\nThorough inquiry occurs at all phases of a successful implementation due to the ill-structured nature of implementing new educator-technology partnerships, the high likelihood of setbacks, and the inevitable need for revision (Moulin et al., 2019). The first opportunity for thorough inquiry is a needs assessment process where the needs of the students, teachers, and organization as a whole are considered before an intervention is selected to meet those needs. A simple example from instructional design is outlining learning objectives before selecting course content and teaching methods. Without a thorough needs assessment, an implementation can fail simply because it does not solve any problems or provide anything of value. Avoid post-hoc justification of a pre-selected intervention. Collaborating with local stakeholders during the needs assessment is essential because the needs of the organization depend on local contextual factors, and different levels within the organization can have unique needs. This collaborative process will likely highlight unique elements of the organizational structure and its interaction with the external environment. Thus, a rigorous needs assessment will influence the teamwork and communication strategies to be used throughout the project.\n\nBeyond the initial needs assessment, there are many contextual factors to be explored and mapped out in detail (Damshroder et al., 2022). This includes (a) the features of the new intervention, (b) the features of the organization, and (c) the environment in which the organization operates. There are many frameworks for itemizing all the contextual factors, but no proven formula for then developing an implementation plan. Thus, it is essential to create a detailed map of the interconnections among these features (Powell et al., 2015; Damshroder et al., 2022). Mapping out the interconnected factors in this way is the Theory of Change approach (Bruer et al., 2016). This carefully mapped out Theory of Change (ToC) generates a set of many possible implementation plans and judgments about which factors are most influential. For example, introducing iClickers to enable interactive engagement in a university lecture setting could influence students\u2019 end-of-semester feedback. Student feedback can influence tenure and promotion decisions, which can influence instructors\u2019 willingness to use the new technology. Deeply exploring these interconnections before foreseeable problems arise leads to much stronger implementation plans.\n\nAs a new intervention is incrementally tested and monitored, there are opportunities for both exploratory and confirmatory measurements. Consider implementing iClickers for interactive engagement in lecture, but in a particular context, such as a cultural context where students are accustomed to silently listening to lecturers rather than interacting during class time. Since the instructional methodology (interactive engagement) has been proven to yield specific positive effects on student\u2019s learning (Freeman et al., 2014; Lovett et al., 2023; Schwartz et al., 2016), it is appropriate to use confirmatory measurements to determine whether the new intervention is producing these benefits as expected. Often these confirmatory measurements will be quantitative, such as students\u2019 test scores. But since the interaction of this methodology with the unique cultural and organizational context is unknown, and thus not all of the effects can be# Teamwork and Communication\n\nThe Los Angeles Unified School district set themselves up for failure when they began the project by talking to high-profile technology vendors about the products they wanted. The year before that, the leadership of Milpitas School District (a nearby district with similar challenges and goals) set themselves up for success by talking to teachers and local principals first. They asked them \u201cIf you could design the school of the future, what would that look like?\u201d (Lin, 2015;)# Anticipated Exploratory Measurements\n\nAnticipated, exploratory measurements are also appropriate. These exploratory measurements are often qualitative, such as student feedback forms. One type of qualitative exploratory measurement that tends to be overlooked is sitting in on classes and observing how students and teachers are interacting with the new technology. Overall, the process of incrementally rolling out a new intervention will go smoother if the main effects of the intervention and its complex interactions with the specific application context are fully explored in the early planning stages. And yet, because unexpected events are inevitable, thorough inquiry is still necessary even in the middle of a project.# Long-term Success Factors\n\nIn the later stages, an intervention that produces the intended effects is not guaranteed to last. For example, implementing online learning technology in traditional in-person classes has allowed many university courses to deliver the bulk of the course content online and refocus instructor-student contact time on coaching students rather than lecturing them. In many STEM courses, the existing classrooms for these engaging coaching sessions are large lecture halls. This physical infrastructure served as such a strong stability mechanism for old lecture practices, that many courses naturally reverted back. Every organization has stability mechanisms that can present a barrier, or can be leveraged for long-term success. Carefully consider what has stayed the same for a long time within the school (e.g., scheduling processes), and what forces are imposing on that stability (e.g., remote work and learning).\n\nAnother feature which yields long-term success are the school\u2019s existing quality assurance processes. Inquire about what the school already does well, and what processes have been in place to maintain that quality. For example, one university engineering department may consistently maintain up-to-date learning objectives and appropriately high grading standards as a result of having processes in place to take in feedback from firms that hire their students. If the department slips in adequately preparing their students, feedback from those firms will reliably initiate corrective updates to the course learning objectives. Another engineering department may meticulously avoid unfairly failing any students because the department has practices in place to listen to students (or students\u2019 parents) who raise concerns about fair grading. If students start getting unfairly low outcomes, the resulting complaints will reliably initiate corrective actions in grading practices. Educational organizations of all kinds have their own unique quality assurance practices. Once the implementation project is over, the existing stability mechanisms and quality assurance process will determine whether the new intervention persists. By knowing what has stayed the same, what is done very well, and why, existing stability and quality assurance mechanisms can be applied to sustain the long-term success of the new intervention. This degree of careful, thorough inquiry requires allocation of time and resources in the initial exploration and preparation phases. It also demands time and resources for monitoring, observation, and collaboration in later phases.# Iterative Revision\n\nEach phase of an implementation project is an iterative process because the project must not advance from one phase to the next until critical milestones are achieved. The LAUSD advanced to purchasing iPads before achieving the milestone of selecting an intervention that met local needs. They then went on to advance to full-scale implementation of those devices in every classroom before achieving the milestone of identifying the technical and organizational challenges that the full-scale implementation would entail.\n\n|Milestone|Description|\n|---|---|\n|1|Selecting an intervention that met local needs|\n|2|Purchasing iPads|\n|3|Full-scale implementation of devices in every classroom|\n|4|Identifying technical and organizational challenges|# Processes for Developing Successful Educator-Technology Partnerships\n\nThe long-term success of a new technology, or any new practice, does not merely depend on it working as promised, but the process by which it is implemented (Aarons et al., 2011). To provide a process which fully utilizes the three core principles and avoids the common pitfalls, we offer two complementary planning tools: the EPIS Framework (Exploration, Preparation, Implementation, and Stabilization) from Implementation Science (Aarons et al., 2011); and SWOT Analysis (Strengths, Weaknesses, Opportunities, and Threats) from Project Planning Research (Madsen, 2016; Namugenyi et al., 2019). The EPIS Framework outlines a process for each milestone that marks readiness to advance from each phase to the next. Approaching each phase as a set of processes to iterate upon (rather than just a linear sequence of steps) allows the project to set high standards in achieving these milestones. If only one or two rounds of the processes in the initial exploration phase has not yet identified an intervention that meets local needs to a high standard, iterate on those processes again until it does. Likewise with each subsequent phase: iterate within that phase until the milestones are achieved to a high standard.\n\nAlong with a readiness to iterate the processes within phase, the project should maintain a readiness to make methodical revisions to the products of each phase (the team, the theory, and the solution). The OLPC project had a clear opportunity to revise their idea and did not take it. Initial production runs of the cheap, rugged laptops revealed that they were not so cheap; and initial tests revealed that they were not so rugged. More importantly, the laptops did not have nearly the expected impact on the communities in which they were intended to empower. As difficult as it must have been for the OLPC to confront a fatal flaw in their central idea, these tests revealed that laptops were simply insufficient to empower low-income communities around the globe. If the initial formation of their idea had been done in collaboration with those communities, there may not have been a need to make such difficult revisions. Nonetheless, the need for revision arose and the plan rolled forward unrevised. Even in properly planned projects, unexpected challenges will arise and unrevised plans rarely succeed (Reich, 2020).\n\nThe nature of ill-structured problems is that it is not possible to anticipate every complication. Yet it is possible, and essential, to craft a strategy whereby unforeseen complications generate meaningful revisions. Plans which allow for only minor revisions of the intervention are not sufficiently flexible. Every aspect of the plan should be open to multiple major revisions. The goals, timelines, evaluation processes, team member roles, collaboration approaches, and resource allocation must all be allowed to adapt to new information and changing conditions on the ground. The Theory of Change is both a basis for seeking these revisions and is itself subject to revision in response to ongoing communication with stakeholders and monitoring of implementation progress. As the following section describes how to conduct a successful implementation, remember that it is not the brilliance of the initial idea that leads to success, but methodical (and sometimes difficult) revision. One of the central purposes of these processes is to enable those revisions.# Planning and Conducting an Implementation Project\n\nSuch that no critical steps in the process are skipped (Moulin et al., 2019), while SWOT analysis stimulates creative problem-solving in selecting strategies to overcome the unique challenges and opportunities present in the local context of a project. This section describes the processes step-by-step. Figure 1 shows what the project is producing: a team to collaborate together, a Theory of Change to understand the problem, and a solution to that problem. Tables 1-4 describe how to plan and conduct the project: the processes in each phase, how the core principles are manifested in each phase, and the pitfalls to avoid. The final section will provide protocols for using the EPIS framework: milestones for determining when to advance from each phase to the next, and instructions for using SWOT analysis at the points where creative brainstorming is appropriate.# Figure 1\n\n|An informa team|A formal team|Ordinary work roles|External support|\n|---|---|---|---|\n|with clear lines of communication|with clear roles and responsibilities|within the school begin to replace the formal team|fully phased-out|\n|A deep understanding of the needs and constraints of teachers and students|detailed map of the intervention, the organization, and the environment|A refined map based on complications that arise in testing|simplified model of the most crucial factors|\n|An intervention that legitimately meets the needs of students and/or teachers|A strategic, revisable plan for how to make the intervention work in the context|Systemic changes to the organization to enable it to use the new intervention|The new intervention is incorporated into the regular operations of the organization|\n\nThe process is divided into distinct phases because the end results of each phase lay the groundwork for solving the problems in the next. A successful educator-technology partnership must be founded on a meticulous exploration of the needs of local stakeholders (primarily students and teachers), multiple possible solutions to meet those needs, and whether these solutions fit the constraints of the local context. Only after finding an intervention that legitimately meets local needs and fits the constraints is it possible to prepare to implement that intervention. Implementing a new technology or a new practice into any type of organization can strain the capacity of that organization. This is particularly important to remember when that organization is a school with an already strained budget and workforce. Even in preliminary planning, this requires thinking ahead about how to build the capacity of the organization, how to offer any needed external support, and how to sustain the intervention once that external support is gone. These plans will almost certainly be modified once small-scale tests reveal unforeseen challenges and early successes. Thus, each phase of the EPIS process depicted in# Figure 1\n\nfocuses on a distinct two-fold problem which must be solved in preparation for the subsequent phase.\n\nThe problem of the exploration phase is to fully understand the needs and constraints of local stakeholders, and to select the correct intervention. Rushing through this problem haphazardly will leave a weak foundation for the next phase. As the project advances from the exploration phase to the preparation phase, the problem shifts to fully understanding the intricacies of the specific educational context, and to figure out how to make the chosen intervention work as expected amid those intricacies. This problem is not fully solved until it produces a plan that is highly adaptable in the face of the problem in the next phase. The problem in the implementation phase is how to interpret the complications that will arise, and which parts of the plan to revise. In the sustainment phase, it will be difficult to solve the problem of cementing the new intervention and phasing out external support if the solution is still not working or causing strain on the organization (Aarons et al., 2011). In the remainder of this section, we explain the essential subprocesses of each phase, how the three core principles are manifested in each, and common pitfalls to avoid. In the next section, we describe the milestones marking the successful completion of each phase, and the ideal mindset to maintain.# Key Considerations in the Exploration Phase\n\nBefore exploring potential interventions, it is essential to fully explore the needs of local stakeholders1: educators, school leaders, parents, and especially students. This helps to avoid making a post-hoc justification of a preselected intervention.\n\n**Table 1. Processes, principles, and pitfalls in the exploration phase of developing an educator-technology partnership.**\n|Processes|Principles|Pitfalls|\n|---|---|---|\n|Needs Assessment|Thorough Inquiry|Premature Commitment to Initial Ideas|\n|Develop a full understanding of local needs before selecting an intervention.|Explore a wide variety of local needs and potential interventions.|Failure to Consider Constraints|\n|Intervention Fit Assessment|Teamwork and Communication|Insufficient Stakeholder Collaboration|\n|Consider several solutions to meet local needs, and scrutinize the compatibility of each need to the local.|Stakeholders within the academic organization take an active role in the early planning, especially needs.| |\n\n1 While the term \u201cstakeholder\u201d has outdated connotations in some contexts, in the context of Implementation Science it is widely used to refer to community members who engage in meaningful partnerships while respecting the variety of intersecting stakes they experience.# Preliminary Revisions\n\nExplore several possible adaptations of the intervention to better meet the needs and fit the context. assessment and fit assessment.# Iterative Revision\n\nContinually re-examine local needs while exploring and adapting possible interventions.# Needs Assessment\n\nEach educational system may have vastly different needs. A detailed understanding of stakeholders' most important needs must precede the selection of an intervention to implement. The depth and formality of the assessment will depend on the scale of the changes implemented. When the United States Navy was preparing for a major curriculum update to their training program for nuclear operators, they first spent several years conducting a multimillion-dollar study itemizing hundreds of decisions that a nuclear operator in the fleet must make. This comprehensive catalog of decisions allowed the Navy to identify all the skills which must be included in the new curriculum. In contrast, when a small working group of instructors from the initial classroom phase of that same training program was planning a project to improve their classroom teaching methodology, they spent several months doing interviews and focus groups with experienced nuclear operators regarding their thought processes during fleet operations. This qualitative understanding of experienced operators' thought processes helped the working group identify when to use certain teaching techniques optimized for learning memorized procedures versus techniques optimized for adaptive decision-making. At the scale of a single classroom, the assessment might take the form of informal conversations between teacher and students, and perhaps a brief survey.\n\nIn any educational setting, the needs will often focus on student learning outcomes, but it is important to consider a broad scope of needs. In addition to students, other stakeholders of a school organization strongly influence student outcomes. Teachers, administration, and other stakeholders such as parents, play distinct roles in helping students thrive. At the university level, potential employers often have a valuable perspective regarding the challenges new graduates will be expected to face. When conducting surveys, focus groups, and interviews, be sure to include all kinds of stakeholders connected to students\u2019 needs. These essential roles will also vary from school to school. Certain needs that are vital to stakeholders in one school may be secondary in another school. However, no school is so unique that one must reinvent the wheel to meet their needs. There will be existing peer-reviewed literature to help better understand the needs that come up during this assessment, and to better understand possible solutions. Some needs may be in direct conflict with others. Thus, explicitly prioritizing those needs is a difficult but absolutely necessary component of the assessment process. Understanding the needs of local stakeholders is inherently linked to understanding the organizational structure and the specific roles of organizational members. The most common and detrimental pitfall is to skip this process or to conduct it with insufficient rigor due to overconfidence in the initial solution. Avoid simplistic pictures of student needs and the needs of# Intervention Fit Assessment\n\nThe most crucial decision of the entire project is to select an intervention that legitimately meets the highest priority needs of students and schools. There are likely many possible mechanisms to meet these needs. The intervention fit assessment is, in essence, a brainstorming process. As with all brainstorming processes, it is best practice to start by prioritizing quantity over quality of ideas, exploring many ideas of varying quality, and later revising and editing. Another best practice for brainstorming is to stay focused on a central issue or question. Thus, if the top priority needs are still unclear, it is better to backtrack to the needs assessment than to blindly press forward. Conjecturing possible solutions can expose previously unconsidered needs. After exploring solutions, explore constraints. Consider if there are any local contextual factors which may enable or inhibit each possible solution. An intervention which is successful in one school may be inappropriate or impractical in another. Temporarily resist the urge to rule out impractical ideas. When exploring constraints, it is easy to slip into the mindset of editing out ideas rather than brainstorming. Continue with the mantra of quantity over quality. Itemizing potential failure mechanisms of the impractical ideas will stimulate a deeper exploration of the practical ones. Conducting a literature review can expedite this exploration. Even with somewhat new technologies, there is likely to be useful guidance in peer-reviewed research on implementing similar interventions in similar contexts. A word of warning: this exploration of contextual constraints must not be a post-hoc justification of a pre-selected intervention. Not only does post-hoc justification of a pre-selected intervention usually indicate a poor choice of intervention, but it also undermines the essential problem-solving processes of the next phase. Approaching the intervention fit assessment (and the exploration phase overall) by legitimately questioning which intervention is best will lead to a much deeper understanding of the important contextual constraints. These constraints will be analyzed in greater detail in the next phase, and additional constraints and enabling factors are likely to be revealed in the early stages of implementation. Moreover, plans which have explored only a few possible solutions are likely to be fragile and sensitive to minor revisions. Plans which have explored many possible solutions are more likely to have identified an intervention which is appropriate for the local context, and are more likely to have sufficient adaptability. The process of outlining these interconnected factors can be aided by using SWOT analysis, which we describe in the following section.# Preliminary Revisions\n\nFollowing the needs and intervention fit assessments, now is the time to edit ideas. Rather than simply crossing off solutions with obvious failure mechanisms, instead ask how each solution needs to be modified in order to be practical and effective in the local context. Since the selected intervention will typically need to be adapted to the local context, narrow down the choices based on the feasibility of the adaptation and the priority of the needs that it meets. Whenever possible, identify the causal mechanism by which the intervention achieves the intended outcome. It defeats the purpose to adapt the intervention to fit the context, but in so doing remove the causal mechanism that makes it work. Do not attempt to plan every revision in.# Key Considerations in the Preparation Phase\n\nSignificant strategic planning is required to manage the complexity of implementing new technologies and associated practices into schools. Without careful planning, even a highly effective intervention will likely fail to have the expected impact. Once an intervention is selected, plans for implementing that intervention are strategized around an in-depth analysis of many possible barriers and facilitators within the school system and school environment. Strong implementation plans are developed in collaboration with local stakeholders from the beginning and throughout the entire process. Unforeseen setbacks and complications are likely to arise and adaptability to these complications requires continuous monitoring and iterative revision of the implementation plan. The plan is ready when it enables the team to adapt to the setbacks, uncertainties, and complexities of the upcoming implementation process. Table 2 shows important processes, principles, and potential pitfalls to consider in the preparation phase of a new technology intervention in education.# Table 2. Processes, principles, and pitfalls in the preparation phase of developing an educator-technology partnership.\n\n|Processes|Principles|Pitfalls|\n|---|---|---|\n|Formal Team Building|Thorough Inquiry|Failure to utilize local stakeholder\u2019s knowledge of organizational capacity and barriers|\n|Establish a team to manage the implementation project consisting of external experts and local members of the school organization. Outline clear roles and establish lines of communication.|A wide variety of local contextual factors are itemized in order to determine the strongest influences on individual and organizational behavior. Organizational capacity and organizational barriers are mapped-out in detail. Lessons learned are gleaned from any similar interventions in similar contexts.|Failure to collaborate with all levels of the organization|\n|Organizational Capacity and Barriers Assessment| |Insufficient resources allocated to team building and planning|\n|Outline how the capacity of the school organization (budgets, infrastructure, etc.)| |Rigid timelines which do not account for inevitable|# Implementation and Sustainment Planning\n\nCraft plans to build organizational capacity, adapt to the unknown setbacks of the implementation process, and ensure the long-term sustainment of the intervention. Select measurement methods to observe the effects of the intervention.# Teamwork and Communication\n\nA special team within the organization is formed to manage the implementation. Local stakeholders take an active role in the capacity and barriers assessments, and in the implementation and sustainment planning.# Iterative Revision\n\nThe intervention and implementation plan are revised in light of organizational capacity and barriers. Multiple possible future revisions are planned for potential issues arising in the implementation and sustainment phases.# Formal Team Building\n\nDecisions about the project are made as a team. Local organizational members (teachers, administrators, etc.) are the best resource for understanding the local context, and they must eventually take sole responsibility for sustaining a high-quality intervention in the long term. They form a team with external experts to manage the implementation process. In higher education, these formal teams are sometimes called Departmental Action Teams (Corbo et al., 2015). While team member roles may adapt throughout the process, there should be no ambiguity as to what those roles are. In particular, team members and local stakeholders know what power they have to make decisions that impact the progress of the implementation. In many organizations, leaders have a great deal of influence, but the specifics of their role will vary from one context to another. Just as local team members should clearly know their role in the implementation process, external experts need to have a clear and detailed understanding of the various roles within the regular operations of the organization. For the implementation plan to have sufficient adaptability, it should be derived from a map of the organization which accounts for all levels of organizational structure and the external environment in which the organization operates. Thus, organizational members at all levels must actively contribute to developing this map and using it to derive an implementation plan. As with the needs assessment process, the level of formality depends on scale. Formal teams will be much less formal at the scale of a single classroom.# Organizational Capacity and Barriers Assessment\n\nA new intervention and the process of implementing it may strain the organizational capacity of an institution or classroom. For example, a school may not have sufficient workforce, training, or technological capacity to utilize the intervention. If so, resources should be applied to appropriately boost organizational capacity or offset the intervention by reducing other strains on.# Implementation and Sustainment Planning\n\nIt is at this point that the intensive strategizing begins. Previously, most of the work was centered around building a full understanding of the problem at hand. Now, much of the work shifts to developing solutions to that problem. This is where the ill-structured nature of implementing human-technology partnerships in education must not be underestimated. No matter how much rigor has been applied to outlining all the contextual factors, there is no formula for strategizing what to do about them (Perry et al., 2019). Thus, it is still a process of creative brainstorming. As is the case with other ill-structured problems, there will be many possible solution paths. Diligence in using SWOT analysis to craft a detailed Theory of Change will pay off here.\n\nWhile we cannot provide a formula for making a plan, we can describe some key features of strong plans. A strong implementation plan must account for the later phases from the beginning. It must include an approach to incremental implementation, measuring relevant effects, monitoring progress, and revising accordingly. It must allocate time and resources to early testing phases. It must account for how quality assurance of the implementation will be managed long-term, and which stability mechanisms exist within the organization. Flexible plans with multiple possible approaches and moveable timelines are preferred to rigid plans with a single approach and fixed timelines. It is much more difficult to begin to leverage stability mechanisms after the intervention has been implemented at full scale. Thus, avoid the pitfall of waiting until the later stages to begin strategizing for long-term stability.\n\nAs always, proposing solutions and making concrete plans may highlight gaps in the team or gaps in the theory. Perhaps the team may require some outside expertise, or additional resources for communication and collaboration. Perhaps the Theory of Change is missing some important nuance. Iterate on building a team, building a theory, and building a solution until the project is ready to move ahead to implementation.# Key Considerations in the Implementation Phase\n\nIn each of the first two phases, it is usually advantageous to iterate on the processes therein until the project is ready to move on to the next phase. The implementation phase is always inherently iterative. Adequate planning in the preceding phases will not fully guarantee that the implementation phase will go smoothly. Rather, the implementation phase should begin with a readiness to adapt to the uncertainties and challenges to come. Table 3 shows important processes, principles, and potential pitfalls to consider in the implementation phase of a new technology intervention in education.# Table 3. Processes, principles, and pitfalls in the implementation phase of developing an educator-technology partnership.\n\n|Processes|Principles|Pitfalls|\n|---|---|---|\n|Capacity Building|Thorough Inquiry|Exceeding organizational capacity|\n|Ensure that the organization is able to effectively use the intervention (training, technology, infrastructure, workforce, budgets, policies/procedures, etc.).|Small-scale tests further explore organizational capacity, barriers, and various effects of the intervention.|Insufficient resources allocated to testing and revision|\n|Incremental Testing|Teamwork and Communication|Failure to adapt to early setbacks|\n|Conduct small-scale tests and observe results to scale up progressively. This includes the intervention itself and also the capacity building efforts.|Early successes are recorded and communicated to local stakeholders. Local stakeholders collaborate with each other on how to capitalize on early success and adapt to early setbacks.|Failure to communicate early successes|\n|Adapting and Scaling-Up|Iterative Revision| |\n|Incorporate lessons learned into adapting the team, the theory, and the solution. Perform capacity building and incremental testing at a larger scale with each test.|Unexpected outcomes are not a problem, but an opportunity to make refinements to the theory of change and the overall plan for the project.| |# Capacity Building\n\nAlthough the project should utilize the organization\u2019s existing strengths, advantages, and resources, implementing new human-technology partnerships will require building the capacity of the organization to effectively use the new intervention. The policies and procedures of the organization may need to be updated. Depending on the type of technology, electrical or# Incremental Testing\n\nThere will inevitably be some unforeseen setbacks and some early successes. Whenever possible, do preliminary tests outside of classes. When moving to real-world tests in an active classroom, begin with small-scale tests in low-stakes settings. Unforeseen setbacks in an elective course taught by a seasoned instructor will have fewer negative consequences than in some other settings. Be considerate in how to frame this choice to the students; no one likes being guinea pigs in someone else\u2019s experiment. While testing the new intervention in a setting that won\u2019t cause too much harm if it goes wrong, test it in a way that will highlight potential failure mechanisms; thereby guiding essential revisions. Exposing these setbacks will enable revisions to all components of the implementation plan, including but not limited to: the features of the intervention, teamwork and communication strategies, measurement and monitoring approaches, understanding of stakeholder needs, understanding of organizational capacity and barriers, implementation goals and timelines, and quality assurance and stability mechanisms.\n\nThis requires both a rigorous genuine attitude of seeking to learn, and a rigorous approach to measurement and monitoring. Including skeptics and naysayers in the team building processes will pay off here. This process has much in common with scientific research. The initial plans function as a hypothesis. A good hypothesis is testable. A good test is capable of falsifying the hypothesis. Unexpected findings lead to careful revisions of the theory. This updated theory generates a new hypothesis, or in other words, a new plan. This is why both qualitative and quantitative measurements are useful when monitoring the effects of incremental tests. Quantitative measurements often support or falsify existing factors in the Theory of Change, while qualitative methods are useful for finding missing factors.\n\nIn well-planned implementations, early attempts will also generate preliminary successes which should be communicated to a variety of stakeholders. Including leaders in the team-building process will pay off here. In many school settings, local leadership can have a powerful effect by offering meaningful support in the face of setbacks and spreading word of notable successes.# Adapting and Scaling Up\n\nThe low-stakes settings appropriate for initial tests can sometimes focus on students with fewer challenges and obstacles than other students. When scaling up to larger tests, there are also issues with organizational structure and policy that may not come to light in small-scale tests. So, be aware that there is still a need to look out for unforeseen setbacks and new failure mechanisms with each subsequent expansion of the implementation. This is why the implementation process is incremental and iterative. The early planning phases should have identified the major factors at play, but early tests may justify major revision to the central premise of the project. As in previous phases, it is better to do more iterations on these processes than it is to rush ahead to the next phase prematurely.# Key Considerations in the Sustainment Phase\n\nThroughout the project, the new intervention may function as an appendage to the regular operations of the organization. The project is not over until the new intervention is incorporated into these regular operations. During the preparation phase, the implementation team should have already considered how the organization conducts regular quality assurance of its existing practices. It should have also considered how the organization has achieved stability of its existing practices. At this point, the Theory of Change is simplified to focus on the most crucial factors of how the new intervention interacts with the organizational context. This way, those factors can be intentionally designed into the schools policies, training programs, culture, and physical infrastructure.# Table 4. Processes, principles, and pitfalls in the sustainment phase of developing an educator-technology partnership.\n\n|Processes|Principles|Pitfalls|\n|---|---|---|\n|Stabilization|Thorough Inquiry|Lack of clarity regarding which organizational roles conduct ongoing quality assurance.|\n|Incorporate the intervention into the regular operations of the organization, including stability mechanisms and quality assurance processes.|Removing external support is an opportunity to expose any ongoing challenges, especially any necessary stability mechanisms and quality assurance processes.|Failure to monitor progress while phasing out external support.|\n|Phasing-Out External Support|Teamwork and Communication| |\n|Gradually allow the local organization to take full responsibility for using the new intervention.|Each component of the school organization has a clear understanding of their role within the new intervention.| |\n| |Once the school organization is managing the stability and| |# Quality Assurance of the Intervention\n\nThe local implementation team may dissolve.# Iterative Revision\n\nThe organization\u2019s ordinary practices for continued improvement and quality assurance may need to be adapted in response to ongoing challenges and opportunities.# Stabilization\n\nLocal organizational members must sustain the intervention with no outside support. Few, if any, interventions provide value to the organization when used haphazardly. Each school will have their own mechanisms for conducting quality assurances of their internal practices. The ongoing quality assurance processes should support and enhance the causal mechanisms that make the intervention work. Thus, the Theory of Change must include those causal mechanisms and be simplified to focus on the most crucial factors. For example, if technology is implemented in a university classroom to enable interactive engagement pedagogy (i.e., active learning), and if the university uses end-of-semester feedback forms as its main quality assurance process, these feedback forms should allow students to comment on the level of interactive engagement in class. The long-term success of the intervention depends on local stakeholders continuing to conduct quality assurance of the intervention as a component of their routine organizational processes. Every organization has internal mechanisms for keeping certain features stable over time. This may include physical infrastructure, formal policies, teacher training programs, and organizational culture. From the earliest stages of the implementation project, the project should have begun by making partners of local stakeholders. If so, the project will have a major advantage at this final stage.# Phasing-Out External Support\n\nThe implementation is not complete if the organization requires external support to train people to use the intervention, conduct ongoing quality assurance, maintain necessary infrastructure, or secure funding. Think of phasing out external support as another form of testing in that it is an opportunity to make observations, and when necessary, corresponding revisions. It may be that previous testing and adaptation has worked out all the bugs, or it could be that removing external support exposes new issues. Depending on the level of ongoing challenges, the sustainment phase may be as iterative as the previous three phases, or it could be mostly linear. If phasing out support exposes new challenges that require backtracking to additional stabilization, then it is better to backtrack and iterate than to push ahead.# Protocols for Implementing Successful Educator-Technology Partnerships\n\nNow that we have described the core principles, processes, and pitfalls of implementing educator-technology partnerships, we offer a set of user-friendly protocols for planning and conducting an implementation project. These include the major milestones that must be achieved before advancing from one phase to the next, the key indicators that these milestones have been fully achieved, and instructions for using SWOT analysis as an aid in brainstorming and strategic planning. Finally, we will characterize a mindset for working through these protocols most effectively.# Milestones and Readiness Indicators\n\nImplementing new human-technology partnerships, or any new practice, is never easy considering the level of planning, attention to detail, and responsiveness to setbacks required. Though the process is unavoidably difficult, it can be broken down into a series of achievable milestones. The goal is that eventually the new educator-technology partnership will no longer be an intervention that functions as an appendage to the organization, but is fully assimilated into the regular day-to-day operations. This level of assimilation can only be achieved if the new human-technology partnership leverages the unique advantages and overcomes the unique barriers in the local context. Revealing these key advantages and barriers is an iterative process of meticulous testing and revision. Meticulous, iterative testing and revision requires a revisable strategic plan. Such a careful plan can only be developed once a good intervention is selected, and selecting a good intervention depends on developing a thorough understanding of students\u2019 most important needs. Many failed implementations of educator-technology partnerships simply never achieved one or more of these basic milestones: the LAUSD did not consider how iPads would meet the needs of students or teachers, the OLPC project did not revise their plan after small-scale tests revealed unforeseen challenges, and many flipped classroom initiatives never leveraged the stability mechanisms within their university departments. In Table 5, we outline four major milestones corresponding to the four EPIS phases where each major milestone must be achieved before moving on to the next phase. Since moving forward with an implementation project without achieving each milestone is detrimental for success, we suggest key indicators of readiness to move forward.# Table 5. Major milestones in each of the four EPIS phases, and indicators of readiness to move to the next phase.\n\n|Exploration Phase|Preparation Phase|Implementation Phase|Sustainment Phase|\n|---|---|---|---|\n|Selected an intervention that meets local needs|Developed a revisable strategic plan|Exposed unforeseen challenges and iteratively refined the plan|Incorporated the new intervention into the regular operations of the|\n|\u3007 Collaborated|\u3007 Built a team| | |# Readiness Indicators\n\n- New challenges and opportunities were identified during the needs assessment\n- Local stakeholders made specific contributions to the needs assessment\n- Conflicting needs were identified during the needs assessment\n- Local stakeholders made specific contributions to the intervention fit assessment# Stakeholder Engagement\n\n- Local stakeholders from all levels of the organization have clear roles in the project\n- School leadership is ready to offer support when setbacks occur\n- ToC includes factors from the intervention, the organization, and the environment in which the organization operates\n- ToC includes the causal mechanisms that makes the intervention work\n- ToC includes stability mechanisms and quality assurance processes that may be leveraged in the late stages of the project\n- Local stakeholders made specific contributions to the capacity and barriers assessment\n- The plan for the new intervention does not strain the capacity of the organization in terms of budgets, workloads, training, or infrastructure\n- Local stakeholders at all levels of the organization have a shared understanding of their role using and maintaining the new intervention for the long-term.# Intervention Development\n\n- Built the capacity of the organization to use the intervention\n- Imposed stability mechanisms and quality assurance processes\n- Brainstormed multiple possible interventions to meet those needs\n- Made preliminary revision to the selected intervention to fit the context\n- Mapped out the advantages and disadvantages within the organization (i.e. a Theory of Change [ToC])\n- Conducted small-scale tests\n- Openly communicated successes and setbacks\n- Adapted the plan, revised the intervention, and scaled-up tests\n- Planned for the implementation and sustainment phases# Monitoring and Measurement\n\nMonitoring and measurement includes both qualitative and quantitative measurements.\n\nThe plan explicitly allocates time and resources for testing and revision.# SWOT Analysis\n\nWorking through these steps of an implementation project involves intensive strategic planning, especially at the key milestones of selecting an intervention, and determining how to make that intervention a long-term success. Any educational organization will present a complex web of interacting advantages and disadvantages that must be understood in detail to achieve these key milestones. Some strategic planning tools which represent this inherent complexity are themselves so complex that they are difficult to use (Powell et al., 2015). SWOT analysis, on the other hand, is a strategic planning tool with a simple organizational scheme. Its purpose is to judge the feasibility of a prospective project by considering advantages and disadvantages for both internal and external sources (Namugenyi et al., 2019). Strengths, Weaknesses, Opportunities, and Threats (SWOT) are organized into a table. Despite being independently developed, the following recommendations for conducting a SWOT analysis (Madsen, 2016; Namugenyi et al., 2019) are remarkably similar to the process described by Implementation Science:\n\n1. Brainstorm factors to fill in the SWOT table in collaboration with all levels of the organization, and with outside sources when needed.\n2. Carefully outline interactions among these factors and prioritize the most critical factors based on the potential impact on the project.\n3. Develop strategies to overcome barriers and leverage advantages.\n4. Adaptively implement the selected strategies with ongoing monitoring of progress, clear communication, and appropriate allocation of resources.\n\nThough these recommendations are a direct analog of the processes described by Implementation Science, there is one important difference. In SWOT analysis, the step of developing strategies is framed as a creative problem-solving process rather than a formulaic process (Jarzabkowski et al., 2007; Whittington, 2003). This fills in a notable gap in Implementation Science, but also benefits from small modifications from Implementation Science. Whereas Project Planning Research considers external and internal influences, Implementation Science additionally considers the unique advantages and disadvantages of the new intervention itself. To capture this, we adjust the SWOT table from its traditional two-by-two format to a two-by-three format, as shown in Table 6.# Table 6. Example of a SWOT analysis based on the One Laptop Per Child project.\n\nIn this case, low-income nations are the external environment, the school system within those nations is the organization, and the provision of laptops for children is the intervention.\n\n|Advantages|Disadvantages|\n|---|---|\n|External Environment|- Bigger problems in low-income nations than low computer literacy|\n|- Premise is appealing to funders in high-income nations|- Unreliable or non-existent electricity and telecommunications infrastructure|\n| |- Low income in the Global South reduces the relative cost effectiveness of technological solutions over human-centered solutions such as creating schools and libraries|\n| |- Minimal interest in computers|\n|Organization|- Minimal interest in the role of computers in education|\n|Leadership Structure|- Minimal local capacity to provide tech support|\n|Staffing|- Minimal capacity to provide training for students on how to use the units|\n|Training and Expertise| |\n|Emotional Factors| |\n|Policy and Procedures| |\n|Technological Capacity| |\n|Budgets| |\n|Intervention|- Manufacturing and logistics are insufficient for large-scale production and distribution|\n|- Cost per unit decreases with increasing scale|- Cost per unit is extremely high relative to the yearly income of the users|\n|Scalability| |\n|Adaptability|- Requires reliable electricity and telecommunications infrastructure|\n|Context Dependence|- Requires training to integrate into schools|\n|Cost|- Requires ongoing tech support|\n|Technological Constraints| |\n|Required Expertise|- Units are durable|\n|Causal Mechanisms| |\n|Etc.| |# Support to Use and Maintain the Units\n\nWe provide an example of this adapted SWOT analysis for the OLPC project in Table 6. With the benefit of hindsight, this project can serve as an example of how to consider stakeholder needs and important contextual constraints in an implementation project. The SWOT analysis helps identify various interrelated challenges and opportunities for this example project. On one hand, simply building schools and libraries could have a greater impact at less cost. On the other hand, the laptops would target the specific problem of assisting users without literacy in their own language. The SWOT analysis leads to a possible strategy of focusing on schools and libraries in most cases, and using the laptops in cases where additional support for developing literacy is needed and there is sufficient local infrastructure to support the laptops. In these special cases, the project would need to provide external technical support. The plan for this strategy should involve piloting the laptops and carefully monitoring the level of external support required, and measuring the additional benefit beyond simple schools and libraries.\n\nThis strategic analysis tool cannot compensate for failure to fully explore local needs prior to selecting an intervention. Neither can this tool force planners to attend to the constraints faced by the stakeholders; it merely provides an opportunity to attend to those constraints (Madsen, 2016; Namugenyi et al., 2019). This example also demonstrates the value of considering multiple possible solutions to meet stakeholder needs amid their constraints. SWOT analysis is unlikely to rescue a project that moves ahead with a single pre-selected intervention based on a poor understanding of the stakeholders' needs and constraints. Thus, we recommend first performing an assessment of local needs, then considering multiple possible interventions, and then conducting a SWOT analysis to help explore important constraints. The SWOT table should be filled out in moderate detail for each potential intervention being considered in the exploration phase, and it should be filled out in great detail for the selected intervention in the preparation phase. In the preparation phase, the SWOT table combines the findings of the needs assessment, the intervention fit assessment, and the barriers and capacity assessment. Craft a detailed Theory of Change by finding the interconnections between the factors in the SWOT table and prioritizing the strongest influences on the project.# Think Like an Expert\n\nWithout the proper mindset, it is possible to work through these protocols in a perfunctory manner and generate underwhelming results. When designing a project to implement a new educator-technology partnership; researchers, educators, and technology developers often reproduce common errors made by novice designers. Novices tend to make the same mistakes in any field. Thus, anyone without vast experience working to implement new technology and practices into educational organizations would be prudent to consider themself a novice. The purpose of this mindset is to be on alert for common pitfalls, and to consciously mimic an expert-like approach.# Notable Failures in Technology Implementation\n\nNotable failures like the OLPC laptops and LAUSD iPads are not surprising considering that people often do not continue generating ideas once they have even one solution in mind (Binz & Schulz, 2023; Jansson & Smith, 1991). Though experts also experience the phenomenon of fixation on initial ideas, they tend to utilize strategies to overcome this problematic mental barrier whereas novices tend to make choices which exacerbate it (Crismond & Adams, 2013; Atman et al., 2013).\n\nSpecifically, experts will conjecture multiple solutions as a way of exploring constraints in the problem (Cross, 2004). Whereas in mechanical engineering, the relevant constraints include materials, manufacturing, and maintenance; in education, these constraints include teacher training, classroom infrastructure, limited workforce, etc. After exploring constraints, experts will then re-analyze the problem and represent it in a way that is entirely independent of how they frame the solutions (Fricke, 1999).\n\nOnce the problem is defined, experts will search for pre-existing solutions (Crismond & Adams, 2013). They will expect errors in their initial design and allocate sufficient time and resources for testing and revision. Novices tend to analyze the problem with a particular solution in mind, and frame the problem in terms of their initial solution (Fricke, 1999; Flynn, 2020). Framing the problem in this novice manner contributes to the common pitfall of failure to consider constraints, including potential failure modes and critical needs of the client or end-user (Loweth et al., 2020).\n\nNovices will therefore tend to be overconfident in the effectiveness and uniqueness of their solution (Atman et al., 2013). This can lead to \u201creinventing the wheel\u201d rather than searching for existing solutions. It also leads to planning for rigid timelines that lack sufficient time or resources allocated to testing and revision (Flynn, 2020; Crismond & Adams, 2013).# Poor Team Dynamics\n\nPoor team dynamics are another major contributor to failure in nearly any field (Leifer, 1998; Whitcomb & Whitcomb 2013; Maier et al, 2021). Often because novices do not make intentional strategic choices about their approach to teamwork and communication. This results in poorly defined roles among team members (Jantzer et al, 2020).\n\nExperts include the client or end-user in the early collaborative process of framing the problem and outlining constraints. In contrast, novices do not frame the problem in collaboration with the end-user, leading to a poorer understanding of user needs (Loweth et al., 2020; Flynn, 2020; Atman et al., 2013).\n\nBy applying so much rigor to fully understand a problem, experts are able to construct a testable predictive framework for the project at hand. This predictive framework is equivalent to a Theory of Change (Rapport et al. 2022; Breuer et al., 2016) in that it is a detailed map of the strongest factors influencing the outcomes, and is subjected to iterative testing and revision throughout the course of the project.# The Expert Mindset\n\nThe essence of the expert mindset is to anticipate revision, and thus approach any project in a way that is inherently inquisitive, collaborative, and iterative (Crismond & Adams, 2013; Flynn, 2020). The value of the expert strategies and the consequences of the novice mistakes are magnified when working on ill-structured problems that have any combination of conflicting definitions of success, multiple stakeholders, complex causal relationships, ambiguous data, and intricate human factors (Simon, 1973).\n\nDeveloping human-technology partnerships in education will always be an ill-structured problem. Although these partnerships can be successfully implemented using some of the most basic principles of Implementation Science, itis important to appreciate the level of careful planning required, and adopt a mindset suited to the challenge.\n\nWhile it is an appealing notion that educators will spontaneously adopt any new technology that eases the burdensome tasks they must perform, we recommend an alternative outlook. New technology, just like non-technological improvements, are far more likely to achieve long-term adoption if they are developed in collaboration with the end-user and implemented via carefully-planned processes. With a mindset that implementing new technology into educational organizations is an ill-structured problem, educators and technology developers alike may utilize the problem-solving strategies they already know. We offer the EPIS Framework and SWOT Analysis (each slightly modified) as complementary tools to find long-term success in meeting the needs of teachers and students.",
        "context_id": 7,
        "question": "What was the budget allocated by the Los Angeles Unified School district for iPads in 2013?",
        "answer": [
            "over $1 billion"
        ],
        "context_length": 68136
    },
    {
        "context": "# Applications of Knowledge Distillation in Remote Sensing: A Survey# 1.1. Preliminary\n\nRemote sensing (RS) image analysis plays a pivotal role in interpreting and managing Earth\u2019s natural and human-made environments [1]. This technology harnesses data captured by satellites or high-altitude aircraft, providing crucial insights across a broad spectrum of applications\u2014from agricultural monitoring and disaster management to urban planning and climate science [2]. By enabling timely and efficient observation of vast, inaccessible, or dangerous areas, RS becomes indispensable for tracking environmental changes, predicting weather patterns, and managing natural resources [3]. Consequently, the ability to quickly process and analyze RS images leads to more informed decision-making, enhancing our global capability to respond to challenges such as food security, natural disasters, and climate change [4, 5].\n\nHowever, the complexity of RS tasks varies significantly depending on the specific application, and many of these tasks are inherently challenging and computationally intensive [6, 7]. Key tasks such as image classification, object detection, change detection, and segmentation involve processing high-dimensional data, often characterized by large spatial and spectral resolutions [8, 9]. For instance, distinguishing between different land cover types or detecting minute changes over time in vast geographical areas necessitates sophisticated algorithms capable of handling enormous datasets [10]. Moreover, the presence of noise, variability in lighting conditions, atmospheric distortions, and the need for high precision further compound the complexity of these tasks [11, 12]. These challenges lead to extensive training times, particularly for deep learning models, which require large datasets to achieve high accuracy and generalization. Therefore, optimizing these models to balance accuracy and computational efficiency remains an ongoing challenge in RS [13, 14].\n\nArtificial Intelligence (AI), particularly machine learning (ML) and deep learning (DL), has revolutionized RS image analysis by introducing levels of precision and efficiency previously unattainable with traditional methods [15, 16, 17]. DL models, especially those based on Convolutional Neural Networks (CNNs), are highly adept at handling high-dimensional data from RS imagery [18]. These models excel in tasks such as pattern recognition, object detection, and semantic segmentation, where they can automatically identify features like roads, buildings, or vegetation changes [19]. Furthermore, the deployment of AI enables the processing of large datasets in real-time, significantly improving the accuracy of predictions and analyses. Moreover, DL\u2019s ability to learn feature representations without manual intervention reduces reliance on expert-driven feature design, thus scaling up the analytical capabilities of RS technologies [20].\n\nDespite these advances, the integration of AI and DL into RS presents several significant challenges. One of the foremost issues is the requirement for substantial computational resources, particularly for training large neural network models [21, 22, 23]. This becomes a critical barrier for organizations with limited access to high-performance computing infrastructure [24]. Additionally, DL models often require vast labeled datasets for training, which can be difficult and costly to acquire in the context of RS. Furthermore, these models are prone to overfitting, especially when trained on limited datasets, reducing their ability to generalize well.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 1 of 50# Application of Knowledge Distillation in Remote Sensing\n\n|Abbreviation|Full Form|Abbreviation|Full Form|\n|---|---|---|---|\n|KD|Knowledge Distillation|YOLOv8|You Only Look Once version 8|\n|RS|Remote Sensing|MS2RGB|Multispectral to RGB Knowledge Distillation|\n|CNN|Convolutional Neural Network|PseKD|Phase-shift Encoded Knowledge Distillation|\n|S-T|Student-Teacher|GSGNet|Graph Semantic Guided Network|\n|ARSD|Adaptive Reinforcement Supervision Distillation|LPIS|Land Parcel Identification System|\n|RGB|Red, Green, Blue|DOTA|Dataset for Object Detection in Aerial Images|\n|R-CNN|Region-based Convolutional Neural Network|DIOR|Dataset for Object Detection in Remote Sensing|\n|FPN|Feature Pyramid Network|AID|Aerial Image Dataset|\n|MCFI|Multiscale Core Features Imitation|SSKDNet|Self-supervised Knowledge Distillation Network|\n|SSRD|Strict Supervision Regression Distillation|MSKA|Multi-level Semantic Knowledge Alignment|\n|CFKD|Cross-layer Fusion for Knowledge Distillation|ViTs|Vision Transformers|\n|YOLO|You Only Look Once|FPN|Feature Pyramid Network|\n|HSI|Hyperspectral Image|ERKT|Efficient and Robust Knowledge Transfer|\n|CKD|Collaborative Consistent Knowledge Distillation|TWA|Two-way Adaptive Distillation|\n|GKD|Generalized Knowledge Distillation|NLD|Noisy Label Distillation|\n|DKD|Decoupled Knowledge Distillation|CAMs|Class Activation Maps|\n|SSFD|Spatial Feature Blurring for Distillation|RS-SSKD|Remote Sensing Self-supervised Knowledge Distillation|\n|LEVIR|Large-scale Earth Vision Image Recognition|SAR SSDD|Synthetic Aperture Radar Ship Detection Dataset|\n|UCMerced|University of California Merced Land-use Dataset|NWPU-RESISC|Northwestern Polytechnical University Remote Sensing Image Scene Classification|\n|CMD|Class Mean Distillation|MSW|Maximum Sustained Wind|\n\nto new, unseen data [25]. Another pressing concern is the \"black box\" nature of DL models, which often leads to difficulties in interpreting their decision-making processes\u2014a critical requirement in applications where transparency and understanding are paramount, such as in environmental compliance and strategic planning [26, 27].\n\nTo address some of these challenges, knowledge distillation (KD) emerges as a promising technique. KD involves training a smaller, more efficient student model to replicate the performance of a larger, more complex teacher model [28]. By transferring knowledge from a high-performing neural network to a compact model, KD reduces the computational resources required for deployment, making advanced AI-driven RS technologies more accessible [29]. Moreover, the student model can often achieve comparable accuracy with less data, mitigating the issues of extensive data requirements and overfitting [30]. In resource-constrained environments, KD proves particularly advantageous, as it enables energy-efficient deployment, thereby reducing the carbon footprint of AI systems [31]. Additionally, KD facilitates the transfer of pre-trained models to other domains through fine-tuning, extending the versatility of AI applications even in scenarios with scarce data. Furthermore, KD techniques can assist in generating synthetic training data when annotated data is limited, thus addressing one of the critical bottlenecks in RS [32, 30]. The resulting simpler models from the distillation process also offer easier interpretability, providing clearer insights into their decision-making mechanisms [33, 34]. This interpretability is essential for applications requiring transparency, such as environmental monitoring and regulatory compliance. As a result, KD not only democratizes AI capabilities within RS but also enhances the practical utility of these technologies in critical applications, ensuring a balance between performance, energy efficiency, and scalability across diverse domains [31].# 1.2. Comparison with Existing Reviews\n\nSeveral recent reviews and surveys have provided comprehensive analyses of various aspects of Knowledge Distillation (KD) and its applications across different domains. These works highlight the evolution, challenges, and future directions of KD, focusing on areas such as computer vision, medical applications, and large language models. For instance, [35] offers an in-depth examination of KD within the framework of the Student-Teacher (S-T) learning model, providing a thorough overview of KD\u2019s core concepts, methods, and applications, particularly in vision tasks. The study also identifies key challenges and potential future research directions. Similarly, [36] explores the significance of cross-stage connection paths between teacher and student networks, introducing a novel approach that enhances the effectiveness of KD while maintaining low computational overhead. This framework is shown to improve performance across various tasks such as classification and object detection. Additionally, [37] presents a survey focusing on KD as a model compression and acceleration technique, categorizing KD methods by knowledge types, training schemes, and architectures. The paper discusses challenges like the trade-off between model size and performance and suggests potential research avenues to advance the field further.\n\nIn another study, Yadikar et al. [38] examine the application of KD in target detection within computer vision, focusing on the challenges of balancing detection speed and accuracy. The study highlights how knowledge compression techniques, particularly knowledge refinement, can enhance the performance of target detection algorithms on edge devices with limited computational power. The authors also propose potential improvements and future trends in integrating distillation learning with target detection [38]. Similarly, Alkhulaifi et al. [39] explore KD as a solution for deploying deep learning models on resource-constrained devices. They introduce a \"distillation metric\" to compare different KD methods based on model size and accuracy, providing a detailed survey of techniques such as soft label distillation and logit and feature map distillation, both offline and online. The study also discusses real-world KD applications in domains such as autonomous vehicles, healthcare, and IoT, outlining current challenges and future research directions. Furthermore, Yu et al. [40] review dataset distillation (DD), a technique related to KD that focuses on creating smaller, synthetic datasets that retain the performance of models trained on larger datasets. The study presents an algorithmic framework for DD methods, categorizes existing approaches, and identifies challenges such as privacy, copyright, and data storage, offering insights into future research directions for this emerging field.\n\nAdditionally, Meng et al. [41] explore the use of KD in the medical field, addressing challenges such as deploying large models on lightweight devices and the difficulty of# Application of Knowledge Distillation in Remote Sensing\n\nThe study reviews various KD applications in healthcare, demonstrating how KD can compress complex models while improving their performance in medical tasks. It highlights the potential of KD to alleviate issues related to medical resource shortages by optimizing model deployment effectively. Similarly, Li et al. [42] present a survey on KD in object detection (OD), discussing the evolution of KD-based OD models and their advantages in performance and resource efficiency. The study analyzes different distillation techniques and explores their applications in domains like remote sensing (RS) and the management of 3D point cloud datasets, offering a comprehensive comparison of model performance across various datasets.\n\nFurthering the exploration of KD, Luo et al. [43] provide an overview of modern approaches to distilling Diffusion Models (DMs), focusing on distilling DMs into neural vector fields and reviewing stochastic and deterministic implicit generators. The authors also examine accelerated diffusion sampling algorithms as a training-free method for distillation, offering valuable insights for researchers interested in DM distillation. Additionally, Acharya et al. [44] address the emerging field of symbolic KD in large language models (LLMs), emphasizing the transformation of implicit knowledge within these models into a more explicit, symbolic form. This survey categorizes existing research, highlights the importance of symbolic KD in enhancing interpretability and efficiency, and proposes future research directions to advance this growing field.\n\nIn the context of computer vision, Kaleem et al. [45] provide a comprehensive review of KD techniques, covering major methods such as response-based, feature-based, and relation-based knowledge transfer. The study discusses the benefits and challenges of using KD to compress and optimize deep learning models, especially in resource-constrained environments. It explores the application of KD in tasks such as image classification, object detection, and video captioning, and highlights recent developments in multimodal models with KD. Similarly, Habib et al. [46] focus on KD in Vision Transformers (ViTs), addressing the challenges of deploying these models in environments with limited computational resources. The study reviews various KD approaches for compressing ViTs, emphasizing KD\u2019s role in reducing computational and memory requirements while maintaining model performance. It also provides a comparative analysis of different KD techniques for ViTs and identifies unresolved challenges that warrant further research.# Table 1\n\nComparison of several KD surveys and reviews across various aspects such as focus on vision tasks, the use of teacher-student frameworks, real-world and medical applications, distillation techniques, and future research directions. It highlights which aspects are covered by each reference, along with the proposed study, indicating areas of focus and gaps in the existing literature.\n\nThe proposed review offers a comprehensive and structured analysis of KD, significantly expanding upon previous works by integrating a wide-ranging taxonomy and exploring its diverse applications across various domains, particularly in RS. Unlike existing reviews, which tend to focus on specific aspects of KD such as its role in model compression or its application in computer vision, this review provides a holistic overview, categorizing KD models based on architecture, distillation techniques, and application areas. Furthermore, it delves into advanced topics such as dynamic distillation, layer-wise distillation, and the integration of KD with real-time processing and edge AI\u2014areas that remain relatively underexplored in prior literature. Additionally, the review addresses practical challenges such as data heterogeneity, scalability, and the balance between efficiency and accuracy, offering insights into emerging trends and future directions. This approach not only contextualizes KD within the broader landscape of machine learning but also highlights its potential for innovation in areas like precision agriculture, urban planning, and oceanographic monitoring. Thus, this review serves as a valuable resource for researchers and practitioners aiming to leverage KD in diverse and complex environments. Overall, this review makes several key contributions to the field of knowledge distillation (KD) in RS, which can be briefly summarized into the following:\n\n- Provides a comprehensive and structured analysis of KD, significantly expanding on previous works by integrating a wide-ranging taxonomy.\n- Explores the diverse applications of KD across various domains, with a particular focus on RS.\n- Categorizes KD models based on architecture, distillation techniques, and application areas, offering a holistic overview.\n- Delves into advanced topics such as dynamic distillation, layer-wise distillation, and the integration of KD with real-time processing and edge-AI, which are underexplored in prior literature.\n- Addresses practical challenges, including data heterogeneity, scalability, and the balance between efficiency and accuracy, providing insights into emerging trends and future directions.\n- Contextualizes KD within the broader landscape of machine learning over RS data, highlighting its potential for innovation in areas like precision agriculture, urban planning, and oceanographic monitoring.# 1.3.1. Inclusion and Exclusion Criteria\n\nThe inclusion and exclusion criteria have been identified by including studies that are directly relevant to KD, particularly in the context of RS, with a focus on research published within the last 5 years to capture the latest advancements. Peer-reviewed articles, conference papers, preprints from reputable platforms and book chapters published in English are prioritized, in addition to empirical studies, reviews, case studies, and theoretical papers. Studies that do not specifically address KD in RS or focus on unrelated technologies, as well as outdated research published more than 5 years ago unless it is seminal are excluded, as well as non-peer-reviewed sources such as blog posts, opinion pieces, and non-academic publications.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 3 of 50# Application of Knowledge Distillation in Remote Sensing# Comparison of KD Surveys and Reviews\n\n|Aspect|Focus on Vision Tasks|Teacher-Student Framework|Real-world Applications|Medical Applications|RS Applications|Distillation Techniques|Discussion on Challenges|Future Research Directions|Model Compression Techniques|Introduction of New Metrics|Multimodal Model Applications|Discussion of Existing Datasets|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[35]|\u2713|\u2717|\u2713|\u2713|\u2717|\u2717|\u2713|\u2713|\u2713|\u2713|\u2713| |\n|[36]|\u2713|\u2713|\u2713|\u2717|\u2713|\u2717|\u2713|\u2713|\u2713|\u2713|\u2713| |\n|[37]|\u2713|\u2713|\u2713|\u2713|\u2713|\u2717|\u2713|\u2713|\u2713|\u2713|\u2713| |\n|[38]|\u2717|\u2717|\u2717|\u2717|\u2717|\u2717|\u2713|\u2713|\u2717|\u2713|\u2713| |\n|[39]|\u2717|\u2717|\u2717|\u2717|\u2717|\u2717|\u2713|\u2717|\u2717|\u2713|\u2713| |\n|[40]|\u2713|\u2713|\u2713|\u2713|\u2713|\u2717|\u2713|\u2713|\u2713|\u2713|\u2713| |\n|[41]|\u2713|\u2717|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713| |\n|[42]|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713| |\n|[45]|\u2713|\u2713|\u2713|\u2713|\u2713|\u2717|\u2713|\u2713|\u2713|\u2713|\u2713| |\n|[46]|\u2717|\u2717|\u2717|\u2717|\u2713|\u2717|\u2717|\u2717|\u2717|\u2717|\u2713| |\n| |\u2717|\u2717|\u2717|\u2717|\u2717|\u2717|\u2717|\u2713|\u2717|\u2713|\u2713| |\n| |\u2717|\u2717|\u2713|\u2717|\u2717|\u2713|\u2713|\u2713|\u2713|\u2713|\u2713| |# 1.3.2. Search Strategy\n\nThe search strategy involves using multiple academic databases such as IEEE Xplore, Scopus, Web of Science, Elsevier, Springer Nature, Wiley, Taylor & Francis, MDPI, Google Scholar, etc. to conduct a comprehensive search using relevant keywords like \"Knowledge Distillation\", \"Model Compression\", \"Model Distillation\", \"Feature Distillation\", \"Data Distillation\", \"Remote Sensing\", \"Urban Planning\", \"Precision Agriculture\", \"Land Cover Classification\", etc., refined with Boolean operators (AND, OR, NOT). Initial screening of titles, abstracts, and keywords is performed manually to identify potentially relevant studies. Studies that meet the inclusion criteria are shortlisted for a full-text review, where a detailed evaluation confirms their relevance and quality. Additionally, reference lists of selected studies are screened to identify any further relevant studies that may have been overlooked.\n\nFig. 1 explains the literature screening approach adopted in this study.# 1.4. Organization of the Paper\n\nThe organization of the paper is meticulously structured to provide a thorough exploration of KD and its applications in remote sensing. Section 2 lays the groundwork by covering the fundamentals of KD, starting with a brief overview, defining essential concepts, and discussing the historical evolution of KD techniques. This section also delves into the basic principles and mechanisms of KD, including the objective function and overall loss that guide the distillation process. Additionally, the benefits of KD are highlighted, such as model compression, improved efficiency, enhanced performance on smaller models, and the broader implications for various applications. Following this, Section 3 focuses on RS tasks and the public datasets that are pivotal for applying KD in this domain. Section 4 introduces a comprehensive taxonomy of KD models, categorizing them based on variations in the model or input data, the type of transferred knowledge (including response-based, feature-based, and relation-based distillation), distillation targets (data, model, and feature distillation), and structural relationships within network layers (layer-to-layer and cross-layer distillation). In Section 5, the paper transitions to discussing the applications of KD in remote sensing, with a detailed examination of its use in image/scene classification, object detection, land cover classification, semantic segmentation, precision agriculture, urban planning, and oceanographic monitoring. Section 6 then addresses the challenges and limitations associated with KD, including model complexity, data heterogeneity, overfitting, scalability, real-time applicability, dependency on high-quality data, balancing efficiency and accuracy, and integration complexity. Looking ahead, Section 7 outlines future directions for KD research. It suggests advancements such as dynamic distillation, layer-wise distillation, efficient training and inference techniques, low-cost training algorithms, hardware-aware distillation, and improvements in data quality and robustness. The section also discusses scalability solutions like# Figure 1\n\nSummary of literature screening approach used in this review.\n\nComprehensive Flowchart of the Literature Screening Process# Application of Knowledge Distillation in Remote Sensing\n\ndistributed and incremental distillation, real-time processing enhancements, and the integration of cross-modal and multi-modal distillation. Additionally, it explores the potential for seamless integration with existing workflows through plug-and-play distillation modules, toolkits, and frameworks, as well as enhancing model interpretability through explainable distillation and feature importance preservation. The potential of hybrid approaches, combining KD with other techniques and developing adaptive distillation frameworks, is also considered. Finally, Section 8 offers a comprehensive conclusion, synthesizing the insights gained throughout the paper and highlighting the potential for future advancements in the field of KD in remote sensing.# 2.1.1. Definition and Basic Concepts of KD\n\nKD is a ML technique where a smaller, simpler model (known as the student) is trained to emulate the behavior of a larger, more complex model (known as the teacher) [19]. As shown in Fig. 2, KD relies on two deep neural network models, a more complex one that is called the Teacher and a simpler one that is called the Student. The core idea is to transfer the knowledge from the teacher model, which typically performs better due to its greater capacity, to the student model, which is less resource-intensive [47] and tries to mimic the teacher\u2019s behavior. This can be achieved by aligning the student\u2019s outputs with those of the teacher using a Distillation Loss function that compares the two outputs. Usually, the teacher\u2019s soft target probabilities (the outputs from the softmax layer before applying the final decision function, as depicted in Fig. 2) are used for this purpose. These soft targets provide richer information than hard labels, as they contain insights about the relative probabilities of incorrect answers, giving the student model clues about the underlying data structure and feature relationships that the teacher model has learned [48]. However, apart from this response-based knowledge distillation tactic, the student also can learn the output of intermediate teacher layers, or other representations, making the KD approach very flexible and powerful.# 2.1.2. Brief History and Evolution of the KD Technique\n\nThe concept of KD can be traced back to earlier works in model compression and hints training, where simpler models were trained to mimic more complex ones using additional information from those models. However, the term \u201cknowledge distillation\u201d was popularized by Hinton et al. [49] in a seminal 2015 paper, where they demonstrated the effectiveness of using soft targets to train neural networks. Since then, the field has seen rapid development and broad applications across various domains of Artificial Intelligence. Originally, KD was primarily used to reduce the size and computational demands of large neural networks so that they could be deployed on devices with limited hardware capabilities, such as mobile phones and embedded systems. This was particularly valuable for applications that require real-time processing, such as speech recognition and mobile vision [50].\n\nAs research progressed, the scope of KD expanded beyond model compression. Researchers began exploring its potential to improve model generalization by smoothing the decision boundaries, making them more stable and improving generalization. Using ensembles of teacher networks to train a student network further stabilizes training by distilling the collective knowledge of multiple models into a single model, and facilitates transfer learning across different domains or tasks [51]. The technique has been adapted and refined to include not just direct output mimicry, but also feature-based and relation-based distillation, where intermediate representations and relationships between data points are also transferred from the teacher to the student [52]. Today, KD is an active area of research with ongoing innovations that aim to further enhance its effectiveness and expand its applicability. This includes cross-modal distillation for transferring knowledge between different types of data, such as video-to-text, and self-distillation, where a model is iteratively trained on its own softened outputs to refine its capabilities [53].# 2.2. Basic Principle and Mechanism\n\nThis section provides the mathematical background of KD [54]. Let us denote the output logits (pre-softmax activations) of the teacher model as \ud835\udc67\ud835\udc47 and those of the student as \ud835\udc67\ud835\udc46. The softmax function applied to these logits is given by:\n\n\ud835\udf0e(\ud835\udc67, \ud835\udc47 ) =  \u2211\ud835\udc57 \ud835\udc52\ud835\udc67\ud835\udc57 \u2215\ud835\udc47 ,\ud835\udc52\ud835\udc67\u2215\ud835\udc47\ud835\udc56\n\nwhere \ud835\udc56 indexes the output classes, and \ud835\udc47 is the temperature parameter that controls the softness of the probability distribution. A higher value of \ud835\udc47 produces a softer probability distribution [55].# 2.2.1. Objective Function\n\nThe training of the student network involves minimizing a loss function that typically comprises two terms: the distillation loss and the traditional hard target loss [56].\n\n- Distillation Loss: This loss measures the difference between the softened outputs of the teacher and the student, encouraging the student to mimic the teacher\u2019s generalized behavior. It is often computed using the Kullback-Leibler (\ud835\udc3e\ud835\udc3f) divergence [57]:\n- \ud835\udc3fKD = \ud835\udc47 2 \u22c5 \ud835\udc3e\ud835\udc3f(\ud835\udf0e(\ud835\udc67\ud835\udc47 , \ud835\udc47 )\u2016\ud835\udf0e(\ud835\udc67\ud835\udc46 , \ud835\udc47 ))\n\nThe factor of \ud835\udc47 2 is used to scale the gradients appropriately, as the gradients produced by the softmax function are scaled by \ud835\udc47 [57].\n\nHard Target Loss: This is a standard loss, such as Cross-entropy (CE), used in training neural networks, calculated between the student\u2019s output (at \ud835\udc47 = 1) and the true labels [58]:\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 5 of 50# Application of Knowledge Distillation in Remote Sensing# Teacher Model\n\n|Input Layer|Hidden Layers|Output Layer|\n|---|---|---|\n|Knowledge Distillation|Knowledge Distillation|Knowledge Distillation|\n|Soft Target Probabilities|Soft Target Probabilities|Soft Target Probabilities|\n|Knowledge|Knowledge|Knowledge|\n|Data|Data|Data|\n|Student Model|Student Model|Student Model|\n|Soft Target Probabilities|Soft Target Probabilities|Soft Target Probabilities|\n\nFigure 2: An overview of the knowledge distillation principle.# 2.2.2. Overall Loss\n\nThe total loss function used to train the student model is a weighted sum of the distillation and hard target losses:\n\n\ud835\udc3f = \ud835\udefc\ud835\udc3fCE + (1 \u2212 \ud835\udefc)\ud835\udc3fKD\n\nwhere \ud835\udefc is a hyperparameter that balances the importance of the two loss components. By optimizing this loss, the student learns not only the explicit knowledge represented by the class labels but also the implicit, richer information embedded in the teacher\u2019s output distribution, thus achieving better generalization from a more compact model [59]. Fig. 3 summarizes the main steps of applying KD in RS applications. Fig. 4 illustrates the architecture of a knowledge distillation (KD) framework based on YOLOv8, designed for precision agriculture applications such as weed recognition and variable rate spraying. In this framework, a YOLOv8l model, which has the highest recognition accuracy, was chosen as the teacher network, while a YOLOv8n model, which has the lowest recognition accuracy and the smallest model size, was selected as the student network. The resulting KD model, named YOLOv8n-DT, is specifically tailored for rice field weed recognition and comprises three main components: the teacher network, the student network, and the distillation loss function module, that performs both target and feature distillation.# 2.3. Benefits of KD\n\nKD offers several compelling advantages that make it an attractive technique in the field of ML, particularly when deploying models in resource-constrained environments.# 2.3.1. Model Compression\n\nOne of the primary benefits of KD is model compression. Traditional DL models often require substantial computational resources due to their depth and complexity, which limits their deployment on devices with restricted hardware capabilities such as mobile phones, IoT devices, and embedded systems. KD addresses this challenge by enabling the training of smaller, lighter models (students) that mimic the behavior of larger, more complex models (teachers). This process involves transferring the intricate knowledge and insights learned by the teacher model into a more compact form within the student model. The student thereby learns to approximate the function of the teacher but with fewer parameters and lower computational demands. This compression not only reduces the size of the model but also lessens the energy consumption and heat production, which are critical factors for battery-powered devices.# 2.3.2. Improved Efficiency\n\nEfficiency in model training and inference is another significant advantage of KD. By distilling a cumbersome model into a smaller one, KD effectively reduces the time and computational power needed for training and deploying AI systems. This improved efficiency is particularly beneficial for applications requiring real-time data processing, such as autonomous driving and real-time surveillance. Smaller models also allow for more frequent updates and easier maintenance, which is crucial for systems that need to adapt to changing conditions or data streams.# 2.3.3. Enhanced Performance on Smaller Models\n\nKD not only compresses the size of the models but often also enhances their performance, especially in smaller models. Typically, smaller neural networks are prone to underfitting and may not capture the complex patterns in large datasets as effectively as their larger counterparts. However, when trained through the KD process, these smaller models inherit refined insights from the teacher models, which include soft probabilities and inter-class relationships that are not visible through traditional hard-label training. This enriched training set helps the student models to perform better than if they were trained independently from scratch. Moreover, the nuanced knowledge transferred includes the handling of edge cases and anomalies, which significantly improves the robustness and generalizability of the student models.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 6 of 50# Application of Knowledge Distillation in Remote Sensing# Compute Hard Target Loss LCE\n\n- Tune Temperature T\n- Adjust T\n- KL Divergence between Softened Outputs# Compute Overall Loss L = \u03b1LCE + (1 \u2212 \u03b1)LKD\n\n- Adjust \u03b1\n- Train Student Model\n- Evaluate Performance on Validation Set# End\n\nFigure 3: Principal steps of applying KD in RS applications.# 2.3.4. Broader Implications\n\nThe advantages of KD extend beyond individual model improvements. In educational settings, distillation techniques can democratize access to advanced AI capabilities by enabling more institutions to deploy high-performing AI solutions without the need for expensive infrastructure. Furthermore, in a research context, KD facilitates greater experimental flexibility and faster iteration speeds, accelerating the pace of innovation in AI.# 3. RS Tasks and Public datasets\n\nRS has become a pivotal tool for monitoring and understanding changes in both urban and agricultural environments. RS involves the acquisition and analysis of data from satellite or airborne sensors to observe and interpret features on the Earth\u2019s surface. The main RS tasks encompass a variety of applications that leverage spectral, spatial, and temporal information. These tasks include image classification, object detection, change detection, segmentation, and data fusion. The primary RS tasks are centered around image classification and analysis.\n\nImage classification categorizes pixels in an image into distinct classes, such as different land cover types, using methods like convolutional neural networks (CNNs) for high accuracy. Object detection identifies and locates specific objects within an image, such as vehicles or buildings, making it crucial for applications in urban planning and agriculture. Change detection focuses on identifying differences in images taken at different times, which is essential for monitoring environmental changes like deforestation. Segmentation further refines this process by partitioning an image into meaningful regions, helping extract detailed information about specific features like roads or rivers. Collectively, these tasks highlight that RS primarily involves sophisticated image classification and analysis.\n\nData fusion tasks arise from the need to integrate and analyze data from various sources, such as multispectral, hyperspectral, or LiDAR data, to enhance the comprehensiveness and accuracy of RS applications. This integration is vital when dealing with the complex nature of environmental features that cannot be fully captured by a single sensor type. Consequently, data fusion is an essential approach to\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 7 of 50# Application of Knowledge Distillation in Remote Sensing# Teacher Model\n\n| |Backbone|Head| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | |CZF| |Detect| |Bbox_| | | |\n| | | | | |PS: 20*20*1024|Conv| | | | | |\n| | | |C2F|C2F|40*40*1022|Detect| | | | | |\n| | | |CZF| |P3: 80*80*512| | | | | | |\n| | | | | | | |Loss|Loss| | | |\n| |160*160*256|320*320*123| | | | | | | | | |\n|Cf| | | |P3|Cz|P4|P5|KT|Output| | |\n| | | | | |Feature Distill| | | | |Logical Distill|Output|# Student Model\n\n|Backbone|Head| | | | | | |\n|---|---|---|---|---|---|---|---|\n| | | |CZF|20*20*512|Detect|Bbox_| |\n| | | |conv| | | | |\n| | |CZFF|CZF|40*40*512|Detect| | |\n| | | |Conv| | | | |\n| | |C2F| |P3: 80*80*256| |Cls_| |\n| | | | | | |Loss|Loss|\n|80x80+256|P2: 160*160*128| | | | | | |\n|PI: 320*320*64| | | | | | | |\n\nFigure 4: The YOLOv8n DT network architecture is structured into three primary components: the teacher network, the student network, and the distillation loss function module. This architecture incorporates both feature loss and logit loss within the distillation process to effectively transfer knowledge from the teacher to the student network, thereby enhancing the student\u2019s performance while maintaining efficiency.\n\nAddressing the limitations of individual datasets, providing a more holistic view of the Earth\u2019s surface. All the aforementioned tasks are fundamental to various environmental, agricultural, and urban studies, providing essential insights for decision-making and resource management. During the years, diverse datasets have been developed to support the advancement of instance segmentation techniques in this field, each tailored to specific challenges and applications.\n\nSpaceNet 7 [60] and SpaceNet 4 [61] represent significant contributions to urban development analysis. SpaceNet 7 offers insights into the evolution of building footprints across 100 global locations over two years, using Planet imagery. This dataset is crucial for tracking urban expansion and infrastructure development. Conversely, SpaceNet 4 focuses on the technical challenge of detecting buildings from steep observation angles\u2014up to 54 degrees off-nadir. This is particularly valuable in emergency response situations where quick, accurate assessments are necessary. Similarly, the Microsoft BuildingFootprints dataset [62] provides detailed building footprints across several countries, extracted from Bing imagery. This resource supports urban planning and management by offering extensive building delineations. Additionally, the xView 2 Building Damage Assessment Challenge [63] leverages high-resolution Worldview-3 imagery to assess building damage from natural disasters, a critical component of effective disaster response.\n\nIn the agricultural sector, datasets like PASTIS [64] and the Agriculture-Vision Database [65, 66] are invaluable. PASTIS provides panoptic labels for over 124,000 agricultural parcels in France, captured across Sentinel-2 timeseries images. This dataset aids in the precise monitoring and management of agricultural lands. The Agriculture-Vision challenge, on the other hand, focuses on identifying field anomalies from aerial imagery across the United States, promoting enhanced agricultural practices through detailed monitoring.\n\nFor more specialized applications, datasets like RarePlanes [67], which includes both synthetic and real data for plane detection, and iSAID [68], which covers a wide range of categories from planes to bridges, are particularly noteworthy. RarePlanes is essential for developing models that differentiate between aircraft types, useful in both civilian and defense sectors. iSAID facilitates broad applications in aerial image analysis by providing extensive annotations for diverse objects. Furthermore, the introduction of SpaceNet 6: Multi-Sensor All-Weather Mapping [69] combines SAR data and optical imagery to enhance building footprint detection in challenging weather conditions, illustrating the value of multi-sensor data integration in RS. The technological advancements in datasets like Airbus Ship Detection Challenge [70], which focuses on ship detection using satellite imagery, and novel methodologies in the LPIS agricultural field boundaries dataset highlight the industry\u2019s shift towards more sophisticated and fine-grained analysis capabilities.\n\nThe PASTIS dataset [71] provides detailed panoptic labels for over 124,000 agricultural parcels across France, captured in 2,433 Sentinel-2 image timeseries. This dataset is instrumental for applications in agricultural monitoring, allowing for the differentiation of crops at the parcel level through both instance and semantic segmentation. It is particularly useful for tracking changes in agricultural land over time.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 8 of 50# Application of Knowledge Distillation in Remote Sensing# Knowledge Distillation Taxonomy\n\n|Type of Knowledge Transferred|Training Methodology|Application Area|Supervision Signal|Distillation Strategies|Performance Optimization|\n|---|---|---|---|---|---|\n|Graph-Based Relations|Progressive Distillation|Model Distillation|Hard Labels|Classical Distillation|Energy Efficiency|\n|Relation-Based|Co-Training|Data Distillation|Soft Labels|Contrastive Distillation|Memory Optimization|\n|Pairwise Distances|Online Distillation| | |Adversarial Distillation|Speed Optimization|\n|Attention Maps|Self-Distillation| | |Cross-Layer Distillation| |\n|Feature-Based|Offline Distillation| | |Layer-to-Layer Distillation| |\n|Intermediate Features| | | | | |\n|Probability Distribution| | | | | |\n|Response-Based| | | | | |\n|Logits Matching| | | | | |# 4. Taxonomy of KD Models\n\nKD methods in RS (RS) can be categorized into several key approaches, each with unique attributes and applications. As depicted in Fig. 5, the variations may come from the differences in the data or architecture used by the teacher and student networks resulting to Heterogeneous and Cross-modal KD approaches that are based on the Teacher-Student Architecture, or from the different types of knowledge that are distilled between the teacher and the student, resulting to Response-based, Feature-based, and Relation-based approaches. These approaches are tailored to optimize RS models by transferring knowledge from complex teacher models to more efficient student models, using all the available data per case, thereby enhancing performance in tasks such as object detection, scene classification, and image segmentation. Of course, there are several more variations that depend on the training methodology, the application area, the structural representation, the distillation strategy, etc., as shown in Fig. 5 and explained in the following.# 4.1.1. Heterogeneous KD\n\nHeterogeneous KD (HKD) is a method of transferring knowledge from a teacher model to a student model where the teacher and student models have significantly different architectures [72]. Traditional KD methods typically assume that the teacher and student models have similar architectures, which allows for straightforward layer-by-layer transfer of knowledge. However, in HKD, the architectures may vary greatly, posing a challenge for direct knowledge transfer.\n\nThe study in [73] presents a Generalized KD (GKD) framework for multi-source Earth Observation analysis, specifically for land cover mapping using radar and optical satellite image time series data. This approach tackles data.\n\nY. Himeur, et al.: Preprint submitted to Elsevier# Application of Knowledge Distillation in Remote Sensing\n\nmisalignment due to atmospheric conditions or acquisition costs, using radar data consistently and treating optical data as privileged information. This makes it a case of heterogeneous distillation, where different modalities (radar and optical) are involved, requiring the student model to adapt to a less data-rich environment at test time compared to training. The authors in [74] propose using information from deep convolutional networks to guide the training of shallow Grassmannian manifold networks, addressing the need for high-performance yet small-sized networks in resource-limited scenarios. The approach bridges DL with manifold learning, fitting well within the heterogeneous category, as it involves transferring knowledge between fundamentally different architectures. Moving forward, Yang et al. [75] introduce a two-way assistant distillation method for lightweight object detection in RS. This method incorporates compression and multiscale adaptive modules to address feature disparities and background noise, utilizing a heterogeneous distillation approach by applying complex operations from larger models to enhance smaller, simpler ones.\n\nBesides, Nabi et al. [76] propose a compound loss computed on a Transformer-based student and a CNN teacher for single-label scene classification in RS. The use of heterogeneous architectures, where a CNN and a Transformer are involved, leverages the long-range visual capabilities of the Transformer and the inductive biases of the CNN, aiming to enhance classification accuracy in complex scenes. Similarly, the research in [77] involves a teacher-ensemble learning approach using KD in cross-source content-based image retrieval for high-resolution RS images. The method combines source-shared and source-specific classifiers, constructing an effective heterogeneous ensemble of teacher models to transfer useful information to the student model.# 4.1.2. Cross-Modal KD\n\nCross-modal KD (CMKD) refers to the process of transferring knowledge from a model trained with superior modalities (e.g., depth maps or point clouds) to another model trained with weaker modalities (e.g., RGB images) [78]. The goal is to improve the performance of the student model trained on the weaker modality by leveraging the knowledge from the teacher model trained on the superior modality. This transfer is achieved by aligning the intermediate feature representations and activation maps between the teacher and student models [78]. In CMKD, the knowledge from the teacher model is used as an additional supervision signal to guide the training of the student model, enhancing its learning process and performance.\n\nExpanding on this concept, Geng et al. [79] propose a topological space network for road extraction, where a denser teacher network focused on topological feature extraction guides a lighter student network. This distillation process transfers knowledge about complex road topology from a heavy network, illustrating a clear case of cross-modal architecture distillation by integrating high-dimensional topological features into a simplified network. Similarly, Xiong et al. [80] introduce a discriminative distillation network for cross-source Content-Based RS Image Retrieval (CBRSIR), addressing the challenge of harmonizing features between multispectral and panchromatic images, thereby further exemplifying cross-modal architecture by handling variations between different types of RS data sources. Additionally, Liu et al. [81] propose a cross-modal KD framework designed to improve multispectral scene classification by transferring knowledge from teacher models pre-trained on RGB images to a student model processing multispectral images. This approach highlights the adaptability of CMKD by addressing the differences between modalities and enhancing the student\u2019s performance, particularly in scenarios with limited samples.\n\nFurthermore, Pande et al. [82] contribute to the field with an adversarial training-driven hallucination architecture for modality distillation in RS image classification, focusing on learning discriminative feature representations from multiple sensor modalities, even in the presence of missing data during the model inference phase. This work aligns closely with cross-modal architectures as it effectively distills features across varying sensor modalities, enhancing model robustness. Lastly, Liu et al. [83] present a universal Super-Resolution-Assisted Learning (SRAL) framework aimed at improving the performance and efficiency of salient object detection in RS images. By incorporating super-resolution techniques into a multitask learning framework, this approach distills domain knowledge from the super-resolution task to significantly boost object detection performance, further showcasing the potential of cross-modal knowledge transfer in enhancing model accuracy and efficiency.# 4.2.1. Response-Based (Soft Targets Distillation)\n\nResponse-based KD focuses on the knowledge extracted from the final layer of the teacher model. It aims to align the final predictions between the teacher and the student models. The primary goal is outcome-driven learning, which involves distilling the class probability distribution via a softened softmax function, known as \u2019soft labels.\u2019 This method guides the student model by matching the output distributions of the teacher and student models using various distance functions such as Kullback-Leibler divergence, mean squared error, or Pearson correlation coefficient [84].\n\nThe study in [85] introduces a KD framework applied to RS scene classification. By using the high-temperature softmax outputs from a large, deep teacher model to train a smaller, shallow student model, the study showcases how KD can improve the performance of less complex models on multiple public datasets, increasing accuracy significantly even on smaller and unbalanced datasets. This approach directly employs the response-based distillation technique by leveraging the teacher\u2019s softened output probabilities to enhance the student\u2019s learning process. Moving on, Zhao et al. [86] introduces a novel pairwise similarity KD method for reducing the complexity of CNN models in RS image scene classification, maintaining accuracy while using less computational resources. This method focuses on distilling discriminative information between sample pairs.# 4.2.2. Feature-Based (Intermediate Representations)\n\nFeature-based KD addresses the limitation of response-based KD by providing supervision at intermediate layers of the network. This method focuses on transferring intermediate feature representations, such as feature maps, attention# Application of Knowledge Distillation in Remote Sensing# 4.2.3. Relation-Based (Learning Relationships Between Different Data Layers)\n\nRelation-based KD explores the relationships between different data samples or across different layers within the neural network. Unlike response-based and feature-based KD, which typically handle individual samples, relation-based KD captures cross-sample or cross-layer relationships as meaningful knowledge [84]. This method constructs relational graphs to model dependencies and similarities between instances or layers and uses similarity metrics and distance functions to measure these relationships. The goal is to transfer structured knowledge that encapsulates higher-order dependencies and interactions within the dataset.\n\nChen et al. [96] develop consistency- and dependence-guided KD methods for object detection in RS images. They introduce modules that focus on extracting and transferring discriminative spatial locations and channels, as well as establishing the consistency and dependence of features between the teacher and student models. This approach utilizes relation-based distillation by focusing on the inter-layer and inter-feature relationships to guide the student model\u2019s learning process. Moving on, Li et al. [97] introduce an instance-aware distillation method, which combines feature-based and relation-based distillation techniques. The method enhances the student model\u2019s performance by focusing on instance-related foreground information and constructing relationships between different instances to improve detection accuracy in complex remote-sensing images. Zhao et al. [98] propose a self-supervised KD network (SSKDNet) that uses feature maps of the backbone as supervision signals and transfers the \"dark knowledge\" through KD. This method focuses on enhancing the discriminative feature extraction capabilities by learning the relationships between different data layers in a self-supervised setting.\n\nDong et al. [99] present a cross-model KD framework, distilling segmenters from CNNs and transformers, which uses a channel-weighted attention-guided feature distillation and a target\u2013nontarget KD module to guide the student model in learning complex representations and decision boundaries. This study distinctly focuses on relation-based distillation by leveraging the interdependencies of features and classification decisions between different network architectures. On the other hand, Zhou et al. [100] introduce the Multi-level Semantic Transfer Network (MSTNet), a KD framework designed for dense prediction of RS images. This network utilizes a Multi-level Semantic Knowledge Alignment (MSKA) framework to distill semantic information from a complex teacher model to a more compact student model. The MSKA framework emphasizes cross-layer semantic alignment, dynamic semantic aggregation.\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 11 of 50# Application of Knowledge Distillation in Remote Sensing# 4.3.1. Data Distillation\n\nData distillation refers to techniques that aim to synthesize small, high-fidelity data summaries which capture the most important knowledge from a given dataset [101]. These distilled summaries are optimized to serve as effective substitutes for the original dataset in various data-usage applications such as model training, inference, and architecture search. The goal is to create a concise representation of the data that maintains its critical characteristics, allowing for faster and more efficient model training and evaluation [101].\n\nBuilding on this concept, Zhang et al. [102] introduce a novel noisy label distillation method within an end-to-end teacher-student framework, which distills knowledge from labels across various noise levels. This approach exemplifies data distillation by effectively utilizing knowledge from noisy data to improve classification performance in RS image scene classification. Extending the application of data distillation, Zhao et al. [86] propose a pair-wise similarity KD method for RS image scene classification. By distilling discriminative information from a cumbersome model to a compact model, this study aims to maintain high accuracy while reducing model complexity, demonstrating another facet of data distillation. Furthermore, Yue et al. [103] contribute to this field with a self-supervised learning method that incorporates adaptive distillation for hyperspectral image classification. Their approach, which focuses on generating adaptive soft labels based on spatial-spectral similarity, underscores the importance of utilizing extensive unlabeled data in the data distillation process.# 4.3.2. Model Distillation\n\nModel distillation refers to the process of replacing a complex ML model with a simpler model that approximates the original model\u2019s performance [104]. This technique is used to improve computational efficiency by distilling large or ensemble models into smaller, more manageable models that maintain similar accuracy. The primary goal is to reduce the computational cost associated with deploying large models while preserving their predictive capabilities [104]. Model distillation also aids in model interpretability by converting \u201cblack-box\u201d models, such as neural networks, into more transparent forms.\n\nIn the context of model distillation for RS applications, a variety of approaches have been developed to enhance the performance and efficiency of lightweight models. Zhang et al. [105] introduce a dynamic knowledge distillation (KD) framework that enables CNN models to be lightweight while maintaining high detection accuracy, with an emphasis on selective learning through a dynamic instance selection distillation module. Building on the concept of model distillation, Yang et al. [106] develop a lightweight semantic segmentation network that combines KD with a multiscale pyramidal pooling module and attention mechanisms, resulting in a pruned model that retains high accuracy. Similarly, Wang et al. [107] propose a change detection method that integrates prototypical contrastive distillation and channel-spatial-normalized distillation, allowing the student model to learn complex feature distributions from the teacher, thereby fitting into the model distillation framework.\n\nFurther advancing the field, Chen et al. [108] propose a multi-teacher collaborative distillation approach that uses adaptive weight and feature knowledge exchange to enhance the robustness of student models, while Gu et al. [109] introduce a Context-aware Dense Feature Distillation (CDFD) strategy for CubeSat-based RS object detection, integrating multiple teacher networks to optimize a lightweight detector. Chai et al. [110] contribute to the model distillation category with their Bidirectional Self-Attention Distillation (Bi-SAD) approach, aimed at enhancing cloud detection models by enabling compact models to learn detailed textural and semantic information.\n\nAddressing the challenge of few-shot learning, Liu et al. [111] present a ranking-preserving KD method that improves the generalization capabilities of student models in RS scene classification. Similarly, Wang et al. [112] explore the enhancement of lightweight models through a Phase-shift encoded KD method (PseKD) that improves object orientation prediction. In a broader application, Chen et al. [113] propose a semi-supervised KD framework for global-scale urban object mapping, emphasizing the handling of urban diversity and large-scale sample growth.\n\nComplementing these efforts, Zhao et al. [114] propose a weakly correlated distillation learning framework for RS object recognition with limited samples, leveraging large-scale natural image datasets to enhance small-scale RS datasets. Lin et al. [115] address the issue of denoising by presenting a lightweight model that uses KD to efficiently extract spatial and spectral features while maintaining computational efficiency. Yu et al. [116] focus on incremental learning, introducing a dual KD method to mitigate catastrophic forgetting, which aligns with the incremental learning approach proposed by Xu et al. [117] and Xu et al. [118], who use KD to enhance multimodal learning and hyperspectral image classification, respectively.\n\nLastly, Zhou et al. [119] introduce a graph semantic guided network (GSGNet) for optical RS scene analysis, utilizing knowledge refinement to maintain high inference speed and contextual inference capability. Zhao et al. [120] propose a target detection model distillation framework that uses feature transition and label registration to improve the learning ability of lightweight networks in RS imagery, further contributing to the body of work on model distillation.# 4.3.3. Feature Distillation\n\nFeature distillation refers to a method in which the student network learns to mimic the hidden feature values of a teacher network [121]. This process involves transferring the intermediate representations (features) learned by the teacher network to the student network. Unlike traditional KD that focuses on the output probabilities (logits), feature distillation emphasizes the transfer of internal activations or feature maps. The primary goal is to improve the student network\u2019s performance by leveraging the knowledge encapsulated in the teacher\u2019s feature representations [121].\n\nBuilding upon this concept, Zhou et al. [122] propose a lightweight student network framework for semantic segmentation of high-resolution RS images. By employing a\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 12 of 50# Application of Knowledge Distillation in Remote Sensing\n\nGraph attention guidance network, they distill knowledge from a large teacher network to optimize image features, thereby enhancing segmentation accuracy. This method aligns with feature distillation, where the objective is to boost the student\u2019s feature representation capabilities to closely match those of the teacher. Similarly, Zhang et al. [123] introduce a few-shot classification method for RS scene classification, which also falls under the feature distillation category. This approach utilizes a novel two-branch network and incorporates self-KD during training to generate powerful representations, prevent overfitting, and enhance overall performance.\n\nIn parallel, Hu et al. [124] contribute to the field with a variational self-distillation network designed for RS scene classification. This method hierarchically distills class entanglement information from deep to shallow layers, further illustrating the application of feature distillation by refining and transferring feature information across different network layers. Expanding on these ideas, Xing et al. [125] present a collaborative consistent KD method aimed at improving classification accuracy for RS image scenes on embedded devices. Their approach emphasizes feature distillation across multiple network branches, focusing on reducing parameter redundancy and enhancing model efficiency, thus reinforcing the relevance of feature distillation in RS applications.# 4.4. Varying the Structural Relationship of Network Layers# 4.4.1. Layer-to-Layer Distillation\n\nLayer-to-layer distillation refers to the process where the teacher model\u2019s intermediate layers directly guide the corresponding layers of the student model. This method ensures that the student model learns similar feature representations as the teacher model at different stages of its depth [126, 127].\n\nDirect Mapping: In this approach, each layer of the teacher model is aligned with the corresponding layer in the student model. The outputs of each intermediate layer in the teacher model are used as targets for the corresponding layer in the student model. This direct mapping can help the student model learn hierarchical features similar to those learned by the teacher model [128].\n\nFeature Representation: By mimicking the intermediate representations of the teacher, the student model can capture complex features and patterns, which might be difficult to learn solely from the final output. This method is particularly useful when the student model has a similar or reduced architecture compared to the teacher.\n\nLoss Function: Often, additional loss terms are introduced to minimize the difference between the teacher\u2019s and student\u2019s intermediate layer outputs. This can include mean squared error (MSE) or other similarity measures.\n\nSuppose a deep CNN is used as the teacher model with layers: \ud835\udc471, \ud835\udc472, \ud835\udc473, \u2026 , \ud835\udc47\ud835\udc5b. The student model has corresponding layers: \ud835\udc461, \ud835\udc462, \ud835\udc463, \u2026 , \ud835\udc46\ud835\udc5b. During training, the output of \ud835\udc471 will guide \ud835\udc461, \ud835\udc472 will guide \ud835\udc462, and so on, ensuring that each student layer learns to mimic the feature maps of the corresponding teacher layer.# 4.4.2. Cross-Layer Distillation\n\nCross-layer distillation refers to the process where the teacher and student models do not have a direct correspondence between layers. Instead, the knowledge transfer happens between non-matching layers, for example, higher layers of the teacher model guiding lower layers of the student model or vice versa.\n\nNon-Matching Layers: In this approach, there is no strict one-to-one correspondence between the layers of the teacher and the student. The knowledge from higher (more abstract) layers of the teacher model can be distilled into lower (more detailed) layers of the student model, allowing for flexible guidance. Chen et al. [129] propose Semantic Calibration for Cross-layer Knowledge Distillation (Sem-CKD), which automatically assigns target layers from a teacher model to each student layer using an attention mechanism. This method allows student layers to distill knowledge from multiple teacher layers rather than following a fixed, one-to-one correspondence. Building on this concept, Wang et al. [130] further refine the idea of non-matching layers by using a learned attention distribution to assign appropriate teacher layers to student layers, thereby enhancing cross-layer supervision and subsequently improving student model performance. In addition, Nath et al. [131] introduce Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), which searches for the best teacher layer to supervise each student layer, thus allowing for non-matching layer associations that enhance robustness in neural architectures. Furthermore, Zhao et al. [132] develop Cross-Architecture Knowledge Distillation (CAKD), where non-matching layers are utilized to transfer knowledge from a Transformer-based teacher model to a CNN-based student model, involving the alignment of pixel-wise spatial information across different architectures and expanding the applicability of non-matching layers in cross-architecture scenarios.\n\nLayer Interaction: This method leverages the hierarchical nature of neural networks, where different layers capture different levels of abstraction. By using high-level features from the teacher to guide the student\u2019s learning process, the student can gain a richer understanding of the data. Yao et al. [133] propose Dense Cross-layer Mutual-distillation (DCM), which involves layer interaction by integrating auxiliary classifiers and bidirectional knowledge distillation operations across different layers of the teacher and student models, thereby enhancing knowledge representation and performance. Building on this concept, Su et al. [134] present Deep Cross-layer Collaborative Learning (DCCL), focusing on layer interaction through intermediate cross-layer supervision among peer student models, which integrates features from different layers to enhance representation and learning outcomes. Similarly, Zhu et al. [135] introduce Cross-layer Fusion for Knowledge Distillation (CFKD), which aggregates features from both teacher and student models, allowing for rich layer interactions that further enhance the student model\u2019s learning process. In a related effort, Hu et al. [136] propose an online knowledge distillation method with layer-level feature fusion modules that connect sub-networks, thereby facilitating mutual learning through enhanced layer interaction among student networks. Expanding on the concept, Nguyen et al. [137]# Application of Knowledge Distillation in Remote Sensing# Summary of Studies on KD in RS\n\n|Ref.|Model Used|Main Contribution|Database|Task/Appl.|Best Performance Value|Limitation|\n|---|---|---|---|---|---|---|\n|[85]|Small and shallow student models|Introduced a KD framework for scene classification.|AID, NWPU-RESISC, EuroSAT|Scene Classification|Increased accuracy by 1% to 5%|Performance on small and unbalanced datasets|\n|[88]|Lightweight object detector|Developed ARSD to enhance detection capability through feature and regression distillation.|DOTA, DIOR, NWPU VHR|Object Detection|Outperforms SOTA methods|Noise in training due to complicated backgrounds|\n|[96]|Consistency and dependence-guided model (CDKD)|Improved object detection with structured discriminative modules and consistency techniques.|RSOD|Object Detection|92% mean average precision|High model volume and computation in RS images|\n|[87]|Incremental learning model with FPN|Employed feature pyramid and KD for incremental learning in object detection.|Various RS datasets|Object Detection|Comparative performance to SOTA|Challenges with object size diversity and directions|\n|[105]|Dynamic KD (DKD)|Developed a dynamic KD framework to improve model performance on edge devices.|DOTA, NWPU VHR-10|Object Detection|SOTA|Complex model deployment on low-computation devices|\n|[122]|GAGNet with KD|Utilized graph attention and dense fusion for semantic segmentation.|Potsdam, Vaihingen|Semantic Segmentation|Excellent performance on datasets|Resource-intensive model deployment|\n|[123]|RS-SSKD for few-shot classification|Introduced a two-branch network with self-KD for few-shot classification.|NWPU-RESISC45, RSD46-WHU|Scene Classification|Surpasses current SOTA|Requires high model adaptability to new data|\n|[97]|Instance-aware distillation (InsDist)|Combined feature-based and relation-based KD for object detection.|DIOR, DOTA|Object Detection|Noticeable gains over other methods|Integration complexity with existing detectors|\n|[124]|Variational self-distillation network (VSDNet)|Implemented a VKT module for robust and end-to-end optimization.|Multiple RS datasets|Scene Classification|Significant improvement over backbones|Managing uncertainty and perturbation in images|\n|[125]|Collaborative consistent KD (CKD)|Designed a KD method for high classification accuracy on embedded devices.|SIRI-WHU, NWPU-RESISC45|Scene Classification|0.943 and 0.916 average accuracy on devices|Redundancy and parameter management|\n|[89]|DKD Model with DA and SS|Dual KD with dual attention and spatial structure modules|AID, NWPU-45|Scene Classification|Improved accuracy by 7.57% and 7.28%|Model complexity and computational cost|\n|[83]|SRAL Framework|Super-resolution-assisted learning for salient object detection|Multiple datasets|Object Detection in RSIs|Superior to 20+ algorithms|High computational cost of high-resolution processing|\n|[90]|Oriented R-CNN, CF-ORNet|Two-stage fine-grained object recognition with KD|VEDAI, HRSC2016|Object Recognition in HR-RSIs|Competitive performance|Limited by size of geospatial objects|\n|[98]|SSKDNet|Self-supervised KD network for feature learning|Multiple datasets|Scene Classification|Effective feature extraction|Difficulty in training self-supervised networks|\n|[106]|KD-MSANet|Lightweight semantic segmentation with multiscale pooling and attention|Vaihingen, Potsdam|Semantic Segmentation|Accuracy near 99.30% of teacher model|Reduced model size might impact some complex scene parsing|\n|[107]|CDKD Method|Change detection with prototypical contrastive and channel-spatial-normalized distillation|Public CD datasets|Change Detection|Comparable to large models|Requires careful tuning of distillation parameters|\n|[102]|NLD Method|Noisy label distillation for robust training on noisy datasets|UC Merced Land-use, NWPU-RESISC45, AID|Scene Classification|Outperforms fine-tuning methods directly|Performance variability with noise levels|\n|[99]|DSCT Framework|Cross-model KD from CNNs and transformers for semantic segmentation|ISPRS Potsdam, Vaihingen, GID, LoveDA|Semantic Segmentation|Outperforms state-of-the-art KD methods|Complexity of integrating CNNs and transformers|\n|[91]|MS2RGB-KD|MS-to-RGB KD for scene classification using RGB images|EuroSAT|Scene Classification|Effective compared to KD baselines|Dependent on quality of MS teacher model|\n|[100]|MSTNet with MSKA|Dense prediction using multi-level semantic transfer and KD|Vaihingen, Potsdam|Dense Prediction in RSIs|Excellent performance with reduced parameters|Balancing between model complexity and performance|\n\nDevelop CLAFusion, a framework that employs cross-layer alignment for fusing neural networks with different numbers of layers, leveraging layer interaction to improve model accuracy and efficiency. Finally, Zhang et al. [138] propose Patch Aware Knowledge Distillation (PAKD), which emphasizes cross-layer patch alignment and interaction within and across instances, guiding the student\u2019s learning of multi-level information and further reinforcing the importance of layer interaction in knowledge distillation.\n\nHierarchical Guidance: Cross-layer distillation can help in scenarios where the student model is significantly smaller or has a different architecture compared to the teacher. It allows the student to learn abstract representations earlier in its layers. Imagine a teacher model with layers: \ud835\udc471, \ud835\udc472, \ud835\udc473, \u2026 , \ud835\udc47\ud835\udc5b, and a student model with layers: \ud835\udc461, \ud835\udc462, \ud835\udc463, \u2026 , \ud835\udc46\ud835\udc5a. In cross-layer distillation, \ud835\udc47\ud835\udc5b (the final layer of the teacher) might guide \ud835\udc463 (a middle layer of the student), \ud835\udc473 might guide \ud835\udc461, and so on, depending on the distillation strategy and the specific architecture of the models. In this regard, Zou et al. [139] develop CoCo DistillNet, which utilizes cross-layer correlations to guide the student model in learning abstract representations from a teacher model in the context of pathological image segmentation, thereby enhancing the student model\u2019s performance in resource-constrained environments. Building on this concept, Zou et al. [140] propose Graph Flow Distillation, a method that transfers cross-layer variations from a large teacher network to a compact student network.\n\nY. Himeur, et al.: Preprint submitted to Elsevier# Application of Knowledge Distillation in Remote Sensing# Figure 1: Applications of Knowledge Distillation in Remote Sensing# Figure 6: Applications of KD in RS.\n\nIn medical image segmentation, enabling the student model to learn from both high-level and low-level abstractions of the teacher. In a similar vein, Zhai et al. [141] introduce a method that uses the deepest feature maps from the teacher to guide the shallow layers of the student model, providing hierarchical guidance that effectively balances performance and efficiency. Furthermore, Guo et al. [142] propose Alig-nahead++, an online knowledge distillation framework for GNNs that transfers structure and feature information across layers, facilitating hierarchical guidance and significantly improving performance on edge devices. Together, these studies underscore the importance of hierarchical guidance in enhancing the efficiency and effectiveness of knowledge distillation across various architectures.# 5.1. Tasks\n\nAs previously described, KD has emerged as a transformative approach in RS, enabling the development of more efficient models that handle RS tasks with the same or even better performance across various applications. The main applications of KD is RS are depicted in Fig. 6.# 5.1.1. Image/Scene Classification\n\nIn the context of the classification of RS images/scenes, KD can be particularly beneficial. High-resolution satellite or hyperspectral images, which are rich in spatial and spectral information, can be computationally intensive to process online using large models. By employing KD, a large, powerful model (teacher) that has been trained on such images can pass on its learned representations and decision-making capabilities to a smaller, more efficient model (student). This allows the student model to achieve high classification accuracy while significantly reducing computational and storage requirements. Techniques such as spatial feature blurring can be incorporated to enhance the student\u2019s learning by making the training data more challenging, which helps in better generalization and improved classification performance. Various studies have been proposed in the literature to enhance RS image classification, focusing on KD, model efficiency, feature extraction, and handling noisy or incomplete data. Table 3 provides the main features of these works.\n\nBuilding on this, Xu et al. [118] propose a hyperspectral image classification method based on class-incremental learning to learn new land-cover types without forgetting the old ones. This method uses a KD strategy to recall information of old classes and a channel attention mechanism to effectively utilize spatial-spectral information, demonstrating high accuracy on three hyperspectral image datasets. Similarly, Chi et al. [92] introduce a self-supervised learning method with KD for HSI classification, termed SSKD, which generates soft labels for unlabeled samples by considering spatial and spectral distances. This method significantly improves classification accuracy on three public HSI datasets. In addition, Xing et al. [125] address the challenge of using large deep neural networks on embedded devices by proposing a collaborative consistent KD (CKD) method. This method reduces the number of redundant parameters and improves the classification accuracy when tested on the SIRI-WHU and NWPU-RESISC45 datasets. Furthermore, Chen et al. [85] focus on scene classification using a KD framework to improve the performance of smaller and shallower network models. Their method increases the overall accuracy when tested on AID, UCMerced, NWPU-RESISC, and EuroSAT datasets. Along similar lines, Song et al. [143] present ERKT-Net, an efficient and robust knowledge transfer network designed for lightweight yet accurate CNN classifiers, demonstrating superior accuracy and compactness on three RSI datasets. Likewise, Wu et al. [1] propose the TAKD method, which reduces background disturbance and improves the accuracy.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 15 of 50# Application of Knowledge Distillation in Remote Sensing# Comparison of KD-based RS Image/Scene Classification Studies\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance Value Achieved|Limitation|\n|---|---|---|---|---|---|\n|[118]|learning Class-incremental|PaviaU|KD with channel attention mechanism|99.91% OA|Bias towards new classes|\n|[125]|sistent KDCollaborative con-|SIRI-WHU, NWPU-RESISC45|Multi-branch fused redundant feature mapping|0.943 accuracy (SIRI-WHU)|Parameter redundancy|\n|[92]|Self-supervised learning with KD|Three HSI datasets|Adaptive generation of soft labels|7.09% improvement|Limited labeled samples|\n|[85]|KD framework|AID, UCMerced, NWPU-RESISC, EuroSAT|KD training method for small and shallow models|5% accuracy improvement (UCMerced)|Computationally expensive|\n|[143]|ERKT-Net|Three RSI datasets|Efficient and robust KD network|22.4% OA (NWPU45)|Slight accuracy sacrifice|\n|[98]|SSKDNet|AID|Self-supervised KD network|95.98% accuracy|Complex training|\n|[82]|Adversarial training|HSI datasets|Handling missing modalities with hallucination architecture|98.17% accuracy (Houston)|Modality dependency|\n|[123]|RS-SSKD|NWPU-RESISC45, RSD46-WHU|Few-shot classification with CAMs and KD|86.26% accuracy (NWPU-RESISC45)|Overfitting risk|\n|[102]|NLD|UC Merced, NWPU-RESISC45, AID|Handling noisy labels with end-to-end KD|99.08% accuracy (UC Merced)|Noisy data handling|\n|[73]|GKD framework|Dordogne study site|Handling data misalignment with privileged information|64.27% F-Measure|Incomplete coverage|\n|[75]|TWA distillation|LEVIR, SAR SSDD|Reducing background noise and feature disparities|95.4% AP50 (SAR SSDD)|Background interference|\n|[116]|ing Incremental learning|CIFAR100, RESISC45|Dual KD to prevent catastrophic forgetting|6.9% accuracy improvement|Stability-plasticity dilemma|\n|[2]|DKD with SFB module|Four HSI datasets|Spatial feature blurring for better KD|97.55% OA (Salinas)|Fixed receptive fields|\n\nof student models for RS scene classification on three benchmark datasets. Moreover, Ienco et al. [73] propose a Generalized KD (GKD) framework to manage information misalignment between training and test data, demonstrating improved classification results using radar and optical satellite image time series data. Similarly, Zhang et al. [102] address the challenge of noisy labels in RS image scene classification by proposing a noisy label distillation (NLD) method, which effectively distills knowledge from labels across a range of noise levels, achieving high accuracy on UC Merced Land-use, NWPU-RESISC45, and AID datasets.\n\nIn another approach, Zhao et al. [98] propose a self-supervised KD network (SSKDNet) that uses feature maps as supervision signals and dynamically fuses feature maps to extract discriminating features, showing excellent performance on three datasets. Furthermore, Yang et al. [75] introduce the TWA distillation method for RS object detection, reducing background information and addressing feature disparities, achieving superior performance on the LEVIR and SAR SSDD datasets. Additionally, Pande et al. [82] tackle the problem of missing modalities in RS image classification by proposing an adversarial training-driven hallucination architecture. This method shows that the student model can surpass the teacher model\u2019s performance on HSI datasets. In a similar vein, Yu et al. [116] propose a two-stage training method for incremental learning that includes dual KD to prevent catastrophic forgetting, improving accuracy on CIFAR100 and RESISC45 datasets. Finally, Xie et al. [2] introduce an improved decoupled KD (DKD) strategy for HSI classification using a spatial feature blurring (SFB) module, achieving high overall accuracy on the Salinas dataset.\n\nMoving forward, Zhang et al. [123] present RS-SSKD for few-shot RS scene classification, which uses Class Activation Maps (CAMs) and self-KD to generate powerful representations, achieving high accuracy on NWPU-RESISC45 and RSD46-WHU datasets. As the availability of airborne and satellite imagery increases, the challenge in RS (RS) scene classification has shifted from data scarcity to the lack of ground truth samples. Addressing these challenges, especially in unfamiliar environments with limited training data, few-shot classification offers a promising solution within meta-learning by extracting rich knowledge from minimal data. In [123], the authors introduce RS-SSKD, a method designed for few-shot RS scene classification that focuses on generating robust representations for downstream meta-learners. This approach features a two-branch network that uses three pairs of original-transformed images and incorporates Class Activation Maps (CAMs) to focus on the most relevant category-specific regions, ensuring the creation of discriminative embeddings. Additionally, a self-KD is applied to prevent overfitting and enhance performance (see Fig. 7).# 5.1.2. Object Detection\n\nIn RS, object detection is crucial for identifying specific features such as buildings, vehicles, and vegetation. KD helps in creating lightweight models that maintain high accuracy, making it feasible to run these models on devices with limited computational power. Several studies focus on the use of KD for improving object detection in RS images, each introducing innovative strategies to address specific challenges. Algorithm 1 outlines a process for KD in RS object detection. It starts by training a teacher model on a dataset, and then defines a student model with a simpler architecture. The teacher model generates soft targets, which are probability distributions over classes, using a softened softmax function. The student model is trained using a combined loss function that includes the cross-entropy loss and the Kullback-Leibler divergence between the teacher\u2019s and student\u2019s outputs. The process iterates over several epochs, optimizing the student model to mimic the teacher while also learning from the original labels. Finally, the trained student model is deployed. The main works on the use of KD in the object detection task in RS images and their main features are summarized in Table 4.\n\nFor instance, Yang et al. [88] propose an adaptive reinforcement supervision distillation (ARSD) framework to enhance lightweight object detectors. This method focuses# Application of Knowledge Distillation in Remote Sensing# SSKD embedding module\n\n|M-way classification|Knowledge distill module|y\u0302|\n|---|---|---|\n|Base dataset|task \ud835\udcafi|f\u03d5|\n|support set|f\u03d5|\u03bc|\n|query set|f\u03d5|f\u03d5 (x*)|\n|Category representation|Category representation|Similarity metric|\n|sampled tasks|sampled tasks|y\u0302*|\n\nFigure 7: The overall framework includes the SSKD module for embedding learning and the meta-learning module based on ProtoNets. The parameter \ud835\udefe is used to adjust cosine similarity in the meta-learning process. It aims at learning a powerful embedding, without any additional annotation effort that offers more discriminative representations to the downstream meta-learner. The meta-learning module is based on ProtoNets with an additional parameter \u03b3 to scale cosine similarity.\n\nOther studies address different aspects of RS. Zhang et al. [53] combine detection and tracking in a joint framework for small objects in complex backgrounds. Zhang et al. [105] introduce a dynamic KD (DKD) framework, leveraging dynamic global distillation and instance selection distillation to enhance object detection in cluttered scenes. Another study by Zhang et al. [144] presents Orientation Distillation (OD) to address issues with boundary discontinuity and spatial feature ossification for detecting arbitrary-oriented objects in RS images. The authors further propose an adaptive composite feature generation (ACFG) strategy to improve feature mapping and handling of foreground and background loss in object detection [145].\n\nFeng et al. [146] introduce an Instance-aware Distillation approach for Class-incremental Object Detection (ID-COD), which helps in preserving old class knowledge while learning new classes, thus mitigating catastrophic forgetting. Chen et al. [3] propose Discretized Position KD (DPKD), which focuses on transferring high-quality bounding box position and pose information to improve object detection performance. Pang et al. [4] present a pyramid KD (PKD) framework to handle the limitations of model compression, utilizing a hybrid online\u2013offline smooth distillation strategy to enhance recognition accuracy while avoiding knowledge explosion and offset.\n\nDu et al. [55] add a detection head specifically for small targets in the YOLOv5 model, proposing a network KD framework for improved small-scale target detection in RS images. Gao et al. [147] design a feature super-resolution fusion framework using cross-scale distillation to improve the detection accuracy of small objects by enhancing feature expression capability. Yang et al. [148] propose a weakly supervised object detection method using self-attention distillation and instance-aware mining to handle varying scales and dense object proximity in RS images.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 17 of 50# Application of Knowledge Distillation in Remote Sensing# Algorithm 1: KD for RS Object Detection\n\nInput: Training data \ud835\udc37, Teacher model \ud835\udc47, Student model architecture \ud835\udc46, Temperature \ud835\udc47\ud835\udc52\ud835\udc5a\ud835\udc5d, Loss weights \ud835\udefc, \ud835\udefd, Number of epochs \ud835\udc41\n\nOutput: Trained Student Model \ud835\udc46\n\n1. Train the Teacher Model\n\n\ud835\udc47 \u2190 TrainTeacherModel(\ud835\udc37, \ud835\udc47)\n2. Define the Student Model\n\n\ud835\udc46 \u2190 DefineStudentModel(\ud835\udc46)\n3. Compute the Soft Targets from the Teacher Model\n\nfor each batch (\ud835\udc65, \ud835\udc66) \u2208 \ud835\udc37 do\n\n&nbsp;&nbsp;&nbsp;\ud835\udc67\ud835\udc47 \u2190 \ud835\udc47 (\ud835\udc65)\n\n&nbsp;&nbsp;&nbsp;\ud835\udc5d\ud835\udc47 \u2190 Softmax(\ud835\udc67\ud835\udc47 \u2215\ud835\udc47\ud835\udc52\ud835\udc5a\ud835\udc5d)\n\nend\n4. Define the Loss Functions\n\n\ue238\ud835\udc36\ud835\udc38 \u2190 CrossEntropy(\ud835\udc46(\ud835\udc65), \ud835\udc66)\n\n\ue238\ud835\udc3e\ud835\udc37 \u2190 KLDiv(LogSoftmax(\ud835\udc46(\ud835\udc65)\u2215\ud835\udc47\ud835\udc52\ud835\udc5a\ud835\udc5d), \ud835\udc5d\ud835\udc47 ) \u00d7 \ud835\udc47 \ud835\udc52\ud835\udc5a\ud835\udc5d2\n\n\ue238\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59 \u2190 \ud835\udefc\ue238\ud835\udc36\ud835\udc38 + \ud835\udefd\ue238\ud835\udc3e\ud835\udc37\n5. Train the Student Model\n\nfor epoch = 1 to \ud835\udc41 do\n\n&nbsp;&nbsp;&nbsp;for each batch (\ud835\udc65, \ud835\udc66) \u2208 \ud835\udc37 do\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\ud835\udc5d\ud835\udc47 \u2190 ComputeSoftTargets(\ud835\udc47 , \ud835\udc65, \ud835\udc47\ud835\udc52\ud835\udc5a\ud835\udc5d)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\ud835\udc67\ud835\udc46 \u2190 \ud835\udc46(\ud835\udc65)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\ue238 \u2190 ComputeLoss(\ud835\udc67\ud835\udc46 , \ud835\udc66, \ud835\udc5d\ud835\udc47 , \ud835\udefc, \ud835\udefd, \ud835\udc47\ud835\udc52\ud835\udc5a\ud835\udc5d)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update \ud835\udc46 by minimizing \ue238\n\n&nbsp;&nbsp;&nbsp;end\n\nend\n6. Deploy the Student Model\n\n\ud835\udc46 \u2190 Trained Student Model\n\nOn the other hand, traditional KD-based object detection methods have limitations, such as ignoring crucial background information and focusing solely on global context. To overcome these issues, Attention-based Feature Distillation (AFD) is proposed in [159], which distills both local and global information. AFD enhances local distillation with a multi-instance attention mechanism and reconstructs pixel relationships, resulting in state-of-the-art performance in object detection while remaining efficient. Fig. 8 illustrates the architecture of the proposed Attention-based Feature Distillation (AFD) method. This framework improves upon traditional KD-based object detection by incorporating both local and global information from the teacher network. The multi-instance attention mechanism within AFD allows the model to distinguish between background and foreground elements effectively. Additionally, the method reconstructs pixel relationships, ensuring that both local details and broader context are accurately transferred from the teacher to the student detector, resulting in enhanced detection performance.# 5.1.3. Semantic Segmentation\n\nKD is beneficial for semantic segmentation in RS applications, which involves classifying each pixel in an image into predefined categories. The teacher model is first trained on the segmentation task using high-resolution RS data [160]. Due to its complexity and larger capacity, it can learn intricate patterns and detailed features from the data. Once trained, the teacher model\u2019s predictions, along with its internal representations, are used to guide the training of the student model. The student model, being smaller and more efficient, aims to mimic the performance of the teacher model while maintaining lower computational costs and faster inference times [161]. Moreover, KD is particularly advantageous because it allows for the deployment of effective semantic segmentation models on edge devices or in scenarios with limited computational resources [162]. By leveraging the distilled knowledge from the teacher model, the student model can achieve high segmentation accuracy despite its reduced size. This is crucial for applications such as real-time environmental monitoring, disaster response, and agricultural analysis, where timely and accurate segmentation of satellite or aerial imagery is needed. The distillation process also helps the student model generalize better to new and unseen data, enhancing its robustness and reliability in diverse RS tasks [163].\n\nThe studies on semantic segmentation in RS show significant advancements but also face several limitations. For instance, Gao et al. [164] introduced the FoMA framework, which significantly improves segmentation performance by leveraging foundation models, but it struggles with data scarcity in novel classes and balancing segmentation performance across classes. Similarly, Zhou et al. [122] proposed a lightweight student network (GAGNet-S*) with KD that achieves excellent segmentation performance but faces challenges related to scalability and complexity in deployment on resource-limited equipment. Dong et al. [99] addressed the limitations of CNNs and transformers by proposing the DSCT framework, which enhances segmentation performance through cross-model KD. However, this approach requires high computational complexity and massive data resources.\n\nStudies focusing on KD methods, such as the MGSAD by Zhang et al. [165], and MTKD by Li et al. [166] with MTKD, contribute innovative techniques but encounter challenges such as the need for extensive computation and handling of domain shifts. Liu et al. [167] proposed a three-stage UDA method that shows better performance but relies heavily on large-scale annotated data and struggles with domain shift handling. Similarly, Shi et al. [168] introduced DSANet, which effectively handles spatial and semantic feature enhancement but reduces the model\u2019s characterization ability for these features.\n\nIncremental learning and domain adaptation are other areas where significant contributions have been made but also face limitations. Rong et al. [169] proposed a generalized framework for CSS but struggled with the challenge of old classes collapsing into the background. Rui et al. [170] and Le et al. [171] focused on incremental learning methods but faced high computational costs and the complexity of adapting to incremental domains and partial multi-task learning, respectively. Shan et al. [172, 173] developed class-incremental segmentation methods that address catastrophic forgetting but require balancing old and new class learning and managing feature generation complexity. Li [174] proposed DSSN with weakly-supervised constraints to handle cross-domain segmentation, but the method heavily depends on labeled data and struggles with geographic variation.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 18 of 50# Application of Knowledge Distillation in Remote Sensing# Comparison of Studies on KD-based Object Detection in RS Imagery.\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance|Limitation|\n|---|---|---|---|---|---|\n|[88]|ARSD framework|DOTA, DIOR, NWPU VHR-10|Adaptive reinforcement supervision distillation for lightweight object detection|Outperforms SOTA methods|High complexity due to adaptive modules|\n|[105]|DKD framework|DOTA, NWPU VHR-10|Dynamic KD for multi-scale feature imitation|Suitable for various detectors|Potential overfitting to specific datasets|\n|[144]|Orientation Distillation (OD)|Multiple datasets|Anti-ambiguous location prediction and feature calibration|Improved performance on non-axially arranged objects|Limited accuracy in complex scenes|\n|[145]|ACFG strategy|DIOR, DOTA|Adaptive composite feature generation for KD|Better performance than SOTA KD algorithms|Complexity in composite mask generation|\n|[146]|IDCOD|DOTA, DIOR, RT-DOD, PASCAL VOC|Instance-aware distillation for class-incremental detection|mAP@0.5 of 74.0% on DIOR|Challenge in handling new classes post-deployment|\n|[3]|DPKD|DOTA, HRSID|Discretized position KD for object detection|mAP of 79.82% on DOTA|Overlooks certain localization knowledge|\n|[4]|PKD framework|Aircraft, FGSC-23|Pyramid KD to avoid knowledge explosion and offset|Effective with ResNet and VGG networks|Complexity in finding optimal configuration|\n|[55]|Enhanced YOLOv5|NWPU VHR-10|KD framework for small-scale target detection|Detection accuracy of 43.9%|High computational cost|\n|[147]|SSRFPN with CSD|NWPU VHR-10, DIOR|Feature super-resolution fusion for small object detection|AP0.5 of 95.0% on NWPU VHR-10|Difficulty in feature extraction for very small objects|\n|[148]|WSOD with SAD and IAM|NWPU VHR-10, DIOR|Weakly supervised learning for object detection|Accurate bounding boxes|Struggles with varying scales and dense objects|\n|[53]|OKD-JDT|JiLin-1|Joint detection and tracking framework|State-of-the-art performance|Limited to certain types of satellite videos|\n|[149]|MGFAFNET|SyluDrone|Efficient detection method for UAV platforms|AP of 52.7%, AP50 of 93.6% on SyluDrone|Balancing detection speed and accuracy|\n|[150]|DC-KD|xView|Distillation scheme for object detection in satellite images|3.88% mAP50 improvement on xView|Data distribution differences|\n|[151]|HMKD-Net|Multiple datasets|Hybrid-model KD with CNN-ViT ensemble|Max accuracy improvement of 22.8%|Handling variances during KD|\n|[152]|Visual knowledge-oriented WSOD|NWPU VHR-10, DIOR|Leveraging visual cues as pseudo labels|mAP of 84.25% on NWPU VHR-10|Handling noise in object proposals|\n|[153]|WSA-GAN, BGNet|Various RS datasets|Multitask learning for image translation and saliency detection|Outperforms other approaches|Complexity in multimodal context learning|\n|[154]|TDKD-Net|Various RS datasets|Tensor decomposition and KD for UAV detection|High generalization and robustness|Handling imbalanced issues|\n|[155]|Coarse-to-fine network|VisDrone, UAVDT|Density-aware scale adaptation for small object detection|Superior detection in UAV images|Issues with scale variation|\n|[156]|Self-distillation YOLO|KITTI|Multi-scale self-distillation for object detection|2.8% accuracy improvement|Inefficiencies in knowledge transfer|\n|[157]|DTCNet|AID|Distillation Transform-CNN for super-resolution|PSNR of 28.73 dB, SSIM of 0.7904|High model complexity|\n|[158]|TGN with KMDN and CDTG|DIOR, FGSC-23, DOTA|Text-guided tail-class generation for long-tailed distribution|Superior performance on tail classes|Data distribution imbalance|\n\nLastly, Guo et al. [175] and Cao et al. [176] proposed methods to balance effectiveness and compactness in segmentation models, but they face high computational demands and challenges in handling noise and redundant features. Zhou et al. [119] introduced GSGNet with high inference speed but had to balance this with contextual reasoning capabilities. Bai et al. [177] and Wang et al. [178] focused on domain adaptation, but they faced difficulties in aligning high-dimensional image representations and managing intermediate domain learning. Michieli et al. [179] addressed incremental learning with various KD techniques but struggled with catastrophic forgetting and internal feature representation complexity. Lastly, Pena et al. [180] introduced DeepAqua for water detection, which improves segmentation accuracy but lacks specific details on datasets and segmentation scenarios. Table 5 provides a summary of the works that use KD for semantic segmentation of RS images.\n\nBesides, [164] produces a Foundation Model Assisted (FoMA) for Generalized Few-Shot Semantic Segmentation (GFSS) in RS images, aimed at improving segmentation performance under data scarcity conditions. FoMA leverages foundation models through three strategies: Support Label Enrichment (SLE) to enhance support labels, Distillation of General Knowledge (DGK) to transfer generalizable knowledge, and Voting Fusion of Experts (VFE) to combine zero-shot and few-shot predictions. The method demonstrates state-of-the-art performance on the OpenEarthMap few-shot challenge dataset. Fig. 9 illustrates the architecture of the FoMA framework, which effectively integrates a vision-language foundation model\u2019s general knowledge into the GFSS task for RS images.# 5.2.1. Land Cover Classification\n\nKD improves the classification of land cover types by refining the feature extraction capabilities of student models. This leads to better segmentation and classification of different land cover types, essential for environmental monitoring and urban planning. Several studies have proposed innovative methods to improve land cover classification and other RS tasks using KD and multimodal data fusion. For example, Xu et al. [117] developed a two-branch patch-based CNN with an encoder-decoder (ED) module to fuse multimodal RS (RS) data. They introduced a KD in model (DIM) module for better multimodal data fusion and a cross-model (DCM) module to enhance single-modal classification using multimodal knowledge. Their approach demonstrated superior performance on hyperspectral (HS) and light detection and ranging (LiDAR) data as well as HS and synthetic aperture radar (SAR) data. Fig. 10 depicts the approach proposed in [117]. Wang et al. [182] proposed the cross-modal graph knowledge representation and distillation learning (CGKR-DL) framework, which combines CNN and graph convolutional network (GCN) to enhance land cover classification. Their method addresses the limitations of\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 19 of 50# Application of Knowledge Distillation in Remote Sensing# IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 61, 2023# Fig. 2. Architecture of our KD method.\n\nThe enhancement of AFD is based on three points. Our new KD approach distills both local and global information from the teacher network. For local distillation, a multi-instance attention mechanism is proposed to identify the background from the foreground. Attention Figure 8: The KD architecture enhances AFD through three key advancements [159]. Firstly, the method extracts both local and global information from the teacher network. For local distillation, a multi-instance attention mechanism is introduced to effectively distinguish foreground elements from the background. Secondly, the approach reconstructs the relationships between different pixels, facilitating a more comprehensive transfer of knowledge from the teacher to the student detector through both traditional CNN-based cross-modal distillation methods and local and global distillation strategies.# Equations\n\nWe can write the local channel and spatial masks (Lch and Lsp) as follows:\n\nLch, p = Mcha( f pT) + Mcha( f pS)\n\nLch = \u2297(Lch,1, Lch,2, . . . , Lch,P)\n\nLsp, p = Mspa( f pT) + Mspa( f pS)\n\nLsp = \u2297(Lsp,1, Lsp,2, . . . , Lsp,P)# The Generalized KD (GKD) framework\n\nhas been introduced by Ienco et al. [73] to handle data misalignment between training and test phases. Their method, applied to radar and optical satellite image time series data, improved land use land cover mapping, especially for agricultural classes. A multimodal online KD (MMOKD) framework significantly improves performance on various multimodal RS datasets. Kanagavelu et al. [185] and Gbodjo et al. [186] explored federated learning and multisensor data integration, respectively, to enhance land cover mapping and monitoring. The former work federated teacher-student structure for better generalizability and performance in land cover classification. The latter developed a self-distillation strategy within a CNN framework to combine multitemporal SAR and optical data for improved land cover classification.# Table 6\n\nThe works that use KD for the classification of land cover and their main characteristics are summarized in Table 6.# Fig. 3.\n\nActivations for the input image before and after normalization. This process fills the gap between the patterns of the teacher detector and the student detector, providing a more effective and smoother transfer of knowledge.\n\nTo advance this field, a two-branch, patch-based CNN with an encoder-decoder (ED) module for effective multimodal data fusion is proposed in [117]. Typically, a KD in model (DIM) module to guide cth channel in a batch of FPN outputs; therefore, we can obtain the normalized values from the teacher T and the student S.# Normalization Process\n\nIt is also important that the normalization follows the convolution property, to ensure that features are normalized uniformly at different regions of the feature map. Let V represent the whole set of feature map values that include the components of mini-batch and its spatial locations. Therefore, for a u-size mini-batch and h \u00d7 w-size feature maps, we take the functional.# Global Channel and Spatial Masks\n\nThe global channel and spatial masks (Gch and Gsp) can be written as follows:\n\nGch = Mcha(F T ) + Mcha(F S)# Application of Knowledge Distillation in Remote Sensing# Comparison of Various Studies on Semantic Segmentation in RS\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance Value Achieved|Limitation(s)|\n|---|---|---|---|---|---|\n|[164]|FoMA Framework|OpenEarthMap|Introduces GFSS with three strategies: SLE, DGK, and VFE for improved segmentation|Improvement of 28.94% in segmentation performance, with 31.79% for novel classes and 24.64% for base classes|Data scarcity in novel classes and complex balancing in segmentation performance|\n|[122]|GAGNet-S* (with KD)|Potsdam, Vaihingen|Proposes a lightweight student network framework with KD|Achieved excellent segmentation performance on Potsdam and Vaihingen datasets|Scalability and complexity in deployment on resource-limited equipment|\n|[99]|DSCT Framework|ISPRS Potsdam, Vaihingen, GID, LoveDA|Cross-model KD using CNNs and transformers|Outperforms state-of-the-art KD methods on four datasets|High computational complexity and massive data resource requirements|\n|[165]|MGSAD|Not specified|Proposes a multi-granularity semantic alignment distillation method for semantic segmentation|Not specified|Details on datasets and specific performance metrics are not provided|\n|[166]|MTKD|Not specified|Multi-task KD for weather-degraded image segmentation|Achieves 0.038 s in semantic segmentation for a 2048 \u00d7 1024 image|Specific performance values not provided, computation-intensive|\n|[167]|Covariance-based Channel Attention Module|ISPRS 2-D Semantic Labeling, Urban Drone Dataset (UDD)|Proposes three-stage UDA method with KD for RS images|Shows better performance compared with state-of-the-art methods|Domain shift handling and reliance on large-scale annotated data|\n|[168]|DSANet|ISPRS Potsdam, Vaihingen|Effective deep supervision-based attention network for RSIs|79.19% mIoU on Potsdam, 72.26% mIoU on Vaihingen with 470.07 FPS on 512 \u00d7 512 images|Reduces model characterization ability for spatial and semantic features|\n|[181]|Cross-modal KD|Not specified|Uses optical images to train a student model for SAR images through cross-modal KD|Increase of 5-20% IoU score compared to training from scratch|Small training datasets and complexity in cross-modal learning|\n|[169]|Generalized Framework for CSS|iSAID, GCSS|Proposes historical information-guided modules for CSS in RS images|Outperforms state-of-the-art methods in most incremental settings|Challenge of old classes collapsing into the background|\n|[170]|Domain-Incremental Learning|LoveDA-rural|Proposes domain-incremental learning for multi-source RS data|Achieves mIoU of 0.6233 on LoveDA-rural at step 5|High computational cost and complexity in incremental domain learning|\n|[171]|Partial Multi-Task Learning with KD|ISPRS 2D Semantic Labeling Contest|Enhances partial multi-task learning performance using KD|mIoU of 68.97% on Vaihingen dataset|Lack of all-task annotations and reliance on soft labels|\n|[172]|DFD and LM Modules|Aerial images dataset|Proposes class-incremental segmentation method without old data storage|6.2% and 15% mIoU gains from DFD and LM modules respectively|Catastrophic forgetting and balancing old and new class learning|\n|[173]|PFG and TKD Modules|Not specified|Effective class-incremental segmentation framework without storing old data|More than 4.5% gains compared with state-of-the-art methods|Limited detail on dataset performance and complexity in feature generation|\n|[174]|DSSN with Weakly-Supervised Constraints|Not specified|Proposes DSSN for cross-domain RS image segmentation|Mean F1Score: 60.76%, Mean IoU: 44.53%|High dependency on labeled data and difficulty in geographic variation handling|\n|[175]|CLNet-T and CLNet-S (with KD)|MFNet, PST900|Proposes a balance between effectiveness and compactness using KD|MFNet: mAcc 76.6%, mIoU 58.2%; PST900: mAcc 95.59%, mIoU 80.77%|High computational demands and complexity in terminal device deployment|\n|[176]|C3Net with Multi-Level KD|ISPRS Vaihingen|Proposes efficient C3Net for multi-modal data semantic segmentation|Overall Accuracy: 91.3%, High mean F1 score for car class|Noise and redundant feature handling and high running time|\n|[119]|GSGNet with KD|Vaihingen, Potsdam|Proposes GSGNet for ORSI scenario analysis with high inference speed|Outperforms most advanced methods with 19.61 M parameters|Balancing high inference speed and contextual reasoning capability|\n|[177]|Contrastive and Adversarial Learning|Not specified|Proposes a model for domain adaptation in representation space and spatial layout|Not specified|Specific performance values and dataset details not provided|\n|[178]|TDARS|Three domain adaptation datasets|Proposes transitive domain adaptation for RS images|Effectively handles domain shift problem compared to other methods|High complexity in intermediate domain learning and transfer|\n|[179]|Various KD Techniques|Pascal VOC2012, MSRC-v2|Proposes incremental learning for semantic segmentation with KD|Highest Accuracy: 97.5% (Abisoye et al. 2024), Lowest Error: 0.032 MAE (De 2024)|Catastrophic forgetting and complexity in internal feature representation handling|\n|[180]|DeepAqua|Not specified|Proposes an unsupervised method for water detection in RS|Improves accuracy by 3%, IoU by 11%, F1-score by 6%|Specific details on datasets and segmentation scenarios not provided|# 5.2.2. Precision Agriculture\n\nKD is crucial in smart agriculture as it allows for the development of lightweight models that maintain high accuracy while being deployable on resource-constrained edge devices, such as drones or sensors. This is particularly important for precision agriculture tasks like early weed detection and crop monitoring, where efficient and accurate models are needed for real-time decision-making. By transferring knowledge from larger, more complex models to smaller ones, KD helps optimize these tasks, enhancing agricultural.\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 21 of 50# Application of Knowledge Distillation in Remote Sensing# Tunable\n\nA photo of a [cls] \"A photo of a [cls] \"# Vision-Language Foundation Model (VLFM)\n\n|RSI|DGK|SLE|\n|---|---|---|\n|Prediction|Voting|Voting Fusion|\n|Backbone|Pixel|Classifier Base|\n|Encoder|Decoder|Classifier Novel|\n|Update|Self-supervised Optimization|Shared GFSS Learner|# Text Prompt\n\nFoundation model as labeler (a) The FoMA GFSS framework Enriched Label Tert Foundation model as teacher\n\nVLFM# (c) DGK\n\nFigure 2. The overall architecture of the proposed Method. This approach introduces the general knowledge of the vision-language foundation model learned from natural images into the remote sensing image GFSS task through two modules: SLE integrates the foundation model\u2019s results on support images as pseudo-labels into the GFSS learner\u2019s training process. Concurrently, DGK distills the exceptional performance achieved by the foundation model on novel classes from query images into the GFSS learner. Furthermore, a voting fusion strategy is used to effectively merge the results of the foundation model and the GFSS learner across all classes, ensuring more accurate prediction results for the model.\n\nThese efforts focus on enhancing model efficiency and accuracy in tasks such as crop monitoring, weed detection, and resource management. For instance, Liangde et al. [199] develop a model distillation approach to enhance agricultural named entity recognition, leveraging a BERT-based model enhanced by BiLSTM and CRF for precise entity detection from a constructed agriculture knowledge graph. Ghofrani and Mahdian Toroghi [200] focus on plant disease detection, using a KD approach to enable smaller CNN architectures, like MobileNet, to achieve near high-end model accuracy on the Plantvillage dataset. On the same line, Hu et al. [201] and Dong et al. [202] address crop disease detection. The former approach optimizes YOLOv5s for maize disease detection, while the latter uses ECA-KDNet for efficient apple disease diagnosis on mobile devices. Finally, Huang et al. [203] develop multistage KD to create lightweight models for diagnosing multiple crop diseases effectively.\n\nIn the task of image segmentation, Angarano et al. [204] introduce a method for robust crop segmentation using KD, aimed at improving the generalization across different environmental conditions for robotic field management. Similarly, Li et al. [205] employ KD for efficient panoptic segmentation, creating lightweight networks capable of detailed scene understanding at high speeds, whereas Jung et al. [206] improve plant leaf segmentation using KD to maintain high-quality instance segmentation. The work of Pag\u00e9-Fortin [207] investigates class-incremental learning methods to address the challenge of learning new plant species and diseases incrementally, focusing on mitigating catastrophic forgetting.# 3.2.1 Support Label Enrichment\n\nGiven a support image I, in our challenge setting, only one novel class is labeled. It indicates that even though some other novel classes appear in the image I, the pixels are also labeled as background, making it semantic ambiguous during the training of the novel classifier. Therefore, we aim to leverage off-the-shelf foundation models to enrich the information of current limited support labels. We name it Support Label Enrichment (SLE).\n\nTo obtain accurate pseudo labels, we utilize off-the-shelf lightweight semantic segmentation model for identifying grape picking points, enhancing the picking efficiency in vineyard environments, whereas Hollard and Mohimont [209] apply KD to enhance grapevine detection for early yield prediction, focusing on lightweight model deployment for embedded devices. As far as it concerns disease detection, Musa et al. [210] propose a low-power DL model for detecting plant diseases in hydroponic systems, aiming at efficiency and reduced resource consumption and Zhang and Wang [211] improve plant leaf disease recognition using a.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 22 of 50# Application of Knowledge Distillation in Remote Sensing# Comparison of Studies on KD-based Land Cover Classification\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|\n|---|---|---|---|\n|[117]|Two-branch patch-based CNN with ED and DIM modules|Hyperspectral (HS) and LiDAR data (Houston2013)|Developed a KD in model (DIM) and cross-model (DCM) module for better LC classification|\n|[182]|CGKR-DL framework with CNN and GCN|HS-LiDAR, SAR, HS-SAR-DSM datasets|Proposed cross-modal graph knowledge representation and distillation learning|\n|[73]|Generalized KD (GKD) framework|Radar (Sentinel-1) and optical (Sentinel-2) SITS|Managed information misalignment between training and test data|\n|[11]|MMOKD framework|Optical and SAR images|Developed multimodal online KD framework for land use/cover classification|\n|[185]|Federated UNet model with KD|Satellite and street view images|Improved efficiency and privacy of real-time climate tracking|\n|[183]|DH-ADNet with MSIS|Coregistered optical and SAR datasets|Introduced dynamic-hierarchical attention distillation for land cover classification|\n|[54]|KD-MSI with CAMs|WHU-CD, DSIFN-CD, LEVIR-CD datasets|Weakly supervised change detection using KD|\n|[184]|Transfer learning framework with CMD and high-temperature softmax|Various RS datasets|Improved land cover classification using teacher-student structure|\n|[187]|DAGDNet with IG-FGM and MS-ADL|Coregistered optical and SAR datasets|Efficient dense adaptive grouping distillation network for MLCC|\n|[186]|Patch-based multibranch CNN|Multitemporal SAR/optical data|Integrated multisensor RS data using self-distillation strategy|\n|[188]|Hallucination network with KD|PAN-MS image pairs, hyperspectral dataset|Provided robust solution for missing modalities using hallucination module|\n|[189]|CloudSeg framework with multi-task learning|M3M-CR, WHU-OPT-SAR datasets|Addressed semantic segmentation under cloud cover using KD|\n|[190]|Segment Anything (SAM) model|Planetary images|Rapid annotation for geological mapping using KD|\n|[191]|Distill and refine strategy with CNN|Sentinel-1 data|Addressed spatial transfer challenge for mapping irrigated areas|\n|[192]|Lightweight model with KD|UC Merced Land Use dataset|High accuracy and efficiency for RS image retrieval|\n|[193]|MRF-NAS with self-training UDA|OpenEarthMap, FLAIR #1 datasets|Lightweight neural networks for UDA in RS|\n|[194]|Cross-modal distillation framework|Sen1Floods11 dataset|Improved flood detection with cross-modal distillation|\n|[195]|GCPNet with GCN and ASPM|Various satellite datasets|Enhanced pansharpening using GCN and KD|\n|[196]|Domain knowledge-guided self-supervised learning|Onera Satellite Change Detection dataset|Improved unsupervised change detection using domain knowledge|\n|[197]|VGG13 (teacher), ResNet8 (student)|SMAP satellite data|Improved soil moisture prediction using KD|\n|[198]|LSAW with adaptive weights|CCF, Potsdam, Vaihingen datasets|Addressed catastrophic forgetting in incremental learning|# Best Performance Value Achieved\n\n|Best Performance Value|Limitation|\n|---|---|\n|Improved LC classification performance on two multimodal RS datasets|The study mainly focuses on LC classification; does not cover other RS applications|\n|Significant improvement in land cover classification accuracy|Focuses on classification; not on other types of RS tasks|\n|Accuracy: 65.01%, F-Measure: 64.27%, Kappa: 0.5775|Limited to cases where radar data is always available|\n|Outperformed other networks in both full- and missing-modality scenarios|Large semantic gap between modalities poses a challenge|\n|Accuracy above 95%|Focus on semantic segmentation, not other RS tasks|\n|State-of-the-art results in the privileged information scenario|Limited to privileged information scenarios|\n|F1-score: 0.854 on WHU-CD|Focuses on change detection; not applicable to other RS tasks|\n|Average increase in mIoU: 9.9%, 2.1%, 4.3%|Requires large datasets for teacher model training|\n|Superior performances on representative datasets|Limited to scenarios with privileged modality|\n|Accuracy: 94% (Reunion island), 88% (Dordogne)|Requires sparsely annotated ground-truth data|\n|Overall accuracy: 97.01%|Focused on scene recognition and image classification|\n|mIoU improvement: 3.16% (M3M-CR), 5.56% (WHU-OPT-SAR)|Focuses on cloudy conditions; not applicable to cloud-free scenarios|\n|Comparable to state-of-the-art on mapping planetary skylights|Limited to geological mapping tasks|\n|Best performance in spatial transferability|Focused on spatial transfer; not on other RS tasks|\n|mAP: 0.9680 with 3.8M parameters|Limited to image retrieval tasks|\n|mIoU: 59.38% (OpenEarthMap), 51.19% (FLAIR #1)|Focus on UDA; not on other RS tasks|\n|IoU improvement: 6.53% on test split|Limited to flood detection; not other RS tasks|\n|Outperformed state-of-the-art visually and quantitatively|Limited to pansharpening tasks|\n|Kap: 53.34%, F1: 55.69%|Focused on change detection; not other RS tasks|\n|High prediction accuracy with efficient student model|Focused on soil moisture prediction; not other RS tasks|\n|Best datasets results on three novel data augmentation-based KD framework, enhancing recognition accuracy in natural environments.|Focus on incremental learning; not other RS tasks|\n\nMore advanced and complex tasks have also been addressed using KD. In the aquaculture domain, Yin et al. [212] propose a novel fish individual recognition method using KD within a vision transformer framework, improving accuracy. In the same application domain, Li et al. [213] focus on underwater fish species classification using a novel two-tier KD method to enhance model accuracy and reduce computational demands. Back to the plants and trees images, Yang et al. [214] developed a fast pest detection algorithm using lightweight feature extraction and KD to enhance performance on edge devices and Wu et al. [215] presented Deep BarkID, a lightweight CNN for tree species identification from bark images, tailored for use in forest environments with limited computing resources. Finally, Yamamoto [216] utilized CNNs to distill crop models to accelerate understanding of plant physiology, applying DL to evaluate environmental impact on grain yield.\n\nResearchers have also contributed to the development and deployment of lightweight student models on low capacity devices on the edge. Wenjie et al. [217] discuss structured model compression via KD, transferring knowledge from a complex VGG16 model to a lightweight MobileNet. This approach significantly reduces model size and improves performance, making it suitable for deployment on devices with limited resources. Wang et al. [218] explore.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 23 of 50# Application of Knowledge Distillation in Remote Sensing# IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024# Figure 10\n\nIllustration of the framework proposed in [117]. The \"Conv\" block comprises a 3 \u00d7 3 convolutional layer, followed by batch normalization, a 2 \u00d7 2 max-pooling layer, and a ReLU activation function. The \"FC\" block includes a fully connected layer, batch normalization, and a ReLU activation function. Both the \"Shared Classifier\" and \"Classifier\" share the same structure, composed of \"FC\" blocks and a softmax layer for final classification.# 1) Knowledge\n\nDIM: The self-distillation component that supports our network to learn from itself. The per-source output layer is trained to mime the behavior of the final fused output with the goal of distilling knowledge from fused information to the per-source stream. Using VGG as a teacher network, a student network is trained with KD, achieving high recognition accuracy and speed, particularly for coffee leaf pest and disease identification.\n\nIn detail, to align the dimension v, z1, and z2, a FC layer is applied to z1 and z2:\n\nzs,i = f(Ws zs,i + bs)# 5.2.3. Urban Planning\n\nKD has emerged as a vital technique in urban planning, particularly in the context of enhancing the efficiency and accuracy of models used for complex tasks such as environmental monitoring, infrastructure management, and resource allocation. For instance, KD has been effectively used to improve the real-time detection of building defects, optimize building extraction from noisy datasets, and enhance the accuracy of traffic flow prediction and travel time estimation.\n\nThis technique is particularly valuable in scenarios involving large-scale urban data, where it enables the deployment of sophisticated models on resource-constrained devices, such as UAVs and edge computing frameworks, facilitating more efficient management of urban infrastructure and services.# 2) Knowledge DCM\n\nThe DCM module leverages a distillation component that transfers the knowledge of the multimodal from powerful teacher models to more efficient student models. KD supports the development of robust, scalable solutions that are essential for modern urban planning and the creation of smarter, more responsive cities.\n\nFor instance, Rithanasophon et al. [228] proposed a method that leverages deep CNNs (DCNNs) and KD to evaluate QoL for pedestrians using walkability data collected through virtual reality tools, achieving significant improvements in model performance.# Table 7\n\nSummary of the main works that employ KD in the agriculture domain.\n\n|Reference|Application|Model Type|Performance Improvement|\n|---|---|---|---|\n|[219]|Instance-based semantic segmentation|DCNN|High accuracy and speed|\n|[220]|Animal behavior classification|Compact model|Real-time performance|\n|[221]|Weed mapping using UAVs|Vision Transformer|Effective weed mapping|\n|[222]|Plant growth monitoring|Transformer-based network|Enhanced performance|\n\nAuthorized licensed use limited to: University of Dubai. Downloaded on August 14, 2024 at 12:26:54 UTC from IEEE Xplore. Restrictions apply.\n\nY. Himeur, et al.: Preprint submitted to Elsevier# Application of Knowledge Distillation in Remote Sensing# Comparison of Studies on KD in Agriculture\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|\n|---|---|---|---|\n|[199]|BERT-ALA + BiLSTM + CRF|Agriculture named entity data|Enhanced agricultural entity recognition using model distillation|\n|[200]|MobileNet, Xception|PlantVillage dataset|Plant disease recognition with KD|\n|[201]|Improved YOLOv5s|Maize leaf disease dataset|Lightweight model for maize leaf disease detection|\n|[205]|ResNet-34|Various datasets for panoptic segmentation|KD for panoptic segmentation|\n|[206]|Identical architecture for teacher and student|Large dataset for plant leaf segmentation|Improved instance segmentation using spatial embedding and KD|\n|[202]|ECA-KDNet|Apple leaf dataset|Lightweight model for apple leaf disease diagnosis|\n|[203]|YOLOR model variants|PlantDoc dataset|Multistage KD for plant disease detection|\n|[223]|Multilevel distillation framework|CIFAR100 and CIFAR10|Addressing low resolution identification problems|\n|[208]|Lightweight semantic segmentation model|Custom dataset for grape picking point localization|Efficient grape picking point localization in complex environments|\n|[210]|Low-power DL model|Hydroponic systems|Plant disease detection in low-power IoT devices|\n|[211]|Data augmentation-based KD framework|PlantDoc dataset|Enhanced recognition accuracy for plant leaf diseases|\n|[209]|Knowledge-distilled models|Datasets for grapevine detection|Early grape detection and yield prediction with KD|\n|[218]|Lightweight model using VGG for KD|Coffee leaf dataset|High accuracy in coffee leaf disease identification with a lightweight model|\n|[220]|GRU-MLP models, ResNet|Animal behavior datasets|In-situ animal behavior classification on wearable devices|\n|[217]|Distilled-MobileNet|Common diseases of crops|Lightweight disease recognition model for limited-resource devices|\n|[219]|Instance-based semantic segmentation with Mask2Former|Agricultural datasets|KD for instance semantic segmentation|\n|[221]|Lightweight Vision Transformer|WeedMap dataset|Mapping weeds with drones using KD|\n|[222]|PA-RDFKNet|Various datasets for plant growth monitoring|RGB-depth fusion for plant age estimation with KD|\n|[224]|KD from Multi-head Teacher (KDM)|Bio-HSI|Efficient hyperspectral image segmentation with a compact student network|\n|[225]|UNet with various backbones|On-field images of pomegranate fruit|Effective segmentation of pomegranate fruits for agricultural automation|\n|[212]|Vision Transformer with chunking method|DlouFish dataset|Enhanced fish individual recognition using a novel KD strategy|\n|[214]|C3Faster with KD|CropPest6 dataset|Fast and efficient crop pest detection suitable for edge devices|\n|[215]|Lightweight CNN models|Indiana Bark Dataset|Portable tree species identification system for smartphones|\n|[216]|CNN|Crop growth dataset|Learning plant physiology from crop models generated by a crop model to enhance model portability|\n|[213]|Two-tier KD (T-KD)|Fish37 dataset|Improved accuracy and reduced parameters for underwater fish species classification|\n|[226]|KD from multispectral to RGB models|Mullus Marbatus family dataset|Fish quality estimation using RGB cameras with knowledge from multispectral images|\n|[227]|ResNet50 and a lightweight student model|Dataset of Ethiopian medicinal plants|Accurate identification of medicinal plants using a distilled knowledge approach|# Best Performance\n\n|Performance|Limitation|\n|---|---|\n|Macro-F1 increased by 3.3%|High time and space complexity|\n|Accuracy of 97.58%|Limited to small architectures|\n|mAP(0.5): Increased by 3.8%|Only focuses on maize; may not generalize to other crops|\n|Improved panoptic quality by up to 4.1 points|Requires extensive fine-tuning of balancing weights|\n|Enhanced segmentation accuracy|High dependency on the quality and size of the dataset|\n|Accuracy of 98.28%|Focused only on apple leaves, might not generalize|\n|60.4% mAP@.5|Model complexity and distillation stages may be challenging to manage|\n|Improved low-resolution recognition accuracy|Specific to low-resolution datasets|\n|91.08% accuracy in picking point localization|Limited to grape picking, may not extend to other fruits|\n|Accuracy of 99.4%|Focus on hydroponics; broader application unknown|\n|Improved accuracy by up to 3.06%|Performance heavily dependent on data augmentation quality|\n|Improvement in various metrics, e.g., 13.63% in mAP50-95|Predominantly focused on early detection stages|\n|Accuracy of 96.73%|Generalization to other crop diseases not demonstrated|\n|MCC of 0.882 (ResNet)|Mainly applicable to animal behavior, not crops|\n|Accuracy of 97.62%|Limited to specific diseases and crops|\n|AP improvement of 1.8 for ResNet-50|Focused on specific types of segmentation|\n|F1 score of 0.863|Specific to drone-based RS|\n|MSE reduced from 2 to 0.14 weeks|Focused on plant growth, might not extend to other agricultural tasks|\n|mIoU of 90.03%|Over-compression degrades performance without medium-sized teacher assistants|\n|F1 score of 90.35% for VGG19 backbone|Dependency on the choice of backbone for performance|\n|Accuracy of 93.19%|Specific to underwater environments|\n|97.5% mAP|Reduced feature extraction capability in lightweight models|\n|96.12% accuracy|Limited to specific tree species in Indiana|\n|MSE of 52.9 during training|Limited by synthetic data generation from crop models|\n|Top-1 accuracy of 97.20%|Requires large model sizes for initial training|\n|Classification accuracy of 84.3%|Limited to specific types of fish and conditions|\n|96.91% accuracy|High accuracy dependent on extensive data preprocessing|\n\nperformance and computational efficiency. Similarly, Liu et al. [229] introduced UrbanKG, a knowledge graph system that integrates KD for urban data fusion, showing promising results in boosting performance across various urban computing applications. Xu et al. [230] also addressed the challenges of limited training samples in building polygon extraction by proposing BPDNet, a KD-based framework that effectively integrates generalization knowledge from large datasets with task-specific characteristics, resulting in superior performance in complex urban environments.\n\nFederated learning frameworks have also benefited from KD, particularly in the context of land use monitoring and environmental impact assessment. Kanagavelu et al. [185] demonstrated the potential of integrating KD with federated UNet models for the semantic segmentation of satellite and street view images, achieving high accuracy and significant.# Application of Knowledge Distillation in Remote Sensing\n\nModel compression. In a similar vein, Xu et al. [231] developed a KD-based building extraction method that reduces the impact of noise on model performance while maintaining generalization, achieving notable improvements in precision, recall, and IoU metrics. In the context of transportation systems, KD has been utilized to enhance travel time estimation (TTE) models and improve traffic flow prediction. Yang et al. [106] proposed KDTTE, a deep neural network model that employs KD to reduce computation and memory costs while increasing accuracy, significantly outperforming state-of-the-art baselines in TTE tasks. In a different but related task, Li et al. [232] applied deep KD to traffic flow prediction in spatio-temporal networks, demonstrating improvements in both local and global feature perception and achieving better accuracy in traffic predictions.\n\nIn the autonomous driving domain and the task of off-road environment segmentation, KD has been instrumental in improving model efficiency and accuracy. Pan et al. [233] developed an end-to-end lane detection method using KD to guide polynomial regression under complex road conditions, achieving competitive results in efficiency and accuracy. Similarly, Kim and An [234] proposed a KD method for segmenting off-road environment range images, resulting in a favorable trade-off between segmentation performance and computational cost, highlighting its effectiveness for autonomous systems.\n\nLee et al. [235] proposed a high-speed detection method for multi-class defects on residential building fa\u00e7ades using KD. The study demonstrated that applying KD to a lightweight DL model significantly improved mean average precision (mAP) by approximately 20% and reduced inference time by 2.5 times, making it more suitable for real-time applications. Moving on, Chen et al. [108] introduced a novel approach to building extraction that utilizes KD to enhance the robustness of the distilled student model. The study employed a multi-teacher collaborative distillation strategy to transfer comprehensive feature knowledge from teacher networks to the student model. The approach demonstrated state-of-the-art performance on multiple datasets, including the Massachusetts Roads Dataset, LRSNY Roads Dataset, and WHU Building Dataset, achieving high IoU scores and improving learning capabilities. Geng et al. [79] developed a lightweight topological space network for road extraction from optical RS images, leveraging KD. The study addressed the challenge of extracting topological features from complex road networks by proposing a topological space loss calculation model. The method resulted in significant improvements in accuracy and computational efficiency, demonstrating a good balance between performance and model size.\n\nBesides, Li et al. [236] proposed an off-policy imitation learning method for autonomous driving that employs task KD. This approach was designed to clone human driving behavior and transfer driving strategies to new, unseen scenarios. The method showed promising results in transferring knowledge to different illumination and weather conditions, enhancing route-following performance in realistic urban driving scenes. Hong et al. [237] introduced a hierarchical edge-decision framework for intelligent transportation systems (ITS) that incorporates KD. The framework enables vehicle-road-cloud cooperation to enhance real-time motion planning by distilling complex spatial-temporal event reasoning into efficient decision-making processes. The method was validated on autonomous driving scenarios, demonstrating improved adaptability to complex environments. Luo et al. (2022) [238] presented the KeepEdge framework, which integrates deep neural networks into an edge computing system for UAV-assisted parcel delivery. By employing KD, the study created a lightweight model that maintained high accuracy while reducing the computational load on UAVs. This approach proved effective in complex environments where traditional GPS-based positioning might fail. Pelizari et al. (2023) [239] developed a deep multitask learning (MTL) architecture for building characterization using street-level imagery. The study incorporated KD to encode cross-task interdependencies, which improved the generalization capabilities of the model across multiple natural hazards. The proposed MTL methods outperformed traditional single-task learning (STL) models, achieving higher accuracy and efficiency. The aforementioned studies that demonstrate the versatility of KD in enhancing the efficiency, accuracy, and scalability of models in various prediction and classification tasks in urban planning and intelligent transportation systems, using RS data are summarized in Table 8.# 5.2.4. Oceanographic Monitoring\n\nKD can significantly enhance the efficiency and practicality of AI applications in ocean and sea studies by simplifying complex models for deployment on resource-constrained devices. This includes improving marine wildlife detection, real-time oceanographic monitoring, and underwater object detection by compressing large models into smaller, more efficient versions without compromising accuracy. Additionally, it can aid in climate change prediction and fisheries management, making advanced AI models more accessible and effective for monitoring and analysis in remote or resource-limited environments. In this direction, the authors in [244] explore the application of CNNs in ocean RS, highlighting their effectiveness in tasks such as 3D ocean field reconstruction, image super-resolution, and ocean phenomena forecasting. The study demonstrates significant improvements in classification accuracy for sea ice and open water areas in SAR images and a notable enhancement in image resolution using CNN-based models.\n\nSeveral studies focus on underwater environments, where detection and analysis face unique challenges due to poor visibility and environmental complexities. Chen et al. [245] propose an online KD framework, Online-XKD, to enhance the accuracy and generalizability of underwater object detection models while maintaining their lightweight nature. Similarly, Ben Tamou et al. [246] present a CNN-based approach for classifying live reef fish species in underwater environments, using incremental learning to maintain high accuracy as new species are added. Another underwater-focused study introduces WaterMono [247], a framework for depth estimation and image enhancement in underwater scenes, leveraging KD to address challenges such as dynamic scenes and image degradation.\n\nIn the domain of geophysical field reconstruction, Adapt-Deep [248], a self-supervised framework designed, has been proposed to reconstruct fine-grained spatial structures from coarse-scale geophysical data. The proposed method effectively identifies and recovers detailed information in sea.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 26 of 50# Application of Knowledge Distillation in Remote Sensing# Comparison of KD Studies for Urban Planning\n\n|Citation|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance Value Achieved|Limitation|\n|---|---|---|---|---|---|\n|[228]|DCNNs, LSTM, KD|VR-based questionnaire data|Evaluates walkability using AI and enhances real-time performance through KD|MSE of 7.19 \u00d7 10\u22123 (within-city) and 9.73 \u00d7 10\u22123 (across-cities)|Limited to VR data, may not generalize to all environments|\n|[229]|FedUKD, UNet|Satellite and street view images|Integrates knowledge graphs for urban data fusion|97% accuracy on Chennai land use dataset|May struggle with dynamic, heterogeneous urban data|\n|[230]|BPDNet|Building polygons|Distills knowledge for generalization in building extraction tasks|IoU of 66.54%|Performance may drop in complex urban settings|\n|[185]|FedUKD|Satellite and street view images|Reduces communication costs in land use classification via federated learning|Above 95% accuracy with significant compression|Scalability to other urban data types may be limited|\n|[106]|KDTTE|Travel time estimation datasets|Improves travel time estimation with KD|86.8% accuracy improvement on Porto dataset|Limited generalization to diverse traffic conditions|\n|[240]|UrbanKG|Urban spatial-temporal data|Develops an urban knowledge graph for data fusion|Effective in various urban applications|Requires extensive setup and integration|\n|[231]|UPerNet, Swin Transformer|Noisy RS images|Enhances building extraction from noisy images with KD|IoU of 81.61%|Dependent on noisy label quality|\n|[235]|DCNN|Building fa\u00e7ade images|Accelerates defect detection on building fa\u00e7ades using KD|20% mAP increase, 2.5x faster inference|Limited to fa\u00e7ade defects, may not generalize|\n|[108]|U-Net, DeepLabV3Plus|Road and building datasets|Enhances model robustness via multi-teacher distillation|IoU scores: 48.56%, 79.51%, 81.35%|Teacher weight optimization is still needed|\n|[232]|Deep KD Model|Traffic flow datasets|Improves spatio-temporal traffic flow prediction using KD|Accuracy improvement of 0.19 and 0.18 on respective datasets|Focused on local data, may miss global patterns|\n|[233]|End-to-End Lane Detection with KD|TuSimple, CULane Datasets|Lane detection method using auxiliary supervision|Competitive accuracy, high efficiency|Post-processing still needed for some tasks|\n|[241]|Interaction-aware Trajectory Planning with KD|Real-world driving scenarios|Combines DL with optimization for trajectory planning|Fivefold improvement in computation time|Integration with control paradigms is complex|\n|[242]|Lightweight Location Prediction Model|Next Mobility data|Efficient next-location prediction with reduced inference time|6.57% error reduction, 99.8% faster inference|Focuses on reducing computational load|\n|[243]|MJPNet-S*|RGB-T/D data|Trimodal joint-perception network for crowd density estimation|92% faster, 83% fewer parameters|Reduced resource consumption may impact generalization|\n|[79]|TSKD-Road|RS images|Topological network for road extraction with KD|Road IoU: 59.16%, mIoU: 78.49%, F1: 74.15%|Limited to road extraction tasks|\n|[234]|MobileNet_v2 DeepLabV3+ with SLKD|Off-road environment dataset|Lightweight model for off-road segmentation using KD|mIoU of 57.28%, low computational cost|Trade-off between accuracy and efficiency|\n|[237]|GSCNN|Autonomous driving scenarios|Edge-decision framework for motion skill enhancement|Improved adaptation to dynamic environments|Complexity in real-time implementation|\n|[238]|DNN|UAV delivery environments|Edge intelligence framework for UAV positioning|High accuracy with reduced model complexity|Dependent on visual data quality|\n|[239]|Deep MTL|Street-level imagery|Cross-task interdependency modeling for building characterization|accuracy = 88.43%|Complexity in MTL model training|\n\nSurface temperature fields, demonstrating the potential of domain adaptation techniques in enhancing data resolution and accuracy. Moving on, Tropical cyclone (TC) wind radii estimation is the focus of Jin et al. [249], who propose a multimodal fusion network, MT-TCNet, and its distillation variant, MT-TCNet-Distill. These models utilize a combination of satellite infrared images, wind field reanalysis, and maximum sustained wind speed data to estimate TC wind radii, achieving superior performance even in scenarios with incomplete data.\n\nIn the domain of water segmentation, the challenge of accurately segmenting water areas for unmanned surface vehicles (USVs) has been presented in [250]. The study introduced a multimodal fusion method combining 2D camera images and 3D LiDAR point clouds, utilizing transformers and KD to improve segmentation accuracy and processing speed. Lastly, Yang et al. [251] focus on sea ice segmentation, proposing a CNN-based method enhanced with data augmentation, a novel loss function, and multiscale strategies. Their study achieves high segmentation accuracy using the HRNet-W48 backbone, demonstrating the effectiveness of innovative DL techniques in environmental monitoring. Table 9 provides a summary of the studies that employ KD techniques to improve model performance in the oceanographic remote imaging domain.# 6. Challenges and Limitations\n\nDespite the many advantages that KD techniques offer and the wide range of their applications, they still face several limitations as portrayed in Fig. 11. These challenges are mainly related to the deployment of the models to resource-constrained devices and to keeping the performance of these models high when handling heterogeneous data or data from new, unseen distributions. Finding the balance between model efficiency and prediction accuracy is the key challenge as explained in the following.# 6.1. Model Complexity and Deployment\n\nIn RS applications, KD is often employed to create smaller, more efficient student models by transferring knowledge from a larger, more complex teacher model. The main goal of KD is to retain the high accuracy of the teacher model while reducing the computational load and model size.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 27 of 50# Application of Knowledge Distillation in Remote Sensing# Table 9\n\n|Ref.|Model(s) Used|Dataset/Data Type|Brief Description of Main Contribution|Best Value Achieved|Performance|Limitation|\n|---|---|---|---|---|---|---|\n|[244]|CNNs|Various ocean RS data|Applied CNNs across multiple ocean RS tasks, including 3D ocean field reconstruction and image super-resolution.|ACC=92.36% for sea ice and open water areas in SAR images|High computational cost|and model interpretability challenges.|\n|[245]|Online-XKD|URPC2020 dataset|Enhanced feature extraction and generalization in underwater object detection using mutual knowledge transfer in a distillation framework.|3.6 mAP improvement in student model detection accuracy|Complexity may hinder deployment in low-resource environments| |\n|[246]|CNN with incremental learning|LifeClef 2015 Fish dataset|Developed an incremental learning strategy for live reef fish species classification, maintaining high performance on previously learned species.|81.83% accuracy on LifeClef 2015 Fish benchmark dataset|Scaling to larger datasets|or complex environments could be challenging.|\n|[248]|AdaptDeep|Coarse and fine-scale geophysical field data|Proposed a self-supervised framework for fine-grained reconstruction of geophysical data using domain adaptation and contrastive learning.|Recovered 81.2% detailed information in sea surface temperature fields|Performance depends on|the availability of coarse-scale data and temporal correlations.|\n|[247]|WaterMono|Underwater images|Introduced a self-supervised depth estimation framework with image enhancement for underwater environments using KD.|RMSE: 0.945, RMSE log: 0.152|Limited generalization to|diverse camera angles and extreme conditions.|\n|[249]|MT-TCNet, TCNet-Distill|Multimodal data including satellite IR images, reanalysis wind fields, and MSW speed|Developed a multimodal fusion network and distillation method for robust TC wind radii estimation with both complete and missing modalities.|R34 estimation: RMSE 22.458 nmi, MAE 16.577 nmi, R-value 0.855; RMW estimation: RMSE 7.958 nmi, MAE 5.689 nmi, R-value 0.738|Reliance on reanalysis data|limits real-time applicability.|\n|[250]|Transformer-based multimodal fusion|2D camera images, 3D LiDAR point clouds|Proposed a water segmentation method using transformers and KD for improved 2D image-based segmentation with faster speed.|Approx. 1.5% improvement in accuracy and MaxF, speed of 15-110 fps|High computational load|during training phase, though reduced with distillation.|\n|[251]|CNN with HRNet-W48 backbone|Large sea-ice segmentation dataset|Introduced innovative data augmentation, loss function, and multiscale strategies for accurate sea ice segmentation with KD for real-time application.|FWIoU score of 97.8439|High computational resource requirement|for real-time processing.|\n|[252]|Tiny YOLO-Lite|SSDD, HRSID, large-scene SAR images|Developed a lightweight SAR ship detector using network pruning and KD to reduce model size and computation while maintaining high accuracy.|Average Precision (AP)MB model size, of 89.07%, 2.8|inference speed >200 fps|Performance may decline with further model size reduction.|# Knowledge Distillation Challenges in Remote Sensing\n\n- Mathematical Optimization and Loss Computation\n- Model Complexity and Deployment\n- Data Heterogeneity\n- Heterogeneity Error in Multi-Modal Data\n- Generalization Error due to Limited Data\n- Overfitting and Generalization\n- Scalability\n- Computational Cost for Large Datasets\n- Inference Time and Real-Time Constraints\n- Limited Real-Time Applicability\n- Dependency on High-Quality Data\n- Impact of Data Quality on Model Performance\n- Efficiency-Accuracy Trade-offs\n- Balancing Efficiency and Accuracy\n- Integration Complexity\n- Complexity in Integration with Other Techniques# End\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 28 of 50# Application of Knowledge Distillation in Remote Sensing\n\nwhich is crucial for deployment on resource-constrained devices commonly used in RS. However, the process of optimizing the distillation loss function to achieve this balance between model size and performance is inherently complex and computationally demanding. The distillation loss *\u2113KD, which combines the cross-entropy loss \u2113CE and the Kullback-Leibler divergence \u2113KL*, needs to be carefully minimized to ensure that the student model effectively approximates the teacher model. This optimization process becomes more challenging as the complexity of the teacher model increases, leading to a higher computational burden during training [253]. Furthermore, when deploying the distilled student model on resource-constrained devices, such as those used in RS for on-board data processing, the reduced model complexity must still meet the real-time processing requirements and maintain high accuracy. The complexity of optimizing the KD process for deployment is expressed by the computational cost associated with the gradient of the distillation loss with respect to the student model parameters. As this complexity increases, it can lead to longer training times, higher energy consumption, and potentially suboptimal model performance, particularly when deployed in environments with limited computational resources.\n\nIn this regard, given a teacher model *T with parameters \u03b8T and a student model S with parameters \u03b8S*, the distillation loss is defined as:\n\n*\u2113KD(\u03b8S) = \u03b1\u2113CE(y, S(x; \u03b8S)) + (1\u2212\u03b1)\u2113KL(T(x; \u03b8T), S(x; \u03b8S))* (5)\n\nThe challenge related to the complexity of optimizing this loss function for deployment is defined as follows:\n\n*Complexity \u221d O(\u2202\u2113KD)* (6)\n\nThis complexity can affect the feasibility and effectiveness of deploying KD models in real-world RS scenarios, where computational efficiency and model robustness are critical.# 6.2. Data Heterogeneity\n\nRS data often comes from multiple modalities, such as optical, SAR, and multispectral sensors. Integrating knowledge across these heterogeneous data sources while maintaining accuracy is challenging, as the characteristics of the data can vary significantly. For multi-modal RS data *x1, x2, \u2026 , xm* from different modalities, the aggregate loss is:\n\n*\u2113multi-modal(\u03b8S) = \u2211i=1m wi \u22c5 \u2113KD(T(x; \u03b8T), S(x; \u03b8S))* (7)\n\nTypically, the heterogeneity error is defined as:\n\n*Heterogeneity Error = \u2211i=1m |\u2113KD(T(x; \u03b8T), S(x; \u03b8S))| - \u2113KD(Tj(xj; \u03b8T), S(xj; \u03b8S))|* (8)# 6.3. Overfitting and Generalization\n\nWhile KD helps reduce the size of models, it can also lead to overfitting, particularly when the student model is trained on a limited dataset. This results in poor generalization to new, unseen data, which is critical for the success of RS applications [35, 254]. The generalization error is given by:\n\n*Generalization Error = \u2113KD(\u03b8S; Dtest) \u2212 \u2113KD(\u03b8S; Dtrain)* (9)\n\nOverfitting occurs when: Generalization Error \u226b 0. The impact of overfitting on the performance of knowledge distillation in RS applications includes: Overfitting causes the student model to perform well on the training data but poorly on unseen test data, which can significantly reduce the model\u2019s effectiveness in real-world RS applications where data variability is high; Overfitted models are often overly sensitive to noise in the training data. This sensitivity can lead to incorrect predictions when the model encounters noisy or outlier data in RS, where data quality can vary widely across different sensors and conditions; In RS, data is often collected from multiple modalities (e.g., optical, SAR, multispectral). An overfitted student model might fail to generalize well across these different modalities, leading to inconsistent performance and reduced reliability in practical applications; Overfitting can limit the ability of the student model to transfer learned knowledge to new tasks or domains within RS, reducing the versatility and adaptability of the distilled model.# 6.4. Scalability\n\nAs the size of RS datasets increases, the computational complexity of applying KD also increases. This scalability issue can limit the practicality of deploying distilled models on large datasets.\n\nFor a dataset of size *N*, the computational complexity scales as:\n\n*Scalability \u221d O(N \u22c5 C(\u2113KD))* (10)\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 29 of 50# Application of Knowledge Distillation in Remote Sensing# 6.5. Limited Real-Time Applicability\n\nIn RS applications, the need for real-time processing is critical, as delays in data processing can render the information outdated and less useful for immediate decision-making. Knowledge Distillation (KD) aims to create more efficient models, but even with distilled models, achieving the required inference speed can be challenging, especially when dealing with complex student models. The inference time tinference, which depends on both the size of the student model \u03b8S and the computational complexity of evaluating the distillation loss \u212d(\u2112KD), must be kept within the real-time processing limit treal-time. If tinference exceeds treal-time, the performance of the KD model is compromised, as it may not be able to process data quickly enough to be useful in time-sensitive RS applications. This limitation can reduce the effectiveness of KD in scenarios where immediate data analysis and decision-making are required [35, 254].\n\ntinference \u2264 treal-time (11)\n\nwhere tinference is the inference time and treal-time is the allowable time for real-time processing:\n\ntinference \u2248 O(\u03b8S) + O(\u212d(\u2112KD)) (12)# 6.6. Dependency on High-Quality Data\n\nThe effectiveness of Knowledge Distillation (KD) is highly contingent on the quality of the training data, which plays a critical role in the success of the distillation process. In RS applications, the data is often noisy, sparse, or collected under varying conditions, leading to inconsistencies that can adversely impact the KD process. The distillation loss, which combines the cross-entropy loss \u2112CE between the student model\u2019s predictions and the true labels, and the Kullback-Leibler divergence \u2112KL between the teacher and student model outputs, assumes that the input data is of high quality. However, when the data is of poor quality, the student model may struggle to learn effectively from the teacher model, leading to increased errors and reduced generalization capability. The formula for the impact of data quality indicates that as the proportion of low-quality data increases, the overall performance of the KD model diminishes. This reduction in performance can result in less robust models, which may fail to accurately process and interpret RS data, ultimately hindering the effectiveness of KD in real-world RS tasks [255, 256, 96].\n\n\u212dKD(\u03b8S) = \u2211i=1n(\u2112CE(yi, S(x; \u03b8S)) + \u2112KL(T(xi; \u03b8T), S(x; \u03b8S))) (13)\n\nTypically, the impact of data quality is defined as follows:\n\nData Quality Impact = n1 \u2211i=1n I(quality(x, y) < \u03b5) (14)# 6.7. Balancing Efficiency and Accuracy\n\nOne of the key challenges in Knowledge Distillation (KD) is striking the right balance between model efficiency and accuracy. In the context of RS applications, where the stakes are often high, such as in disaster monitoring or environmental protection, compressing the student model too much in the pursuit of efficiency can lead to a significant loss in accuracy. This reduction in accuracy could result in the failure to correctly interpret RS data, leading to erroneous decisions [257, 258]. The trade-off between efficiency and accuracy is represented by the following relationships:\n\nEfficiency \u221d Model Size(\u03b8S), Accuracy \u221d 1/\u212dKD(\u03b8S) (15)\n\nwhere Model Size(\u03b8S) refers to the number of parameters in the student model, and \u212dKD(\u03b8S) is the distillation loss, which is inversely proportional to the accuracy of the student model. The optimization problem, therefore, involves maximizing the product of efficiency and accuracy:\n\nmax\u03b8S Model Size(\u03b8S) \u22c5 1/\u212dKD(\u03b8S) (16)\n\nHowever, this optimization is complex because increasing efficiency (i.e., reducing model size) often leads to a rise in distillation loss \u212dKD, which in turn decreases accuracy. Conversely, maintaining high accuracy may require a larger model size, reducing efficiency. In RS, where both computational resources and model performance are critical, failing to achieve an optimal balance can limit the effectiveness of KD. This trade-off must be carefully managed to ensure that the compressed model performs adequately in practical RS scenarios, where both speed and accuracy are essential.# 6.8. Integration Complexity\n\nIntegrating Knowledge Distillation (KD) with other techniques such as multi-modal fusion or domain adaptation introduces significant complexity to the model and its training process, affecting its performance in RS applications. Integrating these techniques requires careful balancing of multiple loss functions, as the overall performance now depends on the combined effectiveness of KD, multi-modal learning, and domain adaptation. This added complexity can make the training process more computationally expensive.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 30 of 50# Application of Knowledge Distillation in Remote Sensing\n\nHarder to optimize, and more prone to issues like overfitting or convergence to suboptimal solutions. For instance, when integrating KD with other techniques, the overall loss function is expressed as a combination of multiple loss components, each weighted by a coefficient (e.g., \ud835\udefc, \ud835\udefd, \ud835\udefe). The need to fine-tune these coefficients to achieve the desired balance between KD, multi-modal fusion, and domain adaptation further complicates the training process [116, 259]. Moreover, the integration complexity, represented by the derivative of the total loss function concerning the student model parameters, reflects the increased difficulty in optimizing the student model. As the complexity increases, the risk of inefficient training or suboptimal performance also rises, making it challenging to achieve the desired accuracy and efficiency in real-world RS applications.# 7.1.1. Dynamic Distillation\n\nDynamic Distillation is a technique within the broader category of advanced model compression. It aims to optimize the student model\u2019s performance by dynamically adjusting its complexity based on the specific characteristics of the input data or the task at hand. The core idea behind dynamic distillation is to create a flexible and adaptive student model that can efficiently learn from the teacher model without being overly constrained by a fixed architecture or predetermined level of complexity [260, 261]. In many RS applications, the complexity of the input data can vary significantly. For example, a satellite image of a dense urban area may contain more intricate features than an image of a rural landscape. A static, one-size-fits-all student model may struggle to balance performance across such diverse inputs. Dynamic distillation allows the student model to adapt its architecture or parameters based on the specific task, ensuring that it allocates resources efficiently [262, 263]. Not all inputs require the same level of processing. Dynamic distillation enables the student model to adjust its complexity (e.g., the number of layers, the size of feature maps, or the degree of feature extraction) depending on the input. For instance, simpler inputs might be processed with a reduced version of the student model, while more complex inputs trigger a more detailed processing approach [264]. The term \"dynamic\" implies that these adjustments occur in real-time or near-real-time, during the inference phase. This is particularly useful in resource-constrained environments, such as edge devices or real-time RS applications, where computational resources are limited. By making on-the-fly adjustments, the model can maintain high performance while conserving resources [105]. The dynamic distillation process can be expressed as an optimization problem:\n\nmin\ud835\udd3c\ud835\udc65\u223c\ue230[\ud835\udf06(\ud835\udc65) \u22c5 \ue238KD(\ud835\udc47 (\ud835\udc65; \ud835\udf03\ud835\udc47 ), \ud835\udc46(\ud835\udc65; \ud835\udf03\ud835\udc46 ))] \ud835\udf03\ud835\udc46\n\nWhere \ud835\udf03\ud835\udc46 represents the parameters of the student model that need to be optimized. The input sample, denoted as \ud835\udc65, is drawn from the data distribution \ue230. The teacher model, parameterized by \ud835\udf03\ud835\udc47, produces an output \ud835\udc47 (\ud835\udc65; \ud835\udf03\ud835\udc47) for the input \ud835\udc65, while the student model, with parameters \ud835\udf03\ud835\udc46, generates an output \ud835\udc46(\ud835\udc65; \ud835\udf03\ud835\udc46) for the same input. The KD loss function, \ue238KD, typically quantifies the difference between the teacher\u2019s and student\u2019s outputs. Additionally, \ud835\udf06(\ud835\udc65) is a dynamic weighting function that adjusts the contribution of each input \ud835\udc65 to the overall loss, depending on its complexity or the specific requirements of the task.\n\nSpecifically, in dynamic distillation, the factor \ud835\udf06(\ud835\udc65) acts as a critical gatekeeper, adjusting the influence of each input on the student model\u2019s training. For complex or critical inputs, \ud835\udf06(\ud835\udc65) increases, prompting the student model to allocate more resources, such as deeper layers or enhanced feature extraction. Conversely, simpler inputs lead to a lower \ud835\udf06(\ud835\udc65), allowing the student model to process the data more efficiently with reduced resources. The KD loss function \ue238KD is central to this process, focusing on minimizing the difference between the teacher and student models\u2019 outputs to ensure the student effectively mimics the teacher. This approach is generalized across the entire dataset, as captured by the expectation \ud835\udd3c\ud835\udc65\u223c\ue230, optimizing the student model\u2019s performance across diverse inputs.\n\nDynamic distillation enhances resource efficiency by dynamically adjusting the student model\u2019s complexity based on the input, ensuring that computational resources are used optimally, especially in resource-constrained environments. This adaptability allows the student model to maintain or even surpass the performance of static models, particularly when dealing with heterogeneous datasets like those in RS. Additionally, the scalability of dynamic distillation makes it a versatile solution that is suitable for deploying ML models across various environments, from cloud-based systems to edge devices. Typically, in RS, dynamic distillation could be applied to urban monitoring where satellite images vary significantly between dense urban areas and sparse rural regions. For instance, when analyzing satellite imagery for urban heat island detection, the student model could dynamically adjust its complexity, using more layers and features for complex urban environments with varied structures, while simplifying its approach for less complex rural landscapes, thus optimizing processing efficiency and accuracy.# 7.1.2. Layer-Wise Distillation\n\nLayer-wise distillation is an advanced technique in model compression that focuses on transferring knowledge from a teacher model to a student model at a more granular level [265]. Unlike traditional KD, which typically focuses on aligning the final outputs of the teacher and student models, layer-wise distillation involves aligning the outputs of corresponding layers in both models [266]. This approach ensures that the student model learns not only the final output distribution but also the intermediate representations that the teacher model uses to arrive at that output [267]. Typically, layer-wise distillation enables a more effective transfer of knowledge by focusing on the outputs of different layers within a complex model, where each layer captures varying levels of abstraction\u2014from basic features like edges (17) to more complex patterns [268]. This approach allows for tailored compression by assigning different importance to each layer, ensuring that critical features are preserved while less important layers are compressed more aggressively. As a result, the student model becomes more compact and maintains or improves performance, particularly in tasks requiring detailed understanding, such as detecting and\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 31 of 50# Application of Knowledge Distillation in Remote Sensing\n\nclassifying intricate patterns in RS applications [269]. The layer-wise distillation process can be expressed as follows:\n\n\ud835\udc3f\n\n\ue238KD-layer =\u2211\ud835\udc64\ud835\udc59 \u22c5 \ue238KD(\ud835\udc47 \ud835\udc59(\ud835\udc65), \ud835\udc46\ud835\udc59(\ud835\udc65)) (18)\n\nwhere \ud835\udc47 \ud835\udc59(\ud835\udc65) and \ud835\udc46\ud835\udc59(\ud835\udc65) denote the outputs of the \ud835\udc59-th layer in the teacher and student models, respectively, while \ue238KD represents the knowledge distillation loss function applied to these corresponding layer outputs. The term \ud835\udc64\ud835\udc59 is a weight assigned to each \ud835\udc59-th layer, indicating its significance in the distillation process, and \ud835\udc3f denotes the total number of layers in the model.\n\nIn layer-wise distillation, \ud835\udc47 \ud835\udc59(\ud835\udc65) and \ud835\udc46\ud835\udc59(\ud835\udc65) represent the outputs of the \ud835\udc59-th layer for a given input \ud835\udc65 in the teacher and student models, respectively, ensuring the student model learns the same feature representations as the teacher model at each stage. The KD loss function, \ue238KD, measures the difference between these outputs, and when applied layer-wise, it ensures close alignment between the corresponding layers of both models. The layer-specific weight \ud835\udc64\ud835\udc59 allows for fine-tuning the importance of each layer, with critical layers in the teacher model being given higher weights to ensure their knowledge is effectively transferred. Finally, the summation across all layers \ud835\udc3f ensures that the distillation process comprehensively covers the entire model, enabling the student model to replicate the full range of the teacher model\u2019s capabilities.\n\nLayer-wise distillation offers several practical benefits, including enhanced feature preservation, where focusing on each layer ensures that the student model retains the critical features learned by the teacher, leading to greater accuracy and capability. The flexibility in compression, enabled by layer-specific weights, allows for optimizing the trade-off between model size and performance, depending on the application\u2019s needs. Additionally, this approach fosters better generalization, as the student model is trained to replicate the hierarchical representations of the teacher model, making it more adept at handling new data, particularly in tasks that require detailed feature extraction and classification. In RS, layer-wise distillation can be applied to multispectral image classification, where different spectral bands capture varying levels of detail. By aligning the outputs of corresponding layers in both the teacher and student models, this technique ensures that the student model effectively learns the intermediate features critical for distinguishing complex land cover types, such as differentiating between various crop types or identifying subtle changes in vegetation health.# 7.2.1. Low-Cost Training Algorithms\n\nLow-cost training algorithms aim to reduce the computational burden associated with training both teacher and student models, which is particularly important in the context of KD where the goal is to make the student model as efficient as possible. These algorithms focus on optimizing various aspects of the training process to minimize costs while maintaining or even enhancing the performance of the distilled models [270]. Developing more efficient training algorithms that reduce the computational burden of training both teacher and student models is crucial. This can involve leveraging techniques such as federated learning, transfer learning, or smaller proxy datasets for initial training to minimize the overall training cost [271].\n\nFederated learning is a distributed approach that allows training to occur across multiple devices or servers without the need to centralize the data. This can significantly reduce the computational cost associated with data processing and model training by distributing the workload [272]. Each device trains a local model using its data and periodically shares updates with a central server, which aggregates these updates to improve the global model. This approach not only reduces the computational burden on individual devices but also enhances privacy since raw data is not shared [273, 274].\n\nTransfer learning involves taking a pre-trained model (often trained on a large dataset) and fine-tuning it on a smaller, task-specific dataset. This approach can drastically reduce the training cost because the model has already learned general features from the larger dataset, and only minimal additional training is required to adapt it to the new task. In the context of KD, transfer learning can be used to initialize the teacher model, which then distills its knowledge to a student model with minimal additional training [275, 276].\n\nUsing smaller proxy datasets for initial training can also reduce costs. Proxy datasets are subsets of the original data or synthetic datasets that approximate the characteristics of the full dataset but are much smaller in size. Training on these datasets requires fewer resources and can provide a good initial model that can be further refined with the full dataset. This approach is particularly useful in scenarios where obtaining labeled data is expensive or time-consuming [277, 278].\n\nThe cost of training both the teacher and student models can be expressed as:\n\n\ud835\udc41\n\n\ud835\udc36train =\u2211(\ud835\udc36data(\ud835\udc65\ud835\udc56) + \ud835\udc36model(\ud835\udc47 , \ud835\udc46)) (19)\n\nwhere \ud835\udc36data(\ud835\udc65) denotes the cost associated with processing each data sample \ud835\udc65\ud835\udc56, which encompasses activities such as data loading, augmentation, and preprocessing, while \ud835\udc36model(\ud835\udc47 , \ud835\udc46) refers to the cost incurred during the training process for updating both the teacher model \ud835\udc47 and the student model \ud835\udc46. This formulation highlights that the total training cost \ud835\udc36train is the sum of the costs associated with processing all data samples and updating the models. By optimizing these components\u2014such as by reducing the size of the data samples with proxy datasets, distributing the training workload with federated learning, or leveraging pre-trained models with transfer learning\u2014the overall training cost can be significantly reduced [271].\n\nLow-cost training algorithms enhance resource efficiency, allowing organizations to train effective models even in environments with limited computational resources, which is particularly valuable in fields like RS that involve large datasets and complex models. These algorithms also support scalability, enabling the handling of extensive datasets and sophisticated models without a proportional increase in resource demands, making them adaptable to various environments from cloud servers to edge devices [279]. Additionally, by reducing training costs and time, these algorithms facilitate faster development cycles, allowing for quicker iteration and deployment of ML models, which is essential in rapidly evolving fields like AI [280].# Application of Knowledge Distillation in Remote Sensing\n\napplied to disaster response scenarios, where real-time analysis of satellite imagery is crucial. For example, federated learning can be used to train models across multiple local servers situated near disaster zones, enabling rapid analysis of satellite images for damage assessment without the need for extensive centralized computing resources. Transfer learning can further enhance this process by fine-tuning pretrained models on smaller, region-specific datasets, ensuring swift deployment and effective monitoring during critical events.# 7.2.2. Hardware-Aware Distillation\n\nIntegrating KD with hardware-aware design principles can optimize models specifically for the hardware on which they will be deployed, such as edge devices or GPUs. This approach aims to balance the distillation process with the computational capabilities of the target hardware [281]. Hardware-aware distillation integrates KD with hardware-specific optimization strategies to create models that are not only efficient in terms of performance but are also tailored to the computational constraints of the hardware on which they will be deployed. This approach is particularly useful for scenarios where the model needs to be run on edge devices, GPUs, or other specialized hardware, ensuring that the distilled model operates within the physical and computational limits of the target platform [282].\n\nHardware-aware distillation seeks to balance the effectiveness of the knowledge transfer process with the computational efficiency required by the target hardware. The goal is to ensure that the student model retains as much of the teacher model\u2019s performance as possible while also fitting within the hardware\u2019s resource constraints [283]. This involves careful consideration of factors such as memory usage, processing speed, and power consumption, which are critical in environments like mobile devices, embedded systems, or cloud-based GPUs [284]. Different hardware platforms have varying capabilities and limitations. For example, GPUs excel at parallel processing but may have limited memory bandwidth, while edge devices often have strict power and computational limits. Hardware-aware distillation tailors the student model to leverage the strengths of the target hardware while minimizing its weaknesses. This could involve optimizing the model\u2019s architecture to reduce the number of parameters, simplify computations, or increase parallelism, depending on the hardware\u2019s characteristics [285].\n\nThe regularization parameter \ud835\udf06 in the hardware-aware distillation framework controls the trade-off between the accuracy of the distilled model and its hardware efficiency. A higher \ud835\udf06 places more emphasis on minimizing computational costs, potentially sacrificing some accuracy for greater efficiency. Conversely, a lower \ud835\udf06 prioritizes accuracy, allowing for more complex models that may require more computational resources. The choice of \ud835\udf06 depends on the specific requirements of the application and the hardware. The optimization objective for hardware-aware distillation can be expressed as:\n\nmin \ue238KD(\ud835\udc47 , \ud835\udc46) + \ud835\udf06 \u22c5 \ud835\udc36hardware(\ud835\udf03S)\n\nwhere \ue238KD(\ud835\udc47 , \ud835\udc46) refers to the knowledge distillation loss function, which quantifies the discrepancy between the outputs of the teacher model \ud835\udc47 and the student model \ud835\udc46. The term \ud835\udc36hardware(\ud835\udf03S) represents the computational cost associated with running the student model \ud835\udc46 on specific hardware, encompassing factors such as inference time, memory usage, and power consumption. The regularization parameter \ud835\udf06 is introduced to balance the trade-off between reducing the distillation loss and optimizing hardware efficiency. This formulation ensures that the student model is not only accurate but also optimized for the computational environment in which it will be deployed. Typically, hardware-aware distillation enhances the feasibility of deploying advanced ML models in resource-constrained environments by tailoring the student model to the specific hardware, making it particularly valuable for edge computing scenarios with strict power, memory, and processing limits. This approach leads to improved performance on the target hardware, offering faster inference times, lower power consumption, and more efficient memory usage, thereby optimizing model deployment in real-world applications. Additionally, hardware-aware distillation allows for customization to meet the unique requirements of various deployment environments, ensuring that models are optimized whether deployed on high-performance GPUs in data centers or low-power microcontrollers in IoT devices [286].\n\nHardware-aware distillation can be applied to real-time monitoring on edge devices, such as drones used for precision agriculture. By optimizing the student model to operate efficiently within the power and computational constraints of these drones, the model can quickly process high-resolution imagery to detect crop health issues or identify weeds, enabling swift, in-field decision-making without relying on cloud-based resources. This approach ensures that advanced analysis can be performed directly on the edge, even in remote or resource-limited environments.# 7.3.1. Robust Distillation Against Noisy Data\n\nThe effectiveness of KD heavily relies on the quality and quantity of training data. Robust distillation techniques need to be developed to handle noisy, sparse, or imbalanced datasets, which are common in RS. Robust Distillation Against Noisy Data focuses on enhancing the resilience of the KD process when dealing with imperfect data [287]. In real-world applications, particularly in RS, datasets often contain noise, inconsistencies, or imbalances that can degrade the performance of ML models. This approach aims to mitigate the impact of such issues by incorporating robustness into the distillation process, ensuring that the student model can still learn effectively even when the data is not ideal [288].\n\nIn RS, noisy data is common due to various factors like sensor errors, atmospheric interference, or mislabeling during data collection. Standard KD techniques may struggle with such data, leading to poor model performance [289, 290]. Robust distillation techniques address this by explicitly modeling and compensating for the noise during the training process. This can involve using techniques that identify and either correct or down-weight the influence of noisy samples on the student model [291, 292]. The key to robust distillation is modifying the loss function to account for the presence of noise. The traditional KD loss# Application of Knowledge Distillation in Remote Sensing\n\nfunction, which measures the difference between the teacher and student models, is augmented with a term that penalizes the model based on the amount of noise in the data. This term is controlled by a weighting factor \ud835\udf02, which determines how much influence the noise has on the overall learning process. By doing so, the student model becomes more robust to the effects of noise, learning to focus on cleaner, more reliable data [293]. Robust distillation techniques must strike a balance between learning from noisy and clean data.\n\nWhile it is important to minimize the negative impact of noise, completely ignoring noisy data could result in a loss of valuable information [294]. Therefore, these techniques aim to optimize the learning process by allowing the student model to still extract useful knowledge from noisy data while minimizing the distortion it causes [295]. The formulation for robust distillation against noisy data can be expressed as:\n\n&#x1D4C8;KD-robust = \ud835\udd3c\ud835\udc65,\u0303\ud835\udc65 \u223c&#x1D4C8;[&#x1D4C8;KD(\ud835\udc47 (\u0303 ), \ud835\udc46(\ud835\udc65)) + \ud835\udf02 \u22c5 Noise(\ud835\udc65,\u0303\ud835\udc65 )]\n\nwhere &#x1D4C8;KD(\ud835\udc47 (\u0303 ), \ud835\udc46(\ud835\udc65)) denotes the standard knowledge distillation loss, capturing the discrepancy between the teacher model\u2019s output on noisy data \ud835\udc65 and the student model\u2019s output on corresponding clean data \ud835\udc65. The weighting factor \ud835\udf02 regulates the influence of the noise term in the overall loss function, with a higher \ud835\udf02 placing greater emphasis on noise correction, thereby enhancing the model\u2019s robustness to noise. The term Noise(\ud835\udc65,\u0303\ud835\udc65 ) quantifies the noise level between the clean and noisy data, using metrics such as mean squared error (MSE) or other relevant measures of data corruption.\n\nThis aforementioned formulation ensures that the distillation process remains effective even in the presence of noisy data by integrating a mechanism to handle noise directly within the training objective. Robust distillation techniques enhance model robustness by enabling student models to handle real-world data imperfections, resulting in more reliable performance in practical applications like RS where data quality can vary [296]. These techniques also improve generalization across diverse data conditions by incorporating noise handling into the training process, ensuring that models perform well in both clean and noisy environments.\n\nAdditionally, robust distillation is particularly beneficial for sparse or imbalanced datasets, allowing student models to learn effectively from limited or unevenly distributed data while minimizing the risk of overfitting to noisy or rare examples [297]. In RS, robust distillation against noisy data can be applied to cloud detection in satellite imagery, where cloud cover often introduces noise that obscures ground features. By employing robust distillation techniques, a student model can be trained to accurately detect clouds even in images with varying levels of noise caused by atmospheric conditions, ensuring more reliable and consistent results for subsequent analyses, such as land use classification or vegetation monitoring.# 7.3.2. Semi-Supervised and Unsupervised Distillation\n\nSemi-Supervised and Unsupervised Distillation represents an emerging area of research in KD, particularly relevant for fields like RS where labeled data is often scarce or expensive to obtain. The traditional KD process relies heavily on labeled datasets to transfer knowledge from a teacher model to a student model [298]. However, in many practical scenarios, especially in RS, obtaining a large volume of labeled data is challenging. To address this, semi-supervised and unsupervised distillation techniques aim to leverage the abundant unlabeled data available, reducing the dependency on labeled datasets and making the distillation process more robust and scalable [299].\n\nWhile it is important to minimize the negative impact of noise, completely ignoring noisy data could result in a loss of valuable information [294]. Therefore, these techniques aim to optimize the learning process by allowing the student model to still extract useful knowledge from noisy data while minimizing the distortion it causes [295]. The formulation for robust distillation against noisy data can be expressed as:\n\n&#x1D4C8;KD-semi = \ud835\udefc\u22c5&#x1D4C8;KD(\ud835\udc47 , \ud835\udc46; &#x1D4C8;labeled)+(1\u2212\ud835\udefc)\u22c5&#x1D4C8;KD(\ud835\udc47 , \ud835\udc46; &#x1D4C8;unlabeled)\n\nThe overall loss function includes &#x1D4C8;KD(\ud835\udc47 , \ud835\udc46; &#x1D4C8;labeled), which represents the KD loss computed using the labeled dataset &#x1D4C8;labeled, and &#x1D4C8;KD(\ud835\udc47 , \ud835\udc46; &#x1D4C8;unlabeled), which is the distillation loss calculated from the unlabeled dataset &#x1D4C8;unlabeled where the teacher model\u2019s predictions serve as pseudo-labels. The weighting factor \ud835\udefc adjusts the influence of the labeled and unlabeled data on the overall loss function.\n\nThe above formulation allows the student model to learn from both labeled and unlabeled data, making the training process more flexible and less dependent on extensive labeled datasets. Semi-supervised and unsupervised distillation techniques offer significant practical advantages by reducing the reliance on large, high-quality labeled datasets, making model training more feasible in resource-constrained environments. These methods enhance generalization by exposing models to a wider variety of data patterns, allowing them to perform well on new, unseen data\u2013an essential capability in fields like RS, where data diversity is high.\n\nAdditionally, these techniques provide a scalable approach to model training, enabling organizations to leverage vast amounts of unlabeled data to build robust models without the need for extensive manual labeling efforts. Semi-supervised and unsupervised distillation can be applied to satellite imagery for land cover classification, where acquiring labeled data for every land type is impractical. By using semi-supervised distillation, a model can initially learn from a small set of labeled images and then leverage the large pool of unlabeled satellite images, where the teacher model provides pseudo-labels to refine the student model\u2019s performance.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 34 of 50# Application of Knowledge Distillation in Remote Sensing# 7.4.1. Distributed Distillation\n\nDistributed distillation is an advanced approach designed to scale the KD process across larger datasets by leveraging distributed learning frameworks. This method spreads the computational load of both the teacher and student models across multiple nodes or devices, making it more feasible to handle large-scale data. The key idea is to perform distillation in a parallel or distributed manner, where each node or device processes a subset of the data, thus reducing the individual computational burden and allowing for more efficient training [303]. In distributed distillation, the overall training task is divided among multiple nodes in a distributed learning framework. Each node independently performs a portion of the distillation process, working with its local subset of data. This division of labor helps in managing the computational load effectively, enabling the distillation process to scale with the size of the dataset. The use of multiple nodes allows for parallel processing, which significantly speeds up the training process [304]. Each node in the distributed system hosts a teacher model and a student model, denoted as \ud835\udc47\ud835\udc58 and \ud835\udc46\ud835\udc58, respectively, where \ud835\udc58 represents the node index. These models operate on the local data available to that node. The student model on each node learns from its corresponding teacher model, capturing knowledge specific to that subset of the data. This localized learning allows the student models to collectively capture diverse knowledge from the entire dataset when combined [305]. After the distributed distillation process is complete, the student models from all nodes are aggregated to form a comprehensive model that incorporates the knowledge distilled from all parts of the dataset. The aggregation can be done by averaging the model weights, or by combining the outputs of the student models. The objective function \ue238KD-distributed is averaged over all nodes, ensuring that the final student model reflects a balanced learning from the entire distributed dataset [306]. The primary advantage of distributed distillation is its scalability. By distributing the workload, it becomes feasible to train on extremely large datasets that would be otherwise impractical to process on a single machine. This approach is particularly useful in scenarios like RS, where data is often collected in large quantities from multiple sources and needs to be processed efficiently. Distributed distillation allows for more rapid training and deployment of models, making it an effective solution for handling big data in ML [307, 308].\n\nThe loss function for distributed distillation is given by:\n\n\ud835\udc3e\n\n\ue238KD-distributed = 1 \u2211\ue238KD(\ud835\udc47\ud835\udc58, \ud835\udc46\ud835\udc58) (23)\n\n\ud835\udc3e  \ud835\udc58=1\n\nwhere \ue238KD(\ud835\udc47\ud835\udc58, \ud835\udc46\ud835\udc58) represents the KD loss computed on the \ud835\udc58-th node, with \ud835\udc47\ud835\udc58 and \ud835\udc46\ud835\udc58 being the teacher and student models on that node. The term \ud835\udc3e denotes the total number of nodes involved in the distributed learning process. By averaging the distillation losses across all nodes, the overall objective function ensures that the student model benefits from the collective knowledge distributed across the dataset [306]. Distributed distillation can be utilized in the analysis of global satellite imagery, where the vast amount of data is processed across multiple computational nodes. For instance, in mapping land use changes over large geographic regions, distributed distillation allows each node to handle different segments of the satellite images, collectively training a student model that integrates insights from all segments, leading to a comprehensive and scalable approach for detecting and classifying land cover changes [309].# 7.4.2. Incremental Distillation\n\nIncremental distillation is a technique designed to efficiently handle the continuous influx of new data without the need to retrain the student model from scratch each time new information becomes available. This method is particularly beneficial in scenarios where datasets grow over time, such as in real-time applications or when new data is periodically added, as it allows for the model to be updated incrementally [310]. The key idea behind incremental distillation is to enable the student model to learn from new data as it arrives, while also retaining the knowledge gained from previous training sessions. At each time step \ud835\udc61, the student model \ud835\udc46\ud835\udc61 is trained to mimic the behavior of the teacher model \ud835\udc47, which is typically derived from the latest data [311]. A critical aspect of incremental distillation is the preservation of previously learned knowledge. As the student model is updated with new data, it is important to ensure that it does not forget what it learned in earlier stages. This is achieved by incorporating a historical distillation term \ue238history in the loss function, which measures the difference between the current student model \ud835\udc46\ud835\udc61 and its previous version \ud835\udc46\ud835\udc61\u22121. This regularization term helps in maintaining continuity and stability in the learning process, preventing catastrophic forgetting [312]. The overall loss function for incremental distillation, \ue238KD-incremental, includes two components:\n\n- KD Loss: \ue238KD(\ud835\udc47, \ud835\udc46\ud835\udc61) ensures that the student model learns from the current teacher model at time \ud835\udc61.\n- Historical Distillation Loss: \ue238history(\ud835\udc46\ud835\udc61, \ud835\udc46\ud835\udc61\u22121) penalizes deviations from the knowledge previously acquired by the student model, helping it retain important information from past iterations.\n\nThe parameter \ud835\udefd controls the balance between learning new information and preserving old knowledge. A higher \ud835\udefd places more emphasis on retaining historical knowledge, while a lower \ud835\udefd allows the model to adapt more quickly to new data [313]. Incremental distillation provides significant practical benefits by allowing the student model to be updated incrementally, thereby avoiding the high computational costs associated with full model retraining. This approach is especially advantageous for large-scale applications where datasets are continuously evolving, such as in RS or streaming analytics [314]. It ensures that the model remains adaptive to new data trends, maintaining its relevance and accuracy over time. Additionally, the use of a historical distillation term enhances the stability of the learning process, minimizing the risk of performance degradation as new data is incorporated [315]. Incremental distillation can be used for real-time monitoring of deforestation in satellite imagery. As new satellite images become available, the student model is updated incrementally to learn from the latest data while preserving its ability to recognize previously identified deforestation patterns, thus allowing continuous and efficient.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 35 of 50# Application of Knowledge Distillation in Remote Sensing# 7.5.1. Real-Time Distillation Algorithms\n\nReal-time distillation algorithms are critical in scenarios where timely decision-making is essential, such as in RS applications that involve disaster monitoring or autonomous systems. These algorithms are designed to optimize both the accuracy and speed of the inference process, ensuring that the student model can deliver predictions within the stringent time constraints required for real-time operations [307]. In real-time applications, there\u2019s a trade-off between model accuracy and inference speed. A highly accurate model may be too slow for real-time processing, while a faster model might sacrifice accuracy. Real-time distillation algorithms aim to strike a balance by training the student model to achieve an acceptable level of accuracy while ensuring that it can make predictions quickly enough to meet real-time requirements [317]. The inference time, denoted as \ud835\udc61inference, is the time it takes for the model to process an input and produce an output. For real-time applications, this must be less than or equal to a predetermined threshold \ud835\udc61real-time. The distillation process incorporates this requirement into the training by adding a penalty term in the loss function if the student model\u2019s inference time exceeds the threshold. This ensures that the final model is optimized not only for accuracy but also for speed [318]. The penalty term \ud835\udefe \u22c5 \ud835\udd40(\ud835\udc61inference \u2264 \ud835\udc61real-time) in the loss function introduces a strong disincentive for any delay in inference time. Here, \ud835\udd40(\u22c5) is an indicator function that activates the penalty whenever the inference time \ud835\udc61inference is greater than the real-time threshold \ud835\udc61real-time. This encourages the model to adhere strictly to time constraints, making it suitable for deployment in environments where every millisecond counts, such as in disaster response or real-time traffic management [319]. In RS, where data often needs to be processed and acted upon quickly (e.g., detecting natural disasters, monitoring climate changes, or guiding autonomous vehicles), real-time distillation ensures that models are both accurate and fast enough to be practical. This is particularly important in disaster monitoring, where delays in data processing could result in missed opportunities to mitigate damage or save lives [320]. The loss function for real-time distillation, \ud835\udcdbKD-real-time, combines the traditional KD loss with a penalty for exceeding the inference time threshold:\n\n\ud835\udcdbKD-real-time = \ud835\udcdbKD(\ud835\udc47 , \ud835\udc46) + \ud835\udefe \u22c5 \ud835\udd40(\ud835\udc61inference \u2264 \ud835\udc61real-time) (24)\n\nwhere \ud835\udcdbKD(\ud835\udc47 , \ud835\udc46) represents the standard Knowledge Distillation (KD) loss, which measures how effectively the student model replicates the teacher model\u2019s behavior. The hyperparameter \ud835\udefe controls the strength of the penalty applied when the inference time exceeds the real-time threshold. Additionally, the indicator function \ud835\udd40(\u22c5) activates this penalty if the inference time \ud835\udc61inference surpasses the allowed real-time limit \ud835\udc61real-time. This formulation ensures that the student model not only learns effectively from the teacher model but also adheres to the necessary timing constraints for real-time deployment [321]. The development of real-time distillation algorithms provides several practical benefits, including the ability to make timely decisions, which is critical in applications such as disaster monitoring where delays can have severe consequences. These algorithms efficiently balance the trade-offs between speed and accuracy, ensuring models are both effective and practical for real-time use. Additionally, their versatility makes them suitable for a wide range of fields that require real-time processing, from RS to autonomous driving and other time-sensitive applications [322]. Real-time distillation algorithms can be utilized in monitoring wildfires through satellite imagery, where immediate detection and response are crucial. The student model is trained to rapidly process satellite images and detect fire outbreaks in real-time while adhering to stringent time constraints, ensuring timely alerts for emergency services and minimizing potential damage [323].# 7.5.2. Edge-AI Distillation\n\nEdge-AI distillation focuses on optimizing ML models specifically for deployment on edge devices, such as sensors, drones, or other low-power, resource-constrained hardware commonly used in RS applications [324]. These devices often have limited computational power, memory, and battery life, which necessitates the development of highly efficient models that can perform complex tasks with minimal resources. The goal of edge-AI distillation is to ensure that the distilled student model is not only accurate but also energy-efficient and capable of running with low latency on edge devices [325].\n\nThe key to achieving this lies in balancing the KD process with the energy consumption constraints of the target hardware. The distillation loss function denoted as \ud835\udcdbKD(\ud835\udc47 , \ud835\udc46), is typically used to align the outputs of the student model \ud835\udc46 with those of the teacher model \ud835\udc47. However, in the context of edge-AI, an additional term, Energy(\ud835\udf03S), is introduced into the objective function to account for the energy consumption of the student model on the edge device [326].\n\nThis approach involves minimizing not just the distillation loss but also the energy consumption associated with running the student model. The regularization parameter \ud835\udeff is used to balance the importance of energy efficiency against the need to maintain high model performance. By carefully tuning this parameter, it is possible to develop models that are both effective in their predictive capabilities and efficient in terms of energy usage [327].\n\nEdge-AI distillation can be employed in drones for real-time wildlife monitoring, where models need to process and analyze video feeds on the device itself. By optimizing the model for low power consumption and fast inference, drones can efficiently identify and track animal movements without draining their batteries or relying on constant data transmission to central servers.# 7.6.1. Cross-Modal Knowledge Transfer\n\nCross-modal distillation involves transferring knowledge from one modality (e.g., optical images) to another (e.g., SAR or multispectral images). This approach can improve model generalization across different data sources [328, 329].\n\nY. Himeur, et al.: Preprint submitted to Elsevier# Application of Knowledge Distillation in Remote Sensing\n\n\ud835\udc40\n\n\ue238KD-cross = \u2211\ud835\udc5a=1\ud835\udc64\ud835\udc5a \u22c5 \ue238KD(\ud835\udc47\ud835\udc5a(\ud835\udc65\ud835\udc5a), \ud835\udc46(\ud835\udc65\ud835\udc5a)) (25)\n\nwhere \ud835\udc40 is the number of modalities, and \ud835\udc64\ud835\udc5a is the weight associated with each modality \ud835\udc5a.\n\nCross-modal knowledge transfer is a specialized technique within the broader context of KD, focusing on transferring knowledge from one data modality to another. In RS, data is often collected from various sources, each providing unique and complementary information about the environment. For instance, optical images capture visible light, SAR (Synthetic Aperture Radar) images provide microwave data, and multispectral images cover a range of wavelengths beyond the visible spectrum. Each modality offers distinct advantages and limitations, making it valuable to develop models that can generalize across these diverse data types [330]. The essence of cross-modal distillation lies in the ability to leverage a teacher model trained on one modality (e.g., optical images) to improve the performance of a student model operating on a different modality (e.g., SAR or multispectral images). This process enhances the student model\u2019s ability to generalize and perform well across different data sources, which is crucial in RS tasks that require robust performance across varying environmental conditions and sensor types [329].\n\nThe formulation of cross-modal distillation is encapsulated in the loss function \ue238KD-cross. This loss function aggregates the KD process across multiple modalities, denoted by \ud835\udc40. For each modality \ud835\udc5a, a specific weight \ud835\udc64\ud835\udc5a is assigned, reflecting the importance or relevance of that modality in the distillation process. The objective is to minimize the weighted sum of the KD losses \ue238KD for each modality, where \ud835\udc47\ud835\udc5a(\ud835\udc65\ud835\udc5a) represents the output of the teacher model for modality \ud835\udc5a, and \ud835\udc46(\ud835\udc65\ud835\udc5a) is the corresponding output of the student model [331]. By carefully selecting and tuning the weights \ud835\udc64\ud835\udc5a, the cross-modal distillation process can be tailored to emphasize certain modalities over others, depending on the specific requirements of the application. For example, in scenarios where optical images are more informative, the weight \ud835\udc64\ud835\udc5a for optical data can be increased, ensuring that the student model learns more effectively from that modality [332]. Cross-modal knowledge transfer is particularly beneficial in RS, where data from different modalities may be abundant but not always consistently available. By enabling models to transfer knowledge across modalities, this approach ensures that the student model remains robust and effective even when some data sources are missing or degraded. This capability is critical in applications such as environmental monitoring, disaster response, and resource management, where reliable and accurate information from diverse data sources is essential for decision-making [333].\n\nAs mentioned above, cross-modal knowledge transfer can be applied in integrating SAR and optical satellite imagery for flood monitoring. By leveraging a teacher model trained on high-resolution optical images, a student model can be optimized to accurately interpret SAR images, enhancing the detection of flood extents and water levels in areas where optical data might be obstructed or unavailable.# 7.6.2. Multi-Task Distillation\n\nMulti-task distillation is an advanced technique in KD where a single student model is trained to perform multiple tasks simultaneously, such as classification, segmentation, and object detection. This approach aims to create a more versatile and efficient model that can handle a variety of tasks without compromising performance in any of them. By distilling knowledge from teacher models specialized in different tasks, the student model learns to balance these tasks effectively, making it highly valuable in applications where multi-functionality is essential, such as in RS or autonomous systems [334]. Multi-task distillation involves training a student model to handle multiple tasks concurrently. Each task has its own teacher model, and the student model learns from these teachers to perform all tasks simultaneously. For example, in RS, one teacher model might be specialized in land cover classification, while another is focused on detecting specific objects like vehicles or buildings. The student model is designed to learn from both these tasks, allowing it to perform land cover classification and object detection within the same framework [335].\n\nThe loss function for multi-task distillation, denoted as \ue238KD-multi-task, is a weighted sum of the distillation losses from each task. The weight \ud835\udf06\ud835\udc61 assigned to each task allows for prioritization based on the importance or difficulty of the task. For instance, if segmentation is more critical for a specific application than classification, a higher weight can be assigned to the segmentation task to ensure the student model focuses more on that aspect [336].\n\n\ud835\udc47\n\n\ue238KD-multi-task = \u2211\ud835\udc61=1\ud835\udf06\ud835\udc61 \u22c5 \ue238KD(\ud835\udc47\ud835\udc61, \ud835\udc46\ud835\udc61) (26)\n\nWhere \ud835\udc47 represents the total number of tasks, and \ue238KD(\ud835\udc47\ud835\udc61, \ud835\udc46)\ud835\udc61 is the KD loss for task \ud835\udc61 between the teacher \ud835\udc47\ud835\udc61 and the student model \ud835\udc46. One of the key challenges in multi-task distillation is balancing the learning of different tasks. Since each task might require different levels of focus or complexity, the student model must be carefully trained to avoid underperforming in one task while excelling in another. The weight \ud835\udf06\ud835\udc61 helps manage this balance by allowing certain tasks to have more influence on the model\u2019s learning process [337]. By combining multiple tasks into a single model, multi-task distillation reduces the need for deploying separate models for each task, saving computational resources and simplifying the deployment process. This makes the distilled model more efficient and versatile, particularly in environments where resources are limited or where real-time processing of multiple tasks is required [338].\n\nIn practical scenarios, such as RS, a multi-task distilled model could simultaneously analyze satellite images for land classification, detect changes over time, and identify specific objects or features. This approach is not only more efficient but also enables the model to leverage shared information across tasks, leading to better overall performance and more coherent results across the different tasks [339]. While multi-task distillation offers numerous advantages, it also comes with challenges, such as the potential for task interference, where learning one task might negatively impact another. Careful design of the distillation process and appropriate weighting of tasks are essential to ensure that the student model performs well across all tasks [340]. A multi-task distilled model can be used for comprehensive.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 37 of 50# Application of Knowledge Distillation in Remote Sensing\n\nUrban monitoring from satellite images, where it simultaneously performs land cover classification, detects infrastructure changes (such as new buildings or roads), and identifies specific objects (like vehicles or trees). This enables efficient and integrated analysis of diverse data, enhancing the ability to track urban development and infrastructure changes in a single model [341].# 7.7. Seamless Integration with Existing Workflows# 7.7.1. Plug-and-Play Distillation Modules\n\nCreating plug-and-play distillation modules that can be easily integrated into existing RS workflows offers several benefits [342]. These modules are designed to be seamlessly incorporated with minimal adjustments to the current infrastructure, reducing the barriers to adopting knowledge distillation (KD) in RS applications [33]. The formulation of this integration can be expressed as:\n\n\ud835\udc40\n\n\ue238KD-plug = \ue238KD(\ud835\udc47 , \ud835\udc46) +\u2211\ue238mod(\ud835\udc46, \ud835\udc40\ud835\udc5a)\n\n\ud835\udc5a=1\n\nwhere \ue238KD(\ud835\udc47 , \ud835\udc46) represents the standard KD loss between the teacher model \ud835\udc47 and the student model \ud835\udc46, and \ue238mod(\ud835\udc46, \ud835\udc40\ud835\udc5a) denotes the additional loss terms introduced by integrating existing modules \ud835\udc40\ud835\udc5a with the student model \ud835\udc46. The modularity of this approach allows different components to be swapped or upgraded independently, facilitating the adoption of KD in diverse scenarios. This scalability ensures that KD remains applicable across a wide range of RS tasks, from small UAV datasets to extensive satellite imagery. By leveraging pre-built modular KD techniques, developers and researchers can save time on implementation, accelerating the development and deployment of RS models, which ultimately enhances their performance [343, 344].# 7.7.2. Toolkits and Frameworks\n\nDeveloping comprehensive toolkits and frameworks for KD in RS can significantly enhance its performance and adoption. These toolkits provide standardized implementations of KD methods, ensuring consistency and reliability across different RS tasks [345]. The complexity of integrating various modules within the KD process can be expressed as:\n\n\u2211\ud835\udc36integration(\ud835\udc40, \ue238KD)\n\nToolkit Complexity \u221d \ud835\udc41\n\n\ud835\udc56=1\n\nwhere \ud835\udc36integration(\ud835\udc40, \ue238KD) represents the complexity of integrating module \ud835\udc40\ud835\udc56 with the KD process. This standardization reduces the variability in performance that can arise from ad-hoc implementations, leading to more predictable results and making KD a more reliable option for RS applications. Additionally, these toolkits lower the technical barriers for practitioners by providing user-friendly interfaces and comprehensive documentation. Frameworks often include optimized routines for tasks such as hyperparameter tuning or data preprocessing, which can lead to better model performance and faster training times, especially in computationally intensive RS tasks. The community-driven development of these toolkits and frameworks leads to faster identification of bugs, new feature additions, and overall better support for the technology. This collective improvement makes KD more practical and effective for RS tasks, contributing to enhanced model performance and more efficient applications [346].# 7.8.1. Explainable Distillation\n\nExplainable distillation is an advanced approach in KD that not only focuses on transferring the predictive performance of the teacher model to the student model but also ensures that the student model\u2019s decision-making process is interpretable. The goal is to create models that are not just accurate but also transparent, providing insights into how they arrive at their predictions [347]. This is particularly important in critical applications like RS, healthcare, and autonomous systems, where understanding the model\u2019s reasoning is crucial for trust and accountability [348]. Explainable distillation mainly relies on the following key concepts. Traditional KD emphasizes performance, often at the cost of interpretability. However, in many applications, it is essential to know why a model makes a certain decision. Explainable distillation aims to bridge this gap by integrating interpretability into the distillation process. The student model is trained not only to mimic the teacher\u2019s outputs but also to generate explanations for its decisions that are comprehensible to humans [349]. The process of explainable distillation involves an additional term in the loss function, denoted as:\n\n\ue238KD-explain = \ue238KD(\ud835\udc47 , \ud835\udc46) + \ud835\udf09 \u22c5 \ue238explain(\ud835\udc46) (27)\n\nwhere, \ue238KD(\ud835\udc47 , \ud835\udc46) is the standard KD loss, and \ud835\udf09 is a regularization parameter that controls the trade-off between accuracy and interpretability. The term \ue238explain(\ud835\udc46) guides the student model toward generating explanations that are either inherently interpretable or match the explanations provided by the teacher model if available [348]. In domains such as RS, where decisions based on model predictions can have significant real-world impacts, the ability to explain model outputs is critical. For example, when a model identifies a potential disaster area in satellite imagery, it is not enough to simply flag the area; stakeholders need to understand the reasoning behind the decision, such as the specific patterns or features that led to the prediction [349]. Various explainable AI techniques can be incorporated into the distillation process, such as saliency maps, attention mechanisms, or feature attribution methods [351]. These techniques help to visualize and understand which parts of the input data are most influential in the model\u2019s decision-making process. By integrating these techniques into the distillation process, the student model becomes not only a distilled version of the teacher but also a more interpretable and transparent model [352].\n\nAll in all, explainable distillation holds significant potential in fields where trust in AI systems is paramount. By producing models that are both accurate and interpretable, it enhances the usability and acceptance of AI systems in sensitive areas. Moreover, it aids in regulatory compliance.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 38 of 50# Application of Knowledge Distillation in Remote Sensing\n\nAs interpretable models can more easily meet legal and ethical standards regarding AI transparency [353]. Besides, while explainable distillation is a promising approach, it comes with challenges, such as defining and quantifying interpretability in a way that aligns with both human understanding and model performance. Future research may focus on developing more sophisticated explainability metrics and integrating them seamlessly into the distillation process, ensuring that models are not only effective but also understandable and trustworthy [354].# 7.9.1. Combining Distillation with Other Techniques\n\nHybrid approaches in ML involve the integration of multiple learning paradigms to create more powerful and versatile models. When applied to KD, these hybrid approaches can significantly enhance the performance and efficiency of models, especially in complex and data-rich fields like RS [360]. For instance, a hybrid approach relies on combining KD with transfer learning and federated learning. Typically, transfer learning involves leveraging knowledge gained from one task or domain to improve the performance on a related but different task. In the context of hybrid approaches, transfer learning can be integrated into the distillation process to help the student model acquire additional knowledge from pre-trained models on related tasks [361]. The transfer learning loss component, \ud835\udcdbTransfer(\ud835\udc46), represents the cost associated with adapting the student model to the new task or domain. Moving on, reinforcement learning is a paradigm where models learn to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. In hybrid distillation approaches, RL can be incorporated to optimize the student model\u2019s performance through trial and error, especially in scenarios where decision-making under uncertainty is critical. The RL loss component, \ud835\udcdbReinforce(\ud835\udc46), measures how well the student model performs in achieving its objectives within the environment [362].\n\nBesides, active learning is another technique that can be integrated with KD. It involves selectively querying the most informative data points for training, thereby improving the model\u2019s performance with fewer labeled examples. While not explicitly included in the equation, active learning can complement the distillation process by ensuring that the most critical data points are used for training the student model [363, 364]. The hybrid loss function in this approach combines the losses from KD, transfer learning, and reinforcement learning:\n\n\ud835\udcdbKD-hybrid = \ud835\udefc\u22c5\ud835\udcdbKD(\ud835\udc47 , \ud835\udc46) + \ud835\udefd\u22c5\ud835\udcdbTransfer(\ud835\udc46) + \ud835\udefe \u22c5\ud835\udcdbReinforce(\ud835\udc46) (29)\n\nIn this context, \ud835\udefc, \ud835\udefd, and \ud835\udefe are weighting factors that determine the contribution of each component to the overall loss function. The term \ud835\udcdbKD(\ud835\udc47 , \ud835\udc46) ensures that the student model closely mimics the behavior of the teacher model, while \ud835\udcdbTransfer(\ud835\udc46) aids the student model in adapting to new tasks or domains by utilizing previously acquired knowledge. Additionally, \ud835\udcdbReinforce(\ud835\udc46) enhances the student model\u2019s decision-making abilities through interaction with an environment, thereby optimizing its performance [365].\n\nBy integrating multiple learning paradigms, hybrid approaches can create models that are not only smaller and faster (through distillation) but also more knowledgeable and capable (through transfer learning) and more adaptive (through reinforcement learning). Hybrid models are better.\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 39 of 50# Application of Knowledge Distillation in Remote Sensing\n\nEquipped to handle a variety of tasks and environments, as they combine the strengths of different learning methods. This is particularly useful in RS, where data can vary widely in type, quality, and context. The ability to combine different learning strategies makes hybrid approaches highly versatile. For example, a hybrid model might use transfer learning to understand basic image recognition tasks while using reinforcement learning to make real-time decisions based on that understanding [366].\n\nHybrid distillation approaches are particularly relevant in complex fields such as RS, where models need to process and analyze vast amounts of data from multiple sources. For instance, in disaster monitoring, a hybrid model could use transfer learning to recognize different types of terrain, reinforcement learning to predict the spread of a fire, and KD to ensure the model is efficient enough to run on edge devices deployed in the field [367]. However, combining multiple learning paradigms can increase the complexity of the model and its training process. Careful tuning of the weighting factors (\ud835\udefc, \ud835\udefd, \ud835\udefe) is necessary to achieve the desired balance between the different learning objectives. Moreover, while the goal of distillation is to create efficient models, the integration of transfer learning and reinforcement learning can demand additional computational resources during training, which may limit the scalability of the approach [368].# 7.9.2. Adaptive Distillation Frameworks\n\nDeveloping adaptive distillation frameworks that can adjust the distillation process based on the complexity of the task or the availability of data could lead to more flexible and robust models. Traditional knowledge distillation methods apply a fixed strategy for transferring knowledge from the teacher model to the student model, which might not be optimal for all scenarios, particularly in RS where data heterogeneity and varying task requirements are common [269]. An adaptive distillation framework dynamically adjusts the distillation process by incorporating additional mechanisms that account for task complexity, data quality, or computational constraints. This dynamic adjustment allows the distillation process to be more responsive to the specific needs of the application, improving both the efficiency and effectiveness of the student model [369].\n\nFor instance, in scenarios where the task is relatively simple or where data is abundant and high-quality, the framework could prioritize rapid model convergence by assigning higher weights to the knowledge distillation loss. Conversely, in more complex tasks or when dealing with sparse or noisy data, the framework could allocate more resources to the adaptation process, fine-tuning the student model to handle these challenges more effectively [370]. The adaptive distillation process can be represented as:\n\n\ud835\udcdbKD-adaptive = \u2211t=1T \ud835\udefct \u22c5 \ud835\udcdbKD(\ud835\udc47t, \ud835\udc46) + \ud835\udf07t \u22c5 \ud835\udcdbadaptive(\ud835\udc46)\n\nwhere \ud835\udefct and \ud835\udf07t are time-varying weights for the KD and adaptation loss components, respectively. These weights are not static but rather evolve over time \ud835\udc61 based on factors such as the current performance of the student model, the difficulty of the current task, and the quality of the data available at each step [371].\n\nThe term \ud835\udcdbKD(\ud835\udc47t, \ud835\udc46) represents the traditional knowledge distillation loss, which measures the discrepancy between the outputs of the teacher and student models. The term \ud835\udcdbadaptive(\ud835\udc46) is an additional adaptation loss that could include penalties for model complexity, regularization terms to prevent overfitting, or other factors that enhance the student model\u2019s ability to generalize across different tasks or datasets [372]. By allowing the distillation process to adapt to varying conditions, these frameworks can produce student models that are not only smaller and faster but also more versatile and capable of maintaining high performance across a range of tasks and environments. This adaptability is particularly valuable in RS, where the conditions under which models operate can vary significantly, and the ability to generalize effectively is crucial [373].",
        "context_id": 8,
        "question": "What type of neural networks are highly adept at handling high-dimensional data from RS imagery according to the document?",
        "answer": [
            "Convolutional Neural Networks"
        ],
        "context_length": 212578
    },
    {
        "context": "# 1 Introduction\n\nThe solid-state, or \u201cceramic\u201d method is a simple and ubiquitous technique for synthesizing inorganic crystalline solids in which powder precursors are heated to elevated reaction temperatures under controlled atmospheric conditions4. This method is used at both the small-scale (e.g., research laboratories attempting the synthesis of new materials) and large-scale (industrial manufacturing processes) to produce a wide variety of important functional materials such as battery cathodes (including LiMnPO45 and LiFePO46), the ferroelectrics BaTiO37, YMnO38, and BiFeO39, and many superconductors, including FeSe0.8810, YBa2Cu3O6+x11, and MgB212. Despite the ubiquity of the method, no conventional system for designing or modeling solid-state synthesis recipes exists. Instead, recipes have long been designed primarily using expert knowledge (e.g., precursor selection from a common library or via phase diagram analysis) and heuristic guidelines13;14.\n\nThe difficulty in modeling solid-state synthesis reactions can be illustrated by drawing a comparison to organic molecular synthesis in which recipes can be generated by working backward from a desired product molecule to a set of known precursors via a series of mechanistically well-defined steps in a process known as retrosynthesis15. In contrast, high-temperature solid-state synthesis proceeds by spontaneous thermodynamic reactions which lack clearly defined intermediates and reaction mechanisms. However, enabled by the recent rise of high-throughput density functional theory (DFT) calculations16;17 and the databases generated by them1;18;19;20, several new automatable methods for designing solid-state synthesis recipes have emerged. These methods, which include measures for determining the synthesizability of a desired target21;22, metrics for comparing the selectivity of reaction recipes23;24, tools for extracting synthesis data directly from natural language25;26;27, and reaction networks which identify thermodynamically favorable pathways between specified precursor and target materials28, have yielded early success in guiding synthesis recipe design, despite being built on zero-temperature simulations of ordered crystalline structures. Furthermore, advances in autonomous synthesis have increased throughput for synthesis experiments29;30;31 and motivated the development of synthesis design algorithms that utilize experimental results to improve their planning32. While each of these methods provides an element of recipe design guidance, none of them allows for the quantitative prediction of the time and temperature-resolved emergence and consumption of phases during the execution of a synthesis recipe.\n\nThough no a priori simulation exists for predicting the progression of solid-state synthesis reactions, other reaction classes have been captured by simulation methods that are not neatly transferable to the solid-state case. For example, the kinetic Monte Carlo method is frequently used to model the evolution of species in gas or liquid phase molecular reactions (often in conjunction with reaction rates calculated using transition state theory33;34). This method assumes integer numbers of discrete particles that transform via reaction into other sets of discrete particles, and it assumes that these particles are available to interact with each other with no heed paid to their spatial arrangement35. These two assumptions do not hold for solid-state reactions; instead, solid phases transform in continuous amounts from reactant to product and reactant particles do not move as freely as they do in liquid phase reactions (barring the presence of a molten flux or gas transport). Surface reactions have been successfully modelled by lattice Monte Carlo simulations to determine heterogeneous catalytic behavior, but they explicitly treat the motion of individual atoms36. This method is not feasible for modeling the evolution of the powder contents of a solid-state reaction vessel because the large number of atoms are involved (often on the order of 1010 - 1020 atoms or more) leads to intractable computing requirements. Indeed, any atomistic method presents similar limitations. Finally, phase field models have been used to model ionic diffusion during solid-state metathesis reactions37, but these methods require significant assumptions.# 1. Introduction\n\nIn light of these challenges, we present in this work a simulation framework (ReactCA) that predicts the time-dependent, quantitative evolution of phases over the course of a prescribed solid-state reaction as a function of precursor ratio, heating profile, and reaction atmosphere. To achieve this, we leverage the cellular automaton (CA) formalism which offers a flexible framework for addressing the unique challenges posed by solid-state reactions.\n\nA CA is defined by a grid of sites, each of which is assigned a state value. At each step in the evolution of the automaton, the state in each site is updated according to its own current value and the states of the sites neighboring it (the \u201cneighborhood\u201d). The specific nature of the state values and the rule governing evolution (the \u201cevolution rule\u201d) can be chosen to best suit the simulation problem at hand. As a result of this flexibility, cellular automata have been used in materials science and chemistry to model a variety of processes, including grain growth, crystallization, and surface adsorption/desorption.\n\nDue to the fundamentally spatial nature of the neighborhood and the flexibility of the evolution rule, the CA structure is a natural choice for modeling solid-state synthesis.\n\nThe simulation framework described here utilizes zero-temperature thermodynamic properties of ordered crystalline compounds from the Materials Project as its primary input data. Importantly, some of the compounds we simulate here (and many of those found in the Materials Project) can accommodate disorder, which would increase the entropy of these phases and affect the energetics of the reactions which form them, especially at higher temperatures. To date however, no existing database of computed material properties contains rigorous representations of configurationally disordered materials and their entropy.\n\nAs a result, for the ordered materials in the Materials Project, we consider only the vibrational entropy contribution to the Gibbs formation energies as estimated by the machine learning method of Bartel et al. These estimates, in conjunction with the cellular automaton formalism, are used to capture the thermodynamic and spatial features of solid-state reactions in addition to some rudimentary kinetic effects based on machine learning estimates of phase melting points. Our framework offers new functionality in automated solid-state synthesis planning in that it enables the facile prediction of quantitative reaction outcomes a priori as a function of temperature profile, reaction atmosphere and precursor choice.\n\nWe envision our framework being used as a straightforward, easily implemented method for testing hypothesized recipes before attempting them in the lab, for implementing a digital twin in an autonomous laboratory designing its own synthesis recipes, and for refining synthesis parameters when used in conjunction with optimization frameworks.# 2.1 Cellular Automaton Model\n\nThe solid-state reaction cellular automaton simulation described herein (ReactCA) is constructed based on the pairwise model of solid-state reactions. This model states that solid-state powder reactions proceed predominantly via sequential reactions at pairwise interfaces (i.e., between only two solid species at a time). This model has its theoretical basis in the spatial geometry of the contact regions between particles, and has been verified with in situ experiments.\n\nThe solid-state reaction process, illustrated in Figure 1a, proceeds via the diffusion of atomic species driven by chemical potential differences across the interface between two reactant particles. As the reaction# Modeling Solid-State Reactions\n\nProgresses, nuclei of one or more stable product phases form and grow at the interface, converting reactant material into product. Importantly, the local composition of the interface region is determined by the kinetic availability of reacting species and not constrained to reflect the overall composition of the precursor mixture. The first product phase to form is then a function of the \u201clocal\u201d composition (as opposed to the overall composition) and the relative energetics of the possible product phases41.# b) Composition\n\nReaction Sites\nPrecursors mixed\nReactions initiate at interfaces\nProduct phases form# Evolution\n\n|Calculate phase energies|Initial State|A + B|\n|---|---|---|\n|Enumerate|Score Rxns|2A + 3D - 2C + B|\n|Specify Recipe|B + 3D - - 2F +|Precursor Ratios|\n|Heating Profile|Final State|Reaction coordinate|\n\nFigure 1: Modeling solid-state reactions with a cellular automaton. a) The progression of the initial stages of a solid-state reaction occurring via the pairwise interface reaction model; b) convex hull schematic representing the thermodynamics of reactions between two hypothetical solid precursor phases drawn as orange squares and light blue hexagons, with possible products given as the pink diamond and blue oval phases; c) schematic illustrating the main stages of the simulation: i) formation energies are obtained from the Materials Project and machine learning estimators are used to calculate the vibrational entropy part of the Gibbs energy of formation as a function of temperature and the melting point for each phase, reactions are enumerated and scored, and a recipe is specified which defines the desired precursors, a heating profile, and a reaction atmosphere; ii) a random initial arrangement of particles is generated, and the evolution rule is repeatedly applied to simulate the reaction; iii) simulation steps are concatenated into a trajectory which is analyzed to determine phase evolution over the reaction pathway.\n\nThe thermodynamics of the pairwise interface reaction model are conveniently represented by the convex hull construction, in which the Gibbs free energy is shown as a function of the mixing ratio (i.e., mixture composition) of the two precursor phases (Figure 1b). The interior points (i.e., pink diamond and blue oval) represent product phases that can form as the result of the reaction.# 2.2 Phase Data Acquisition\n\nThe input data for ReactCA is determined by the desired synthesis recipe which includes precursor ratios, a heating profile, and a reaction atmosphere (currently gaseous atmospheres consisting of only a single element are supported, e.g. N, O2 or Ar). The heating profile is defined by the user as a list of heating stages which each have a temperature and duration (specified by number of simulation steps). Once this recipe is defined, ordered crystal structures in the chemical system spanned by the reaction atmosphere and precursor phases are identified and their calculated formation energies are acquired from the Materials Project. Note that these formation energies are calculated via zero-temperature DFT while solid-state reactions occur at elevated temperatures. However, exact calculations of finite temperature formation energies are not available from existing high-throughput databases. To bypass this data deficiency, a machine learning descriptor given by Bartel et al.3 is used to estimate the vibrational entropy contribution to the Gibbs energy of formation for ordered solid phases at each of the temperatures specified in the reaction recipe. The Gibbs free energies of formation for common liquid/gaseous phases are acquired from experimental thermochemistry data (NIST-JANAF tables)42. Finally, the melting points of all phases are estimated using the graph neural network model from Hong et al.2.# 2.3 Reaction Enumeration and Scoring\n\nWith the phases and energies acquired from the Materials Project, the reaction-network28 Python package is used to identify all stoichiometrically possible reactions and calculate the changes in Gibbs free energy associated with them at each of the specified temperatures. While no general strategy exists for estimating the rate of solid-state reactions, predicting the evolution of phases during a reaction necessitates a model for relative reaction rates. To accomplish this, a score, S, is calculated for each reaction at each temperature using a heuristic function (Eq. 1), which returns the relative likelihood of each reaction occurring:\n\nS = 1ln[1 + exp(a \u2192 ( Tm,reactant2 Trxn \u2191 b))] \u2192 [1 + erf(\u2191c(!Grxn + d))] (1)# Figure 2: Scoring the likelihood of reactions as a function of reaction energy and temperature.\n\n|a)| |AG rxn|b)|Tit|0.67|c)| | |\n|---|---|---|---|---|---|---|---|---|\n| |3.5|3.5|2.5|2.5|1.5|1.52| | |\n|0.5|0.5| | | | | | | |\n| |-0.3|-0.2|-0.1|0.1|0.2|0.4|0.6|0.8|\n\nThe score function described by Eq. 1 is composed of two primary terms: a softplus function term and an error function term. The shape of this function (shown in Figure 2) captures: 1) the spontaneity of exergonic reactions, 2) the onset of reactions at temperatures equal to two-thirds of the melting point of the lowest melting point precursor (i.e., Tamman\u2019s Rule) and 3) the increase of reaction rate with temperature. The scaling parameters a = 14 and b = 0.8 were chosen to shift/scale the softplus function such that its \u201conset\u201d is around the Tamman temperature (Trxn Tm = 23) and the parameters c = 35 and d = 0.03 were chosen to shift/scale the error function such that it is centered on the region just below \u0394G = 0 eV/atom. Other values near the ones shown here for a, b, c and d were experimented with, but the variations did not significantly alter the simulation outcomes.\n\nThe softplus function was chosen to encode Tamman\u2019s rule because of its \u201csoft\u201d activation (i.e., above the Tamman temperature). The error function was chosen to encode spontaneity because it behaves as a dial that abruptly \u201cramps up\u201d for exergonic reactions. While these effects could also have been encoded using piecewise functions (e.g., a rectified linear unit in place of the softplus function, or Heaviside function in place of the error function), we opt for smooth alternatives which \u201csmear\u201d the onset of each effect over a range of values. This smearing allows for a degree of accommodation for uncertainty in our input Gibbs energy and melting point estimates. Most importantly, the scoring function can easily be updated to accommodate more sophisticated functionality, e.g. based on kinetics and local availability of reactive species.# 2.4 Simulation Evolution\n\nAfter phase data is collected and reactions are enumerated and scored, an initial simulation state is defined. The simulation box for this automaton is a three-dimensional region of space subject to periodic boundary conditions and discretized into a grid of cubic cells. To establish an initial state, each cell is randomly assigned a phase occupancy according to the precursor ratios given by the reaction recipe along with a volume equal to 1.0. We assign no scale or unit to this value.because only the ratio of the volumes of neighboring cells is relevant to this simulation. This value is used in achieving conservation of mass and should not be interpreted as a literal measure of the physical extent of the simulation.\n\nThe evolution rule determines the phase occupancy and volume value of the selected cell in the next simulation time step. It is applied to a single cell at a time, selected at random, meaning that this simulation is an asynchronous cellular automaton (as opposed to a standard cellular automaton, in which every cell is updated simultaneously)43. This rule ensures that one of two actions occurs: 1) a swap, or 2) a reaction. These actions are illustrated in Figure 3. The definitions for the simulation state and shape, along with the evolution rule were implemented using the pylattica Python package44.# 2.4.1 Action 1: Melted Phase Swap\n\nIf the current reaction temperature is above the melting point of the phase in the selected cell, the state of the selected cell is swapped with one of its neighbors, chosen at random, as shown in Figure 3. To accommodate uncertainty in the estimation of the melting point, the onset of the likelihood of this swap is smeared over a range of relative temperature values. Specifically, the swap likelihood begins ramping up as a function of reaction temperature at Trxn = 0.8Tm, increases to a 95% probability when Trxn = Tm, and reaches a 99% probability at Trxn = 1.2Tm. This behavior is shown graphically in Supplementary Figure S1. The swapping motion facilitates movement of the reaction vessel contents and can capture heightened reactant movement during flux-mediated reactions in which the presence of a liquid phase makes reactants more able to access each other. This is crucial for capturing more realistic reaction dynamics in many solid-state reactions.# 2.4.2 Action 2: Reaction Progression\n\nIf the phase occupying the selected cell is determined to be solid (i.e., it has a melting point higher than the current temperature), a reaction is selected. In this step, the reaction library is consulted to identify possible reactions between the selected cell and each of its neighbors. Reactions between the phase in the selected cell and the reaction atmosphere are also considered. From this list of possible interactions, a reaction is chosen randomly with a probability that is proportional to its score obtained from Eq. 1. As a result, reactions with higher scores occur more frequently than reactions with lower scores. This scheme has the net effect that higher-scored reactions proceed faster. Once a reaction is chosen, the reaction proceeds at each of the reacting cells; note that only a single cell is involved if the other reactant is contained in the atmosphere (e.g., gaseous O2). This procedure entails several steps (Figure 3):\n\n1. A probability distribution over the reactants is constructed. The stoichiometric coefficients taken from the reaction are used as the weights in this distribution.\n2. A random draw from the resulting distribution is performed. If the resulting phase matches the phase of the reacting cell, the process proceeds to the next step. If it does not match, the step ends and the cell is left unchanged.\n3. If the reaction proceeds, a second distribution is constructed over the reaction products (again using their stoichiometric coefficients as weights).\n4. A draw from this distribution is used to select a product phase.# Figure 3: Evolution of phases in the simulation according to the evolution rule.\n\nIn the top panel a cell is randomly selected (pale green) from the simulation and its neighbors are identified (teal). Next, if the simulation temperature is above the melting point of the phase in the selected cell, the Melted Phase Swap action occurs (middle-right). If not, reactions between the selected cell and its neighbors are enumerated and a reaction is randomly chosen using the reaction scores as probabilities (middle-left). Finally, each of the reacting cells\u2019 phases are replaced (or not) according to probabilities given by the stoichiometric coefficients of the selected reaction. These probabilities are indicated by the small fractions decorating each arrow in the bottom panel. Note that the CA implemented in this work uses a three-dimensional simulation state, but only two dimensions are shown here for clarity.\n\n9# 5\n\nThe reacting phase is replaced with the product phase.# 6\n\nThe volume of the cell is scaled according to the ratio between the volume of the products and the reactants. Importantly, the process described above utilizes probability distributions over the stoichiometric coefficients of the reaction to maintain conservation of mass within the automaton. For a given reaction, the coefficients of the reactants provide the probability that each will be consumed during a given occurrence of that reaction. This ensures that the reactants are consumed at the correct rate relative to each other. The coefficients on the products of the reaction provide the probability that each one will be produced by a given occurrence of the reaction, similarly ensuring that the products of each reaction are produced at the correct rate relative to each other. Finally, scaling the volume of the simulation cell after its contents have been replaced during a reaction ensures that the correct amount of product phase is produced relative to the amount of reactant consumed by the reaction. A more detailed explanation of this process in conjunction with an example is provided in the Supplementary Information.# 2.5 Trajectory Construction and Analysis\n\nA simulation run typically entails hundreds of thousands of applications of the evolution rule described above. When the simulation is complete, the results are concatenated into a trajectory that can be analyzed to understand the reaction pathway as a series of steps and discrete intermediate species. Because ReactCA relies on random draws from probability distributions over the possible actions, reactions, and product phases, several simulations are run in parallel, each utilizing a different random starting state. Though the choice of starting state does not affect the qualitative outcome of the simulation, each run is characterized by differing fluctuations, and represents a unique sampling of the distributions in the automaton. As shown in Figures S2 and S3, when a sufficiently large simulation box is used, the standard deviation of the maximum and final mass fractions attained by each phase across a set of parallel trajectories is reduced to less than 1%. To construct the final result, the individual outputs of these parallel simulations are ensemble averaged to yield an overall trajectory.# 3 Results and Discussion\n\nTo test the efficacy of ReactCA in describing real solid-state reactions, we apply it to model several case studies selected from the literature where high-quality in situ phase evolution data is available. For each case, we compare the predicted and observed final products of the synthesis reaction as well as the appearance (or disappearance) of intermediate and impurity phases.# 3.1 Product Selectivity in BaTiO3 Recipes\n\nBarium titanate is a well-known multiferroic material with a significant body of synthesis literature. While there are a number of well-known recipes for producing this material, we refer to the recent solid-state reaction selectivity study of Ref. 24, which tested and compared nine different BaTiO3 synthesis recipes characterized over a range of temperatures with synchrotron X-ray diffraction (XRD). A selection of these recipes and the corresponding reactions are simulated here using ReactCA to illustrate the way reaction selectivity is expressed in a phase evolution prediction.# Experiment\n\n|Recipe|BaCO3|TiO2|\n|---|---|---|\n|Recipe I: BaCO3 TiO2|0.8|1200|\n| |0.6|1000|\n| |0.4|800|\n| | |600|\n| | |400|\n|Recipe II: Ba2TiO4| |1200|\n| |0.8| |\n| |0.6|1000|\n| |0.4|800|\n| |0.2|600|\n|Recipe III: BaCl2 Na2TiO3| |900|\n| |0.8|800|\n| |0.6|700|\n| |NaCl|600|\n| |Na TiO|500|\n| |BaTiO|400|# Reaction Coordinate\n\nFigure 4: Simulated (left) and experimental (right) reaction evolution plots for the selected BaTiO3 recipes. In each of these plots, the x-axis corresponds to the reaction coordinate, and the y-axis corresponds to mass fraction. The background of each plot is colored according to the amount of precursor (grey), impurity/intermediate (pink), and target/byproduct (light green) over the course of each simulation or experiment. Each of the traces represents the amount of each phase during the reaction. Traces marked with circles correspond to precursor phases, those marked with crosses correspond to intermediate or impurity phases, and those marked with diamonds correspond to BaTiO3, the expected target product phase. The dashed red lines show the heating profile used for each simulation or experiment.\n\nSelectivity was assessed in previous work according to two metrics: primary competition, which quantifies the likelihood of impurities forming from reaction of precursors, and secondary competition, which quantifies the likelihood of subsequent reactions consuming the desired products after they form. In the case of both of these metrics, a lower value corresponds to a more selective reaction, that is, one that is more likely to form only the desired product phase. To illustrate the way that selectivity presents itself in ReactCA simulations, three recipes were selected: the conventional recipe (Recipe I - BaCO3 and TiO2), a recipe with improved selectivity but a lower overall driving force (Recipe II - Ba2TiO4 and TiO2), and a metathesis reaction with excellent selectivity (Recipe III - BaCl2 and Na2TiO3).\n\nRecipe I. The simulation results for Recipe I are shown in Figure 4a and the corresponding experimental outcome is shown in Figure 4b. This reaction received a low primary competition score.# Results and Discussion\n\nbut a perfect (zero) secondary competition score, suggesting that there were competing phases that could form from the original precursors but that if the desired products were formed, they would be unlikely to be consumed by any secondary reactions. In the corresponding experiment from Ref. 24, the BaTi2O5 phase formed as an impurity in conjunction with the product, BaTiO3, at around 1100 K. The result of the reaction automaton simulation for this recipe, shown in Figure 4a, predicts the formation of the target phase BaTiO3 as well as the BaTi2O5 impurity phase at the same onset temperature (1100 K). However, it also predicts the appearance of two additional phases (Ba2TiO4 and BaTi4O9). The three impurities grow at a similar rate to the desired product, BaTiO3, an effect also seen in the experiment. This result illustrates the way that secondary competition appears in a reaction: competing phases form during the reaction of the precursor materials, but once the desired product, BaTiO3, is formed, it is not consumed by any subsequent reaction.# Recipe II\n\nThe second reaction chosen here was shown 24 to improve the selectivity of the first at the cost of lowering the driving force of the reaction by choosing compositional members toward the interior of the complex hull as precursors. The ReactCA simulation (shown in Figure 4c) predicts a majority BaTiO3 formation, and the formation of the impurity BaTi2O5. In the corresponding experiment (shown in Figure 4d), BaTi4O9 is observed as an impurity but the simulated BaTiO5 is notably absent. This will be discussed below.# Recipe III\n\nThe final BaTiO3 reaction selected for simulation is a metathesis reaction using NaTiO3 as the Ti source and BaCl2 for the Ba source. In the original work, this reaction was selected for its strong exergonicity, and its strong selectivity scores. The high selectivity of this reaction is on display in the ReactCA prediction, and the prediction (shown in Figure 4e) is in good agreement with the experimental results (Figure 4f) \u2013 the dominant products are the intended metathesis products: BaTiO3 and NaCl. The main discrepancy between the prediction and the experiment is that the automaton predicts no other Ba-Ti-O phase formation, while the experiment indicates appearance of BaTi2O5, though it is only a trace amount.\n\nAcross these three reactions, the selectivity differences between the recipes are apparent in the results from the CA simulations. The size of the green regions in Figure 4 show the overall trend from low selectivity (in the case of Recipe I), to increased selectivity (in the case of Recipe II) and finally to perfect selectivity (in the case of Recipe III). We also note a tendency for ReactCA to predict the appearance of unobserved impurity phases (particularly in the case of Recipe I, shown in Figure 4a). This is a result of the evolution rule which samples reactions based on their calculated rates. The effect is especially strong for the Ba-Ti-O chemical system which contains many phases with similar energetics and which have similar melting points (the two features which are used in ReactCA to calculate reaction rates). Finally, in the reactions shown here, we also highlight a tendency of the automaton to overpredict accumulation of Ba-rich Ba-Ti-O ternary phases. For example, in the Recipe I simulation the most prevalent byproduct is Ba2TiO4 (Ba-rich), but in the corresponding experiment it is BaTi2O5 (Ba-poor). In the Recipe II simulation BaTiO5 (Ba-rich) is the primary impurity, but in the experiment only BaTi4O9 (Ba-poor) appears.\n\nFinally, in Recipe III, the simulation predicts the formation of pure BaTiO3 (Ba-rich) while in the experiment, small quantities of BaTi5 (Ba-poor) were also observed. In light of this observation, we hypothesize that - given the similar energetics of these compounds - the preferential formation of Ba-deficient compounds in experiments may be related to kinetics of the ionic species across the reaction interface. This discrepancy between simulation and experiment motivates future work to develop new reaction rate estimators using system-specific kinetic models, perhaps in conjunction with yet-unrealized high-throughput databases of kinetic calculations.\n\n12# 3.2 Intermediate Identification in CaZrN2 Synthesis\n\nTernary nitride systems provide a wealth of materials discovery and synthesis opportunities. Recently, Rom et al. identified metathesis synthesis pathways that allowed them to produce the novel ternary nitrides CaZrN2 and CaHfN246. Using in situ XRD analysis, they constructed trajectories for each phase present during their synthesis reaction which used precursors Ca3N2 and ZrCl4 to yield CaZrN2. This trajectory is reproduced in Figure 5a. We performed a simulation for this reaction using a simulation box with side length of 15 cells, a heating profile consisting of a ramp phase from room temperature to 1400K, and an N2 reaction atmosphere. The resulting phase trajectories are shown alongside the experimental results from Ref. 46 in Figure 5b. We note that in panel a) of this figure, the phase prevalence trace for Ca4Cl6O is excluded. This phase appeared in the experiment due possibly to either impure precursor material or reaction with the quartz ampule46, two effects which certainly represent practical synthesis considerations but are not relevant to the ideal environment represented by the automaton.\n\n|Experiment - CaxNy|ZrCl4|\n|---|---|\n|Ca4Cl6O| |\n|CaN2| |\n|CaZrN2| |\n\nFigure 5: Resulting phase trajectories from simulation and experiment for CaZrN2 synthesis. The experimental phase trajectories for the reaction using a) Ca3N2 and ZrCl4 as precursors and b) the simulated phase trajectories for the reaction using CaN2 and ZrCl4 as precursors. In a) the steep drop-offs of CaCl2 and ZrCl4 are caused by their melting and sublimating, respectively.\n\nThis simulation successfully captures the reaction pathway present in the experiment. The first intermediate phase to reach its peak in the simulation is ZrCl3, which agrees with the early reduction of ZrCl4 to ZrCl3 in the experiment. The two intermediates which then follow in the experiment (a small amount of Zr6NCl15 and significant Ca2NCl) are also present in the simulation with the same relative prevalences. Finally, the simulation predicts the major product (CaZrN2) and byproduct (CaCl2) with confidence, though some unreacted Ca2NCl also remains. In addition to this pathway, the simulation predicts the appearance of two phases which are not observed in the experiments, ZrNCl and ZrN. In the following analysis we discuss the mechanisms by which these phases appear in the simulation and explain why those mechanisms may not have been.# Active During the Experiment\n\nZrNCl appears in the simulation at roughly the same stage in the temperature trajectory as Ca2NCl and the three reactions which facilitated the bulk of its formation are shown in Table 1. The most frequent of these reactions, Reaction (1), may have incorrectly occurred in the simulation because ReactCA, in its current form, does not include sublimation. Reaction (1) consumes Ca2NCl, which only forms in the experiment (Figure 5a) when the temperature has reached 600K and ZrCl4, the other reactant, has sublimated. As a result, the two reactants may never have been sufficiently available to one another for this reaction to occur in the experiment. In contrast, since ReactCA has no method for estimating sublimation temperatures, ZrCl4 remains available when Ca2NCl appears, allowing Reaction (1) to proceed. This hypothesis is supported by another experiment by Rom et al. in which Ca2NCl and ZrCl4 were reacted directly as the initial solid precursors. In that experiment, both phases are present as solids and ZrNCl appears as a prominent intermediate46, suggesting that Reaction (1) does occur if both precursors are present.\n\n|Reaction| | |Score (600K)|! \u201d rxn|eV|Occurrences|\n|---|---|---|---|---|---|---|\n|(1) Ca2NCl + ZrCl4 \u2191\u2191\u2193 2 CaCl2 + ZrNCl| | |0.165|-0.562|6683| |\n|(2) 2Ca3N2 + ZrCl4 \u2191\u2191\u2193 ZrNCl + 3 Ca2NCl| | |0.165|-0.636|5103| |\n|(3) Ca3N2 + 2 ZrCl4 \u2191\u2191\u2193 3 CaCl2 + 2 ZrNCl| | |0.165|-0.788|4533| |\n\nTable 1: The most frequently occurring ZrNCl-forming reactions in the automaton simulation shown in Figure 5b. We use the notation !\u201d rxn as opposed to !G rxn to indicate that the relevant thermodynamic potential in this system is a grand potential with N2 as the open species.\n\nReactions (2) and (3) in Table 1 consume the same precursors: Ca3N2 and ZrCl4. In the experiment however, Rom et al. propose that these phases instead react according to the following reaction:\n\nCa3N2 + 6 ZrCl4 \u2191\u2191\u2193 6 ZrCl3 + N2 + 3 CaCl2\n\nS = 0.165, !\u201d rxn = \u21910.194 atom (T = 600K)eV\n\nThis reaction and the three reactions in Table 1 are all assigned the same score because they are highly exergonic (the thermodynamic component of the score function is maximized for all of them) and they share a lowest melting point precursor, ZrCl4 (so the melting point component of the score takes on the same value for all of them). Consequently, ReactCA does not differentiate the rates of these reactions, and they all occur with similar frequencies during the simulation. It is surprising that evidence for neither Reaction (2) nor Reaction (3) appears in the experiment, given their energetics (they are even more exergonic than the reaction proposed by Rom et al.) but there may be important differences in the kinetic accessibility of their product phases. In particular, the formation of the two binaries, ZrCl3 and CaCl2, (along with the release of gaseous N2) from these two compositionally dissimilar precursors may be more kinetically facile than the formation of the nitrogen-containing ternary phases ZrNCl and Ca2NCl. In support of this hypothesis, we note that Rom et al. observe the formation of a small amount of Zr6NCl15 (Figure 5a), which could be interpreted as an incomplete incorporation of nitrogen while transforming of ZrCl3 into ZrNCl. Additionally while CaNCl does appear in the experiment, Rom et al. suggest that its formation2 is facilitated by the reaction of more compositionally similar binaries, CaCl2 and Ca3N2 (which is precisely how it is formed in the simulation). When ZrNCl does form, it is in the second experiment performed by Rom et al. (reacting CaNCl with ZrCl4).# 3.3 Recovery of Observed Reaction Pathways in YMnO3 Synthesis\n\nThe multiferroic YMnO3 has been the recent focus of a number of synthesis investigations into the effect of precursor selection, reaction atmosphere, and reaction temperature on both reaction pathways and also on the identity of the dominant product8;28;47. In the first of these studies, Todd et al. propose reaction pathways at work during the formation of YMnO3 in a flux-assisted metathesis reaction and explain the lower reaction temperature required by their recipe in terms of the interplay between these pathways8. Building on this work, McDermott et al. were able to confirm using a reaction network that the suggested pathways were thermodynamically predicted by data within the Materials Project28. We use this example here to illustrate the ability of ReactCA to predict temperature-dependent reaction pathways and intermediate and product mass fractions and to provide insight into the interactions between simultaneously occurring pathways.\n\nThe simulation for this reaction was configured to use a simulation box with a side length of 15 cells and a heating profile consisting of a ramp phase to 1300K followed by a hold phase at 1300K. We note that the peak temperature used here is slightly higher than the experimental maximum temperature of 1100K. This choice was made in order to accommodate uncertainty in the melting point and vibrational entropy estimates. A view of the resulting trajectory for this simulation is shown in Figure 6a and a longer trajectory which includes the stabilization of the product phases is available in Supplementary Figure S4. The overall result shown here predicts YMnO3 as the dominant product phase and a number of intermediate phases, including YOCl, Mn8Cl3O10, YMn2O5, and Mn3O4. In Figure 6b, a magnified view of the bottom of the trajectory is shown for those phases which never formed greater than 2.5% of the overall mass content in the simulation box. The multitude of phases present in this result illustrate the way that the reaction automaton samples many possible reaction pathways that can be traced from the initial precursor set.\n\nAmidst the complexity shown in Figure 6, the reaction pathways identified by Todd et al. are remarkably well predicted by the ReactCA simulation. From XRD refinements, Todd et al. calculated the trajectory of each intermediate phase, and plotted them in groups according to cation. We reproduce these plots for the experimental data alongside similar plots generated from# Figure 6: Resulting trajectory from simulation of the Li2 CO3 -YCl3 -Mn2 O3 reaction.\n\n|a)|0.6|YCla|1200|\n|---|---|---|---|\n| |0.5|YMnO3|1000|\n|J|0.4| | |\n| |0.3|YOCI|800|\n| |0.2|MnzC|Licl|\n| |3|Mnsc Cl3|600|\n| |Lizo|Mn; 04 YzO3| |\n| |0.1|YMnz|400|\n|b)|0.025|LiMno|'304CI|\n| |0.02| |LiYoz|\n| |0.015| |Mno|\n| |0.01|2Mnzc| |\n| |0.005|Li,Mno_| |\n\nthe simulation data in Figure 7. Every intermediate phase identified by Todd et al. is predicted by the reaction automaton. The relative amounts of these phases as well as the order of their appearance are also predicted with good accuracy, though Y O4 Cl appears in only trace quantities3 in the simulation. However, we note that previous work highlights the high degree to which both Y3O4Cl and YOCl accommodate defects and disorder48. Indeed, in another study Todd et al. assert that the transformation between these phases proceeds through an off-stoichiometric YO1+\u03c9Cl1-\u03c9 phase48. Because our input data is limited to only the ordered, perfectly crystalline phases present in the Materials Project we do not include the effects of defects and disorder. As a result, we neglect the likely significant contribution of configurational entropy to the stability of these phases. This omission may explain the underestimation of YO4Cl in this simulation result.3\n\nIn addition to prediction of reaction intermediates, the ReactCA trajectory contains information about the specific reactions which yielded each phase. Of particular interest here is the formation mechanism for the product phase, YMnO3. Todd et al. propose dual mechanisms for the formation of this phase - first, the faster, lower temperature ternary metathesis reaction between\n\n16# Experiment\n\n| |LizCO3|Licl|\n|---|---|---|\n|L|Mn|YMnO3|\n|2|Mn;C 0|2|\n| |Mn;C 04| |\n| |YCI;|YCI;|\n| |YOCI|YMnO3|\n| | |Yzo_|# Simulation\n\n| |Liz CO3|Licl|\n|---|---|---|\n|1|Mnz|YMnO3|\n|2| |Mn;C 04|\n| |YOCI| |\n| |YMnO3| |\n| |Yzo| |# Figure 7:\n\nReaction pathways extracted from both a) the experimental synthesis results from Todd et al.8 and b) the simulation trajectory of the reaction between Li2CO3, YCl3 and Mn2O3. The top panels illustrate the early emergence and subsequent plateau of LiCl, the middle panels capture the emergence and then recession of the reduced MnO4 phase and the bottom panel shows the ordering and relative prevalence of the three key Y-O-Cl intermediates. Note that LiCl disappears from the experimental phase trajectory in a) (which was generated using XRD data) only because it melts.\n\nLiMnO2 and YOCl, and second, the higher temperature reaction between Mn2O3 and Y2O3 (the latter of which is formed in part by consumption of YOCl). Within the ReactCA trajectory, we identify two major classes of reactions that align with these two proposed reaction pathways. The first of these classes (Class I) is the reaction of LiMnO2 with one of a number of yttrium-containing intermediates (the most frequently occurring of which is YOCl, followed by YCl). These reactions correspond to the low temperature, ternary metathesis step. The second of the two classes of reactions (Class II) producing YMnO3 are reactions between the refractory Y2O3 and either Mn2O3 and Mn3O4. That reactions in the first class occur with any substantial frequency in this simulation is a surprising finding because the total amount of LiMnO2 never exceeds 3% by mass, suggesting that the phase is consumed at nearly a rate nearly equal to that at which it is produced. This may be a reasonable prediction, however, because the failure of this phase to accumulate in our simulation agrees strongly with the experiment performed by Todd et al, in which LiMnO2 appears in only trace quantities (its XRD pattern is poorly resolved from MnO4 which implies that the data in Figure 7a suggests the appearance of only small quantities of both Mn3O4 and LiMnO2 in the experiment).# Reaction Coordinate\n\n|Counts|Class I (LiMnO2 intermediate)|Class II (Y2O3 reactions)|\n|---|---|---|\n|1200|800|1000|\n|600|400|200|\n\nFigure 8: Histogram illustrating the dominance of different classes of YMnO3-forming reactions over the course of the simulation. The counts shown in red correspond to reactions which involve the LiMnO2 intermediate (the ternary metathesis route, i.e. Class I), and the counts in blue show reactions between the refractory Y2O3 and phases in the Mn-O chemical system (i.e. Class II).\n\nBy counting the frequency of each of these classes of reactions as a function of the reaction coordinate, we illustrate in Figure 8 that the first ternary metathesis reaction class dominates early in the simulation at lower temperatures, and that the second reaction class which consumes the refractory Y2O3 occurs later in the simulation at higher temperatures. This result reflects the mechanism that Todd et al. proposed based on their experimental observations - ternary metathesis dominates at lower temperatures, and Y2O3 reactions proceed at higher temperatures, and indeed both mechanisms contribute to the formation of YMnO3.\n\nWe emphasize that the analysis presented here represents an important increase in capability over the previous reaction network approach. In particular, the reaction network requires a set of expected product phases as input while ReactCA predicts the product without any prior target input. Additionally, the reaction network identified isolated pathways of a finite length at a single temperature at a time. In contrast, ReactCA can explore pathways of unlimited length (and allow them to interact) over a range of temperatures in a single simulation. These improvements both simplify the analytic process and broaden the range of reaction behavior that can be predicted compared to the reaction network.\n\nIn addition to the reaction intermediates identified by Todd et al., the ReactCA simulation predicts a number of other unobserved intermediates, most of which appear in the simulation at only trace levels (less than 1-2% by mass, as shown in Figure 6b). These unobserved intermediates reveal the alternate pathways which are present in the trajectory as a result of the automaton sampling many available reactions during its evolution.# Investigating even low-prevalence intermediates, such as LiMnO2, can yield insights into overall reaction pathways, the prevalence of a given intermediate in a ReactCA simulation generally reflects the relative amount predicted to appear during the synthesis reaction. In other words, while we do show that LiMnO2 plays a role in the reaction that aligns well with the experimental hypothesis, its low prevalence suggests that it may not accumulate in significant quantities during the reaction. Similarly, because these other low-prevalence phases appear in only trace quantities, the ReactCA simulation should be interpreted as assigning low likelihood to the appearance of those phases in experimental characterization.\n\nBesides these low-prevalence unobserved impurities, two others (YMn2O5 and Mn8Cl3O10) achieve significant amounts comparable to the predictions for the observed intermediates (&gt;10% by mass). The first of these phases, YMnO5, does not appear in the experiment originally presented by Todd et al., but is recognized in a later work by the same authors as a common impurity in the synthesis of YMnO3 by this metathesis method47. The second of these phases, Mn8Cl3O10, was recently synthesized by the solid-state method from precursors MnCl2 and MnO2 at 600\u00b0C, hence its appearance is not implausible in this reaction.\n\nHowever, by examining the reactions which consume Mn8Cl3O10 during the ReactCA simulation, we find that its primary role is as an intermediate between reactants YCl3 and Mn2O3 (which react to form it), and experimentally verified downstream intermediates (most significantly, LiMnO2, Mn3O4, and LiCl). As a result, we hypothesize that MnCl3O10 functions similarly to ZrN in our discussion of the CaZrN2 synthesis8 above. That is, MnCl3O10 may be the best representation the simulation can provide of a process which is actually facilitated by highly defective or amorphous intermediates with a similar Mn-Cl-O composition that are not present among the ordered, crystalline, stoichiometrically exact phases on the Materials Project. This uncertainty further emphasizes the necessity of future work to improve the ability of the automaton to navigate more sophisticated intermediate landscapes.# 4 Conclusions\n\nWe present ReactCA, a new simulation framework based on the cellular automaton formalism for predicting the evolution of crystalline phases during the course of a solid-state reaction. This simulation utilizes thermodynamic data from the Materials Project and machine learning estimated melting points in conjunction with a cost function to assign reaction rates as a function of temperature. The evolution of the reacting material is determined by a rule based on the pairwise interface reaction model that considers reactions between neighboring particles. The flexibility of the form of both the cost function and the evolution rule lend great extensibility, meaning that as new data become available from as of yet unrealized high-throughput methods or machine learning frameworks, both empirical rules and heuristics based on those data can be incorporated into ReactCA to improve its performance.\n\nWe illustrate the current performance of the simulation framework using three case study systems, the first of which serves as a platform for viewing the relative selectivity of various reaction recipes and the importance of precursor choice with regard to product purity, and the second and third of which demonstrate power of ReactCA in predicting likely reaction pathways, the temperature dependence of those pathways, and the order and amounts of intermediate phases which appear during the course of complex ternary metathesis reactions.\n\nWhile we believe that ReactCA is of immediate utility in both \u201ctesting\u201d reaction recipes before utilizing physical or monetary resources to experimentally execute them, we also foresee the simulation being used to determine recipe parameters using an optimization framework or to facilitate the autonomous design of synthesis recipes by acting as a digital twin for experimental.# 5.1 Thermochemistry Data\n\nTo prepare data for the simulation of the BaTiO3, CaZrN2 and YMnO3 recipes, entries from the Materials Project from the Ba-Ti-O-C, Ba-Ti-O-Na-S, Ba-Ti-O-Na-Cl, Ca-Zr-N-Cl, and Y-Mn-Cl-Li-C-O chemical systems were collected. Additionally, a density functional theory structure relaxation calculation was performed at the GGA level of theory using parameters from the MPRelaxSet in atomate2 to obtain a formation enthalpy for Y3O4Cl, a phase known to appear during the synthesis of YMnO3, but not present within the Materials Project. For each entry in these chemical systems, the machine learning descriptor by Bartel et al. was used to estimate the vibrational entropy contribution to the Gibbs energy of formation at 300K.\n\nPhases with formation energies greater than 30 meV/atom above the hull were removed at this temperature. This value for a metastability cutoff filter was chosen based in part on the work of Sun et al., which present statistics on the metastability of compounds in the Materials Project and connects them to synthesizability. We also removed compounds which were marked as \u201ctheoretical\u201d on the Materials Project unless they appeared explicitly in the experimental results. In addition, several phases which have previously only been successfully produced using synthesis methods other than the solid-state method considered here were excluded. These are ZrCl (synthesized using high pressure methods), ZrCl2 (synthesized via hydrogen disproportionation reactions), CaN2 (synthesized using high pressure methods), CaN6 (synthesized in solution), Ca2N (synthesized via metallothermic reduction), and Zr3N4 (synthesized via high pressure methods or ammonolysis).\n\nData from the Materials Project was collected using the mp-api package and the possible reactions in each of these systems were enumerated using the reaction-network Python package. The CA model was implemented using the pylattica Python package.",
        "context_id": 9,
        "question": "What simulation framework is presented in this work to predict the evolution of phases in solid-state reactions?",
        "answer": [
            "ReactCA"
        ],
        "context_length": 50216
    },
    {
        "context": "# 1. INTRODUCTION\n\nModern Searches for Extraterrestrial Intelligence (SETI) tend to focus on trying to detect unnaturally narrowband radio transmission. In natural radio science contexts, this can mean signals over frequency ranges less than &gt;100 kHz, but in a SETI context narrowband usually means signals with widths measured in Hz (n.b. some SETI work does focus on broader signals, e.g. Gajjar et al. 2022; Suresh et al. 2023, and the tools we use here are sensitive to these signals as well). While some terrestrial digital communication applications often opt for the higher data transmission rates of broadband modulation schemes, narrowband signals are useful as carrier waves for low signal-to-noise (SNR) cases, making it easy to find and lock onto a signal Doppler-drifting through frequency space, particularly in the case of our deep space probes like Voyager (Derrick & Isaacson 2023a). In the limits of our sensitivity to interstellar radio transmission, for a given transmitter power, such narrowband signals offer the best possibility of detection.\n\nSufficiently powerful narrowband signals also have the potential to address ambiguous interpretations. Humans have developed technology to produce much narrower radio transmissions (\u21ad 1 Hz) than natural astrophysical processes, suggesting that non-human sources of such signals must be artificially constructed by some biologically derived entity. The search for radio technosignatures is motivated by the fact that modern radio receivers on Earth can detect signals produced by the largest modern-day transmitters over interstellar distances (Cocconi & Morrison 1959). However, disentangling such distantly sourced signals from signals of anthropogenic origin can be tricky, particularly as more of the radio spectrum is being utilized for various terrestrial communication purposes. Ever since the very first Search for Extraterrestrial Intelligence (SETI) project, parsing out radio frequency interference (RFI) has been a significant problem (Drake 1961). Indeed, a major component of most modern radio SETI projects include both observational and data analysis strategies for RFI mitigation (Tarter 2001).\n\nThe standard observational strategy for single-dish radio SETI is to employ an on/off nodding cadence, in an attempt to discriminate persistent signals localized to a particular source (Lebofsky et al. 2019). However, radio arrays such as the Allen Telescope Array (ATA) employ interferometric techniques to combine the data from each dish in a wide field of view. Beamforming \u2014the interferometric addition of signals from many elements to form tight synthetic beams within the primary field of view\u2014 can be used to synthesize many simultaneous, smaller and spatially separated beams, which provides a similar effect to the on/off nodding cadence without having to slew away from the target. This is the primary observational strategy employed at the ATA to distinguish RFI from sky-localized astrophysical signals (Harp 2005).\n\nThe newly upgraded ATA was originally designed and recently upgraded with SETI in mind as the primary scientific driver (Sheikh et al. 2023a). The main focus of the# A n ATA S E T I S e a rc h o f T R A P P I S T - 1# 3\n\nThe research presented in this paper was to build a generalized pipeline for analyzing the SETI data products at the ATA and then apply it to a specific SETI observation. The TRAPPIST-1 system was selected as the primary science target for observation. Since the publication of its discovery in 2016 (Gillon et al. 2016), TRAPPIST-1 has provided an intriguing case study for exoplanet science. The star is a cool M-dwarf\u2014the most abundant spectral type of star\u2014and currently one of the best prospects for detecting and characterizing orbiting terrestrial-sized planets with solid surfaces due to the relative masses and radii between the star and these kinds of planets. Such stars, particularly later M-dwarfs, have very low luminosities that are difficult to observe at large distances. But the proximity of TRAPPIST-1, at approximately 12.5 parsecs, allows extremely precise measurements of the system\u2019s characteristics (Agol et al. 2021).\n\nThe TRAPPIST-1 system is almost perfectly edge-on from our line of sight, hosting seven known transiting planets. The planets are evenly spaced and in orbital resonance, with planets d, e, f and g spanning the extreme limits where liquid water could theoretically exist on a planetary surface given stellar irradiance and a sufficiently thick atmosphere (Hill et al. 2023). Much work has been done to determine the system\u2019s viability for biological development (Turbet et al. 2020; Krissansen-Totton 2023; Gillon 2024).\n\nThe precisely measured orbital characteristics of this compact multi-planet system suggest that the system is so close to edge-on that several planets may occult each other in what is called a planet-planet occultation (PPO). There are many nearby multi-planet nearly co-planar systems that may exhibit PPOs, but the TRAPPIST-1 system is unique among these in how well constrained its many planets\u2019 orbital parameters are and how closely aligned they all are. The TRAPPIST-1 system is the perfect laboratory to both predict and potentially observe PPOs. Photometrically observing PPOs outside of transit is at the cusp of our technological capability, and it could be a useful observational tool in the near future (Luger et al. 2017).\n\nEven without being able to directly observe PPOs, these events may be critical observation windows for SETI if accurately predicted. On Earth, the Deep Space Network (DSN) is frequently used to transmit and receive radio signals between ground stations and the artificial probes on and around other planetary bodies in the solar system (Mudgway 2000). The beams of transmitted signals are often powerful and wide enough to wash over their target and propagate out into deep space, where they could be received with a serendipitously aligned receiver. Similarly, by definition, Earth is favorably aligned to receive powerful signals transmitted from one planetary body and washing over another during a PPO event. For 2 of the 7 PPO events found in this work, the transmission source planet is within the system\u2019s habitable zone (Gillon 2024). But sources of leaked transmission are not necessarily limited to planets in the habitable zone. Satellites or ground-based relays on uninhabited worlds could be set up and used to communicate for reasons not limited to our own orbiters and rovers on other planets within the solar system. If powerful transmissions are being sent between planetary bodies during these events, wide enough to not be entirely...# 4\n\nblocked by their intended target and catching the Earth within the opening angle of the propagating beam, this unintentionally \u201cleaked\u201d radio emission can be detected as a technosignature (Siemion et al. 2013; Wright 2021).\n\nThe purpose of this paper is to:\n\n1. Report the results of a search for narrowband radio technosignatures from the TRAPPIST-1 system.\n2. Demonstrate an application of the PPO method to observational data.\n3. Describe the application of and current state of the NbeamAnalysis pipeline.\n\nWe used the newly upgraded Allen Telescope Array (ATA) at the Hat Creek Radio Observatory (HCRO), to observe the TRAPPIST-1 system for 28 hours from 0.9 to 9.3 GHz. We subsequently analyzed the data for narrowband radio technosignatures over the entire data set and then more closely examined it during predicted PPO events within the observational windows. The details of the observations are discussed in \u00a72. The pipeline created to analyze the data is detailed in \u00a73. The results of the analysis are highlighted in \u00a74. And finally, \u00a75 concludes with a discussion of the project and future work.# 2. OBSERVATIONS AND DATA\n\nThe ATA is a 42-element radio interferometer located in Hat Creek, California at the Hat Creek Radio Observatory (HCRO). It is comprised of 6.1 m offset Gregorian antennas, and is currently in the process of being upgraded with new \u201cAntonio\u201d feeds: dual-polarization log-periodic feeds which are sensitive from 1\u201311 GHz and cryogenically-cooled to 70 K. The analog signals from the dishes are sent to the HCRO signal processing room over optical fiber, where they are mixed with four local oscillators (LOs) to produce four independently-tuned signal chains. After digitization (and fringe rotation and phase centering to the center of the primary beam), the signals are ingested by the digital signal processing backend, which provides a correlator, for imaging and beamformer calibration, and a beamformer.\n\nFor this project, signals from 20 dishes were digitized by RFSoC boards using two tunings, leading to the ability to record two 672 MHz tunings simultaneously. The beamformer was configured to create two spatially separated beams within the primary field of view, the on-beam placed at phase-center of the array and centered on the target, and the off-beam fixed at a constant 9 arcminutes away, producing high-frequency resolution filterbank data files for each beam (Lorimer 2011). The FWHM of the tied beam varies from \u2192 3 arcminutes at the bottom part of the observed frequency band, to \u2192 0.5 arcminutes at the top part. This corresponds to a separation of \u2192 3 \u2191 18\u2193 FWHM between the on and off-beams, and an attenuation of at least 10 dB between them according to beam-pattern measurements. The entire# A n ATA S E T I S e a rc h o f T R A P P I S T - 1# 5\n\nFigure 1. Frequency coverage (x-axis) by day (y-axis) of the TRAPPIST-1 system observations using the Allen Telescope Array at Hat Creek Radio Observatory in Fall 2022. The standard radio frequency bands are shaded in the background and labeled. These observations covered a continuous 0.9\u20139.3 GHz in radio frequency, over two weeks in October\u2013November 2022.\n\nThe frequency range of the data was recorded in this way over 8 compute nodes with a coarse channel size of 0.5 MHz, a narrow channel resolution of 1 Hz and binned to 16 s over every integration, yielding a total data volume of 65 TB. The ATA RFSoC boards perform the coarse channelization using a polyphase filterbank before streaming the channlized voltages to the acquisition servers. Fine channelization is achieved by running a GPU-accelerated real-time \u2192500k-point FFT on the data acquisition servers.\n\nThe ATA was used to search the TRAPPIST-1 system for narrowband radio emission over the dates and tunings shown in Table 1 and Figure 1. Each observation was conducted in 10 minute integrations for a total of 240 minutes per frequency band, one of which had to be broken up between 2 different observation days to be completed. These are treated separately in the analysis to account for temporal differences in the RFI environment.# 2022 ATA Observations of TRAPPIST-1\n\n|Date (UTC)|Ranges (GHz)|Duration|Frequency|Total Hits|PPO Events|Event Duration|PPO Hits|Dish Diameter|Transmitter Power Range|EIRP|\n|---|---|---|---|---|---|---|---|---|---|---|\n|Oct 27, 2022|6.3 - 6.9 & 8.1 - 8.7|200 min|6767|de|23.0 min|38|3.4 cm|0.023 - 0.72 GW|2.17 - 69.1 TW| |\n|Oct 28, 2022|0.9 - 2.1|240 min|2167448|cf, cd|54.7 min|812|5.89 m|0.15 - 2.43 GW|2.55 - 40.8 TW| |\n| | | | | |77.8 min|694|3.4 m|0.46 - 7.29 GW| | |\n|Oct 29, 2022|2.1 - 3.3|240 min|774082|ge, dfb|95.0 min|537|4.81 m|0.15 - 3.32 GW|4.12 - 91.7 TW| |\n|Oct 30, 2022|3.3 - 4.5|240 min|3187921|gb|60.5 min|358|7.6 m|0.065 - 1.48 GW|8.40 - 190 TW| |\n|Nov 01, 2022|6.3 - 6.9 & 8.1 - 8.7|40 min|1147|-|-|-|-|-|4.28 - 137 TW| |\n|Nov 02, 2022|4.5 - 5.7|240 min|27903|hd|37.4 min|21|6.8 m|0.018 - 0.57 GW|2.9 - 92.8 TW| |\n|Nov 05, 2022|5.7 - 6.3 & 6.9 - 7.5|240 min|24422|bc|8.6 min|7|3.4 cm|0.090 - 2.88 GW|6.45 - 207 TW| |\n|Nov 09, 2022|7.5 - 8.1 & 8.7 - 9.3|240 min|5157|-|-|-|-|-|13.3 - 421 TW| |\n\nTotal: 0.9 \u21919.3 GHz, 28 h, 6194847, 7, 6.0 h, 2467, -, -, 2.17 - 421 TW\n\na The number of signals detected by turboSETI utilizing frequency scrunching and subsequently surviving spatial filtering across beam-formed beams.\n\nb This potential event was not counted, though a slightly smaller dish would have created a wider beam that would have triggered an event, as explained in \u00a73.3.\n\nc The assumed dish diameter in these events is larger than the smallest dish needed in the most extreme misalignment scenario due to uncertainties described in \u00a73.3 and illustrated in Figure 9. However, it is much smaller than the smallest dish needed in the most favorable alignment of the extreme case, and these events occur at less geometrically extreme orbital positions.# 3.1. Signal Detection with turboSETI\n\nThe tree de-Doppler algorithm from turboSETI, an open-source software package developed by Breakthrough Listen (Enriquez et al. 2017), is the primary publicly available tool currently being used to search for drifting narrowband signals in high-frequency resolution filterbank data. We applied the algorithm to the TRAPPIST-1 data to identify signals or hits in both beams above a signal to noise ratio (SNR) of 10. Figure 2 shows diagnostic histograms for all of the signals detected over all.# A n ATA S E T I S e a rc h o f T R A P P I S T - 1# Drift Rate Distribution\n\n|SNR|Frequency (GHz)|Drift Rate (nHz)|\n|---|---|---|\n|200|600|1000|\n|2000|40|6000|\n|8000| | |\n\nFigure 2. Distributions of the relevant signal parameters from all 8 observations combined. The SNR is truncated at 1000 for readability, but the data extends out several orders of magnitude, up to a maximum of 1.15 \u00d7 107 for a few (\u21921% of the data) powerful nearby signals. Most signals were detected at lower frequencies where the major bands are known to be more crowded with RFI. The drift rate distribution extends beyond the target maximum of 15 nHz due to bin doubling during the \u201cfrequency scrunching\u201d process, but is still strongly clustered around 0 nHz due to an abundance of terrestrial RFI.\n\nThe observations covered different frequency ranges, leading to widely-varying RFI environments and, thus, numbers of hits. Unsurprisingly, the lower frequency bands, L and S, tended to have more hits than the higher frequencies, C and X.\n\nWithout knowing the transmitted frequency, a Doppler shift alone is degenerate between a transmitter frequency offset and a non-zero line-of-sight velocity between the source and the observer. However, the \u201cdrift rate\u201d of a signal measures the rate at which a transmitted signal shifts through frequency space as it is observed on Earth due to the changing relative motion between the source and the observer. The drift rate is therefore more useful in identifying signals from distant sources with an expected acceleration relative to us as observers. The de-Doppler algorithm used by turboSETI calculates this drift rate in each signal it identifies. One limitation of this de-Doppler technique is its sensitivity loss at higher drift rates than the one-to-one pixel rate of the data due to power smearing over multiple pixels at higher drift rates (Enriquez et al. 2017).\n\nWe configured the \u201cbeamformer mode-h\u201d data produced by the Breakthrough Listen Accelerated DSP Engine (BLADE) back-end to have a frequency resolution of 1 Hz and a time resolution binned to 16 seconds. Thus, any signals with drift rates higher than 1/16 Hz s-1 will have their power smeared over multiple pixels and the sensitivity of the tree summation in the deDoppler algorithm will be significantly reduced. The loss in sensitivity at these higher drift rates scales roughly as 1 \u2191 \u03c9/x, where \u03c9 is the pixel ratio, 1/16 Hz s-1 in this case, and x is the actual drift rate of the signal (Margot et al. 2021).\n\nRadio SETI searches must generally remain agnostic about the relative motion of the transmitter source, especially for systems without known planets. It has been shown that planetary systems with high accelerations, e.g., those with tightly orbiting planets or highly eccentric orbits, could produce narrowband signals with large drift rates due to their relatively high orbital motion, thus complicating the search for.# 8\n\nSuch signals (Sheikh et al. 2019; Li et al. 2023). The recommendation from Sheikh et al. (2019) suggested that drift rates up to 200 nHz (1 kHz s\u22121 at 5 GHz) should be considered in searches targeting stars with little prior knowledge on the orbital characteristics of planets within a given system. (The unit of nHz was there developed and has since gained traction in the field as a frequency-independent unit of drift rate and a measurement of the acceleration over the speed of light, a/c. The more traditional drift rate units of Hz s\u22121 are recovered by multiplying a/c, in nHz, by the observed frequency in GHz.)\n\nHowever, it is assumed that the TRAPPIST-1 planets are tidally locked due to their proximity to their host star and will have a negligible rotational contribution to the drift rate of a transmitter on their surfaces. Additionally, their orbital parameters are well constrained, making it possible to calculate the drift rate contributions from their orbital motion. Satellite transmitters in circular orbit around each planet could produce much higher drift rates, up to an additional \u219245 nHz on top of the contribution from the planet\u2019s orbit around the star. However, we have chosen to limit our scope to analogues of our deep space communications, the strongest of which are surface transmitters to deep space probes. Although our space probes and satellites have transmitters that return data to Earth, these are typically significantly weaker in power, making it much more practical to search first for more powerful surface transmitter signals.\n\nRather than calculating the drift rate from a particular body based on the precise orbital configuration during the time of each observation, we determined a maximum drift rate of 15 nHz for a surface transmitter in the TRAPPIST-1 system using the methodology from (Li et al. 2023) for the innermost planet \u2014 this was used as a conservative value to ensure that our search is sensitive to accelerations from transmitters on any of the planetary surfaces in the system. A more rigorous calculation of the anticipated drift rate range during a PPO event and subsequent data analysis would be warranted in the event of an interesting signal candidate, but this extra consideration ultimately proved unnecessary.\n\nEven at the lower end of the frequency range of \u21921 GHz for these observations, the resulting maximum drift rate of \u219215 Hz s\u22121 is far above the 1/16 Hz s\u22121 pixel ratio where power loss could become a concern. In an attempt to recover some of the power loss during signal detection for these higher drift rates, we used a technique called \u201cfrequency scrunching\u201d or \u201cfscrunching.\u201d Similar to the methods used by Siemion et al. (2013) and Sheikh et al. (2023b), fscrunching bins power over frequency channels to increase the one-to-one pixel ratio of the data, as described below.\n\nWe first ran the de-Doppler algorithm over an absolute value drift rate range of 0 to 1/16 Hz s\u22121. Although we typically only refer to positive drift rate ranges, the algorithm mirrors these values in the negative direction and searches the equivalent negative drift rate range as well, so it should be understood that the negative value range is included in the search. Then fscrunch was applied to bin adjacent frequency channels together, doubling the pixel ratio, and the deDoppler search was run again.# A n ATA S E T I S e a rc h o f T R A P P I S T - 1\n\nfrom 1/16 to 1/8 Hz s\u21921. This doubling of frequency bins was continued until the maximum drift rate of 15 nHz was covered for all frequencies. Up to 12 iterations of this technique were needed to include the maximum drift rate of 139.5 Hz s\u21921 at the highest frequency of 9.3 GHz, extending the search out to an actual maximum of 256 Hz s\u21921 or 27.5 nHz. The minimum number of iterations was 8 at the lowest frequency. The distribution of drift rates can be seen in Figure 2, although the vast majority of signals are found closer to 0 Hz s\u21921, as expected for a local RFI-dominated environment.# 3.2. NbeamAnalysis Pipeline\n\nWe developed the NbeamAnalysis1 pipeline as an open-source filtering tool for signals identified in beamformed filterbank data products at the ATA. Similar in purpose to the post-signal detection filtering functions of turboSETI, the NbeamAnalysis pipeline produces a filtered list of signals with their relevant parameters along with overall diagnostic plots for quick review of the results. The resulting list of filtered signals can then be fed into a versatile plotting tool to plot the dynamic spectra of individual signals for visual comparison across beams. The pipeline is designed to handle any number of beam-formed data products, though the data volume for N >2 may be prohibitive.# 3.2.1. Spatial Filtering\n\nThe search strategy identified &gt;24 million signals across all observations. However, the majority of these hits were found with similar power at the same frequency (within 2 Hz) over the same frequency range in both beams. This indicates that they are likely local RFI, washing over the entire field of view, and not spatially distinct within the individual beams. Due to the sparse array configuration of the 20-element ATA, the synthesized beam possesses high sidelobe levels. Therefore, even a true sky-localized signal could appear in the off-target beam with only moderate power attenuation. From both analytical estimates and empirical tests on sky-localized X-band downlinks from orbiters around Mars (see Section 3.2.4), the minimum attenuation factor between the on- and off-beam was determined to be roughly 4. This attenuation requirement provides an additional RFI filter. By cross-referencing each hit in each beam, we identify hits having the same frequency as well as attenuation factors less than this factor of 4 as RFI. These hits were removed from further consideration and we refer to this rejection method as \u201cspatial filtering.\u201d Execution of the spatial filtering process reduced the total number of hits to &gt;6 million for further processing.# 3.2.2. DOT\n\nWe developed and incorporated the DOT algorithm within the NbeamAnalysis pipeline as a novel filtering technique to identify candidate signals after the optional.\n\n1 https://github.com/SETIatHCRO/ATA-Utils/tree/master/NbeamAnalysis# 3.2.3. Injection Recovery\n\nTo test the effectiveness of scoring signals with the DOT algorithm, we used setigen to inject artificial signals into the data (Brzycki et al. 2022). As a preliminary test, three signals were injected; The first was a strong signal injected in a region of pure noise; The second was injected on top of RFI with a signal strength much greater than the RFI signal detected by turboSETI; The third was injected on top of RFI with a signal strength comparable to the RFI signal strength. The pipeline was then run on the data with these injected signals.\n\nThe injection test indicates that a narrowband signal appearing in only one beam should produce scores very near 0, unless overlapping with much stronger RFI. Figure 3 shows the resulting waterfall plot of the two beams where a signal has been injected in a region devoid of obvious RFI in the target beam. The resulting correlation (DOT) score very near 0 demonstrates that a strong signal localized in one beam of a beamformed data product is easily identifiable.\n\nTypically, narrowband RFI shows up in both beams and produces correlation scores significantly closer to 1 than 0, even for signals with low SNR. Figure 4 shows that even for a powerful signal injected on top of relatively weaker RFI, the pipeline can...# A n ATA S E T I S e a rc h o f T R A P P I S T - 1# Figure 3\n\nAn injection-recovery test using setigen used to score an isolated signal with the DOT algorithm. The isolated linearly-drifting signal seen in the target panel was injected in a region of frequency space known to contain only noise. The signal was given a width of 2 Hz, a drift rate of 0.07 Hz s-1, and an SNR of 42,000. turboSETI calculates SNR independently from the SNR input into setigen, but nevertheless indicates a relatively strong signal as intended. The low correlation score of 0.019 indicates that the signal detected in the target panel is distinct from the pure noise in the o! panel.# Figure 4\n\nThe second injection recovery test using setigen to score a strong signal on top of weaker RFI. The added signal seen only in the target panel was injected on top of previously identified RFI, originally calculated by turboSETI as 42.25. The signal was given a width of 2 Hz, a drift rate of 0.07 Hz s-1, and an SNR of 1000 times that of the RFI. turboSETI calculates SNR independently from SNR input into setigen, but nevertheless indicates a relatively strong signal as intended. The RFI o!set from 0 in these frames shows why this was not removed by spatial filtering. The somewhat low correlation score of 0.276 could indicate an interesting signal for follow-up, depending on the user-defined cuto! value.\n\nRecover a relatively low score that may indicate to the user that further investigation is warranted. However, in the case of a signal comparable or weaker in power than the RFI it overlaps, as in Figure 5, the stronger RFI signal is keyed in on by turboSETI and the hits in both beams over this region are registered at the same frequencies. Thus, if spatial filtering is applied in the pipeline, this signal could be filtered out. Even without spatial filtering, the power from the RFI dominates the data slice and the resulting score is so close to 1 that the score alone is unlikely to flag this for further.# Figure 5.\n\nThe third injection recovery test using setigen to score a weak signal on top of stronger RFI. The added signal seen only in the target panel was injected on top of previously identified RFI, originally calculated by turboSETI as 78.88. The signal was given a width of 2 Hz, a drift rate of 0.07 Hz s\u22121, and an SNR equal to that of the RFI. turboSETI calculates SNR independently from the SNR input into setigen, but the reported SNR is relatively close to the original RFI. The RFI centered on 0 in these frames shows why this was removed by spatial filtering. The very high correlation score of 0.983 is unlikely to indicate an interesting signal for follow-up investigation. The potential for true signals to be lurking in frequency space rejected by RFI is a pervasive problem in these kinds of searches. Recent attempts at employing machine-learning techniques have seen some success and may prove to be an effective tool in overcoming this problem (Ma et al. 2023, 2024).# 3.2.4. Mars Probes\n\nAs an additional, practical test of the NbeamAnalysis pipeline, in mid-July and early August 2023, a series of high-frequency resolution observations targeting Mars were performed at the ATA utilizing the same beamforming process. Several deep space probes expected to be transmitting in the observed frequency range were catalogued for reference. The data collected during these observations were then processed with the NbeamAnalysis pipeline, and this test of the pipeline revealed that signals from deep space detected in the target beam were also being picked up in the off-target beam despite significant spatial separation. Through investigation of these data, we found that the application of the beamforming software was capturing power from the target beam in the off-target beam, with an attenuation factor of at least 4 between the beams. Therefore, sufficiently strong signals localized to a distant source within the target beam will not disappear in the off-target beam.# Figure 6\n\nshows the results of including SNR-ratio (detailed in \u00a73.2.5) as a discriminating parameter to measure attenuation between beams. In order to isolate the bulk of expected RFI within the 2D parameter space of DOT score and SNR-ratio, while accounting for an indeterminate amount of uncertainty, we designed a nominal cutoff as a power law relation based on an empirical description of the data. We define it as:\n\ny = 0.9A(x \u2191 0.05)1/3, (2)# A n ATA S E T I S e a rc h o f T R A P P I S T - 1# Figure 6.\n\nResults of Mars test observations showing recovery of 37 Mars orbiter downlinks during the time of observation. 3 signals were classified as RFI below the nominal cutoff, and 1 false positive showed an attenuated signal at a known transmitting frequency for STEREO-A, which was not in either beam at the time. Signals that are powerful enough to show up in both beams, overwhelming the DOT scoring algorithm and trending toward 1.0, are still attenuated in the off-beam and identified as potentially interesting for follow-up. Candidate signals localized to a distant source may show up only weakly in the target beam at the limits of the telescope\u2019s sensitivity, and are expected to appear in the top left of this kind of plot.\n\nwhere A is the attenuation value (assumed to be roughly 4 for the ATA\u2019s current configuration), x is the DOT score and y is the SNR-ratio.\n\nDiscriminating the signals in this way allowed us to reject all the RFI and recover all of the true signals, including an additional \u201cFalse Positive\u201d that turned out to be the STEREO-A spacecraft in heliocentric orbit and not in the beams at the time of observation, likely being picked up through a side-lobe. For comparison with the synthetic signals in X-band, figure 7 shows a few representative spectra of signals that were recovered in this test, including the additional false positive from STEREO-A.\n\nThe variation in attenuation factor for the real signals is mainly due to differences in on-sky target positions between the observations. During each observation, the off-beam is fixed at a constant relative 9 arcminute separation from the target-beam, but these results comprise several observations where the target is located at different positions on the sky, leading to variations in the observed beam pattern. Discussion of ongoing work to account for this variability in future observations can be found in section \u00a75.# Figure 7.\n\nThree representative signals in the on- and off-beams resulting from test observations of Martian probes that were subsequently filtered through the NbeamAnalysis pipeline. The detected frequency of these signals were cross-referenced with known transmission frequencies for various Martian probes to identify their source. The top panel shows a signal from the Emirates Hope orbiter. The middle panel shows a signal from the Mars Odyssey orbiter. The bottom panel shows the \u201cFalse Positive\u201d signal noted in figure 6 from STEREO-A, a heliocentric satellite. The colorbar indicates the amount of power in each beam, normalized to the peak of the target beam signal and displayed in a logarithmic scale.\n\nCalculation of the SNR-ratio between beams for a given signal was added to the pipeline as a second major filtering parameter based on tests explained in \u00a73.2.4. For a given signal, there is an expected attenuation value of \u21924 between the target and off-target beams from the beamforming process employed to create these data. A sufficiently powerful signal coming from a distant target within the target beam should# A n ATA S E T I S e a rc h o f T R A P P I S T - 1\n\nshow up at least 4 times weaker in the o!-target beam. Thus, by calculating the SNR within every data slice that contains a hit as well as the same frequency-sliced data in the o!-target beam, the SNR-ratio can be calculated and compared with the expected attenuation factor.\n\nTo calculate the SNR of a given data slice, we measured the standard deviation of the middle 90 percent of the pixels within the data as described in \u00a73.2.2. We identified signal in each data slice as any power greater than 10\u2193 the standard deviation of the noise. The SNR for data slices not meeting this criteria was set to 1. In data slices where signal was identified, we then calculated the signal strength to be the median of the N highest power elements, where N is the number of time bins, which is 38 in the 10-minute integrations of the TRAPPIST-1 data. We determined the SNR by dividing the signal by the standard deviation of the noise, and then finally, the SNR for the on- and o!-beams was divided to produce the SNR-ratio for each hit.# 3.3. PPOs\n\nThorough analysis to determine the orbital parameters of the TRAPPIST-1 planets by Agol et al. (2021) suggests that the planets in this system are very co-planar and nearly edge-on, with some small degree of offset and uncertainty. But even a small deviation in the relative inclinations of the planets can turn a syzygy into a near miss.\n\nHowever, for the purposes of searching for radio spillover, we use a broader definition of PPOs that do not require syzygy. Here, a \u201cPPO event\u201d defines a window of time wherein Earth could receive radiation from a beamed radio signal from the further planet towards the nearer one. The range of valid geometries is based on the width of the opening angle, which itself depends on the frequency and dish diameter. Figures 8 & 9 provide cartoon illustrations of favorable geometries from different points of view.\n\nThe three points of the transmitting planet, receiving planet, and the Earth constitute a plane. In this plane, the radio transmission is beamed along a line between the two planets in an expanding cone with some opening angle, \u03b5 = 1. 15\u03d1/D, where \u03d1 is the wavelength of transmission and D is the diameter of the transmitter dish. In this way, \u03b5 is defined as the FWHM of the beam. For the sake of simplicity and because we are only defining windows in which to focus our search based on notional dish sizes, we assume most of the power is contained within this angle and drops off steeply enough to be approximated as a top-hat function with the FWHM as its width. When half of that opening angle, \u03b5, is larger than the angle between the central beam line and Earth, \u03d6, there is a PPO event. In this way, syzygy is not required for a PPO event with a sufficiently large transmitted beam.\n\nThe DSN typically uses a 34 m dish to transmit to probes on Mars. The separation of planets in the TRAPPIST-1 system is an order of magnitude smaller than the distance between Earth and Mars, meaning that dish diameters can be an order of magnitude smaller than those used by the DSN to achieve the same gain and signal strength at a given power. We therefore started with an assumed dish diameter of 3.4 m for# Figure 8.\n\nA cartoon illustrating that PPO events, in this work, can occur in scenarios broader than strict occultation. Specifically, PPO events occur where the opening angle of the transmitted beam, \u03c9/2, is greater than the angle \u03b5 between the line of the planets and the \u2191\u02c6 z direction, toward Earth. The top panel shows the projected xy-plane view. The bottom panel shows a bird\u2019s-eye, xz-plane view of the same scenario where a transmitter on the surface of planet b is sending a radio signal toward planet c. This sketch is not to scale, but it provides a visual reference for the geometry considered in calculating a PPO event.# A N ATA S E T I S e a rc h o f T R A P P I S T - 1# 17\n\nFigure 9. Sketch of the geometry of a PPO event where the effects on our observations due to uncertainties in the semimajor axis and inclination would be maximized. When the opening angle of the transmitted beam, \u03c9/2, is greater than the angle \u03b5 between the line of the planets and the \u2191\u02c6 direction, spillover emission from a PPO event could be detected.\n\nCommunication between neighboring planets and added an additional scaling factor. The DSN uses larger dishes for communication with our deep space probes further away, so we included a scaling factor to account for the possibility of larger dishes being used to communicate to planets further apart. This scaling factor was chosen as the square root of the difference between the numbered order of each planet. This choice was made to yield reasonably sized dishes and resulting beams that essentially vary with target distance and provide a reasonable constraint on a detectable transmitting beam width. The scaled dish diameter and the maximum frequency of each observation were used to calculate the opening angle of a transmitted beam, which determined whether that triggered a PPO event. The duration of each event was calculated by totaling the number of steps in the simulation, with a resolution of 2.9 minutes, where \u03b5/2 > \u03d6 for the relative positions of that pair of planets with respect to Earth.\n\nA modified version of the NbodyGradient algorithm originally developed by Agol et al. (2021) was used to determine the orbital positions of the bodies in the TRAPPIST-1 system from August 23, 2015, 10:20:51.81 UT through the end date of the last observations for this project. Some of the modifications included adding radii of the bodies in the system, calculating an impact parameter to determine planet-planet occultations, and using smaller time steps (0.002 days down from 0.05 days) to ensure small timescale PPOs could be identified at any orbital phase. The modified code was tested to accurately replicate the results of the original NbodyGradient, which has been shown to predict mid-transit times based on transit timing variations for this particular system within microsecond agreement of similar N-body integrators, like TTVFast (Agol et al. 2021). At every time step, the algorithm calculates and records the positions of every body in the system. We assume these positions to be accurate outside of transit; the alignment of multiple bodies along the line of sight of the Earth was calculated during the times of these observations.\n\n2# Figure 10.\n\nSimulated potential PPO events during our observations. Online viewers will see a concatenated video of the orbital configuration of the system during each of the observations, including any potential PPO events that we found to occur during those windows. A still image of a PPO event during the observation on Oct 29, 2022 is included where the animation is not accessible. The top panel shows a bird\u2019s-eye view of the system with planet radii scaled up for better viewing. The distances and beam sizes are to scale, assuming a beam created with a 3.4m dish at 3.3 GHz (the maximum frequency observed during this particular session) from the surface of planet g aimed at planet e. The bottom panel shows the edge-on view with planet sizes scaled with distance, showing how much of the beam spills over the planet toward the direction of Earth in the negative z-direction. The red dashed lines in the illustrated beam is the inner angle blocked by the occulting planet, e. The blue dashed lines show the outer angle of the beam that would spill over the planet. The window for this event lasted roughly 95 minutes.\n\n| |0.05|0.04|0.03|0.02|0.01|0.00|-0.01|-0.02|-0.03|-0.04|-0.05|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|3|0.01|0.00| | | | | | | | | |# A n ATA S E T I S e a rc h o f T R A P P I S T - 1# 4. RESULTS\n\nUncertainties in the inclination, i, and semi-major axis, a, of the planets could still lead to alignments that disallow a PPO event. Figure 9 shows the geometry for which uncertainties in a and i have the maximum effect on observability of spillover radiation. The most extreme case was calculated for each pair of planets during a potential PPO event. In most cases, the maximum dish diameter that could have been assumed and still created a PPO event at the frequencies observed was much larger than the dish diameter assumed.\n\nThe orbital configuration of the system during the time of observation based on the output of the NbodyGradient code was simulated as animated gifs. Figure 10 shows a concatenation of the gifs, and the representative still frame captures the PPO event on October 29, 2022. The duration of the 7 PPO events during the 28 hours of observations ranged from 8.6 minutes to 99.4 minutes. Additional simulations of this system outside these windows indicate that over the course of just a few days many PPOs are likely to occur with a similarly wide range of durations, showing good agreement with previous studies, and offering encouraging results as a search strategy (Luger et al. 2017; Ashtari 2023).\n\nThe NbeamAnalysis pipeline does not output a definitive identifier of ETI, but instead maps each signal in the data onto a 2D parameter space that can be used as a guide for sorting and filtering the list of signals as candidates for follow-up. The two parameters are the DOT score, which scores similarity of signals in each beam, and the SNR-ratio, which measures the attenuation of signal power in each beam. The expectation is that true-positive, sky-localized signals will tend to have low DOT scores and high SNR-ratios, while local RFI will tend to have high DOT scores and low SNR-ratios.\n\nThe 2D parameter space for each observation of the TRAPPIST-1 data is plotted in Figure 11, showing that the vast majority of signals, over 99.8% of the combined data, are identified as RFI. Each observation is treated separately to account for any temporal dependence in the RFI environment at the time of observation.\n\nAfter filtering and scoring the list of signals, the pipeline also contains a plotting tool to show comparison dynamic spectra (waterfall plots) of each beam for a given signal, with the color scale representing the signal power and normalized to the power in the target beam. For practical considerations, the default configuration plots up to 500 of the most interesting candidate signals, though this number is adjustable as an input parameter. A nominal cutoff, defined in \u00a73.2.4, is first used to identify obvious RFI with high DOT scores and low SNR-ratios, and if still more than 500 signals remain, the default plotting scheme will plot up to the lowest 500 scoring candidates above the default attenuation value.\n\nIn the case of the TRAPPIST-1 observations specifically, 11127 candidate signals were plotted for visual inspection, of which 1627 were found to be within the PPO.# Figure 11.\n\nSNR-ratio vs. DOT scores for each day of the TRAPPIST-1 observations. Each observation was analyzed independently to account for temporal variability in the RFI environment. The red dashed line represents a conservative cutoff defined by an empirical power law designed to isolate the bulk of the RFI. RFI is expected to have high DOT scores and low SNR-ratios while true signals are expected to trend toward lower DOT scores and higher SNR-ratios. All signals above the cutoff are plotted as individual black points while signals captured below the cutoff are represented as a heatmap, with the colorbar showing the base 10 exponent of the number of signals represented in a given region.# A n ATA S E T I S e a rc h o f T R A P P I S T - 1# Figure 12\n\nAn example of some one-off \u201cblip\u201d results. The red framed central panel is the 10-minute integration from the observations taken on Nov 05, 2022 in which the signal was found, showing reasonable attenuation but lacking the drifting narrowband morphology and persistence in time expected from a true signal in this kind of search. It appears to mimic other confirmed RFI in morphology, though at a distinct and isolated frequency. The observing time progresses from the bottom to the top of each panel. The bottom and top rows show adjacent integrations at the same frequency.\n\nWe do not see a notable difference in the number of signals detected during PPO windows, and none of the signals indicated a definitive technosignature detection of non-human origin, though a handful warranted extra consideration.# Figure 12\n\nshows an example of a one-off \u201cblip\u201d occurring over a timescale less than the 16-second time bins in the recorded data. Figure 13 shows the most enigmatic example of various signals with an unexpected morphology. Many of these blips and morphologically interesting signals recovered by the pipeline were easily discounted as RFI when looked at more carefully, but a small subset, like the one shown in Figure.# Figure 13.\n\nAn example of unique and unexpected RFI in the primary panel framed in red. The observing time progresses from the bottom to the top of each panel. The other rows show adjacent integrations at the same frequency. The apparently alternating pings in the primary frame explain the low correlation score and spark curiosity. However, the low SNR-ratio, small drift rate, and bright features in later integrations all suggest that the source of the signal is likely not localized to the TRAPPIST-1 system.# A n ATA S E T I S e a rc h o f T R A P P I S T - 1\n\n12 held too little information and dissimilarity to other signals in nearby frequency space. It may be worth searching for these signals in future observations. However, like the famed \u201cWow!\u201d signal (Kraus 1979), it seems imprudent to assign confidence to signals that do not persist and could easily be attributed to an increasingly crowded and complicated RFI environment.\n\nFrom this null result, we can calculate an upper limit on the signals that we were sensitive to in this search. The narrowband form of the radiometer equation from Enriquez et al. (e.g., 2017) provides the spectral flux density Smin,narrow in terms of the SNR, System Effective Flux Density (SEFD) of the instrument, the transmitter bandwidth #\u03f1t, the channel width #\u03f1, the number of polarizations npol, and the total integration time per scan, \u03c2obs:\n\nSmin,narrow = (SNR)SEFD\u221anpol \u03c2obs#\u03f1t\u221a#\u03f1\n\nThe majority of these values were chosen when determining the data dimensions and survey design, however the SEFD must be calculated separately for each tuning and observing day as a measure of the sensitivity of the ATA under those particular conditions.\n\nWe follow the same SEFD derivation procedure as Sheikh et al. (2024). Briefly, each observing session began with a 10 min calibrator scan of a flux calibrator \u2014 one of 3C286, 3C48, 3C295, and 3C843. We used the measured gain values from the calibrator scan and the expected flux values for each calibrator (generally from Perley & Butler 2017, except in the case of 3C84), to determine the SEFD of each antenna-polarization combination, and then average these values to get the SEFD of the beamformer as a whole.\n\nWe thus calculate Smin,narrow for each session-tuning combination using SNR = 10, #\u03f1t = #\u03f1 = 1 Hz, npol = 2, \u03c2obs = 10 min, and the SEFD for that session-tuning combination. We can then convert this spectral flux density into a required transmitter Equivalent Isotropic Radiated Power (EIRP) by using the following equation:\n\nEIRPmin = 4\u03c6d2 Smin #\u03f1t\n\nand plugging in d = 12.5 parsecs, we find a range of potential EIRPs that we might be sensitive to, as reported in Table 1.# 5. DISCUSSION & CONCLUSION\n\nTRAPPIST-1 is inarguably an excellent laboratory for high precision multi-planet transiting exoplanet science. And by extension, it is perfect for refining targeted technosignature search strategies for leaked radio emission. The observational data\n\n3C84 is actually suitable for gain calibration, but not generally for flux calibration. In this case, for the first observing session only, 3C84 was accidentally used as the calibrator. In Summer 2023, we derived a set of coefficients for flux calibration with the ATA \u2014 these values were used to use 3C84 as a pseudo-calibrator for this dataset.# 24\n\npresented here provide a useful reference for this system and the current RFI environment. The pipeline developed to search for and filter interesting signals in beamformed data at the ATA performs as well as current single dish search strategies, with similar limitations. However, long stares at every nearby multi-planet edge-on system is impractical.\n\nPPO events comprised roughly 21% of the total observation duration in this experiment. And, while the TRAPPIST-1 system is one of the most favorable systems for PPOs, having 7 closely aligned known transiting planets, most other planetary systems where PPOs could plausibly occur are likely to have far fewer PPOs, leading to a smaller percentage of total time when it would be optimal to observe. Therefore, observing during predicted PPO events would significantly enhance the efficiency of targeted searches for leaked transmission in these systems.\n\nThe DSN transmits continuously as it tracks space probes, and frequently schedules tracking for several hours at a time. We cross-referenced recent scheduling from the Canberra station4 with a recent NASA audit of the DSN5 that includes tracking hours for the most subscribed satellites to estimate how often the DSN is transmitting toward a particular target. Focusing just on Mars transmissions and adjusting the reported time by 80% to account for target acquisition and other set up overhead, we estimate that the DSN transmits to Mars about 30-35% of the time over a given period of months, typically for several hours at a time.\n\nThe PPO event durations are significantly shorter than typical DSN transmissions and each PPO event found during these observations constitutes a unique pairing of planets. We treat each PPO event as an individual probability event within the scope of the entire observations and assume the duration of each PPO event does not significantly contribute to the overall probability of detecting an equivalently intermittent DSN. Therefore, we estimate the probability of having intercepted a transmission during one of these events to be roughly 33% per PPO event. To have a significant chance of intercepting leaked emission from an intermittent transmission source similar to the current DSN, one would want to observe for a minimum of 3 PPO events for each pairing of planets.\n\nThere are of course many caveats to this consideration, as it does not account for all possible factors that could affect the probability of detection from an equivalent ETI DSN. We cannot guess at the true nature of an alien communication system, so we offer this calculation as a first order approximation based entirely on the primary system in use by the only currently known species to communicate through deep space: the human-made DSN. We leave the inclusion and refinement of additional factors that may better inform search strategies along this vein to future research.\n\nThe TRAPPIST-1 system has many advantages that have led to the feasibility of determining and predicting PPO events, but the same process remains difficult and\n\n4 https://www.cdscc.nasa.gov/Pages/trackingtoday.html\n\n5 https://oig.nasa.gov/docs/IG-23-016.pdf# A n ATA S E T I S e a rc h o f T R A P P I S T - 1\n\nMJD: 59879_18519 || fmax: 8647.378070 MHz || Drift Rate: 0.061 Hzls (0.007 nHz) || Correlation Score: 0.495 || SNR ratio: 6.791 || SNR: 11.703\n\n|Beam|SNR|\n|---|---|\n|Target Beam|11.703|\n|Off Beam|1.723|\n\nCentered at 8647.377945 MHz\n\nFrequency [kHz]\n\nMJD: 59879_18519 || fmax: 8647.428070 MHz || Drift Rate: 0.061 Hzls (0.007 nHz) || SNR: 11.703 || Correlation Score: 0.872 || SNR ratio: 1.123\n\n|Beam|SNR|\n|---|---|\n|Target Beam|11.703|\n|Off Beam|10.425|\n\nCentered at 8647.377945 MHz\n\nFigure 14. Original plot of a signal over the expected frequency span based on drift rate in the top row, and the same signal zoomed out in frequency in the bottom row. The true morphology of this broadband signal can be seen over a wider bandwidth and is reflected in the increase in correlation (DOT) score. This demonstrates how small slices of broadband signals are often misinterpreted by current de-Doppler algorithms as narrowband signals.\n\nTime-intensive for other systems. Future work will include adapting such techniques to other applicable systems, especially as higher-precision orbital data becomes available for similar, co-planar systems.\n\nWhile great strides have been made to automate the process of detecting signals and filtering out obvious RFI in very large datasets with very high resolution like this, some challenges remain difficult to overcome. As mentioned in \u00a73.2.3 and shown in Figure 5, low-power signals from a distant source overlapping with much stronger RFI at the same frequency may easily be missed with this and other similar search and filter algorithms.\n\nAnother confounding problem is broadband RFI as illustrated in Figure 14. Slight fluctuations in power between beams often presents as different power signatures in narrowband channels over smaller scales than the true width of the broadband signal. In some cases, this may be erroneously interpreted as distinct narrowband signals by the search algorithm. This can also lead to low correlation (DOT) scores in the filtering pipeline. Although the signals are easily discerned as part of the same broadband feature when zoomed out to larger bandwidths, identifying and classifying.# 26\n\nThese structures as broadband interference is a poorly defined problem and difficult to automate.\n\nRecent attempts at employing machine-learning techniques have seen some success in retrieving previously undetected signals in non-AI driven pipelines (Ma et al. 2023, 2024). Machine learning algorithms have proven very effective as classifiers and identifying key features in image data, in certain instances. It is very possible that some adaptation of these techniques may be useful for mitigating these problems of broadband characterization and signals hidden behind RFI.\n\nThe plotting software in the NbeamAnalysis pipeline offers the ability to stack integrations, which can help track persisting signals and provide additional information for identifying the nature of the signal. However, as seen in Figure 12, many signals occur over shorter timescales than the 16s time binned data, which provides almost no information on its drift rate unless the signal pings again. Preserving the sampling at shorter timescales without binning may help, but this would significantly increase an already dense data volume.\n\nSimilarly, along the orthogonal axis, this project attempted to apply frequency binning (fscrunching) as a potential solution to help recover power lost at higher drift rates. But, when applying the fscrunching technique, the de-Doppler algorithm as currently implemented with turboSETI is run anew on the frequency binned data. Therefore, the algorithm will sometimes trigger on and assign a higher drift rate to the same signal at wider frequency ranges. So, many of the hits, particularly at higher drift rates, are seen to be duplicates of the same signal upon further inspection. Attempting to recover weak signals at high drift rates may be better handled through direct integration of frequency binning into the de-Doppler algorithm from the start, if not an entirely new process to recover signals at high drift rates without substantial loss of power in the signal.\n\nThe limits of the DOT algorithm on ATA data need to be better characterized, including determining a more quantitative cut-off value for future searches. This should be paired with a more robust injection-recovery analysis. In this project, a simple beam-localization procedure was used: the on-beam was always at the phase-center of the array, and the off-beam was always placed a constant separation, of 9 arcminutes, from the phase center within the primary beam. However, the beam isolation would be improved by using a variable off-beam location, adjusted to fall within a \u201cnull\u201d of the on-beam\u2019s pattern. Work is ongoing to incorporate a model of the beamshape that would also allow for a more precise determination of the attenuation in the off-beam (not just the threshold value of 4 used in this work), reducing the number of false positive signals that need to be manually vetted.\n\nThe analysis of the observations presented here demonstrates that precise characterization of ideal systems, like TRAPPIST-1, enabling orbital dynamical modeling and prediction of PPO events offer practical application for leaked emission searches. This provides SETI a powerful new observational tool and search strategy. As signal# A n ATA S E T I S e a rc h o f T R A P P I S T - 1\n\ndetection and RFI mitigation pipelines improve, the inclusion of PPOs to provide narrow search windows may make it more feasible to increase time resolution and sensitivity at higher drift rates.\n\nThe minimum detectable transmitter power by the ATA during these observations, listed in Table 1, show that much greater sensitivity is needed to detect transmissions with power on the order of the DSN, which typically operates at tens of kiloWatts of directed transmission, and \u219210 GW EIRP(Derrick & Isaacson 2023b). It is technically feasible that an Arecibo-like transmitter\u2014operating at 20 TW EIRP\u2014would be detectable by the ATA at low to moderate drift rates, but such powerful transmitters are generally seen as overkill, especially in the compact TRAPPIST-1 system, and not regularly used as part of the DSN for interplanetary communication. However, future observatories, such as the Square Kilometer Array may provide the sensitivity required to detect levels of leaked emission comparable to the DSN (Braun et al. 2019). Detection of leaked radio emission from nearby planetary systems may be on the horizon.",
        "context_id": 10,
        "question": "What spectral type is the star TRAPPIST-1?",
        "answer": [
            "M-dwarf"
        ],
        "context_length": 57094
    },
    {
        "context": "# Cognitive Hierarchy in Day-to-day Network Flow Dynamics# 1 Introduction\n\nWardrop\u2019s first principle characterizes a steady-state User Equilibrium (UE) wherein no driver can reduce their travel time by unilaterally changing the route. However, due to the ever-changing nature of a transportation network (e.g., induced by traffic incidents, capacity modifications, and network structure changes), traffic states may always be in a disequilibrium state (Kumar and Peeta, 2015). To explain how travelers adjust their route-choice behaviors and predict those out-of-equilibrium states, various day-to-day traffic-network flow models have been proposed in the literature over the years.\n\nMost day-to-day models, either continuous or discrete, focus on modeling the impact of past flow evolution on travelers\u2019 route choice behavior of today. The differences among the experienced travel times are the internal driving force that induces the flow change. For example, a rational behavior adjustment process (RBAP) proposed in Yang and Zhang (2009) generalizes a kind of flow evolution process along which the total travel cost of today will decrease based on the experienced cost of yesterday. A bunch of day-to-day models, including the proportional-switch adjustment (Smith, 1984), the projected dynamical system (Nagurney and Zhang, 1997), the network tatonnement process (Friesz et al., 1994), the evolutionary traffic dynamic (Sandholm, 2010) and the simplex gravity flow dynamic (Smith, 1983), can be categorized into this framework. Some studies also use the learning behavior (specifically an exponential smoothing filter) to model the impact of the whole historical cost trajectory on today\u2019s route choice (Horowitz, 1984; Bie and Lo, 2010; Xiao et al., 2016, 2019).\n\nLargely ignored in the existing modeling efforts is the fact that travelers may predict what other travelers will do in the future (upcoming day). Substantial evidence in psychological studies demonstrates that when making decisions, people engage in a certain degree of reasoning about what others will do, i.e., a theory of mind, the ability to understand another person\u2019s mental state (Lo, 2017). In a route-choice context, suppose now a transportation network is in a disequilibrium state, and travelers in the costlier route have the incentive to switch to the shortest route with the minimum travel time. A traveler may reason like: \u201cif many travelers choose the shortest route, then I will try to avoid being on that route\u201d. This phenomenon was somehow manifested in a recent day-to-day virtual experiment (Ye et al., 2018), but the models calibrated in that study did not take it into account. Without modeling such a logic, the prevailing day-to-day models are more like a way of calculating the final equilibrium (e.g., Hazelton# Dynamic Routing Behaviors and Cognitive Hierarchy Theory\n\nand Watling, 2004) rather than explaining the inherent reasons for traffic flow evolution. Intuitively, if all the selfish travelers have yesterday\u2019s complete information and do not consider what the others will do, the shortest one will be the only choice. Under this circumstance, the flow evolution trajectory may oscillate permanently among different routes.\n\nTo model the travelers\u2019 prediction behaviors, this paper employs the idea of Cognitive Hierarchy (CH) theory from Camerer et al. (2004), in which travelers form their beliefs of their opponents using an iterated reasoning process. In the CH theory, the heterogeneous players differ in their strategic-reasoning levels (i.e., cognitive capacities): while lower-step players do not carefully think through the whole game, the higher-step players would try to reap benefits by predicting how these lower-level \u201ccareless\u201d players respond to the current situation. The CH model provided superior explanations in many game-theoretic settings, including guessing games (Costa-Gomes and Crawford, 2006) and extensive-form games (Ho and Su, 2013). It can also explain many phenomena in economics, marketing, and operation management, including market entry competition (Goldfarb and Xiao, 2011) and capacity allocation games (Cui and Zhang, 2017). It is worth mentioning that although the idea stemmed from Camerer et al. (2004), our proposed model\u2019s novelty lies in its dynamic nature. In contrast, most applications of the CH model in the literature are static one-shot games such as the p-beauty game.\n\nIn the literature, two pieces of work have recognized and modeled such prediction behavior; see He and Liu (2012) and He and Peeta (2016). However, they still have their limitations. The former work predicted the potential future congestion only once, and the prediction memory gradually vanished as time wore on. The latter work focused on the case where all the travelers are homogenous 1-step travelers; i.e., they all believe that the other travelers except herself are 0-step ones who myopically switch to shorter routes based on previous experience. Moreover, the model was developed using ordinary differential equations. Despite their tractability for mathematical analysis, the continuous models bring challenges in calibration with real data because the real-world system of repeated daily trips is discrete. Some properties (e.g., stability) in continuous time can only be considered a weaker result compared to the discrete-time counterpart (Li et al., 2018). Under some scenarios, conclusions drawn from continuous- and discrete-time systems may even contradict each other (Ye et al., 2021).\n\nIn this paper, we establish a general framework of day-to-day network flow dynamics based on the CH theory to analyze travelers\u2019 dynamic routing behaviors. In this framework, we allow the travelers to be heterogeneous in their strategic-reasoning capabilities and to form their# Current Research on Strategic-Reasoning Capabilities\n\nWe extend two widely-studied dynamics in the literature, the network tatonnement process (NTP) dynamic (for deterministic UE, DUE) and the Logit dynamic (for stochastic UE, SUE), into our CH framework and call them CH-NTP and CH-Logit dynamic, respectively. A previous online virtual experiment in Ye et al. (2018), which conventional day-to-day models could not fit, can now be calibrated by our proposed CH-NTP dynamic reasonably well. Moreover, the models predict that strategic-reasoning behaviors would lead the system to multiple equilibria, one of which being the classical user equilibrium. Jacobian matrices at the equilibrium points are derived and used to form analytical criteria on local stability around a user equilibrium. Theoretical results and numerical experiments unveiled several insights into how key parameters affect the local stability.# 2.1 Setups\n\nWe consider a strongly-connected transportation network \ud835\udc3a(\ud835\udc41 , \ud835\udc3f, \ud835\udc4a) consisting of a set \ud835\udc41 of nodes, a set \ud835\udc3f of links, and a set \ud835\udc4a of origin\u2013destination (OD) pairs. Each OD pair, \ud835\udc64 \u2208 \ud835\udc4a, has a travel demand of \ud835\udc51\ud835\udc64 and is connected by a set \ud835\udc45\ud835\udc64 of routes. Travelers in the network are categorized into a set \ud835\udc3e (with |\ud835\udc3e| denoting the cardinality) of classes (steps) by their cognitive-capacity levels. (Throughout this paper, the terms \u201cclass\u201d and \u201cstep\u201d are used interchangeably.) We assume for simplicity that the proportion of \ud835\udc58-step travelers, \ud835\udc5d\ud835\udc58 (\ud835\udc58 \u2208 \ud835\udc3e and \u00cd|\ud835\udc3e|\u22121\ud835\udc5d\ud835\udc58 = 1), \ud835\udc58=0 is the same across all the OD pairs. This assumption allows us to write the scaled feasible route flow set of \ud835\udc58-step travelers as \u03a9\ud835\udc5d\ud835\udc58 \u2261 {\ud835\udc99\ud835\udc58 |\u0393\ud835\udc99\ud835\udc58 = \ud835\udc5d\ud835\udc58 \ud835\udc85, \ud835\udc99\ud835\udc58 \u2265 0}, where \ud835\udc99\ud835\udc58 \u2261 (\ud835\udc65\ud835\udc5f\ud835\udc64 , \ud835\udc5f \u2208 \ud835\udc45\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a)\ud835\udc47\ud835\udc58 is the route flow vector (pattern) of class \ud835\udc58 \u2208 \ud835\udc3e, \ud835\udc85 \u2261 (\ud835\udc51\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a)\ud835\udc47 is the OD demand vector and \u0393 is an OD-route incidence matrix indicating if a route belongs to an OD pair. The superscript \u201cT\u201d represents the transpose operation. With a little abuse of notation, we denote \u03a9\ud835\udf02 as the feasible# 2.2 Flow update rules\n\nOn day \ud835\udc61, the route flow pattern of \ud835\udc58-step is denoted as \ud835\udc99\ud835\udc58,(\ud835\udc61), and the aggregate observed route flow pattern \u02dc\ud835\udc99(\ud835\udc61) = \u00cd\ud835\udc58 \ud835\udc99\ud835\udc58,(\ud835\udc61). By observing \u02dc(\ud835\udc61) (or receiving it from the advanced traveler information system), a \ud835\udc58-step traveler before trip will predict the route flow pattern on (\ud835\udc61 + 1)-th day as per her cognitive capacity level (see subsequent Section 2.3). The predicted route flow pattern and its corresponding travel cost pattern are denoted as \ud835\udf45\ud835\udc58,(\ud835\udc61+1) and \ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1).\n\nWith \ud835\udc99\ud835\udc58,(\ud835\udc61) and \ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1), the flow pattern of \ud835\udc58-step travelers on the next day \ud835\udc61 + 1 can be updated as:\n\n\ud835\udc99\ud835\udc58,(\ud835\udc61+1) = \ud835\udc3b\u03a9\ud835\udc5d\ud835\udc58h\ud835\udc99\ud835\udc58,(\ud835\udc61), \ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1); \ud835\udefc, \ud835\udf01i \u2261 \ud835\udefc\ud835\udc9ah\ud835\udc99\ud835\udc58,(\ud835\udc61), \ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1); \ud835\udf01i + (1 \u2212 \ud835\udefc)\ud835\udc99\ud835\udc58,(\ud835\udc61),\n\nwhere \ud835\udc9a is the \u201ctarget\u201d flow pattern in \u03a9\ud835\udc5d\ud835\udc58 determined by a specific day-to-day dynamical model parameterized by \ud835\udf01; and the exponential-moving-average coefficient, \ud835\udefc \u2208 (0, 1), reflecting travelers\u2019 inertia or reluctance to change. (The larger the \ud835\udefc, the smaller the inertia.) Note that the predicted flow pattern for any \ud835\udc58, \ud835\udf45\ud835\udc58,(\ud835\udc61+1), is always in \u03a91.\n\nOur model does not incorporate memory and learning processes in travelers\u2019 decision-making. If we were to include memory effects, travelers\u2019 route choices might be influenced by a weighted average of past experiences rather than just the most recent one (Horowitz, 1984; Bie and Lo, 2010; Xiao et al., 2016; Ye et al., 2021). By abstracting away memory and learning, we can more clearly isolate and analyze the impacts of travelers\u2019 strategic thinking behavior on network dynamics. This simplification allows for greater analytical tractability and easier interpretation of results.\n\nBy describing the dynamic in such a general way, most day-to-day models in the literature can be applied to the framework. For example, the day-to-day models satisfying the RBAP property (Yang and Zhang, 2009) can be applied, i.e., any \ud835\udc9a[\u00b7] satisfying:\n\nh\ud835\udc99\ud835\udc58,(\ud835\udc61), \ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1); \ud835\udf01i \uf8f4\uf8f1\n\n\uf8f4\n\n\uf8f4 \u2208 \u03a8\ud835\udc58,(\ud835\udc61), \u03a8\ud835\udc58,(\ud835\udc61) \u2260\n\n\uf8f2\n\n\ud835\udc9a \uf8f4= \ud835\udc99\ud835\udc58,(\ud835\udc61), \u03a8\ud835\udc58,(\ud835\udc61) = ,\n\n\uf8f4\n\n\uf8f4\n\n\uf8f3\n\n5# 2.3 Cognitive hierarchy levels\n\nWe now use the CH theory to model how travelers with different cognitive capacities form their beliefs on the next day\u2019s flow pattern. To start, 0-step travelers do not think strategically at all. They deem that the flow pattern of day \ud835\udc61 will remain unchanged on day \ud835\udc61 + 1:\n\n\ud835\udf450,(\ud835\udc61+1) = \u02dc\ud835\udc99(\ud835\udc61). (3)\n\nIt is clear that if the network only contains the 0-step travelers, it degenerates into a conventional day-to-day model. (In a CH model for the one-shot games, 0-step players were assumed to either randomize equally across all strategies or choose a salient strategy using ex-ante information (Camerer et al., 2004). Here we adopt the latter idea.)\n\nAs per the CH theory (Camerer et al., 2004), the \ud835\udc58(\ud835\udc58 \u2265 1)-step travelers try to take advantage by predicting how lower-step players respond to the current flow pattern, but are overconfident and do not realize others are using exactly as many thinking steps as they are. Denote \ud835\udc5e\ud835\udc58\u210e = \ud835\udc5d\u22121\ud835\udc5d\ud835\udc56 as \ud835\udc58-step travelers\u2019 belief about the normalized proportion of \u210e-step travelers and \ud835\udc5e\ud835\udc58 \u210e = 0 for \u2200\u210e \u2265 \ud835\udc58. As consistent with the CH theory, such a setting means that travelers do not know the exact distribution of lower-step travelers \u2014 they only confidently assume the normalized distribution. (This setting was also adopted in other studies; see, e.g., Cui and Zhang (2017) that apply the CH theory to the capacity allocation games in the operation management field.)\n\nBased on the above idea, a 1-step traveler thinks that all the others are 0-step travelers, i.e., \ud835\udc5d0 = 1. Her predicted flow on day \ud835\udc61 + 1 is thus formed by looking one step ahead:\n\n\ud835\udf451,(\ud835\udc61+1) = \ud835\udc3b\u03a901h\ud835\udc5e1 \ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udf450,(\ud835\udc61+1); \u02c6, \ud835\udf01\ud835\udc5e0\u02dc \ud835\udefc \u02c6i = \ud835\udc3b\u03a91 h\ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udc99(\ud835\udc61); \u02c6, \ud835\udf01\u02dc\u02dc \ud835\udefc \u02c6i, (4)\n\nwhere \u02c6 \u2208 (0, 1) and \ud835\udf01\ud835\udefc \u02c6 are the predicted coefficients in the travelers\u2019 minds.# Predicted Flow Pattern\n\nWith this predicted flow pattern, the 1-step travelers will switch the route choice, and the resultant flow pattern becomes:\n\n\ud835\udc991,(\ud835\udc61+1) = \ud835\udc3b\u03a9\ud835\udc5d1h\ud835\udc991,(\ud835\udc61), \ud835\udc84\ud835\udf451,(\ud835\udc61+1); \ud835\udefc, \ud835\udf01i = \ud835\udc3b\u03a9\ud835\udc5d1h\ud835\udc991,(\ud835\udc61), \ud835\udc84\ud835\udc3b\u03a91h\ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udc99(\ud835\udc61); \u02c6, \ud835\udf01\u02dc \u02dc \ud835\udefc \u02c6i; \ud835\udefc, \ud835\udf01i. (5)# 2-Step Travelers\n\n2-step travelers will jointly predict both 0-step and 1-step travelers\u2019 responses. The normalized proportion of these two types are \ud835\udc5e20 = \ud835\udc5d0\ud835\udc5d0\ud835\udc5d1 and \ud835\udc5e2+1 = \ud835\udc5d0\ud835\udc5d1\ud835\udc5d1, respectively. They anticipate that the flow pattern of the next day will be:\n\n\ud835\udf452,(\ud835\udc61+1) = \ud835\udc3b\u03a9\ud835\udc5e20h\ud835\udc5e2\ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udf450,(\ud835\udc61+1); \u02c6, \ud835\udf010\u02dc \ud835\udefc \u02c6i + \ud835\udc3b\u03a9\ud835\udc5e21h\ud835\udc5e2\ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udf451,(\ud835\udc61+1); \u02c6, \ud835\udf011\u02dc \ud835\udefc \u02c6i. (6)\n\nHere we assume that the travelers of \ud835\udc58-step (\ud835\udc58 \u2265 1) share the same predicted parameters, \ud835\udefc\u02c6, and \ud835\udf01\u02c6. This assumption allows us to qualitatively model whether the higher-step travelers over- or under-predict lower-step travelers\u2019 behaviors. It also keeps our model parsimonious. Despite this simple treatment, as we will see in the subsequent Section 4, the model still captures the virtual-experiment data quite well, even when we further set \ud835\udefc\u02c6 = \ud835\udefc and \ud835\udf01\u02c6 = \ud835\udf01!# Route Choices Update\n\nBased on the predicted flow pattern \ud835\udf452,(\ud835\udc61+1), the 2-step travelers update their route choices as follows:\n\n\ud835\udc992,(\ud835\udc61+1) = \ud835\udc3b\u03a9\ud835\udc5d2h\ud835\udc992,(\ud835\udc61), \ud835\udc84\ud835\udf452,(\ud835\udc61+1); \ud835\udefc, \ud835\udf01i h\ud835\udc992,(\ud835\udc61), \ud835\udc84\ud835\udc3b\u03a9\ud835\udc5e20h\ud835\udc5e2\ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udf450,(\ud835\udc61+1); \u02c6, \ud835\udf010\u02dc \u02c6i h\ud835\udc5e2\ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udf451,(\ud835\udc61+1); \u02c6, \ud835\udf011\u02dc \u02c6i; \ud835\udefc, \ud835\udf01i. (7)# Recursive Method for Higher-Step Travelers\n\nThe rest types of travelers (\ud835\udc58 > 2) can be done in a recursive manner as follows:\n\n\ud835\udf45\ud835\udc58,(\ud835\udc61+1) = \u22110\u2264\u210e<\ud835\udc58h\ud835\udc5e\ud835\udc58\ud835\udc99(\ud835\udc61), \ud835\udc84\ud835\udf45h,(\ud835\udc61+1); \u02c6, \ud835\udf01h\u02dc \ud835\udefc \u02c6];\n\n\ud835\udc99\ud835\udc58,(\ud835\udc61+1) = \ud835\udc3b\u03a9\ud835\udc5dk\ud835\udc99k,(\ud835\udc61), \ud835\udc84\ud835\udf45k,(\ud835\udc61+1); \ud835\udefc, \ud835\udf01i. (8)# Behavioral Game Theory Findings\n\nSubstantial experiments in the behavioral game theory found that the average thinking step across all the players lies between 1 and 2 (Camerer et al., 2004; Chong et al., 2016). Hence, the rest of this paper focuses on the case where |\ud835\udc3e| \u2264 3. Note that those experiments were designed for one-shot games with a relatively small number of subjects, in contrast to the dynamic setting with many subjects in this paper. It is also worth mentioning that the methodology in this paper can also be applied to cases where |\ud835\udc3e| > 3.# 2.4 Model interpretation and practical relevance# 2.4.1 Interpretations on the strategic thinking\n\nStrategic thinking behavior is the core component of our proposed modeling framework. As stemmed from the CH theory (Camerer et al., 2004), our model assumes travelers know other travelers\u2019 strategic levels (represented by \ud835\udc5e\ud835\udc58 \u210e). This assumption may seem unrealistic at first glance since individual travelers are unlikely to have explicit information about the distribution of strategic sophistication among other road users. However, it can be interpreted on several grounds:\n\n1. Assuming knowing lower-step travelers\u2019 strategic levels serves as a simplified heuristic that represents the underlying cognitive processes travelers use to anticipate others\u2019 behavior. While individual travelers may not explicitly possess this information, our model captures the essence of strategic thinking in aggregate. This approach is validated by the model\u2019s improved fit to observed data (see Section 4), suggesting it effectively captures important aspects of real-world strategic behavior in transportation networks. Notably, even the 1-step model (|\ud835\udc3e| = 2) provides considerable improvements while requiring only one additional parameter compared to traditional day-to-day models. The fact that a multi-class day-to-day model with additional parameters but without strategic prediction failed to reproduce the experimental data further supports that our model\u2019s success stems from its ability to capture strategic thinking, not merely from increased parameter flexibility.\n2. The \ud835\udc5e\ud835\udc58 values might be interpreted as aggregate beliefs held by a class of travelers, rather than exact knowledge possessed by individuals. This aggregate approach allows us to model the overall effect of strategic thinking without making strong claims about individual cognition. The aggregate beliefs represented by \ud835\udc5e\ud835\udc58\u210e can be seen as emergent properties of the system, arising from the collective experience of travelers over time, rather than explicit individual knowledge.\n3. Our modeling framework aligns with the \u201ctheory of mind\u201d concept from cognitive psychology - the ability to attribute mental states and intentions to others (Lo, 2017). In our transportation context, the strategic levels and associated beliefs represent simplified versions of travelers\u2019 theory of mind about other road users.\n\n8# 2.4.2 Practical relevance\n\nOur modeling framework may be used to characterize the strategic thinking behaviors of major decision-making entities that each control a significant portion of traffic flow. In this context, the flow distribution vector of one class can be viewed as a decision of one large entity. This approach is particularly relevant in modern urban environments where traffic patterns are increasingly influenced by sophisticated navigation systems and centralized management centers. These systems operate at different levels of sophistication in predicting and responding to traffic conditions. At the most basic level (\ud835\udc58 = 0), we have navigation systems or drivers simply reacting to current traffic conditions without strategic planning. The next level (\ud835\udc58 = 1) includes more advanced navigation platforms like Google Maps and Waze, which not only provide route recommendations based on current conditions but also engage in predictive behavior (Lau, 2020). By anticipating the actions of other road users and potential congestion points, they aim to recommend shorter routes for their users, effectively playing a strategic game against other decision-makers in the network. At higher level of sophistication (\ud835\udc58 = 2 or above) are advanced traffic management centers and mobility service providers. These entities attempt to optimize overall traffic flow by considering multiple levels of interaction between different road user groups. Their decisions may require anticipating the responses of various traveler segments and adjusting traffic control measures accordingly.# 3 CH-NTP Dynamic\n\nThe previous section describes the idea using a general day-to-day operator \ud835\udc3b[\u00b7]. This section specifies \ud835\udc3b[\u00b7] as the particular NTP dynamic that admits DUE as a fixed point and analyzes its mathematical properties. The NTP dynamic\u2019s behavioral explanation is identical to a link-based day-to-day model proposed in He et al. (2010): travelers seek to minimize their travel costs while exerting some efforts (or incurring costs) to deviate from incumbent routes. This explanation was also discovered by Tsakas and Voorneveld (2009) in the game theory literature in which the NTP dynamic was called the target projection dynamic. In addition, the NTP dynamic has a unified closed-form formula with the projected dynamical system when route flows only evolve in the interior of the feasible route flow set (Nagurney and Zhang, 1997; Guo et al., 2015; Xiao et al., 2016).\n\n9In a discrete version, for every \ud835\udc58-step traveler, the CH-NTP dynamic reads\n\n\ud835\udc99\ud835\udc58,(\ud835\udc61+1) \u2212 \ud835\udc99\ud835\udc58,(\ud835\udc61) = \ud835\udefc\ud835\udc43\u03a9\ud835\udc5d\ud835\udc58h\ud835\udc99\ud835\udc58,(\ud835\udc61) \u2212 \ud835\udefe\ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1)i \u2212 \ud835\udc99\ud835\udc58,(\ud835\udc61), \ud835\udc58 \u2208 \ud835\udc3e, (9)\n\nwhere \ud835\udc43\u03a9\ud835\udf02 [\ud835\udc9b] is the projection operator that solves the optimization arg min\ud835\udc89\u2208\u03a9\ud835\udf02 \u2225\ud835\udc89 \u2212 \ud835\udc9b\u22252. The \ud835\udf01 in (1) is now replaced by \ud835\udefe(> 0), which captures the costs incurred by deviations from the incumbent routes. With a larger \ud835\udefe, travelers have smaller inertia and are prone to switching.\n\nBased on the modeling framework in Section 2.3, the predicted flow pattern of each class \ud835\udc58 \u2208 {0, 1, 2} is given by:\n\n\ud835\udf450,(\ud835\udc61+1) = \u02dc\ud835\udc99(\ud835\udc61), (10)\n\n\ud835\udf451,(\ud835\udc61+1) = \u02c6\ud835\udc43\u03a91h\ud835\udc99(\ud835\udc61) \u2212 \u02c6\ud835\udc84\ud835\udf450,(\ud835\udc61+1)i\ud835\udefc + (1 \u2212 \u02c6) \u02dc\ud835\udefc \ud835\udc99(\ud835\udc61), (11)\n\n\ud835\udf452,(\ud835\udc61+1) = \u02c6\ud835\udc43\u03a9\ud835\udc5e20h\ud835\udc5e2 \ud835\udc99(\ud835\udc61) \u2212 \u02c6\ud835\udc84\ud835\udf450,(\ud835\udc61+1)i + \u02c6\ud835\udc43\u03a9\ud835\udc5e21h\ud835\udc5e2 \ud835\udc99(\ud835\udc61) \u2212 \u02c6\ud835\udc84\ud835\udf451,(\ud835\udc61+1)i + (1 \u2212 \u02c6) \u02dc\ud835\udefc0\u02dc\ud835\udefe\ud835\udefc1\u02dc\ud835\udefe\ud835\udefc \ud835\udc99(\ud835\udc61), (12)\n\nwhere the predicted parameter \u02c6 in (4) and (6) is replaced by \u02c6 \ud835\udf01 \ud835\udefe > 0. Eqs. (10)-(12) indicate that for each \ud835\udc58, \ud835\udf45\ud835\udc58,(\ud835\udc61+1) is a function of \u02dc\ud835\udc99(\ud835\udc61).\n\nThe following lemma introduces a fundamental property of the projection operator, which will be useful in the following analyses.# Lemma 1. (Facchinei and Pang, 2007)\n\nLet \ud835\udc36 be a nonempty and closed convex subset of \u211b\ud835\udc5b. Then for any \ud835\udc9b \u2208 \u211b\ud835\udc5b and \ud835\udc98 \u2208 \ud835\udc36,\n\n(\ud835\udc43\ud835\udc36 [\ud835\udc9b] \u2212 \ud835\udc9b)T (\ud835\udc98 \u2212 \ud835\udc43\ud835\udc36 [\ud835\udc9b]) \u2265 0. (13)\n\nMoreover, \ud835\udc43\ud835\udc36 [\ud835\udc9b] is the only point in \ud835\udc36 satisfying the above relation.# 3.1 Mixed prediction-based equilibria\n\nFixed points of the dynamical system (9)-(12) are termed as mixed prediction-based equilibria (MPE) and they are characterized in the following proposition.# Proposition 1.\n\nWhen the route cost function \ud835\udc84(\ud835\udc99) is continuous, the dynamical system (9)-(12) admits at least one MPE (i.e., one fixed point). Moreover, a vector \ud835\udc99\u2662 \u2261 (\ud835\udc99\ud835\udc58,\u2662, \ud835\udc99\ud835\udc58,\u2662 \u2208 \u03a9\ud835\udc5d\ud835\udc58, \ud835\udc58 = 0, 1, 2)T is an MPE if and only if the following variational inequality (VI) holds:\n\n\u00d5 \ud835\udf45\ud835\udc58,\u2662  \u02dc\ud835\udc99\u2662T \ud835\udc99\ud835\udc58 \u2212 \ud835\udc99\ud835\udc58,\u2662 \u2265 0, \u2200\ud835\udc99 \u2261 (\ud835\udc990, \ud835\udc991, \ud835\udc992)T \u2208 \u00d6 \ud835\udc84 \u03a9\ud835\udc5d\ud835\udc58, (14)\n\n\ud835\udc58=0,1,2 \ud835\udc58=0,1,2where *\ud835\udf45k,\u2662 is a function of the aggregate flow pattern \u02dc\ud835\udc99\u2662 = \u2211k \ud835\udc99k,\u2662*, given by (10)-(12).# Proof.\n\nContinuity of the RHS of the dynamical system (9)-(12) is guaranteed by the continuities of the projection operator (Penot, 2005) and the route cost function. Hence, according to Brouwer\u2019s Fixed Point Theorem, there exist at least one fixed point to the dynamical system.\n\nAccording to Lemma 1, any fixed point of dynamical system (9)-(12), denoted as *\ud835\udc99\u2662 \u2261 (\ud835\udc99k,\u2662, \ud835\udc99k,\u2662 \u2208 \u03a9pk, \ud835\udc58 = 0, 1, 2)*T, solves the following inequalities:\n\n*\ud835\udc99k,\u2662 = \ud835\udc43\u03a9pkh\ud835\udc99k,\u2662 \u2212 \ud835\udefe\ud835\udc84\ud835\udf45k,\u2662  \u02dc\ud835\udc99\u2662i &nbsp;&nbsp;&nbsp;&nbsp; \u21d4 &nbsp;&nbsp;&nbsp;&nbsp; \ud835\udefe\ud835\udc84\ud835\udf45k,\u2662  \u02dc\ud835\udc99\u2662T \ud835\udc99k \u2212 \ud835\udc99k,\u2662 \u2265 0, \u2200\ud835\udc99k \u2208 \u03a9pk, \u2200\ud835\udc58*, (15)\n\nwhere *\ud835\udf45k,\u2662 is the predicted flow pattern under \u02dc\ud835\udc99\u2662 = \u2211k \ud835\udc99k,\u2662, given by (10)-(12). The right part of (15) is of a VI form and they can be combined since \u03a9pk (\ud835\udc58 \u2208 \ud835\udc3e)* are disjoint from each other (Kinderlehrer and Stampacchia, 2000; Yang et al., 2007).\n\n\u25a1\n\nWe next define a |\ud835\udc3e|-class DUE in Definition 1 (see also in Nagurney, 2000, and Zhou et al., 2020) and show that it is one of the MPE in Proposition 2.# Definition 1.\n\n(Nagurney, 2000; Zhou et al., 2020) A route flow pattern *\ud835\udc99* \u2261 (\ud835\udc99k,\u2217, \ud835\udc99k,\u2217 \u2208 \u03a9pk, \ud835\udc58 \u2208 \ud835\udc3e)*T is said to be a |\ud835\udc3e|-class DUE if the following VI holds:\n\n*\u2211k \u2208 \ud835\udc3e \ud835\udc84k ( \u02dc\ud835\udc99*)T\ud835\udc99k \u2212 \ud835\udc99k,\u2217 \u2265 0, \u2200\ud835\udc99 \u2261 (\ud835\udc99k, \ud835\udc58 \u2208 \ud835\udc3e)T \u2208 \u00d6\u03a9pk*, (16)\n\nwhere *\u02dc\ud835\udc99* = \u2211k \ud835\udc99k,\u2217 and \ud835\udc84k ( \u02dc\ud835\udc99*) is the experienced route cost vector of class \ud835\udc58*.\n\nNote that the aggregate flow *\u02dc\ud835\udc99* equals the classical DUE of a single class in the normal sense. The superscript \ud835\udc58 can also be dropped from \ud835\udc84k ( \u02dc\ud835\udc99*)* because all the travelers have the same experience on each route. In the remainder of the paper, the prefix \u201c|\ud835\udc3e|-class\u201d will be omitted if the context allows.# Proposition 2.\n\nA |\ud835\udc3e|(= 3)-class DUE in Definition 1 is an MPE of the dynamical system (9)-(12), but not vice versa.\n\nProof. We prove the sufficiency, and the necessity can be easily negated by a counter-example, as demonstrated in the numerical experiments (see Section 5.1).At the DUE, first note from (10) that \ud835\udf450,\u2217 equals the aggregate flow pattern, \u02dc\ud835\udc99\u2217. It is well-known that \u02dc\ud835\udc99\u2217 \u2208 \u03a91 of the classical single-class DUE satisfies the following VI:\n\n\ud835\udc84( \u02dc\ud835\udc99\u2217)T (\ud835\udc99 \u2212 \u02dc\ud835\udc99\u2217) \u2265 0, \u2200\ud835\udc99 \u2208 \u03a91. (17)\n\nMultiplying (17) by \u2212 \u02c6 and adding ( \u02dc\ud835\udefe \ud835\udc99\u2217)T (\ud835\udc99 \u2212 \u02dc\ud835\udc99\u2217) to both sides yields:\n\n( \u02dc\ud835\udc99\u2217 \u2212 ( \u02dc\ud835\udc99\u2217 \u2212 \u02c6\ud835\udc84( \u02dc\ud835\udc99\u2217)))T (\ud835\udc99 \u2212 \u02dc\ud835\udc99\u2217) \u2265 0, \u2200\ud835\udc99 \u2208 \u03a91. (18)\n\nwhich, by Lemma 1, implies that:\n\n\ud835\udc99\u2217 = \ud835\udc43\u03a91 [ \u02dc\ud835\udc99\u2217 \u2212 \u02c6\ud835\udc84( \u02dc\ud835\udc99\u2217)]\ud835\udefe. (19)\n\nHence, according to (11), at the DUE, the predicted flow pattern of class-1 player, \ud835\udf451,\u2217 = \u02dc\ud835\udc99\u2217.\n\nIn fact, for any scaling factor \ud835\udf02 > 0 and \u02c6\ud835\udefe > 0,\n\n\ud835\udc43\u03a9\ud835\udf02 [\ud835\udf02 \u02dc\ud835\udc99\u2217 \u2212 \u02c6\ud835\udc84( \u02dc\ud835\udc99\u2217)] = arg min\ud835\udc9a\u2208\u03a9\ud835\udf02 \u2225\ud835\udc9a \u2212 \ud835\udf02 \u02dc\ud835\udc99\u2217 + \u02c6\ud835\udc84( \u02dc\ud835\udc99\u2217)\u22252 = arg min\ud835\udc99 \u2225\ud835\udc99 \u2212 \ud835\udf02 \u02dc\ud835\udc99\u2217 + \ud835\udf02 \ud835\udc99\u2217\u22252. (20)\n\nApplying (20) to the definition of the 2-step travelers\u2019 predicted cost in (12), we obtain that the predicted flow pattern of class-2 player, \ud835\udf452,\u2217 = \u02dc\ud835\udc99\u2217.\n\nSummarizing the above, the predicted flow patterns of all the three classes at DUE are exactly \u02dc\ud835\udc99\u2217, and thus (14) becomes (16). \u25a1# 3.2 Local stability\n\nThe MPE\u2019s non-uniqueness makes global stability analysis difficult, if not impossible, because we cannot construct a global convex Lyapunov function that decreases as time passes. We, therefore, turn to analyze its local stability. The definitions and theorems on local stability used in this paper are relegated to the Appendix A. In this subsection, we first give the Jacobian of the CH-NTP dynamic at the MPE and then analyze the stability near the DUE.# 3.2.1 Jacobian matrix\n\nThe following lemma describes the property of the Jacobian matrix of a projection operator.# Lemma 2.\n\nFor a projection of a vector \ud835\udc9b onto \u03a9\ud835\udf02 \u2261 {\ud835\udc99|\u0393\ud835\udc99 = \ud835\udf02\ud835\udc85, \ud835\udc99 \u2265 0}, its Jacobian is a diagonal block matrix:\n\n|\ud835\udc44[\ud835\udc9b] =|<br/>\ud835\udc44\ud835\udc64=1[\ud835\udc9b1]|0|\n|---|---|---|\n|.|.|\n|\ud835\udc44\ud835\udc64=|\ud835\udc4a |\ud835\udc9b|\ud835\udc4a ||0|\n\nwith each block element \ud835\udc44\ud835\udc64 [\ud835\udc9b\ud835\udc64 ] being Diag(1\ud835\udc64 ) \u2212 |\ud835\udc38(\ud835\udc9b\ud835\udc64 \ud835\udc64)|, where 1\ud835\udc64 is an indicator vector of size |\ud835\udc45\ud835\udc64 | whose \ud835\udc5f-th entry is 1 if \ud835\udc5f \u2208 \ud835\udc38(\ud835\udc9b\ud835\udc64 ) and 0 otherwise. \ud835\udc38(\ud835\udc9b\ud835\udc64 ) is the set of routes on \ud835\udc64 that exhibit positive flow after applying the operator onto \u03a9\ud835\udf02\ud835\udc64 \u2261 {\ud835\udc99|\u0393\ud835\udc64 \ud835\udc99 = \ud835\udf02\ud835\udc51\ud835\udc64 , \ud835\udc99 \u2265 0}, where \u0393\ud835\udc64 is the OD-incidence matrix of OD pair \ud835\udc64. Each \ud835\udc44\ud835\udc64 [\ud835\udc9b\ud835\udc64 ] is positive semidefinite (PSD) whose eigenvalues are either 0 or 1. It is also idempotent; i.e., (\ud835\udc44\ud835\udc64 [\ud835\udc9b\ud835\udc64 ])\ud835\udc5b = \ud835\udc44\ud835\udc64 [\ud835\udc9b\ud835\udc64 ], \u2200\ud835\udc5b \u2208 N = {1, 2, . . .}, \u2200\ud835\udc64. Rank(\ud835\udc44\ud835\udc64 [\ud835\udc9b\ud835\udc64 ]) = \ud835\udc61\ud835\udc5f(\ud835\udc44\ud835\udc64 [\ud835\udc9b\ud835\udc64 ]) = |\ud835\udc38(\ud835\udc9b\ud835\udc64 )| \u2212 1. It has a repeated eigenvalue 1 with an algebraic and geometric multiplicity of both |\ud835\udc38(\ud835\udc9b\ud835\udc64 )| \u2212 1. Rank(\ud835\udc44\ud835\udc64 [\ud835\udc9b\ud835\udc64 ] \u2212 \ud835\udc3c) = |\ud835\udc45\ud835\udc64 | \u2212 |\ud835\udc38(\ud835\udc9b\ud835\udc64 )| + 1.\n\nIf the projected vector lies in the interior of \u03a9\ud835\udf02 \ud835\udc64, \u2200\ud835\udc64 (i.e., all the route flows after projection are positive), \ud835\udc44[\ud835\udc9b] degrades to \u00af with each block of OD pair \ud835\udc64 being \u00af\ud835\udc64 = Diag(1\ud835\udc64 ) \u2212 |\ud835\udc45\ud835\udc64 \ud835\udc64| where 1\ud835\udc64 is an all-one vector of size |\ud835\udc45\ud835\udc64 |.\n\nProof. See Appendix B. \u25a1# Remark 1.\n\nThe algebraic and geometric multiplicities of \ud835\udc44[\ud835\udc9b] in Lemma 2 are used to identify the local stability (instead of local asymptotic stability) when the Jacobian matrix has eigenvalue(s) of 1; see the requirement of the Jordan block of order 1 in condition (ii) of Theorem A.1.\n\nWith Lemma 2, the Jacobian matrix for the CH-NTP dynamic can be derived as follows and used to examine the local stability at any MPE by checking its eigenvalues.# Fact 1.\n\nThe Jacobian matrix for the CH-NTP dynamic with |\ud835\udc3e| = 3 is a 3-by-3 block matrix:\n\n|\ud835\udc3d\ud835\udc43 =|<br/>\ud835\udc3d\ud835\udc430,0|\ud835\udc3d\ud835\udc430,1|\ud835\udc3d\ud835\udc430,2|\n|---|---|---|---|\n|\ud835\udc3d\ud835\udc431,0|\ud835\udc3d\ud835\udc431,1|\ud835\udc3d\ud835\udc431,2|\n|\ud835\udc3d\ud835\udc432,0|\ud835\udc3d\ud835\udc432,1|\ud835\udc3d\ud835\udc432,2|# where each block \ud835\udc3d\ud835\udc43\ud835\udc56,\ud835\udc57 is defined as follows.\n\n\ud835\udc3d\ud835\udc430,0 = \ud835\udefc\ud835\udc440 \u00b7 (\ud835\udc3c \u2212 \ud835\udefe\ud835\udc370) + (1 \u2212 \ud835\udefc)\ud835\udc3c,\n\n\ud835\udc3d\ud835\udc430,\ud835\udc57\u22600 = \ud835\udefc\ud835\udc440 \u00b7 (\u2212\ud835\udefe\ud835\udc370),\n\n\ud835\udc3d\ud835\udc431,1 = \ud835\udefc\ud835\udc441 \u00b7 (\ud835\udc3c \u2212 \ud835\udefe\ud835\udc371) + (1 \u2212 \ud835\udefc)\ud835\udc3c,\n\n\ud835\udc3d\ud835\udc431,\ud835\udc57\u22601 = \ud835\udefc\ud835\udc441 \u00b7 (\u2212\ud835\udefe\ud835\udc371),\n\n\ud835\udc3d\ud835\udc432,2 = \ud835\udefc\ud835\udc442 \u00b7 (\ud835\udc3c \u2212 \ud835\udefe\ud835\udc372) + (1 \u2212 \ud835\udefc)\ud835\udc3c,\n\n\ud835\udc3d\ud835\udc432,\ud835\udc57\u22602 = \ud835\udefc\ud835\udc442 \u00b7 (\u2212\ud835\udefe\ud835\udc372),\n\nwhere \ud835\udc37\ud835\udc58 \u2261 \ud835\udc37\ud835\udf0b\ud835\udc58,(\ud835\udc61+1) representing the Jacobian of the route cost functions evaluated at \ud835\udf0b\ud835\udc58,(\ud835\udc61+1); \ud835\udc44\ud835\udc58 \u2261 h\ud835\udc99\ud835\udc58,(\ud835\udc61) \u2212 \ud835\udefe\ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1)i; \ud835\udc3c the identity matrix of size \u00cdw |\ud835\udc45w; and \ud835\udc440 \u2261 \ud835\udc44h\ud835\udc5e2 \ud835\udc99(\ud835\udc61) \u2212 \u02c6\ud835\udc84\ud835\udf450,(\ud835\udc61+1)i, \ud835\udc442\n\n\ud835\udc441 \u2261 \ud835\udc44h\ud835\udc5e2 \ud835\udc99(\ud835\udc61) \u2212 \u02c6\ud835\udc84\ud835\udf451,(\ud835\udc61+1)i, \ud835\udc442\n\nProof. The derivation is relegated to Appendix C.\n\nAs stated in Section 3.1, the strategic-reasoning behavior causes the original NTP dynamic to have multiple MPE, including those non-DUE ones. Local stability regarding all the MPE can be analyzed by the Jacobian in Fact 1.# 3.2.2 Stability around the DUE\n\nWe are particularly interested in how the travelers\u2019 prediction behaviors would affect the stability near the DUE. Numerically, it is always tractable to calculate \ud835\udc3d\ud835\udc43\u2019s eigenvalues at the DUE and check its local stability. However, general analytical insights on the parametric space may rely on certain assumptions. Recognizing this, we assume that as the day-to-day process evolves, the projection always yields a positive flow on each route. Under this assumption, the NTP dynamic reduces to a closed-form expression (Sandholm, 2010; Xiao et al., 2016). The \ud835\udefc and \ud835\udefe (and \u02c6 and \u02c6) can be simply combined (specifically, multiplied) to represent the \ud835\udefc \ud835\udefe sensitivity to the cost difference between two routes; see Appendix A in Xiao et al. (2016). We can therefore fix \ud835\udefc = \u02c6 = 1 and vary \ud835\udefe and \u02c6 while the behavioral explanation is not compromised.\n\nWe divide the discussion into two parts: (i) when higher-step travelers can exactly predict lower-step travelers\u2019 switching tendency, i.e., \u02c6 = \ud835\udefe; and (ii) when the prediction is inaccurate, i.e., \u02c6 \u2260 \ud835\udefe.The following assumption is made on the route cost function when analyzing the stability around the DUE.# Assumption 1.\n\nThe Jacobian of the route cost function \ud835\udc84(\ud835\udc99) w.r.t. route flow pattern \ud835\udc99 \u2208 \u03a91, denoted by \ud835\udc37[\ud835\udc99], is symmetric and positive semidefinite.\n\nA widely-adopted case of Assumption 1 is when link travel time functions are separable, differentiable, increasing, and additive (i.e., the route travel time is equal to the sum of travel time on all links that constitute the route).# Proposition 3.\n\nFor a dynamical system (9)-(12) featuring any |\ud835\udc3e| \u2208 {1, 2, 3}, if the projection operator can always generate positive flow on each route (hence \u02c6 and \ud835\udefc can be set to 1) and \u02c6 = \ud835\udefe, its DUE\ud835\udefc under Assumption 1 is locally stable if all the moduli of the eigenvalues of matrix \u00af(\ud835\udc3c \u2212 \ud835\udefe\ud835\udc37\u2217) are less than 1, where \ud835\udc37\u2217 is the Jacobian of the route cost function evaluated at the aggregate DUE, \u02dc\ud835\udc99\u2217, and \ud835\udc44 is defined in Lemma 2.# Remark 2.\n\nProposition 3 shows that conditions for ensuring local stability near the DUE are equivalent for any |\ud835\udc3e| \u2208 {1, 2, 3} when \ud835\udefe\u02c6 = \ud835\udefe. In other words, thinking multiple steps ahead does not affect the local stability at the DUE when higher-step travelers can exactly predict lower-step ones\u2019 switching behavior. Moreover, the stability is independent of the distribution of heterogeneous travelers.\n\nThe difference between models with different |\ud835\udc3e| is that (as we shall see in the numerical experiments) the instability condition of |\ud835\udc3e| = 1 leads to a permanent oscillating pattern near the DUE (i.e., crossing the DUE infinitely many times) while for |\ud835\udc3e| \u2265 2 it is more likely that the system will evolve to and stick at an MPE that is not DUE.\n\nThe following lemma describes the property of the stability criterion used in Proposition 3, \ud835\udc44[\ud835\udc9b](\ud835\udc3c \u2212 \ud835\udefe\ud835\udc37[\ud835\udc99]), by relating it to the PSD matrix \ud835\udc44[\ud835\udc9b]\ud835\udc37[\ud835\udc99], where \ud835\udc44[\ud835\udc9b] and \ud835\udc37[\ud835\udc99] are not necessarily equal to \ud835\udc44\u00af and \ud835\udc37\u2217, respectively.# Lemma 3.\n\nUnder Assumption 1, for any given \ud835\udc9b and \ud835\udc99, denote \ud835\udf07\ud835\udc56 as \ud835\udc44[\ud835\udc9b]\ud835\udc37[\ud835\udc99]\u2019s \ud835\udc56-th eigenvalue and \ud835\udf07\ud835\udc56 \u2265 0, \u2200\ud835\udc56. The corresponding eigenvalue of \ud835\udc44[\ud835\udc9b](\ud835\udc3c \u2212 \ud835\udefe\ud835\udc37[\ud835\udc99]) is 1 \u2212 \ud835\udefe\ud835\udf07\ud835\udc56 if \ud835\udf07\ud835\udc56 \u2260 0. Therefore, as \ud835\udefe increases from 0 to \u221e, the modulus of the \ud835\udc56-th eigenvalue of \ud835\udc44[\ud835\udc9b](\ud835\udc3c \u2212 \ud835\udefe\ud835\udc37[\ud835\udc99]), |1\u2212 \ud835\udefe\ud835\udf07\ud835\udc56|, will first decrease# 3.2.2.2 Imperfect prediction when |\ud835\udc3e| = 2\n\nThe consistency of stability conditions under different |\ud835\udc3e| breaks down when \ud835\udefe\u02c6 \u2260 \ud835\udefe. We first turn to analyze a simple toy network to get some intuitions. Consider an OD pair served by two routes with |\ud835\udc3e| = 2. According to Proposition 2, at the DUE, \ud835\udf450,(\ud835\udc61+1) = \ud835\udf451,(\ud835\udc61+1) and thus the Jacobians of route cost function \ud835\udc371 = \ud835\udc370 = \ud835\udc37\u2217. Without loss of generality, we assume \ud835\udc37\u2217 = \ud835\udc4e \ud835\udc4f\ud835\udc4f \ud835\udc50 with \ud835\udc4e, \ud835\udc4f, \ud835\udc50 > 0 and \ud835\udc4e\ud835\udc50 \u2212 \ud835\udc4f2 \u2265 0 (positive semidefiniteness in Assumption 1). Using the similar derivation in Proposition 3, we can obtain that \ud835\udc3d\ud835\udc43 with |\ud835\udc3e| = 2 has four eigenvalues 0, 0, 1 and 1\ud835\udefe \u02c6(\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50)2 \u2212 \ud835\udefe(\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50) + 1, with the last one being denoted as \ud835\udc53 (\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50; \ud835\udefe, \u02c6). Note that the stability only depends on \ud835\udc53 (\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50; \ud835\udefe, \u02c6) and that \ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50 \u2265 0 because \ud835\udc4e\ud835\udc50 \u2212 \ud835\udc4f2 \u2265 0.\n\nWe refer to stable/unstable region as region of \ud835\udefe and \u02c6 that makes the DUE stable/unstable (i.e., whether \ud835\udc53 (\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50; \ud835\udefe, \u02c6) \u2208 (\u22121, 1) holds or not).\n\nThe following observations can be made:\n\n- When \ud835\udefe\u02c6 = \ud835\udefe, \u22121 < \ud835\udc53 (\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50; \ud835\udefe, \ud835\udefe) < 1 simplifies to \ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50 < \ud835\udefe \u02c6 4.\n- When \ud835\udefe > \ud835\udefe, the stable region is still \ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50 < \u02c6 4. Compared to the case of \ud835\udefe = \ud835\udefe, the size of the stable region is shrunk.\n- When 2 < \u02c6\ud835\udefe < \ud835\udefe, the minimum point of \ud835\udc53 (\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50; \ud835\udefe, \u02c6), 1 \u2212 \u02c6 \ud835\udefe, is always > \u22121. Since the zero points of \ud835\udc53 (\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50; \ud835\udefe, \u02c6) = 1 are 0 and \u02c6 \ud835\udefe 4, respectively, the stable region becomes \ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50 < \u02c6 4. Compared to the case of \u02c6 = \ud835\udefe, the size of the stable region expands because \u02c6\ud835\udefe < \ud835\udefe.\n- When \u02c6\ud835\udefe < 2, \ud835\udefe < \ud835\udefe, \ud835\udc53 (\ud835\udc4e \u2212 2\ud835\udc4f + \ud835\udc50; \ud835\udefe, \u02c6) = \u22121 will have two zero points. The stable region is separated. One region ranges from 0 to the first zero point, 2 \u02c6 \u2212 2\u221a\ud835\udefe(\ud835\udefe\u22122 \u02c6), and the other ranges from the second zero point, 2 \u02c6 + 2\ud835\udefe \ud835\udefe \u02c6\ud835\udefe 4. The combined size of these two regions is \ud835\udefe4\u02c6 \u2212 4\u221a\ud835\udefe(\ud835\udefe\u22122 \u02c6). By the inequality of arithmetic and geometric means and after algebraic simplification, we can see that \ud835\udefe4\u02c6 \u2212 4\u221a\ud835\udefe(\ud835\udefe\u22122 \u02c6) \u2265 \ud835\udefe 4. Hence, the total size of the two separated regions is increased.\n\nProof. See Appendix E.\n\n\u25a1# 4 Validation Using a Virtual Experiment\n\nYe et al. (2018) conducted an online route choice experiment to mimic travelers\u2019 decision-making processes from day to day. The experiment collected 268 participants\u2019 route choices on 26 rounds, where each round corresponded to a true calendar day. It was conducted on the well-known Braess paradox network (Figure 1) with one OD pair served by three routes: Route 1 as 1 \u2192 3 \u2192 5; Route 2 as 2 \u2192 5 \u2192 3; and Route 3 as 2 \u2192 4. The travel time on each link1 + 0.15 \ud835\udc63\ud835\udc4e4, with parameters \ud835\udc4e \u2208 \ud835\udc3f was given by the well-known BPR function: \ud835\udc61\ud835\udc4e(\ud835\udc63\ud835\udc4e) = \ud835\udc61\ud835\udc4e0 + \ud835\udc4e\ud835\udc63\ud835\udc4e, marked in the figure in order as (\ud835\udc4e, \ud835\udc61\ud835\udc4e0, \ud835\udc49), where \ud835\udc61\ud835\udc4e is the free-flow travel time and \ud835\udc49\ud835\udc4e the capacity. The observed route flow (i.e., how many participants selected that route) on each day is visualized in Figure 2 with the days indexed from 0 to 25. Note the large oscillations in the trajectories in Figure 2. Readers can refer to Ye et al. (2018) for more details of the experiment.# 4.1 Difficulty of prevailing models without prediction in reproducing the observed pattern\n\nA natural way of calibrating a day-to-day dynamical model is to find the optimal parameters that minimize the sum of squared error between simulated and observed flow evolution trajectories. In doing so, Ye et al. (2018) found that the prevailing day-to-day models could not produce fluctuated trajectories fitting the experimental data; see Figure 3 of that paper. (Due to the intractability of the above \u201csimulation-based\u201d method, they had to turn to a relaxed problem using regression analysis.) Below we give an explanation of the prevailing models\u2019 difficulties.\n\nTo explain, we plot the net flow of three routes on each day \ud835\udc61 in Figure 3. We use different colors and shapes to mark different routes ranked by the travel cost on the day \ud835\udc61 \u2212 1: green square points for the shortest route, yellow triangular points for the second-shortest route, and red circular points for the longest route.\n\nFirst note that for day \ud835\udc61 \u2208 [1, 16], the longest route on the day \ud835\udc61 \u2212 1 would by-and-large out-flow to the other two routes on the day \ud835\udc61. This observation is in line with most day-to-day models\u2019 assumption that travelers would select the routes with lower costs. However, there was no distinction between the shortest and the second-shortest routes. On quite a few days (i.e., 1, 3, 8, 9, 10, and 13), the previous day\u2019s second-shortest route received more inflow than the shortest route. This indiscrimination between the shortest and the second-shortest routes contradicted the best-response-based day-to-day models\u2019 underlying hypothesis that the shortest route would be the most popular (e.g., Friesz et al., 1994; He et al., 2010; Xiao et al., 2016). We argue that this indiscrimination was a consequence of strategic thinking.# Figure 3: Net flow on day \ud835\udc61 for the three routes sorted by travel cost on day \ud835\udc61 \u2212 1 in the virtual experiment.\n\n| |inflow / outflow on day t|\n|---|---|\n| |40|\n| |20|\n| |0|\n| |\u221220|\n| |\u221240|# 4.2 Calibrations of the CH-NTP dynamic\n\nA 1-step traveler might think that other travelers would go for the shortest route of the day \ud835\udc61 \u2212 1 and thus avoided switching to that route on the day \ud835\udc61. As a result, the second-best route becomes her preference.\n\nFor day \ud835\udc61 \u2208 [17, 25], a keen-eyed reader may find that travelers\u2019 preferences seem to become independent of yesterday\u2019s route flow pattern. For example, the net in- and out-flows of the three routes on day 18 are 0, regardless that the DUE had not been achieved. We conjecture that the participants learned after 18 days\u2019 experience that the occurrence of the shortest route was highly non-predictable and hence used a uniformly-random mixed strategy to respond. (We learned this phenomenon from conversations with some participants in the final days.)\n\nCalibration results of our proposed hierarchical model confirm the above discussions, which are presented in the sub-sections to follow.\n\nTo minimize the number of parameters, we consider special cases with perfect prediction (i.e., \ud835\udefe\u02c6 = \ud835\udefe) and \ud835\udefc = \ud835\udefc = 1. By doing so, we intend to demonstrate our model\u2019s general explanation capability rather than \u201coverfit\u201d data using more parameters. (Relaxing these constraints will further reduce the fitting errors.) The 0-step model is exactly the so-called XYY dynamics used.# 4.2.1 Preparations\n\nWe conducted calibration by minimizing the root mean square error (RMSE) between the predicted flows and the ground truth:\n\nRMSE =\n\u221a(\u03a3t=1M(\ud835\udc65(\ud835\udc61) \u2212 \u00af\ud835\udc5f(\ud835\udc61))2) / 3\ud835\udc40\n\nwhere \ud835\udc40 is the number of days (i.e., days 1 to \ud835\udc40) used for calibration, \u02c6\ud835\udc5f\ud835\udc65(\ud835\udc61) is the model-predicted aggregate flow on route \ud835\udc5f of day \ud835\udc61, and \u00af\ud835\udc5f\ud835\udc65(\ud835\udc61) is the observed flow. The RMSE function may be highly non-linear. To avoid being trapped in local optima, we performed a grid search over the parametric space with \ud835\udefe varying from 0.01 to 1.0 with step 0.002, \ud835\udc5d0 and \ud835\udc5d1 ranging from 0.01 to 1.0 with step 0.01. For models with |\ud835\udc3e| \u2265 2, given all the parameters, the initial route flow pattern (on day 0) was chosen such that the predicted aggregate route flow pattern on day 1 was closest to the observed one in terms of RMSE.\n\nFor each CH-NTP dynamic with different |\ud835\udc3e|, the optimal parameters that minimize the RMSE function (23) are used to calculate log-likelihoods and perform likelihood ratio tests. The log-likelihood function is derived as follows.\n\nDenote \ud835\udc54\ud835\udc58\ud835\udc5f\ud835\udc59\ud835\udc57,(\ud835\udc61) as the probability of a traveler \ud835\udc59 from class \ud835\udc58 choosing route \ud835\udc57 on day \ud835\udc61. Then the final total predicted probability, \ud835\udc3a(\ud835\udc61)(\ud835\udc5f\ud835\udc59\ud835\udc57), is an aggregation of all thinking steps weighted by the proportions; i.e., \ud835\udc3a(\ud835\udc61)(\ud835\udc5f\ud835\udc59\ud835\udc57) = \u03a3\ud835\udc58 \ud835\udc5d\ud835\udc58 \ud835\udc54\ud835\udc58\ud835\udc5f\ud835\udc59\ud835\udc57,(\ud835\udc61). Finally, we can form a log-likelihood function by combining all the travelers on all the routes and days:\n\nLL = \u03a3t=1M \u03a3l=1 \u03a3j=1 \ud835\udc56\ud835\udc57,(\ud835\udc61)\u00b7 ln\ud835\udc3a(\ud835\udc61)(\ud835\udc5f\ud835\udc56\ud835\udc57)# Flow Comparisons\n\n| |0|2|4|6|8|\n|---|---|---|---|---|---|\n|Day (a) the first 9 days|100|90|80|70|60|\n| |50|40|30|20|10|\n|Day (b) the first 16 days|100|90|80|70|60|\n| |50|40|30|20|10|\n|Day (c) all the 25 days|100|90|80|70|60|\n\nFigure 4: Comparisons between the calibrated and ground-truth flow evolutions of Route 1.\n\nwhere \ud835\udc3c\ud835\udc59\ud835\udc57,(\ud835\udc61) is the indicator function being 1 if traveler \ud835\udc59 chooses route \ud835\udc57 on day \ud835\udc61 and 0 otherwise.# 4.2.2 Results\n\nThe simulated flow evolutions of Route 1 with the parameters that minimized the RMSE are presented in Figures 4a-c for \ud835\udc40 = 9, 16, and 25, respectively. (The other two routes exhibit similar results and hence are omitted to conserve space.) The likelihood ratio test is reported in Table 1 with the optimal parameters.\n\nFirst note that in all the figures, the nonhierarchical model with |\ud835\udc3e| = 1, i.e., the conventional NTP dynamic, fails to produce a significant fluctuation pattern fitting the ground-truth trajectory; see the dotted curves. This result is in agreement with Ye et al. (2018). The corresponding flow trajectories for \ud835\udc40 = 9, 16, and 25 are identical and the optimal \ud835\udefe = 0.358.\n\nFurther note that we have previously implemented a multi-class day-to-day model where# Table 1: Calibration results of the CH-NTP dynamic\n\n|Number of days|9|9|9|16|16|16|25|25|25| | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n|CH-NTP dynamic with |\ud835\udc3e| =|1|2|3|1|2|3|1|2|3| |\n|Maximum log-likelihood|-2646.2|-2631.3|-2618.8|-4706.1|-4693.8|-4680.2|-7356.0|-7344.3|-7334.4| |\n|Estimated log-likelihood\u2662\u2605|-2604.0|-4652.6|-7297.7| | | | | | | |\n|Likelihood Ratio Test|-|29.8|52.8|-|24.5|51.7|-|23.3|43.0| |\n|\ud835\udc5d-value|-|4.77-e8|1e-12|-|7.58-e7|6e-12|-|1.35e-6|4.62e-10| |\n|Parameters|\ud835\udc5d1\ud835\udefe0|0.358-|0.7080.88|0.5220.37|0.358-|0.5700.94|0.5080.31|0.358-|0.5660.90|0.4920.31|\n| |\ud835\udc5d|-|-|0.23|-|-|0.05|-|-|0.05|\n\n\u2662 Estimated log-likelihood using the optimal parameters that minimized RMSE function (23).\n\n\u2605 The maximum that the Log-likelihood function can achieve, i.e., the entropy of the ground-truth route choice distribution.\n\nEach class is associated with a distinct sensitivity parameter (Zhou et al., 2020). Despite increasing the number of classes to three, this multi-class model still fails to reproduce the observed fluctuation pattern. In fact, the calibrated flow evolution from this model closely resembles the pattern seen in the |\ud835\udc3e| = 1 case (illustrated by the red dotted curve in Figures 4a-c). This outcome strongly suggests that our improved fitting performance is not merely a result of additional parameters, but rather stems from the enhanced explanatory capability of our proposed model.\n\nHappily, significant fluctuation patterns emerge when some travelers think one step further, as revealed by the yellow dash-dotted curves. The fitting error decreases as a consequence. Note in Figures 4b and c how the predicted curves faithfully capture the ground-truth trend from day 5 to 16. This is also confirmed by observing that the hierarchical model of |\ud835\udc3e| = 2 improves the log-likelihood by 24.5 and 23.3 for \ud835\udc40 = 16 and 25, respectively. Moreover, the hierarchical model with |\ud835\udc3e| = 3 almost doubles the log-likelihood improvement and exhibits better fitting performance from the very first day until day 16. All the improvements are statistically significant under the likelihood ratio test with the \ud835\udc5d-value less than 0.0002%.\n\nFitting performance is less satisfying for days 17-25, as shown by the trailing edges of the curves in Figure 4c. A possible reason is that as the experiment proceeded, the participants found themselves gaining little by predicting the population\u2019s behaviors and thus exhibited somehow uniformly-random behaviors in the last eight days (see the explanations on this matter in Section 4.1). Our time-invariant parameters cannot capture such a pattern.# Numerical Experiments\n\nIn this section, we conduct numerical experiments to verify the previous sections\u2019 theoretical results and highlight some features not captured by the theoretical analysis.# 5.1 Evolutions towards multiple equilibria\n\nWe examine how the strategic-reasoning behavior leads the system to multiple equilibria. We use the Braess network in the virtual experiment as our first example. At the DUE, the aggregate route flow pattern is (89.33, 89.33, 89.33)T. We set p0 = p1 = 0.5 (|K| = 2), \u02c6 = \u03b1 = 0.3, \u02c6 = \u03b3.\u03b1. Figures 5a and b depict the evolution trajectories in terms of aggregate route flow on each route, starting from different initial points (distinguished by different colors) for \u03b3 = 1.4 and 0.2, respectively. (Plotting two routes is sufficient as the degree of freedom is only two due to the flow conservation constraint.) At the initial point, all the routes share the same p0 and p1.\n\n| |non-DUE MPE|DUE|\n|---|---|---|\n|initial point|250|250|\n| |200|200|\n| |150|150|\n| |100|100|\n| |50|50|\n| |0|0|\n|flow of route 1|0|0|\n| |50|50|\n| |100|100|\n| |150|150|\n| |200|200|\n| |250|250|\n\n(a) \u03b3 = 1.4\n\n(b) \u03b3 = 0.2\n\nFigure 5: Evolutions of route flows starting from different initial route flow patterns under different \u03b3 = \u02c6 for |K| = 2. p0 = p1 = 0.5, \u02c6 = \u03b1 = 0.3.\n\nFirst note in Figure 5a how different initial points result in different evolution trajectories and steady states under a large \u03b3. It is interesting to see that two initial points close to each other can evolve to totally different equilibrium points, as shown by the two trajectories starting in the middle. This kind of multiple-equilibria phenomenon is rooted in travelers\u2019 strategic-thinking behaviors rather than the asymmetry of the travel cost functions previously studied in the literature (see, e.g., Watling, 1996; Bie and Lo, 2010; Han et al., 2017). We believe this system feature would enrich behavioral explanation capability.\n\nThings change a lot when \u03b3 is small. With a small \u03b3, travelers are reluctant to switch to\n\n23# 5.2 Local stability near the DUE\n\nWe next test stability conditions on a larger network used in Zhang et al. (2015); see Figure 6. It contains 8 routes that connect two OD-pairs with demand (90, 90)T: Route 1 as 1 \u2192 9 \u2192 14; Route 2 as 1 \u2192 5 \u2192 10; Route 3 as 2 \u2192 6 \u2192 10; Route 4 as 2 \u2192 11 \u2192 15; Route 5 as 3 \u2192 11 \u2192 16; Route 6 as 3 \u2192 7 \u2192 12; Route 7 as 4 \u2192 8 \u2192 12; and Route 8 as 4 \u2192 13 \u2192 17. Link travel times are again given by the well-known BPR function with link numbers, free-flow travel times, and capacities marked in the figure in order as (\ud835\udc4e, \ud835\udc61a 0, \ud835\udc49a). The aggregate DUE is (20, 20, 25, 25, 25, 25, 20, 20)T.# 5.2.1 In the interior of the feasible route flow set\n\nWe first assess Proposition 3 where predictions can be perfectly made, and the projection operators yield positive flows on all the routes. Since we focus on the local stability, the initial aggregate route flow pattern in the following experiments is slightly perturbed from the DUE. We set \ud835\udc5d0 = 0.4, \ud835\udc5d1 = 0.6 for all the routes at the initial point, \u02c6 = \ud835\udefc = 1.0 and \u02c6 = \ud835\udefe. We calculate that \u00af \u2248 0.79 in Lemma 3. According to Proposition 3 and Lemma 3, the equilibrium is stable when \ud835\udefe < \u00af and unstable when \ud835\udefe > \u00af. Figures 7a and b plot two selected routes\u2019 flow trajectories for \ud835\udefe = 0.78 and 0.8, respectively. It is expected that for both |\ud835\udc3e| = 1 and 2, a smaller \ud835\udefe(= 0.78) eliminates the small initial perturbations while a larger \ud835\udefe(= 0.8) renders the# Figure 7: Flow evolutions of the CH-NTP dynamic with |\ud835\udc3e| \u2208 {1, 2} under different \ud835\udefe.\n\n| ||K| = 1, Route 1||K| = 1, Route 3||K| = 2, Route 1||K| = 2, Route 3|\n|---|---|---|---|---|\n|0| | | | |\n|20| | | | |\n|40| | | | |\n|60| | | | |\n|80| | | | |\n|100| | | | |# Verification of Proposition 4\n\nWe now verify Proposition 4 where predictions are inaccurate. All the parameters and initial flow patterns are the same as the above case except for the values of \ud835\udefe and \u02c6. Figure 8\ud835\udefe depicts whether or not the system deviates from the DUE after a small perturbation for various combinations of \ud835\udefe and \u02c6 in a resolution of 0.05, where solid circles and crosses denote stable and unstable systems, respectively. The points on the 45\u25e6 solid curve represent the case of \ud835\udefe\u02c6 = \ud835\udefe (perfect prediction), and the solid curve lying beneath represents the case of \u02c6 = 2 \ud835\udefe. The approximate \u00af = 0.79 by Lemma 3 is also marked.\n\nFirst note the green shaded area where a moderately small \ud835\udefe\u02c6, 2 < \ud835\udefe < \ud835\udefe < \ud835\udefe, helps stabilize the dynamic even when \ud835\udefe exceeds \ud835\udefe\u00af. This justifies the finding in Proposition 4 that, a mild under-prediction enlarges the stable region. In contrast, when \u02c6 is very small (i.e., < 2 \ud835\udefe), the stable region is separated into two pieces. The eigenvalues of \u00af\ud835\udc44\ud835\udc37\u2217 that previously satisfy the stability condition when \u02c6 = \ud835\udefe < \u00af fall into the unstable region due to the separation when \ud835\udefe < 2 \u02c6 \ud835\udefe; see the red shaded area. In addition, as expected in Remark 3, a \u02c6\ud835\udefe > \u00af will make the system permanently unstable, regardless of \ud835\udefe; see all the crosses above the horizontal line of \ud835\udefe\u00af = 0.79. Finally, for this particular numerical case, although the stable region size is reduced when over-predictions occur (\ud835\udefe < \u02c6\ud835\udefe < \u00af), The eigenvalues of \ud835\udc44\ud835\udc37\u2217 still satisfy the stability condition in Proposition 4 and thus the points above the curve \u02c6 = \ud835\udefe are stable.# 1.0\n\n\u03b3 = \u03b3\n\n\u03b3 = 0.79\n\n0.8\n\n0.6\n\n\u03b3 = \u03b3\n\n0.4\n\ncompared to the case of \u03b3 = \u03b3, stability is \u201crecovered\u201d by a moderate under-prediction, i.e., \" !< \ud835\udefe < \u03b3 < \ud835\udefe\n\n0.2\n\n0.0\n\n0.0                0.2                0.4               0.6                0.8                1.0              \u03b3\n\n\u03b3 = 0.79\n\ncompared to the case of \u03b3 = \u03b3, stability is \u201ccracked\u2019\u2019 by a severe under-prediction, i.e., \ud835\udefe < \"\n\nFigure 8: Stable and unstable regions w.r.t. combinations of \ud835\udefe and \ud835\udefe\u02c6 for the CH-NTP dynamic with |\ud835\udc3e| = 2, \ud835\udc5d0 = 0.4, \ud835\udc5d1 = 0.6.# 5.2.2 On the boundary of the feasible route flow set\n\nProposition 3 becomes invalid when the projected flow pattern is on the boundary of the feasible set. Under this circumstance, the general analytical results are invalid, and we have to resort to Fact 1 for numerically checking the stability. To test this fact, we set |\ud835\udc3e| = 2, \u02c6 = \ud835\udefc = 0.5, \ud835\udefc \u03b3\u02c6 = \u03b3 \u2208 {0.81, 0.82}, \ud835\udc5d0 = 0.4, \ud835\udc5d1 = 0.6 and the initial route flow pattern as \ud835\udc99\ud835\udc58=0,(\ud835\udc61)=0 = (0, 7.999, 10, 18, 9.98, 10.01, 8.01, 8.01)\ud835\udc47 and \ud835\udc99\ud835\udc58=1,(\ud835\udc61)=0 = (20, 11.99, 15, 7, 14.98, 15.01, 12.01, 12.01)\ud835\udc47, such that \ud835\udc3d\ud835\udc430,0 in Fact 1 is evaluated on the boundary. From Fact 1, we have that the maximum moduli of \ud835\udc3d\ud835\udc43 of |\ud835\udc3e| = 2 for \ud835\udefe = 0.81 and 0.82 are 1 and 1.013, respectively. It is expected that the former is stable while the latter is unstable, as shown in Figure 9. The case of |\ud835\udc3e| = 3 is also consistent with the numerical result of Fact 1, which is omitted here for simplicity.\n\n26# 13\n\n\u02c6\u03b3 = \u03b3 = 0.81, Step-0\n\n\u02c6\u03b3 = \u03b3 = 0.81, Step-1 flow of route 3 \u02c6\u03b3 = \u03b3 = 0.82, Step-0\n\n\u02c6\u03b3 = \u03b3 = 0.82, Step-1# 10\n\n0 50 100 150 200 250 300\n\nday\n\nFigure 9: Validation of Fact 1 for |\ud835\udc3e| = 2 when any projected flow is on the boundary of the feasible set.# 6 CH-Logit Dynamic\n\nIf the day-to-day operator \ud835\udc3b[\u00b7] is replaced by a \u201cstochastic\u201d Logit dynamic (Fudenberg and Levine, 1998; Sandholm, 2010; Xiao et al., 2019), we have the following CH-Logit dynamic:\n\n\ud835\udc99\ud835\udc58,(\ud835\udc61+1) \u2212 \ud835\udc99\ud835\udc58,(\ud835\udc61) = \ud835\udefc\ud835\udc5d\ud835\udc58 \u03a6\ud835\udf03h\ud835\udc84\ud835\udf45\ud835\udc58,(\ud835\udc61+1)i \u2212 \ud835\udc99\ud835\udc58,(\ud835\udc61), (25)\n\nwhere \u03a6\ud835\udf03[\ud835\udc9b] \u2261 \ud835\udc51\ud835\udc64 \u03a6\ud835\udf03[\ud835\udc9b\ud835\udc64 ], \ud835\udc64 \u2208 \ud835\udc4a \ud835\udc47 , \u03a6\ud835\udc64 \ud835\udc64 and \ud835\udf03 is a dispersal parameter capturing travelers\u2019 perception errors. For each \ud835\udc64 \u2208 \ud835\udc4a, as \ud835\udf03 \u2192 0, choices become equiprobable among all the routes in \ud835\udc45\ud835\udc64 and as \ud835\udf03 \u2192 \u221e, choices become extremely concentrated on the least cost route of \ud835\udc45\ud835\udc64. Note that for the Logit dynamic, the flow variable (i.e., the first variable) in \ud835\udc9a[\u00b7] of (1) is not explicitly required.\n\nBased on the CH modeling idea in Section 2.3, the predicted flow pattern of each class \ud835\udc58 \u2208 {0, 1, 2} is given by:\n\n\ud835\udf03[\ud835\udc9b\ud835\udc64 ] \u2261 (\ud835\udc51\ud835\udc64 \ud835\udf11\ud835\udc5f\ud835\udc64 (\ud835\udc9b\ud835\udc64 ), \ud835\udc5f \u2208 \ud835\udc45\ud835\udc64 )\ud835\udc47 , \ud835\udf11\ud835\udc5f\ud835\udc64 (\ud835\udc9b\ud835\udc64 ) =\n\n\ud835\udf450,(\ud835\udc61+1) = \u02dc\ud835\udc99(\ud835\udc61), (26)\n\n\ud835\udf451,(\ud835\udc61+1) = \u02c6\u03a6 \ud835\udf03\ud835\udefc \u02c6h\ud835\udc84\ud835\udf450,(\ud835\udc61+1)i + (1 \u2212 \u02c6) \u02dc\ud835\udefc \ud835\udc99(\ud835\udc61), (27)\n\n\ud835\udf452,(\ud835\udc61+1) = \u02c6\ud835\udc5e20\u03a6 \ud835\udf03\u02c6h\ud835\udc84\ud835\udf450,(\ud835\udc61+1)i\ud835\udefc + \u02c6\ud835\udc5e2\ud835\udefc 1\u03a6 \ud835\udf03\u02c6h\ud835\udc84\ud835\udf451,(\ud835\udc61+1)i + (1 \u2212 \u02c6) \u02dc\ud835\udefc \ud835\udc99(\ud835\udc61). (28)\n\nA significantly large \ud835\udf03\u02c6 can be interpreted as a scenario where higher-step travelers anticipate that lower-step travelers will strongly favor yesterday\u2019s shortest path. This situation closely# 6.1 Mixed prediction-based stochastic equilibria\n\nSimilar to the case of the CH-NTP dynamic, we term fixed points of the CH-Logit dynamic (25)-(28) as mixed prediction-based stochastic equilibria (MPSE) and describe them in the following proposition.# Proposition 5.\n\nWhen the route cost function \ud835\udc84(\ud835\udc99) is continuous, the dynamical system (25)-(28) admits at least one MPSE (i.e., one fixed point). Moreover, a vector \ud835\udc99\u25e6 \u2261 (\ud835\udc99\ud835\udc58,\u25e6 , \ud835\udc58 = 0, 1, 2)\ud835\udc47 is an MPSE if \u2200\ud835\udc5f \u2208 \ud835\udc45\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a , \ud835\udc58 \u2208 \ud835\udc3e,\n\nexp\u2212\ud835\udf03\ud835\udf0b\ud835\udc5f\ud835\udc64 ( \u02dc\u25e6)\ud835\udc58    \ud835\udc99\n\n\ud835\udc65\ud835\udc5f\ud835\udc64 \ud835\udc58,\u25e6= \ud835\udc5d\ud835\udc58 \ud835\udc51\ud835\udc64\u00cd\ud835\udc60\u2208\ud835\udc45\ud835\udc64 exp\u2212\ud835\udf03\ud835\udf0b\ud835\udc60\ud835\udc64 ( \u02dc\ud835\udc58                     \ud835\udc99\u25e6),\n\nwhere \ud835\udf45\ud835\udc58,\u25e6 \u2261 (\ud835\udf0b\ud835\udc60\ud835\udc64 , \ud835\udc60 \u2208 \ud835\udc45\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a)\ud835\udc47 is a function of the aggregate flow pattern \u02dc\ud835\udc58 \ud835\udc99\u25e6 = \u00cd\ud835\udc58 \ud835\udc99\ud835\udc58,\u25e6, dictated by (26)-(28).# Proof.\n\nThe existence of a fixed point is guaranteed by the continuities of the closed-form Logit operator and the route cost function. Setting the RHS of (25) to be 0 and rearranging the results yields (29). \u25a1\n\nWe now give the definition of a |\ud835\udc3e|-class SUE in Definition 2 and show that it is one of the MPSE in Proposition 6 when \ud835\udf03\u02c6 = \ud835\udf03.# Definition 2.\n\nA route flow pattern \ud835\udc99\u2605 \u2261 (\ud835\udc99\ud835\udc58,\u2605, \ud835\udc99\ud835\udc58,\u2605 \u2208 \u03a9\ud835\udc5d\ud835\udc58 , \ud835\udc58 \u2208 \ud835\udc3e)\ud835\udc47 is said to be a |\ud835\udc3e|-class SUE parameterized by \ud835\udf03 if \u2200\ud835\udc5f \u2208 \ud835\udc45\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a , \ud835\udc58 \u2208 \ud835\udc3e,\n\n\ud835\udc58,\u2605= \ud835\udc5d\ud835\udc58 \ud835\udc51\ud835\udc64\u00cd\ud835\udc60\u2208\ud835\udc45\ud835\udc64 exp\u2212\ud835\udf03\ud835\udc50\ud835\udc60\ud835\udc64 ( \u02dcexp\u2212\ud835\udf03\ud835\udc50\ud835\udc5f\ud835\udc64 ( \u02dc\u2605)\ud835\udc58  \ud835\udc99\n\n\ud835\udc65\ud835\udc5f\ud835\udc64                                                            \ud835\udc58  \ud835\udc99\u2605),\n\nwhere \u02dc\ud835\udc99\u2605 = \u00cd\ud835\udc58 \ud835\udc99\ud835\udc58,\u2605.# Proposition 6.\n\nWhen \ud835\udf03\u02c6 = \ud835\udf03, a |\ud835\udc3e|(= 3)-class SUE in Definition 2 parameterized by \ud835\udf03 is an MPSE (i.e., a fixed point) of the dynamical system (25)-(28), but not vice versa.# Proof.\n\nAt the SUE, first note from (26) that \ud835\udf450,\u2605 = \u02dc\ud835\udc99\u2605. Thus, for each \ud835\udc5f \u2208 \ud835\udc45\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a, a 1-step# Traveler\u2019s Predicted Flow\n\nThe traveler\u2019s predicted flow (27) is:\n\n1,\u2605 = \u02c6\ud835\udc5e1 0\ud835\udc51\ud835\udc64\u00cd\ud835\udc60\u2208\ud835\udc45\ud835\udc64 exp\u2212\ud835\udf03\ud835\udc50\ud835\udc60\ud835\udc64 ( \u02dcexp\u2212\ud835\udf03\ud835\udc50\ud835\udc5f\ud835\udc64 ( \u02dc\u2605)\ud835\udc58\ud835\udc99 \ud835\udc99\u2605) + (1 \u2212 \u02c6)\ud835\udc5e1 \ud835\udc65\u26050\u02dc\ud835\udc5f\ud835\udc64 = \u02c6 \u02dc\ud835\udc5f\ud835\udc64 + (1 \u2212 \u02c6) \u02dc\ud835\udc5f\ud835\udc64 = \u02dc\ud835\udc5f\ud835\udc64 .\ud835\udefc\ud835\udc65\u2605 \ud835\udc65\u2605 \ud835\udc65\u2605\n\n\ud835\udf0b\ud835\udc5f\ud835\udc64 \ud835\udefc \ud835\udc58 \ud835\udefc \ud835\udefc (31)\n\nWhere the second equality is due to Definition 2. Accordingly, for each \ud835\udc5f \u2208 \ud835\udc45\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a, a 2-step traveler\u2019s predicted flow (28) at the SUE is:\n\nexp(\u2212\ud835\udf03\ud835\udc50\ud835\udc5f\ud835\udc64 ( \u02dc\u2605))\ud835\udc58 \ud835\udc99\n\n\ud835\udf0b\ud835\udc5f\ud835\udc642,\u2605 = \u02c6(\ud835\udc5e2 \ud835\udefc 0+ \ud835\udc5e21)\ud835\udc51\ud835\udc64\u00cd\ud835\udc60\u2208\ud835\udc45\ud835\udc64 exp(\u2212\ud835\udf03\ud835\udc50\ud835\udc60\ud835\udc64 ( \u02dc\u2605)) + (1 \u2212 \u02c6) \u02dc\ud835\udc5f\ud835\udc64 = \u02dc\ud835\udc5f\ud835\udc64 .\ud835\udc58 \ud835\udefc \ud835\udc65\u2605 \ud835\udc65\u2605 (32)\n\nSubstituting the above predicted costs, \u02dc\ud835\udc99\u2605, into (25) and by Definition 2, (25) becomes 0. Hence, the SUE is an MPSE.\n\nSimilar to the CH-NTP dynamic, there exist some MPSE that are not SUE when |\ud835\udc3e| \u2265 2. We can easily construct a counter-example demonstrating their existence (similar to Figure 5a) to negate the necessity. This example is omitted to conserve space. \u25a1# 6.2 Local Stability\n\nDue to the existence of multiple equilibria, this section investigates the CH-Logit dynamic\u2019s local stability. We first give the Jacobian of the CH-Logit dynamic at the MPSE and then analyze the stability near the SUE.# 6.2.1 Jacobian Matrix\n\nLemma 4. The Jacobian matrix of the Logit operator parameterized by \ud835\udf03 evaluated at a cost vector \ud835\udc84, \u03a5\ud835\udf03[\ud835\udc84], is a block diagonal matrix:\n\n|\u03a5\ud835\udc64=1[\ud835\udc84\ud835\udc64=1]|\ud835\udf03|0|\n|---|---|---|\n|. . .| | |\n|\u03a5\ud835\udf03=|\ud835\udc4a | [\ud835\udc84\ud835\udc64=|\ud835\udc4a | ]|0|\ud835\udc64|\n\nWhere \u03a5\ud835\udc64 \ud835\udf03[\ud835\udc84\ud835\udc64 ] = \u2212\ud835\udf03\ud835\udc51\ud835\udc64Diag\u03a6\ud835\udc64 \ud835\udf03[\ud835\udc84\ud835\udc64 ] \u2212 \u03a6\ud835\udc64 \ud835\udf03[\ud835\udc84\ud835\udc64 ]\u03a6\ud835\udc64 \ud835\udf03[\ud835\udc84\ud835\udc64 ]\ud835\udc47. Moreover, given any \ud835\udc84 \u2261 (\ud835\udc84\ud835\udc64 , \ud835\udc64 \u2208 \ud835\udc4a)\ud835\udc47, each \u03a5\ud835\udc64 \ud835\udf03[\ud835\udc84\ud835\udc64 ] is negative semidefinite (NSD), and so is \u03a5\ud835\udf03[\ud835\udc84].\n\nProof. The result for a single \ud835\udc64 can be obtained from Gao and Pavel (2017) after flipping the positive sign due to the negative utility of travel cost. The independence of routes from different\n\n29OD pairs makes the non-diagonal blocks zero. \u03a5\ud835\udf03[\ud835\udc84] is NSD because a block diagonal matrix is NSD if and only if each diagonal block is NSD.\n\nWith Lemma 4, the Jacobian matrix of Logit dynamic is derived in the following fact.# Fact 2.\n\nThe Jacobian matrix for the CH-Logit dynamic with |\ud835\udc3e| = 3 is a 3-by-3 block matrix:\n\n|\ud835\udc3d\u03a60,0|\ud835\udc3d\u03a60,1|\ud835\udc3d\u03a60,2|\n|---|---|---|\n|\ud835\udc3d\u03a61,0|\ud835\udc3d\u03a61,1|\ud835\udc3d\u03a61,2|\n|\ud835\udc3d\u03a62,0|\ud835\udc3d\u03a62,1|\ud835\udc3d\u03a62,2|\n\nwhere each block \ud835\udc3d\u03a6\ud835\udc56,\ud835\udc57 is defined as follows.\n\n\ud835\udc3d\u03a60,0 = \ud835\udefc\ud835\udc5d 0\u03a5\ud835\udc58=0\ud835\udc370 + (1 \u2212 \ud835\udefc)\ud835\udc3c,\ud835\udf03\n\n\ud835\udc3d\u03a60,\ud835\udc57\u22600 = \ud835\udefc\ud835\udc5d 0\u03a5\ud835\udc58=0\ud835\udc370 ,\ud835\udf03\n\n\ud835\udc3d\u03a61,1 = \ud835\udefc\ud835\udc5d 1\u03a5\ud835\udc58=1\ud835\udc371\ud835\udefc\ud835\udf03 \u02c6\u03a5 \ud835\udc58=0\ud835\udc370 + (1 \u2212 \u02c6)\ud835\udc3c\ud835\udf03\u02c6 \ud835\udefc + (1 \u2212 \ud835\udefc)\ud835\udc3c,\n\n\ud835\udc3d\u03a61,\ud835\udc57\u22601 = \ud835\udefc\ud835\udc5d 1\u03a5\ud835\udc58=1\ud835\udc371\ud835\udefc\ud835\udf03 \u02c6\u03a5 \ud835\udc58=0\ud835\udc370 + (1 \u2212 \u02c6)\ud835\udc3c\ud835\udf03\u02c6 \ud835\udefc ,\n\n\ud835\udc3d\u03a62,2 = \ud835\udefc\ud835\udc5d 2\u03a5\ud835\udc58=2\ud835\udc372h\ud835\udefc\ud835\udf03 \u02c6\ud835\udc5e20\u03a5 \ud835\udc58=0\ud835\udc370 + \u02c6\ud835\udc5e2\ud835\udf03\u02c6 \ud835\udefc 1\u03a6 \ud835\udc58=1\ud835\udc371\ud835\udefc\ud835\udf03\u02c6 \u02c6\u03a5 \ud835\udc58=0\ud835\udc370 + (1 \u2212 \u02c6)\ud835\udc3c\ud835\udf03\u02c6 \ud835\udefc + (1 \u2212 \u02c6)\ud835\udc3c\ud835\udefc i + (1 \u2212 \ud835\udefc)\ud835\udc3c,\n\n\ud835\udc3d\u03a62,\ud835\udc57\u22602 = \ud835\udefc\ud835\udc5d 2\u03a5\ud835\udc58=2\ud835\udc372h\ud835\udefc\ud835\udf03 \u02c6\ud835\udc5e20\u03a5 \ud835\udc58=0\ud835\udc370 + \u02c6\ud835\udc5e2\ud835\udf03\u02c6 \ud835\udefc 1\u03a6 \ud835\udc58=1\ud835\udc371\ud835\udefc\ud835\udf03\u02c6 \u02c6\u03a5 \ud835\udc58=0\ud835\udc370 + (1 \u2212 \u02c6)\ud835\udc3c\ud835\udf03\u02c6 \ud835\udefc + (1 \u2212 \u02c6)\ud835\udc3c\ud835\udefc ,\n\nwhere \ud835\udc37\ud835\udc58 represents the Jacobian of the route cost functions evaluated at predicted flow \ud835\udf45\ud835\udc58,(\ud835\udc61+1) and \u03a5\ud835\udc58\ud835\udf03 \ud835\udf45\ud835\udc58,(\ud835\udc61+1). the Jacobian of Logit operator with \ud835\udf03 evaluated at \ud835\udc84\n\nProof. The derivation can be found in Appendix G.# 6.2.2 Stability near the SUE\n\nIf \ud835\udf03 \u2260 \ud835\udf03\u02c6, the block submatrices in (34) would not commute with each other, and thus |\ud835\udf06\ud835\udc3c \u2212 \ud835\udc3d\u03a6| entails directly calculating the inverse matrix of either one of these submatrices. However, none of these inverse submatrices has a closed-form. Hence, we analytically examine the case of \ud835\udf03 = \ud835\udf03\u02c6 only.# Proposition 7.\n\nDenote \ud835\udf0c\ud835\udc56 as the \ud835\udc56-th eigenvalue of \u03a5\ud835\udf03 \ud835\udc84  \u02dc\ud835\udc99\u2605 \ud835\udc37  \u02dc\ud835\udc99\u2605 at the SUE under Assumption 1 and \ud835\udf0c\ud835\udc56 \u2264 0, \u2200\ud835\udc56. Define function \ud835\udf13(\ud835\udf0c; \ud835\udefc, \u02c6, \ud835\udc5d0 , \ud835\udc5d1 , \ud835\udc5d2) \u2261 \ud835\udefc \u02c6\ud835\udefc2 \ud835\udc5d2 0+\ud835\udc5d1 \ud835\udf0c3 + (\ud835\udefc \u02c6 \u2212 \ud835\udefc \u02c6\ud835\udc5d0 \u2212\ud835\udc5d1 \ud835\udefc \u02c6\ud835\udefc2 \ud835\udc5d2 0+\ud835\udc5d\ud835\udc5d\ud835\udc5d11 )\ud835\udf0c\ud835\udc56 2+ (\ud835\udefc \u02c6\ud835\udc5d0 \u2212 \ud835\udefc \u02c6 + \ud835\udefc)\ud835\udf0c\ud835\udc56 + (1 \u2212 \ud835\udefc) where \ud835\udc5d0 + \ud835\udc5d1 + \ud835\udc5d2 = 1. When \ud835\udf03 = \ud835\udf03 and |\ud835\udc3e| \u2264 3, the\ud835\udefc \u02c6\n\n30# Stability Analysis of SUE\n\nSUE is locally asymptotically stable when \u22121 &lt; \ud835\udf13(\ud835\udf0c; \ud835\udefc, \ud835\udefc\ud835\udc56, \u02c6, \ud835\udc5d0, \ud835\udc5d1, \ud835\udc5d2) &lt; 1, \u2200\ud835\udc56. If |\ud835\udc3e| = 2, then \ud835\udc5d2 = 0.\n\nIf |\ud835\udc3e| = 1, then \ud835\udc5d1 = \ud835\udc5d2 = 0, \ud835\udc5d0 = 1 and \u02c6 = \ud835\udefc.\ud835\udefc# Remark 4\n\nRecall that the stability of the CH-NTP dynamic does not depend on the distribution of strategic thinking levels (i.e., \ud835\udc5d0, \ud835\udc5d1, and \ud835\udc5d2) when the projection operator always yields positive flows (see Proposition 3). In contrast, the proportions would affect the stability of the CH-Logit dynamic even though the Logit operator always generates positive flows.\n\n|\ud835\udf13(\ud835\udf0c; \ud835\udefc, \ud835\udefc, \ud835\udc5d\", \ud835\udc5d#, \ud835\udc5d$)!|\ud835\udf13(\ud835\udf0c; \ud835\udefc, \ud835\udefc, \ud835\udc5d\", \ud835\udc5d#, \ud835\udc5d$)!|\n|---|---|\n|(1,1)|(1,1)|\n|1 \u2212 \ud835\udefc|1 \u2212 \ud835\udefc|\n|\ud835\udf0c!|\ud835\udf0c!|\n|-1|-1|\n|case (i)|case (ii)|\n\n|\ud835\udf13(\ud835\udf0c; \ud835\udefc, \ud835\udefc, \ud835\udc5d\", \ud835\udc5d#, \ud835\udc5d$)!|\ud835\udf13(\ud835\udf0c; \ud835\udefc, \ud835\udefc, \ud835\udc5d\", \ud835\udc5d#, \ud835\udc5d$)!|\n|---|---|\n|(1,1)|(1,1)|\n|1 \u2212 \ud835\udefc|1 \u2212 \ud835\udefc|\n|\ud835\udf0c!|\ud835\udf0c!|\n|-1|-1|\n|case (iii)|case (iv)|\n\nstable region\n\nunstable region# Figure 10\n\nSketches of stable and unstable regions of the CH-Logit dynamic under four possible scenarios of \ud835\udf13(\ud835\udf0c; \ud835\udefc, \u02c6, \ud835\udc5d0, \ud835\udc5d1, \ud835\udc5d2).\n\nScrutinization of the cubic function \ud835\udf13(\ud835\udf0c; \ud835\udefc, \u02c6, \ud835\udc5d0, \ud835\udc5d1, \ud835\udc5d2) shows that it crosses points (1, 1) and (0, 1 \u2212 \ud835\udefc) and exhibits different shapes depending on \ud835\udefc, \ud835\udefc\u02c6, \ud835\udc5d0, \ud835\udc5d1 and \ud835\udc5d2. Figure 10 depicts four possible scenarios when determining the stable region. Here the stable/unstable region refers to the region of the five parameters that make the SUE stable/unstable (i.e., whether or not \ud835\udf13(\ud835\udf0c\ud835\udc56; \ud835\udefc, \u02c6, \ud835\udc5d0, \ud835\udc5d1, \ud835\udc5d2) \u2208 (\u22121, 1), \u2200\ud835\udf0c\ud835\udc56). They are classified based on whether the cubic function has one or three real roots (which may not be distinct) at 1 or \u22121. The analytical results of the# 6.2.2.1 Perfect prediction\n\nEven under \u02c6 = \ud835\udefc, the stability condition of the CH-Logit dynamic given by Proposition 7 is much more complex than the CH-NTP dynamic. We explore the stability numerically. Given an eigenvalue \ud835\udf0c\ud835\udc56, we enumerate all the possible combinations of \ud835\udc5d0 and \ud835\udc5d1 in a resolution of 0.001 (\ud835\udc5d2 is not required as \ud835\udc5d2 = 1 \u2212 \ud835\udc5d0 \u2212 \ud835\udc5d1) with \ud835\udefc \u2208 {0.2, 0.5, 0.8} and check whether the stability condition is satisfied as per Proposition 7. Stable and unstable regions under different values of \ud835\udf0c\ud835\udc56 are depicted in Figure 11. Some observations can be made on these plots:\n\n|\ud835\udc5d!|\ud835\udc5d!|\n|---|---|\n|1.0|1.0|\n|unstable|unstable|\n|stable|stable|\n|0.8|0.8|\n|\ud835\udefc = 0.2|\ud835\udefc = 0.5|\n|0.6|0.6|\n|\ud835\udefc = 0.8| |\n|0.4|0.4|\n|0.2|0.2|\n|0.0|0.0|\n|0.0|0.0|\n|0.2|0.2|\n|0.4|0.4|\n|0.6|0.6|\n|0.8|0.8|\n|1.0|1.0|\n\n(a) \ud835\udf0c\ud835\udc56 \u2265 \u22121\n\n(b) \ud835\udf0c\ud835\udc56 = \u22123\n\n|\ud835\udc5d!|\ud835\udc5d!|\n|---|---|\n|0.8|0.8|\n|unstable|unstable|\n|stable|stable|\n|0.6|0.6|\n|\ud835\udefc = 0.2|\ud835\udefc = 0.2|\n|\ud835\udefc = 0.5|\ud835\udefc = 0.5|\n|\ud835\udefc = 0.8|\ud835\udefc = 0.8|\n|0.4|0.4|\n|0.2|0.2|\n|0.0|0.0|\n\n(c) \ud835\udf0c\ud835\udc56 = \u22126\n\n(d) \ud835\udf0c\ud835\udc56 = \u22129\n\nFigure 11: Stable and unstable regions of the CH-Logit dynamic with \u02c6 = \ud835\udefc w.r.t. different eigenvalues \ud835\udf0c\ud835\udc56.\n\n(i) When \ud835\udf0c\ud835\udc56 \u2265 \u22121, the system is always stable irrespective of any parameter. To explain, note\n\n32# 6.2.2.2 Imperfect prediction when |\ud835\udc3e| = 2\n\nThe cubic function \ud835\udf13(\ud835\udf0c; \ud835\udefc, \u02c6, \ud835\udc5d0 , \ud835\udc5d1 , \ud835\udc5d2) in Proposition 7 reduces to a quadratic or linear function \ud835\udefc when |\ud835\udc3e| = 2 or 1, respectively. In these two cases, the stable region in terms of \ud835\udf0c\ud835\udc56 can be succinctly described by the key parameters. Proposition 8 gives the results, followed by the discussions on how over- and under-predictions will affect the stability.# Proposition 8.\n\nThe CH-Logit dynamic with |\ud835\udc3e| = 1 is locally asymptotically stable at the SUE under Assumption 1 when \ud835\udf0c\ud835\udc56 > \ud835\udefc\u22122, \u2200\ud835\udc56. When \ud835\udf03 = \ud835\udf03, for |\ud835\udc3e| = 2, the SUE under Assumption 1 is locally asymptotically stable when one of the following two conditions holds for all \ud835\udf0c:\n\n1. 1\u2212\ud835\udefc+ \ud835\udefc(1\u2212 \u02c6+ \u02c6\ud835\udc5d02\ud835\udc56)\ud835\udefc > \u22121 and \ud835\udf0c\ud835\udc56 > (\ud835\udc5d011) \u02c6 \u2212 \ud835\udefc;\n2. 1 \u2212 \ud835\udefc + \ud835\udefc(1\u2212 \u02c6+ \u02c6\ud835\udc5d0)24 \u02c6(\ud835\udc5d\ud835\udefc\ud835\udefc0\u22121)\ud835\udefc 1(\u2212r \ud835\udefc(1+ \u02c6\u2212 \u02c6\ud835\udc5d0)+8 \u02c6(\ud835\udc5d0\u22121)+\ud835\udefc(\ud835\udc5d10\u22121) +1), and \ud835\udc531(\ud835\udefc, \u02c6, \ud835\udc5d0) \u2261:< \u22121 and {\ud835\udf0c\ud835\udc56 : (\ud835\udc5d\u22121) \u02c6 \u2264 \ud835\udf0c\ud835\udc56 \u2264 \ud835\udc530(\ud835\udefc, \ud835\udefc, \ud835\udc5d0)} \u222a {\ud835\udf0c\ud835\udc56 \ud835\udefc \ud835\udefc2(\ud835\udc5d0\u22121)\ud835\udefc22 \ud835\udc531(\ud835\udefc, \u02c6, \ud835\udc5d0) \u2264 \ud835\udf0c\ud835\udc56 < 0}.\n\nHere \ud835\udc530(\ud835\udefc, \u02c6, \ud835\udc5d0) \u2261 2 \ud835\udefc \ud835\udefc \ud835\udefc \u02c6\ud835\udefc \u02c6 \ud835\udefc 1(r \ud835\udefc(1+ \u02c6\u2212 \u02c6\ud835\udc5d0)+8 \u02c6(\ud835\udc5d0\u22121)+ 2 \ud835\udefc\ud835\udefc \u02c6\ud835\udefc2(\ud835\udc5d0\u22121)2 \ud835\udefc 2 \ud835\udefc \ud835\udefc\u02c6(\ud835\udc5d10\u22121) + 1).# Proof.\n\nSee Appendix I.\n\nProposition 8 reveals insights into how the strategic-thinking behavior and associated over- and under-predictions affect the stability by comparing the stable region size between CH-Logit models with |\ud835\udc3e| = 1 and 2. The discussion is as follows.# Over-prediction ( \u02c6 \u2265 \ud835\udefc)\ud835\udefc\n\nFor case (i) of |\ud835\udc3e| = 2 in Proposition 8, solving 1 \u2212 \ud835\udefc + \ud835\udefc(1\u2212 \u02c6+ \u02c6\ud835\udc5d0)24 \u02c6(\ud835\udc5d\ud835\udefc\ud835\udefc0\u22121)\ud835\udefc > \u22121 with \ud835\udefc\u02c6 \u2265 \ud835\udefc yields\n\n\ud835\udc5d0 \u2264 \u210e(\ud835\udefc, \u02c6) \u2261 2\u221a2q\u2212 2 \u02c6\ud835\udefc\ud835\udefc\u221222 + \ud835\udefc \u02c6+\ud835\udefc\u22124. Noting that \u210e(\ud835\udefc, \u02c6) is monotonically increasing in \u02c6 and \ud835\udefc \ud835\udefc\ud835\udefc \u02c6\ud835\udefc \ud835\udefc0\u210e(\ud835\udefc, \u02c6 = \ud835\udefc) \u2265 2(\u221a2\u22121), we obtain \u210e(\ud835\udefc, \u02c6) \u2265 2(\u221a2\u22121) \u2248 0.828. The stability condition for |\ud835\udc3e| = 1p\ud835\udefc\ud835\udefc requires all the \ud835\udf0c\ud835\udc56 > \ud835\udefc\u22122\ud835\udefc while the condition for |\ud835\udc3e| = 2 needs all the \ud835\udf0c\ud835\udc56 > (\ud835\udc5d011) \u02c6 \u2212 \ud835\udefc. Comparing these two thresholds yields:\n\n(\ud835\udc5d011) \u02c6 \u2212 \ud835\udefc\u2212\ud835\udefc\u22122 \ud835\udefc < 0 when \ud835\udc5d0 > \ud835\udc54(\ud835\udefc, \u02c6) \u2261 \ud835\udefc \u02c6+\ud835\udefc\u22122 \u02c6\ud835\udefc \ud835\udefc \u02c6\u22122 \u02c6 and (\ud835\udc5d011) \u02c6 \u2212 \ud835\udefc\u2212\ud835\udefc\u22122 \ud835\udefc > 0 when \ud835\udc5d0 < \ud835\udc54(\ud835\udefc, \u02c6). Hence, the stable region is expanded when \ud835\udc54(\ud835\udefc, \u02c6) < \ud835\udc5d0 < \u210e(\ud835\udefc, \u02c6), and shrunk when 0 < \ud835\udc5d0 < \ud835\udc54(\ud835\udefc, \u02c6).\n\nFor case (ii) of |\ud835\udc3e| = 2, \ud835\udc5d0 > \u210e(\ud835\udefc, \u02c6). Combined size of the two separated stable regions over \ud835\udf0c\ud835\udc56 is \ud835\udc530(\ud835\udefc, \u02c6, \ud835\udc5d0) \u2212 \ud835\udefc (\ud835\udc5d\u221211) \u02c6 + 0 \u2212 \ud835\udc531(\ud835\udefc, \u02c6, \ud835\udc5d0) = \ud835\udefc \u02c6(1\u2212\ud835\udc5d0) \u00b71 less than 2\u2212\ud835\udefc\ud835\udefc when \ud835\udc5d0 > \u210e(\ud835\udefc, \u02c6). Hence, the total size of the two separated stable regions is expanded.\n\nFigure 12 summarizes the above discussion by depicting how the stable region alters when |\ud835\udc3e| changes from 1 to 2 under different \ud835\udc5d0, \ud835\udefc, \ud835\udefc. Note how the parametric space of \ud835\udc5d0 and \ud835\udefc\u02c6 with a shrinking stable region enlarges as over-prediction becomes more severe (i.e., \ud835\udefc\ud835\udefc\u02c6 becomes larger).\n\n|1.0| | | | |\n|---|---|---|---|---|\n|0.8| | | |2|\n|0.6| | | |h(\u03b1,\u03b1)|\n|0.4| | | |g(\u03b1,\u03b1)|\n|0.2|\u02c6a = 1|a|\u02c6a = 1.1|a|\n|0.0| | | |\u02c6a = 1.2|\n\nFigure 12: Sketch of how the stable region alters when |\ud835\udc3e| changes from 1 to 2 under different \ud835\udc5d0, \ud835\udefc, \ud835\udefc, and \ud835\udefc\u02c6 \u2265 \ud835\udefc.\n\n34# Scenario (1): \ud835\udefc > 3 \u2212 2\u221a2\u02c6\n\nConsider first case (i) of |\ud835\udc3e| = 2 in Proposition 8 where \ud835\udc53 \u2217(\ud835\udefc, \u02c6, \ud835\udc5d0) > \u22121 and \ud835\udc5d0 < \u210e(\ud835\udefc, \u02c6). The discussion is divided into two sub-cases:\n\n- when \u02c6 \u2265 \u2212\ud835\udefc, solving (\ud835\udc5d011) \u02c6 < \ud835\udefc\u22122\u2212 \ud835\udefc yields \ud835\udc5d0 > \ud835\udc54(\ud835\udefc, \u02c6) and solving (\ud835\udc5d011) \u02c6 > \ud835\udefc\u22122\u2212 \ud835\udefc obtains \ud835\udc5d0 < \ud835\udc54(\ud835\udefc, \u02c6). Hence, the region is expanded when \ud835\udc54(\ud835\udefc, \u02c6) < \ud835\udc5d0 < \u210e(\ud835\udefc, \u02c6) and shrunk when \ud835\udc5d0 < \ud835\udc54(\ud835\udefc, \u02c6). It can be verified that \ud835\udc54(\ud835\udefc, \u02c6) is monotonically increasing in \u02c6 and that \u210e(\ud835\udefc, \u02c6) \u2212 \ud835\udc54(\ud835\udefc, \u02c6) is monotonically decreasing in \u02c6 when 0 < \ud835\udefc, \u02c6 < 1. Hence, the parametric space of \ud835\udc5d0, \ud835\udefc, and \u02c6 (< \ud835\udefc) rendering the stable region shrunk is smaller than the perfect prediction case, while the parametric space leading to an expanded stable region is larger than the perfect prediction case;\n- when \u02c6\ud835\udefc < 2\u2212\ud835\udefc, (\ud835\udc5d011) \u02c6 < \ud835\udefc\u22122\u2212 \ud835\udefc always holds and thus the expanded region is simply \ud835\udc5d0 < \u210e(\ud835\udefc, \u02c6). When prediction behaviors are considered, no stable region of |\ud835\udc3e| = 1 is shrunk.\n\nFor case (ii) where \ud835\udc53 \u2217(\ud835\udefc, \u02c6, \ud835\udc5d0) < \u22121, \ud835\udc5d0 > \u210e(\ud835\udefc, \u02c6), it is verified that \ud835\udc530(\ud835\udefc, \u02c6, \ud835\udc5d0) \u2212(\ud835\udc5d\u22121) \u02c6 \ud835\udefc \ud835\udefc \ud835\udefc 1 \ud835\udefc+ 0 \u2212 \ud835\udc531(\ud835\udefc, \u02c6, \ud835\udc5d0) > 2\u2212\ud835\udefc. Hence, when \ud835\udc5d0 > \u210e(\ud835\udefc, \u02c6), the stable region is separated and the combined stable region size increased. Since \u210e(\ud835\udefc, \u02c6) is monotonically increasing in \u02c6, the parametric space rendering the total size of the stable region greater is larger than the perfect prediction case.\n\nThe above discussion is summarized in Figure 13. Note how the parametric space of \ud835\udc5d0 and \ud835\udefc with an expanding stable region enlarges as a \u201cmoderately\u201d small \u02c6 \u2208 (3 \u2212 2\u221a2, \ud835\udefc) increases. In other words, a mild under-prediction helps stabilize the CH-Logit dynamic, which is very similar to the finding of the CH-NTP dynamic.# Scenario (2): \u02c6\ud835\udefc < 3 \u2212 2\u221a2\n\nThis scenario depends on the comparison between 8 \u02c6\ud835\udefc2 and \ud835\udefc. When 8 \u02c6\ud835\udefc2 > \ud835\udefc, discussion is the same as the case of \ud835\udefc > 3 \u2212 2\u221a2; i.e., it helps stabilize the dynamic. When 8 \u02c6\ud835\udefc2 < \ud835\udefc, \ud835\udc53 \u2217(\ud835\udefc, \u02c6, \ud835\udc5d0) < \u22121 always holds and thus the stable region is separated, while the combined size is increased. The latter case is similar to the CH-NTP dynamic with \u02c6\ud835\udefe < 2 \ud835\udefe.# 1.0\n\nthe stable region is separated and the combined size is increased# 0.0\n\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\na\n\nFigure 13: Sketch of how the stable region changes when |\ud835\udc3e| changes from 1 to 2 under different \ud835\udc5d0, \ud835\udefc,\u02c6 and 3 \u2212 2\u221a2 < \u02c6 \u2264 \ud835\udefc.# Summary\n\nThe CH-Logit dynamic bears similarity to the CH-NTP dynamic on how under- and over-predictions affect the stability near the UE. Over-predictions ( \u02c6\ud835\udefc > \ud835\udefc for CH-Logit dynamic and \u02c6\ud835\udefe > \ud835\udefe for CH-NTP dynamic) render the system less stable while mild under-predictions (3 \u2212 2\u221a2 < \u02c6\ud835\udefc < \u02c6 for CH-Logit dynamic and 2 < \u02c6\ud835\udefc \ud835\udefe \ud835\udefe < \ud835\udefe for CH-NTP dynamic) help stabilize the small perturbations near the UE. Severe under-predictions (\u02c6 \ud835\udefc < 3 \u2212 2\u221a2 and( \u02c6+1)\ud835\udefc8 \u02c6\ud835\udefc2 < \ud835\udefc for CH-Logit dynamic and \u02c6\ud835\udefe < 2 \ud835\udefe for CH-NTP dynamic) separate the stable region into two pieces while the total size of the region is increased.\n\nWhen more travelers tend to strategically predict others\u2019 behaviors (i.e., a larger \ud835\udc5d1), the system becomes less stable and behaves like the over-prediction case.\n\nNumerical experiments pertaining to the CH-Logit dynamic confirm the above theoretical results. They are omitted here for simplicity since the results and findings (such as the multi-equilibria phenomenon) are similar to the case of the CH-NTP dynamic, except that the DUE becomes the SUE.# 7 Conclusions and Future Research\n\nThis paper developed a new modeling framework of day-to-day network flow dynamics that incorporates hierarchical cognitive levels (Camerer et al., 2004) to better capture travelers\u2019\n\n36# Dynamic Re-Routing Behaviors\n\nThe classical NTP and Logit dynamics, on behalf of the DUE and SUE cases, respectively, were extended into the framework. Considering heterogeneity in travelers\u2019 strategic-reasoning levels, the proposed CH-NTP model significantly improved the goodness-of-fit for a recent virtual experiment, which traditional day-to-day models could not achieve. For both dynamics, stationary states and local stabilities near the user equilibria were theoretically analyzed and verified by numerical experiments.\n\nWe find that in addition to converging to the classical user equilibria (DUE/SUE), the proposed dynamics may also converge to other equilibria, depending on the initial conditions and parameters. For example, with a large \ud835\udefe, different initial conditions lead to different equilibria, while under a small \ud835\udefe, various initial points tend to evolve to the same DUE tardily (see again Figures 5a and b). Moreover, when higher-step travelers can accurately predict lower-step ones\u2019 switching propensities, thinking multiple steps ahead does not affect the local stability of the CH-NTP dynamic around the DUE, and the stability condition does not depend on the distribution of strategic-reasoning levels. In comparison, the stability condition for the CH-Logit dynamic largely depends on the proportion of heterogeneous travelers.\n\nFinally, we find that for both dynamics with |\ud835\udc3e| = 2: (i) when 1-step travelers over-predict 0-step ones\u2019 switching propensity, the stable region is shrunk, and when the over-prediction is severe (e.g., \ud835\udefe > \u00af for the CH-NTP dynamic), the UE is always unstable; (ii) when 1-step travelers moderately under-predict 0-step ones\u2019 re-routing tendency, the stable region is increased; and (iii) when the under-predictions are severe, the stable region is separated into two parts.\n\nDespite the fresh findings generated, our models still have limitations, which may direct future research. First, we select two widely-used day-to-day models from the literature as examples of the general modeling framework. Many more classical day-to-day models can be analyzed in the future (e.g., Smith, 1979b, 1983; Nagurney and Zhang, 1997; Jin, 2007; Xiao et al., 2019), including those that consider travelers\u2019 learning behavior (Horowitz, 1984; Cantarella and Cascetta, 1995; Bie and Lo, 2010). We surmise that findings on local stability still generally hold when hierarchical cognitive behaviors are incorporated into these models, provided that the dynamic itself exhibits similar properties (e.g., negative definiteness of the Jacobian).\n\nSecond, the virtual experiment data suggest that travelers\u2019 re-routing tendencies become weaker through repeated game play. Further research can use time-varying parameters to capture such a phenomenon better. Third, future research can investigate how different measures, such as congestion tolling (Tan et al., 2015; Guo et al., 2016; Liu et al., 2017; Han).\n\n37",
        "context_id": 11,
        "question": "Whose principle characterizes a steady-state User Equilibrium in the context of traffic networks?",
        "answer": [
            "Wardrop's"
        ],
        "context_length": 66469
    },
    {
        "context": "# Introduction\n\nA fundamental and ubiquitous visual task is searching the local environment for specific targets. Humans and other primates have foveated visual systems with high spatial resolution in the direction of gaze (in the fovea) and rapidly declining resolution away from the direction of gaze (in the periphery). Thus, visual search typically involves a series of fixations. During each fixation, the scene is covertly searched to detect the target and/or possible target locations. A sensible and common research strategy is to begin by studying the mechanisms of covert search and then generalize to search with eye movements (overt search). This study focuses on covert search.\n\nCarefully controlled studies of covert search typically present the stimuli briefly to prevent eye movements and with the potential target locations arrayed at a fixed distance from the fixation location to hold the target visibility at the different locations approximately constant1\u20135. In some studies, the task is simply to indicate whether the target is present or absent. In other studies, the target is always present, and the task is to indicate the location of the target. The covert search task in our study is more complex and realistic: identify whether the target is present or absent, and if present, indicate its location.\n\nUnder natural search conditions, the number of potential target locations often varies from one situation to the next and hence varying the number of potential locations is a key experimental manipulation. When there is just a single potential target location, the search task reduces to a# Search Performance and Bayesian Decision Theory\n\nSimple identification, discrimination, or detection task. In general, as the number of potential target locations increases, search accuracy and speed decrease. The major scientific questions are what stimulus and neural factors are responsible for the decreases and whether models that incorporate the relevant neural factors can quantitatively predict search performance.\n\nBayesian statistical decision theory provides a principled computational framework for addressing these questions. Specifically, measurements of detection performance for each potential target location, when the location is known, can be used to predict quantitatively the best possible (ideal) performance in the search task when the location of the target is unknown. These predictions provide the normative benchmark for evaluating the various potential stimulus and neural factors affecting search performance. For example, hypothesized factors that cause the Bayesian observer\u2019s performance to fall below the measured human performance can be confidently rejected.\n\nAs mentioned above, in many well-controlled covert-search studies the targets are presented briefly, and the potential target locations are at fixed retinal eccentricity. Studies show that when the task is to report whether a single target is present or absent, there is a fairly wide range of conditions where human search accuracy is consistent with an optimal decision rule given statistical independence at the potential target locations1,2,4,6. However, fixed eccentricity displays are not representative of natural search, where potential target locations are typically more uniformly distributed across the visual field. Also, a target\u2019s visibility typically varies somewhat around a circle at fixed retinal eccentricity7\u201310. Finally, many previous studies have not used backgrounds with dense random noise, which is more typical in natural conditions.\n\nTherefore, we designed covert search tasks in white noise backgrounds for target locations that cover the central 16 deg of the visual field. The stimuli were presented for 250 msec\u2014the typical duration of fixations during overt search11,12. Search performance was measured in human observers. To increase interpretability, we also directly measured the detectability of the target at each potential target location when the target location was known to the observer. Furthermore, we carefully interleaved the detection and search sessions to eliminate differences in practice effects for the two kinds of sessions.\n\nThe results are surprising. First, all four observers performed the search task slightly better than the Bayesian ideal searcher, given the measured detectability when the target locations were cued and the assumption of statistical independence of the responses at the different locations. Second, the Bayesian ideal searcher takes into account the prior probability of the target being present at each potential location (the \u201cprior map\u201d), as well as the detectability of the target at each potential target location (the \u201cd map\u201d). It seems implausible that during the experiment the observers could precisely learn the prior map and their own d map, and then optimally apply this information in making responses. Third, in the search task, all four observers showed a substantial loss of sensitivity in the fovea (\u201cfoveal neglect\u201d), a phenomenon we reported in a recent study of covert search in continuous noise backgrounds13.# Visual System Heuristics\n\nThe visual system does use a heuristic rule, can that rule be estimated from behavioral data even though many heuristics give near optimal overall performance? The answer appears to be at least a partial yes. The dashed curve in Figure 3a is, in fact, the falloff parameter (e2 = 7) that best fits the pattern of corrects and errors. The blue histograms in Figure 2 show the predictions for this falloff parameter.\n\nWe also tested the same heuristic rules for dmax varying from 3.0 to 9.0, while holding e2 at 7.0 (see Figure 3c). Figure 3d shows that again both heuristic decision rules achieve near optimal performance in all those conditions.\n\nUnder natural conditions, the properties of the background scene vary substantially over space and hence the d map is generally different with every new fixation. We simulated this situation by having a baseline d map with various e2 parameter values and then assumed that the random background properties cause the actual d value at each location to be scaled up or down by a random percentage that is normally distributed, with a standard deviation of 20%. Figure 3e shows a single example of a random d map, where the baseline e2 is 7.0 and the baseline dmax is 6.0. Figure 3f shows that the heuristic decision rules in Figures 3a achieve near optimal performance even when the d map varies randomly on each trial.\n\nSimilar results to those in Figure 3 hold for all combinations of e2 and dmax (Figure S7), and for prior probability maps (Figure S9). Indeed, very simple heuristics are often near optimal under a very wide range of natural conditions. Nevertheless, heuristic rules with extreme deviations from the actual d and prior-probability maps are detrimental to search performance. For example, if there are substantial search regions where the d or prior probability go to zero, then no weight should be assigned to those locations in the decision. Similarly, performance can suffer substantially if regions where the d and prior probability are non-zero are given no weight in the decision.\n\nNext, consider the effects of correlated neural noise. Besides statistically independent sources of noise at each potential target location, we assume the existence of noise with common sources that are added to the responses at all potential target locations. These common sources cause the total noise at the different potential target locations to be partially correlated. For simplicity, we assume that the independent and common noise sources are Gaussian and statistically independent of each other, with standard deviations of \u03c3 and 0, respectively. Thus, the total noise variance at each target location is \u03c32 + 02. In the detection task, the correlated noise component necessarily lowers the detectability. However, the common noise has little or no effect on the optimal decision rule in the search task. For example, for the flat heuristic rule, the effect of the correlated noise on d' is cancelled out by the max rule, because the correlated noise causes the same increase or decrease of responses equally at all potential target locations. Also, we note that the optimality of the max rule still holds even when the# Conclusion\n\nIn sum, the combination of the three factors described here provides a plausible, quantitatively accurate explanation of the seeming paradoxical detection and search results shown in Figures 1 and 2. The quantitative predictions of the best fitting heuristic searcher with these three factors are much more consistent with the human search behavior than the ideal searcher without these three factors.# a. Retinotopic map.\n\nThe flattened cortical sheet has a constant density of neurons. The grid of contours shows the retinal locations of the cortical neurons\u2019 receptive fields.# b. The estimated variation in attentional gain with retinal eccentricity (also shown by coloring of map in a).# Discussion\n\nCovert search and cued detection performance were measured for wavelet targets in Gaussian noise under carefully controlled conditions. The detectability (d) map measured in the cued detection task was used to predict covert search performance for the Bayes optimal decision rule, assuming statistically independent sensory responses from the potential target locations. We found that human performance slightly exceeded the predictions of the Bayes optimal decision rule, despite the complexity of the optimal rule, and despite the fact the humans showed a loss of sensitivity in the fovea (foveal neglect). We found that these seeming paradoxical results can be explained quantitatively by three facts: (1) very simple heuristic decision rules, together with local normalization, can achieve essentially optimal performance, (2) correlated neural noise causes the d values measured in the detection task to underestimate the effective d values in the search task, and (3) foveal neglect only reduces the effective d value at the central target location.\n\nAn obvious question is how general these findings are. We have also made some measurements of covert search for 7, 61 and 91 potential target locations (Figure S4, Table S1). The results are generally consistent with those for 19 locations shown in Figures 1 and 2. However, we are less confident about the model predictions for 61 and 91 locations, because the d maps needed to be interpolated and extrapolated from the 19 locations. Importantly, we find that the heuristic decision rules work equally well for 7, 19, 61, and 91 locations. Thus, it seems safe to conclude that the findings will hold over a wide range of potential target locations.\n\nWe chose a 6-cpd wavelet target because the detectability of this target varies substantially over the 16\u00b0 diameter search region. It would be informative to repeat the measurements for other targets, but given what is known from the detection and search literature, and the fact the simple heuristic decision rules are near optimal for a wide range of d maps, it is likely that the findings will generalize well across a wide range of spatially-localized targets.# Bayesian Decision Making in Covert Visual Search Tasks\n\nThe classic view in the Bayesian decision making literature is that taking into account reliabilities and prior probabilities is critical for making decisions that optimize accuracy14\u201316. Our simulations and experimental results show that this is not true for covert visual search tasks and probably many other identification tasks19. While it is important to take into account the regions of stimulus space where reliabilities and priors go essentially to zero, it is not important to take into account the specific pattern of values within the non-zero regions. This finding depends on the assumption of parallel processing, which holds when applying the max rule. Parallel processing also holds for most deep neural networks (DNNs) and hence DNNs also benefit from the fact that they can achieve near optimal overall performance without taking into account detailed variations in prior probability and reliability. It follows that DNNs can differ in their responses to specific stimuli even when their overall performance is similar and near optimal. If processing is more limited and serial, then effective decision rules need to take into account detailed priors and reliabilities. For example, in a covert search task with a 50% prior of target absent, it would be a mistake to spend a limited amount of processing time on a few locations with low prior probability and/or low reliability.\n\nThe max rule is the optimal decision rule not just for covert search, but for many identification tasks. For these tasks, correlated noise (across all the category responses) has a negligible or minor effect on the accuracy of the decision. We showed that correlated noise can create a mismatch between the *d values estimated in the cued detection task and the effective d* values in the covert search task. Such mismatches might occur in other identification tasks, and thus would be an important factor to consider.\n\nOur results do not prove that correlated noise is the source of the supraoptimal accuracy of the observers in our experiments, but it is a plausible hypothesis because there are many likely sources of correlated noise, such as slow modulations in membrane potential and in more indirect neural-response measures such as the bold response36\u201340.\n\nAn interesting possibility is that the nervous system evolved to inject correlated variations into the pathways transmitting information to the circuits or brain areas that perform identification tasks. Because these correlated variations do not hurt identification performance, they could provide an independent (low bitrate) channel for communicating other kinds of information such as reward signals, arousal signals, task-relevant global context information, etc. Such correlated variations could be used to broadcast these other kinds of information to any of the decision-making circuits that use the max rule. Piggybacking on existing major pathways could also reduce the need for separate specialized minor pathways. The benefits of this low bit rate communication channel may outweigh the cost of reduced sensitivity in cued detection tasks. A recent study provides evidence for the related hypothesis that stochastic co-modulation of specific neural populations serves as a label for task relevance in subsequent stages of processing41.# Visual Search Under Natural Conditions\n\nVisual search under natural conditions is a mixture of covert and overt search. During the first part of each fixation there is a covert search event to identify the location of the target or potential target locations. During the second part of each fixation the next fixation location is computed. The optimal decision rule for picking the next fixation location also takes into account the d map and the prior probability map10. An important next step will be to perform a Bayesian heuristic decision analysis for fixation selection. However, what is immediately clear is that a flat heuristic d map will be inadequate. Using a flat map would always result in keeping fixation the same, rather than wasting the saccade time during which no useful covert-search information is obtained. If the human visual system uses the same d map for covert search and fixation selection it could explain why our estimate of the average human heuristic for covert search is not flat, even though the flat heuristic is as good as the foveated heuristic (see Figures 3 and S2).# Methods\n\nThe study included four male observers, aged 19\u201326. They all had normal or corrected-to-normal acuity. All procedures in the experiments were approved by the University of Texas Institutional Review Board.\n\nThe stimuli in the experiments were generated with MATLAB 2023a and the Psychophysics Toolbox42,43. They had a resolution of 30 pixels per visual degree, with each pixel occupying a 2 x 2 screen pixel region on a calibrated Sony GDM-FW900 cathode-ray-tube (CRT) monitor, with a size of 1920 x 1200 pixels, a refresh rate of 85 Hz, and a bit depth of 8. The stimuli were gamma-compressed prior to display on the screen.\n\nCircular cues were used to indicate possible target locations. The luminance outside the circular cues was set to the mean luminance of stimuli within the cues (60 cd m2). Light circular cues had a luminance of 66 cd m2. Dark circular cues had a luminance of 51 cd m2. Backgrounds centered on each potential target location were statistically independent samples of high-contrast Gaussian noise. Each patch had a root-mean-squared (RMS) contrast of 20%. For the 19-location configuration, the target locations were separated by 4 deg, and the backgrounds had a diameter of 3.5 deg. The target was a 6-cycles/deg wavelet windowed to a diameter of 0.8 deg that, when present, was added to the center of the noise patch.\n\nFor the detection task, the observers were asked to indicate target absence with a right mouse click, and target presence with a left mouse click. For search tasks, observers were asked to right click if the target was absent and to left click the judged location if present. We ran preliminary search trials with highly visible targets and found that the human observers made no errors in clicking on the target location, indicating that memory and motor-control limitations were not important factors in the study. Furthermore, we used a reverse counterbalancing design, where the observers completed the detection and search experiments in two opposite orders.",
        "context_id": 12,
        "question": "What is the typical duration of fixations during overt search as used in the stimuli presentation for the covert search tasks?",
        "answer": [
            "250 msec"
        ],
        "context_length": 17716
    },
    {
        "context": "# 1 Introduction\n\nPurely-forward and local algorithms propagate signals from inputs to outputs, without any backward pass, and employ a learning rule that is local in space, using signals only from pre- and post- neurons to update a weight. These algorithms, which include well-known Hebbian learning methods [1], have long been studied to model the brain, as their features align with biological processes, contrarily to error backpropagation [2]. They are also ideal for on-chip learning of low-power embedded hardware. The purely forward architecture addresses the challenge of implementing backpropagation on neuromorphic chips, a key obstacle that has been limiting the development of on-chip training for these systems [3]. The local learning rule allows synaptic devices to be directly updated by signals from neuron devices, eliminating the need to store neural activations for gradient computation, thus significantly reducing memory usage, power consumption as well as training times [4].\n\nThe practical application of these algorithms was historically limited by low accuracy on deep learning image recognition benchmarks, such as CIFAR-10 (labeled) and STL-10 (unlabeled). However, in recent years, accuracy has significantly improved thanks to their integration with automatic differentiation tools and activation functions, combined with the use of standardization and pruning techniques [5\u201314]. As a result, these algorithms have become relevant not only for brain modeling and energy-efficient training of neuromorphic chips, but also for distributed learning of large-scale models across multiple devices [15\u201318].\n\nThese advances have motivated the recent development of novel purely forward and local algorithms aimed at improving accuracy and simplifying implementation for various applications. The Forward-Forward (FF) algorithm is a key example of this new breed of algorithms [19]. Its main advantages are its simplicity and versatility, allowing it in principle to handle both unsupervised and supervised learning, and to process time-series as well as static inputs \u2014 while many other purely forward and local algorithms are limited to a single category. As a result, the FF algorithm has attracted significant interest since its introduction [7, 20\u201327], inspiring variants with improved accuracy and architectures and addressing tasks such as image recognition, temporal sequence classification, and reinforcement learning [14, 28\u201339]. Early demonstrations of FF in silico already cover a wide range of hardware platforms, including microcontrollers [40], in-memory computing systems utilizing Vertical NAND Flash Memory as synapses [41], and physical implementations where neurons are replaced by acoustic, optical, or microwave technologies [42, 43].\n\nFig 1 highlights recent advancements in the Forward-Forward algorithm (FF, shown in purple) [14, 19, 33, 38], comparing its accuracy on image classification tasks with algorithms that are also both purely forward and local (shown in black [5\u201314]). As illustrated, the accuracy of FF supervised training has significantly improved from below 60% [19] to 88.2% recently [44], progressively catching up with the best supervised, purely local, and forward algorithm [5]. In unsupervised learning, where FF\u2019s greatest potential for on-chip training may lie, it has only achieved high accuracy on the MNIST task [19, 45], with limited [38] or no success on more challenging datasets such as CIFAR-10 [46] or STL-10 [47].# Comparison of Algorithms\n\n|Supervised|Unsupervised|\n|---|---|\n|CIFAR-IU (CNM)|CIFAR-J (CMD)|\n|STL IO (CNN)|MNIST (MLF)|\n|SigProp|SCFF (us)|\n|DRTP|HardHebb?|\n|Act: Learning|SoftHebb|\n|FFI|Learning|\n|PEPITA|Hebb+WTA|# Accuracy (%)\n\nFig. 1: Comparison of algorithms that are both purely forward and local across different tasks. The blue frame displays the CIFAR-10 results achieved by various supervised local learning methods. The green frame presents the results of different local and unsupervised learning methods on CIFAR-10, STL-10, and MNIST datasets, respectively. Algorithms related to FF are highlighted in purple. FF1 refers to [19], FF2 to [33], FF3 to [14], FF4 to [44], and FF5 to [38]. Our method, SCFF, is indicated in red. SigProp refers to [5], DRTP to [6], Act.Learning to [7], PEPITA to [8], SoftHebb to [9], CSNNs to [10], HardHebb2 to [12], HardHebb1 to [11], Hebb+WTA to [9] and Hebb-CHU to [13].# Forward-Forward (FF) Algorithm\n\nInspired by the noise contrastive estimation (NCE) method [48], the Forward-Forward (FF) algorithm presents positive and negative examples sequentially to the network inputs. Once trained, the network is expected to produce distinctly different neural responses across all layers for these examples. The key challenge is generating \u201cnegative\u201d examples that closely resemble the training data but still provide enough contrast for the network to learn meaningful representations.\n\nSupervised learning methods applicable to any dataset have been developed, solving tasks like MNIST and CIFAR-10 [19, 32]. However, for unsupervised FF learning, there is no universal method to generate positive and negative examples for all databases, hindering FF\u2019s application to more complex unsupervised tasks beyond MNIST. This limitation is evident in the few FF points in the Unsupervised panel of Fig 1, where only Hebbian and activation learning algorithms have successfully solved CIFAR-10 and STL-10 by combining a local rule with a purely forward architecture.\n\nMoreover, the FF algorithm currently lacks the ability to handle time-varying sequential data, limiting its applicability in neuromorphic systems which often deal with dynamic inputs from the real world. While the original FF paper [19] includes a multi-layer recurrent neural network, its purpose is to model top-down effects, using a static MNIST image repeated over time frames as input. Another implementation demonstrates a limited form of sequence learning with a fully connected network.# Self-Contrastive Forward-Forward (SCFF) Method\n\nBut this architecture could not handle real-time sequential data due to the absence of recurrence. As a result, FF has yet to be extended to effectively handle recurrent network scenarios for time-varying inputs.\n\nIn this work, we introduce the Self-Contrastive Forward-Forward (SCFF) method, where each data sample is contrasted within itself to enable efficient learning. This method is inspired by self-supervised learning to provide a simple and effective way to generate positive and negative examples for any dataset. In SCFF, a positive example is created by concatenating an input with itself, while a negative example is formed by concatenating the input with a randomly selected example from the training set. This simple method not only extends the capabilities of FF to unsupervised tasks but also surpasses the state-of-the-art accuracy of similar algorithms on the MNIST, CIFAR-10, and STL-10 image datasets. It also opens the path to solving sequential tasks by contrasting inputs with FF.# Contributions\n\n- We propose an approach called SCFF that takes inspiration from self-supervised learning to generate positive and negative examples and train neural networks with the Forward-Forward algorithm in an unsupervised way, applicable to a wide range of databases.\n- We show that SCFF significantly outperforms existing unsupervised purely-Forward and local learning algorithms in image classification tasks. With a multilayer perceptron (MLP), we achieved a test accuracy of 98.7% on MNIST. With a convolutional neural network (CNN), we reached 80.75% on CIFAR-10 and 77.3% STL-10 (which includes a small number of labeled examples alongside a much larger set of unlabeled data).\n- We present the first demonstration of the FF approach being successfully applied to sequential data. Our findings show that the proposed SCFF method effectively learns representations from time-varying sequential data using a recurrent neural network. In the Free Spoken Digit Dataset (FSDD), SCFF training results in an accuracy improvement of over 10 percentage points compared to scenarios where hidden connections remain untrained (random).\n- We conduct a theoretical and illustrative analysis of the distribution of negative examples generated with our method in comparison to positive examples within the data space. The analysis reveals that negative data points consistently position themselves between distinct clusters of positive examples. This positioning suggests that negative examples play a crucial role in pushing apart and further separating adjacent positive clusters, thereby enhancing the efficiency of classification.\n\nOur results demonstrate the potential of Self-Contrastive Forward-Forward (SCFF) methods for efficient and flexible layer-wise learning of useful representations in a local and purely forward unsupervised manner. In section 2.1 and 2.2, we will introduce the two foundational methods that SCFF builds upon: the original Forward-Forward algorithm and Contrastive Self-Supervised learning, highlighting their differences and similarities. Next, in section 3, we will present our Contrastive Forward-Forward algorithm and discuss our findings. Finally, we will explore the relationship of SCFF to other purely forward and/or local learning methods in the discussion of section 4.# 2 Background\n\n|FF'$ Negative example|Forward Forward|Contrastive Learning|Self-Contrastive Forward-Forward|\n|---|---|---|---|\n|Mirimite coodrey|Minimite Eoodrey|Encocc|Encodnt|\n|Mjumlie coodnes|Maumlie coodness|hybrid|Positive pair|\n|Mulme roodness|Minimice goodnet)|Klnimlz aercement|Maumle Foooness|\n|Lncoder|Encodtr|Positive input|Negative input|\n|Nleative pair|Positive input|Negative input| |\n\nFig. 2: Comparative diagram illustrating three distinct unsupervised (self-supervised) learning paradigms. a. Generation of a negative example is implemented by hybridization of two different images in the original FF paper [19]. b. In Forward Forward (FF) Learning, the layer-wise loss function is defined so as to maximize the goodness for positive inputs (real images) and minimize the goodness for negative inputs, each of which is generated by corrupting the real image to form a fake image, as shown in a. c. In Contrastive Learning, the InfoNCE loss function determines the similarity between representations of two inputs (two different inputs or two same inputs but with different augmentations) in the end of the network [49]. d. Our proposed Contrastive Forward Forward Learning algorithm combines the principles of Forward Forward Learning and Contrastive Learning algorithms to maximize the goodness for concatenated similar pairs and minimize the goodness for dissimilar pairs with a layer-wise loss function.# 2.1 The original Forward-Forward algorithm\n\nThe original Forward-Forward algorithm is depicted in Fig 2b. For an input example x, each layer\u2019s output is assigned a \u2018goodness\u2019 score Gi(l), where l is the layer index. This score is calculated as the mean of the squared activities of the output neurons at layer l: Gi(l) = M1(l) m y2(l), where M(l) is the number of neurons at layer l and m represents the neuron index.\n\nPredefined positive and negative examples are successively presented to the network\u2019s input. The possibility of a positive example xi being recognized as positive and a negative example (l) and pneg(xj) = \u03c9(#neg\u2192 Gj xj) being recognized as negative by the network are defined as ppos(xi) = \u03c9(Gi(l)\u2192 #pos(l)(l)) respectively. The sigmoid function \u03c9(x) = 1 + e1\u2192x evaluates the effectiveness of the separation, where #pos(l) and #neg(l) are fixed values that serve as hyperparameters of the network.# Inspired by Noise-Contrastive Estimation\n\nInspired by Noise-Contrastive Estimation [48], which aims to distinguish positive examples from noisy examples, the objective of FF learning is to increase the goodness score for positive input examples so that it significantly exceeds the threshold (ppos(x) \u2191 1) and to decrease the goodness score for negative input examples so that it falls well below the threshold (pneg(xj) \u2191 1). Weight updates are performed locally by combining the gradients derived from both positive and negative examples:\n\nLFF = Expos[log ppos(x)] - Exjneg[log pneg(xj)] + EGj,neg[log \u03c9(#neg Gj,neg)]\n\n= EGi,pos[log \u03c9(Gi,pos #pos)]\n\nwhere Gi,pos(l) and Gi,neg(l) respectively correspond to the goodness for the positive and negative examples input at layer l. The final loss is computed over all N examples in the batch.\n\nHinton [19] proposed that FF can be used for self-supervised representation learning by using real data vectors as the positive examples and engineered data vectors as the negative examples. The negative examples should be carefully designed, rather than relying on random corruption methods of the training data like noise injection or occlusion. The negative data needs to be sufficiently challenging to ensure the network learns useful information. To make FF focus on the long-range correlations in images that characterize shapes, the negative data should have very different long-range correlations while maintaining similar short-range correlations. For the MNIST dataset, this can be achieved by creating a mask with large regions of ones and zeros. As proposed in the original framework [19], negative data is then generated by combining one digit image multiplied by the mask with a different digit image multiplied by the inverse of the mask, as illustrated in Fig 2a. Although this method performs well for MNIST (as shown in the Unsupervised panel in Fig 1), it does not easily translate to more complex image databases like CIFAR-10 and STL-10, resulting in limited accuracy on these benchmarks.# 2.2 Contrastive self-supervised learning\n\nIn this article, we draw inspiration from contrastive self-supervised learning methods to construct positive and negative examples for FF, applicable to any image database and beyond. Contrastive learning, illustrated in Fig 2c, is a self-supervised technique designed to learn representations by contrasting positive pairs against negative pairs. This approach relies heavily on data augmentation, where a positive pair consists of two different augmented views of the same data sample, and negative pairs are constructed from different samples. While FF shares strong similarities with contrastive learning, it also has key differences.\n\nBoth methods utilize the concept of contrasting positive and negative examples to guide the learning process. They also both aim to learn meaningful data representations without labeled data in the first phase, allowing to extract information from the hidden layers in the second phase by training a linear classifier.However, their loss functions are fundamentally different. FF focuses on the absolute goodness of positive versus negative examples, decoupling them in the loss. On the other hand, general contrastive losses employ relative comparisons through distances or similarities between positive and negative examples.\n\nAdditionally, the neural network optimization method differs. FF defines a local loss function at each layer and avoids backpropagation through hidden layers, whereas typical contrastive learning approaches train deep networks end-to-end. Lastly, self-supervised FF emphasizes data corruption to generate negative examples, focusing on specific characteristics of corruption to enhance learning, while contrastive learning relies heavily on varied data augmentation techniques.# 3 Self-Contrastive Forward-Forward algorithm and results\n\nFig. 3: SCFF method for processing with Convolutional Neural Network Architecture. a. The original batch of images (top row) is processed to generate positive (middle row) and negative examples (bottom row). b. The generated positive and negative examples undergo a series of convolutional (Conv.) and pooling (AvgPool or Maxpool) operations to extract relevant features. The output neurons which are extracted from each hidden layer after an external average pooling layer are then fed together into a softmax layer for final classification.\n\nWe draw inspiration from contrastive self-supervised learning algorithms to propose the Self-Contrastive Forward-Forward (SCFF) algorithm, which addresses the drawbacks of FF, including the complexity of generating negative examples and its inability to generalize well to convolutional neural networks in a purely forward manner.# 3.1 Creating the negative and positive examples\n\nInstead of contrasting the representations in the feature space as in contrastive self-supervised learning (Fig 2c), SCFF directly takes pairs of positive and negative images as inputs to the neural network (Fig 2d). More specifically, given a batch of N training examples, and for a randomly selected example xk (k \u2208 {1, N}) in the batch, the positive example xi,pos(0) (the number 0 is the layer index) is the concatenation of two repeated xk, i.e., xi,pos = [xk, xk]T. The negative example xj,neg(0) is obtained by concatenating xk with another example xn (n \u2260 k) in the batch, i.e., xj,neg(0) = [xk, xn]T (or [xn, xk]T). Fig 3a shows some instances of generated positive and negative examples from the original training batch for the STL-10 dataset.\n\nConsidering the case of a fully connected network, the concatenated pair of images (positive or negative examples) are sent as inputs to the network. The outputs for the positive and negative examples from the first layer can be written respectively as yi,pos = f(W1xk + W2xk), yj,neg = f(W1xk + W2xn), where f is the ReLU activation function. The weight matrices W1 and W2 correspond to the connections to the two images. In practice, we set W1 = W2 because the gradient of the loss function with respect to W1 and W2 converges to the same value. Setting W1 = W2 accelerates the training speed and improves the performance. Intuitively, this can be understood by recognizing that swapping the positions of xk and xn in the concatenated image should not affect the output neural activities. For a rigorous mathematical proof, see Appendix A.# Relative positions between positive and negative examples (IRIS dataset)\n\n|LDA for positive and negative examples|Mean positions|\n|---|---|\n|Setosa|1|\n|Versicolor|0.05|\n|Negative data| |\n\nFig. 4: Probability distributions of relative positions between positive and negative examples. a Theoretical distributions of positive examples from two different classes with distinct means (2\u03bc1 = 0 and 2\u03bc2 = 15) and identical variance (2\u03c32 = 4) are shown with blue and orange curves, respectively. The theoretical distribution of negative examples derived from the two classes using the formula 2 is depicted by the grey curve. b Continuous probability density of LDA applied to the IRIS dataset, displaying contours for positive examples in green, red, and blue, and for negative examples in grey.# The concept of SCFF\n\nThe concept of SCFF can be understood through the lens of Noise Contrastive Estimation (NCE). In NCE, a key insight is that \u201cthe noise distribution should be close to the data distribution, because otherwise, the classification problem might be too easy and would not require the system to learn much about the structure of the data\u201d[48]. Our method of generating the positive and negative examples aligns with this principle if we treat the negative examples as \u201cnoise data\u201d. We assume that the data samples for each class follow a multivariate Gaussian distribution with a shared covariance matrix $ and that each class is statistically independent of the others\u2014assumptions commonly employed in various statistical models [50]. Furthermore, noting that the input weight matrices W1 and W2 are identical, i.e. W = W1 = W2, the concatenated inputs processed by the network are simplified as follows: the positive(0) example becomes yi,pos = f(W1xk + W2xk) = f(W(2xk)), and the negative example is yj,neg = f(W1xk + W2xn) = f(W(xk + xn)). This is equivalent to treating the positive examples as xi,pos = 2xk and negative examples as xj,neg = xk + xn. Therefore, the distributions of positive examples xi,pos and negative examples xj,neg follow:\n\nxi,pos \u2197 N(2\u03bc1, 2!)\n\nxj,neg \u2197 N(\u03bc1 + \u03bc2, 2!)(0)\n\nwhere \u03bc1 and \u03bc2 are means of two different classes respectively. Theoretically, the negative examples always lie somewhere between two different clusters of positive examples in the sample space, as illustrated in Fig. 4a for the one-dimensional case. For practical analysis with a real-world dataset, we visualized the distributions of positive and negative examples from the IRIS dataset [51] using 2D linear discriminant analysis (LDA), which distinguishes between three different types of irises, as shown in Fig. 4b. This visualization shows that the negative examples are positioned between different clusters of positive examples, suggesting that they contribute to pushing apart and further separating adjacent positive examples as they are mapped into higher-dimensional space during training. Additionally, negative examples are formed by combining two examples from different classes, enriching the diversity of negative examples and leading to more robust training. For a detailed analysis of how the LDA components evolve during training as the input data is mapped into the feature space and more theoretical results, please refer to Appendix B.# 3.2 Training procedure\n\nWe evaluate SCFF on different image datasets including MNIST [45], CIFAR-10 [46] and STL-10 [47] (results in Fig. 1 and Fig. 5), as well as an audio dataset Free Spoken Digit Dataset (FSDD) [52] (results in Fig. 6).\n\nEach layer of the network was fully trained and frozen before training the next one. After unsupervised training with SCFF, we froze the network and trained a linear downstream classifier [9, 53] with the back-propagation method on representations created by the network using the labeled data. The linear classifier was optimized using cross-entropy loss. The accuracy of this classification serves as a measure of the quality.# 3.3 Fully connected network: MNIST\n\nOn the MNIST dataset, when trained on a two-layer fully connected network with 2000 hidden neurons each, SCFF achieves a test precision of 98.7%, which is comparable to the performance achieved by backpropagation [19]. This surpasses previously published benchmarks on other biologically inspired algorithms, such as 97.9% in [9], 98.42% in [8] (supervised training), and 96.6% in [7]. The full comparison is shown in the right column of the green frame in Fig 1.# 3.4 Convolutional neural networks: CIFAR-10 and STL-10\n\n| |SCFF|Backprop|\n|---|---|---|\n|1 layer|90|50|\n|2 layers|83.8|66.2|\n|3 layers|80.8|72.4|\n|4 layers| |77.3|\n\nFig. 5: Comparison of test accuracy at different layers by using SCFF and Back-propagation methods on CIFAR-10 in a and on STL-10 dataset in b.\n\nThe convolutional neural network processes three-dimensional color images. The original images are concatenated along the channel dimension to form positive or negative inputs (see Fig 3). The output of each convolutional layer is represented as a three-dimensional vector yi,pos (or yi,neg) \u2208 RC\u2191H\u2191W. The Loss function at layer (l) is then defined as:# Equations\n\nLSCFF = \u2192 E Gi,pos(l) 1 (l) (l)\n\n[ \u2211\u2211log\u03c9(Gi,h,w,pos \u2192 # pos)]\n\n[ \u2211\u2211log\u03c9(# neg \u2192 Gj,h,w,neg)]\n\n\u2192 E Gj,neg(l) 1 (l) (l) (3)\n\nwhere the goodness of neural activities is calculated over the channels as Gi,h,w,pos =(l)\n1 \u2211c y2(l) (or Gi,h,w,neg(l)= C\u2211c y2(l)1\ni,c,h,w,neg).# Results\n\nFor the CIFAR-10 and STL-10 datasets, we employed convolutional neural networks with architectures identical to those in [9] to extract features. SCFF achieves a test accuracy of 80.75% with a three-layer convolutional architecture (number of filters each layer: 96-384-1536) on CIFAR-10 and 77.3% with a four-layer convolutional architecture (number of filters each layer: 96-384-1536-6144) on STL-10. These results surpass the previous state-of-the-art accuracies for purely-forward unsupervised learning, of 80.3% on CIFAR-10 and 76.2% on STL-10 achieved using the SoftHebb algorithm [9]. This demonstrates the significant potential of the SCFF method to scale effectively to more complex datasets and architectures. The full comparison is shown in the left column of the green frame in Fig. 1.\n\nWe also compared the test accuracies at each layer using SCFF and Backpropagation (BP) methods on CIFAR-10 and STL-10, as shown in 5. Notably, for STL-10, SCFF achieved a final layer performance of 77.3% higher than the one of BP: 77.02% (Fig. 5b). This is because the STL-10 dataset contains a large amount of unlabeled images, which limits the effectiveness of supervised BP training. By fine-tuning SCFF with end-to-end BP on the few labelled STL-10 examples, SCFF\u2019s accuracy further improves to 80.13%. This demonstrates that SCFF is highly suitable for unsupervised pretraining followed by supervised BP training, making it ideal for semi-supervised learning approaches.\n\nUnlike other unsupervised learning methods, where the result is obtained solely from the final layer\u2019s output, SCFF integrates neuron information with the linear classifier from intermediate layers, leading to more comprehensive feature extraction [19]. For CIFAR-10 (Fig. 5 a), the test accuracy for the two-layer and three-layer models was obtained by combining the outputs of all previous layers (pooled information for dimensionality reduction; see Methods section) before feeding them into the final linear classifier. However, because the STL-10 dataset consists of high-quality images, the number of output neurons in each layer is much larger than that in CIFAR-10. Therefore, for the STL-10 dataset, we did not combine outputs from previous layers for training the linear classifier, with the exception of the fourth layer. In this case, we combined the outputs from both the third and fourth layers for the final classification, resulting in a 1% improvement in accuracy compared to using only the fourth layer\u2019s outputs as input to the linear classifier.\n\nBy visualizing and investigating the class activation map, which highlights the importance of each region of a feature map in relation to the model\u2019s output, we\n\n11# 3.5 Recurrent neural network: FSDD audio dataset\n\nThe original FF paper [19] introduces a specialized recurrent neural network (RNN) that models top-down effects using repeated static images as input for each time frame. In contrast, our work focuses on training an RNN that processes time-varying inputs.\n\nWe employ the Free Spoken Digit Dataset (FSDD), a standard benchmark task for evaluating RNN training performance [55\u201357]. The FSDD is a collection of audio recordings where speakers pronounce digits (0-9) in English. We follow the standard procedure consisting in extracting frequency domain information at different time intervals, here through Mel-Frequency Cepstral Coefficient (MFCC) features (39 channels) [58]. Plots of the evolution of MFCC feature with time are shown in Fig. 6 for the digits 3 and 8. The SCFF method forms positive and negative examples by concatenating the same input for positive examples, and different ones for negative examples. Fig. 6a presents a negative example which is generated by concatenating MFCC features from two different digits. The goal of the task is to recognize the digit after feeding in the full sequence, from the output of the network at the last time step.\n\nWe train a Bi-directional Recurrent Neural Network (Bi-RNN) in an unsupervised way using the SCFF method to classify the digits. The procedure that we use for this purpose is illustrated in Fig. 6a. Unlike conventional uni-directional RNNs, where sequential input is processed step by step in a single direction, resulting in a sequence of hidden states from H0 to HT (as depicted in the bottom RNN in Fig. 6a), the Bi-RNN comprises two RNNs that process the input in parallel in both forward and backward directions. This results in hidden states evolving from H0 to HT in the forward RNN and from HT to H0 in the backward RNN, as shown in the top portion of the figure. The red regions in the figure highlight the features at different time steps. This bidirectional structure is particularly advantageous for tasks where context from both preceding and succeeding audio frames is critical, such as speech recognition, enhancing model performance compared to conventional uni-directional RNNs [59].\n\nThe output of each directional RNN for a positive or negative input example is a two-dimensional vector hi \u2208 RM\u00d7T, where T represents the number of time steps and M denotes the number of hidden neurons. The loss function at layer l is then defined as:\n\nLSCFF = EGi,pos(l) [1(l) \u2211t log\u03c9(Gi,t,pos \u2192 #pos)]\n- EGj,neg(l) [1(l) \u2211t log\u03c9(#neg \u2192 Gj,t,neg)]# 1. Training procedure of SCFF on a Bi-RNN\n\nwhere the goodness of neural activities is calculated at each time step as\n\nG(l)posi,t = M1 \u2211m h2(l)i,t,m,pos (or Gi,t,neg = M1(l) \u2211m h2(l)i,t,m,neg).\n\nUnsupervised training on hidden connections (grey arrows)# 2. Comparison of test Accuracy\n\n|Digit|Trained with SCFF|Untrained|Trained with Backprop|\n|---|---|---|---|\n|3| | | |\n|8|1| | |\n\nAt each time step, the features are sequentially fed into the Bi-RNN (RNN and RNN\u2193). The red regions indicate features at different time steps. In the second stage, a linear classifier is trained using the final hidden states from both RNNs, i.e., HT and H0\u2193 as inputs for classification task.# 3. Test accuracy comparison\n\nThe blue, orange and green curves in Fig. 6b depict the test accuracy of the linear output classifier with hidden connections trained using SCFF, with random (untrained) hidden connections, and with Backpropagation methods, respectively. SCFF achieves a test accuracy of 90.3% when trained with a one-layer Bi-RNN containing 500 hidden neurons in each direction (refer to Appendix E for further architectural details). It is below the performance of BackPropagation Through Time, reaching 96% accuracy on this small task, but well above the model with untrained (random) hidden connections which plateaus at 80.7%.\n\nThis result constitutes the first successful application of the FF approach to sequential data in an unsupervised manner. Unlike the BPTT method for training RNNs, SCFF avoids the issues of vanishing and exploding gradients, as the gradients at each\n\n13# Table 1: Comparisons of the learning capabilities of different local learning methods and their test accuracy [%] on CIFAR-10 and STL-10 dataset.\n\n|Method|(Self) Unsupervised1|Local|Sequential2|CIFAR-10|STL-10|\n|---|---|---|---|---|---|\n|SimCLR (Chen et. al. 2020 [49])|\u2701|\u2702|\u2701|94.0|89.7|\n|PNN-SSL (Laydevant et. al. 2023 [62])|\u2701|\u2702|\u2702|77.0|-|\n|Bio-SSL (Tang et. al. 2022 [63])|\u2701|\u2701|\u2702|72.7|68.8|\n|CLAPP (Illing et. al. 2021 [64])|\u2701|\u2701|\u2702|-|73.6|\n|SigProp (Kohan et. al. 2023 [5])|\u2702|\u2701|\u2702|91.6|-|\n|EqProp (Laborieux et. al. 2021 [65])|\u2702|\u2701|\u2702|88.6|-|\n|DualProp (H\u00f8ier et. al. 2023 [66])|\u2702|\u2701|\u2702|92.3|-|\n|FF (Hinton et. al. 2023 [19])|\u2701|\u2701|\u2702|59.0|-|\n|FF (Papachristodoulou et. al. 2024 [33])|\u2702|\u2701|\u2702|78.1|-|\n|FF (Wu et. al. 2024 [44])|\u2702|\u2701|\u2702|84.7|-|\n|PEPITA (Srinivasan et. al. 2023 [8])|\u2702|\u2701|\u2702|53.8|-|\n|Act. Learning (Zhou et. al. 2022 [7])|\u2701|\u2701|\u2702|58.7|-|\n|HardHebb (Miconi et. al. 2021 [11])|\u2701|\u2701|\u2702|64.8|-|\n|HardHebb (Lagani et. al. 2022 [12])|\u2701|\u2701|\u2702|64.6|-|\n|Hebb-CHU (Krotov et. al. 2019 [13])|\u2701|\u2701|\u2702|50.8|-|\n|Hebb-PNorm (Grinberg et. al. 2019 [67])|\u2701|\u2701|\u2702|72.2|-|\n|SoftHebb (Journ\u00e9 et. al. 2022 [9])|\u2701|\u2701|\u2702|80.3|76.2|\n|SCFF (ours)|\u2701|\u2701|\u2701|80.8|77.3|\n\n1 Self-supervised or unsupervised\n\n2 Can handle sequential data or not\n\nTime steps are calculated independently. This eliminates the dependency between time steps, providing a more stable training process.\n\nIt is also interesting to note that the network with untrained hidden and input connections is akin to Reservoir Computing, a method that is often used to leverage physical systems on sequential data for neuromorphic applications [60]. SCFF provides a way to train these input and hidden layer connections in a simple, hardware-compatible way, and opens the path to a considerable gain of accuracy. This achievement opens the door for its extension to more complex tasks involving temporal sequences and its potential use in neuromorphic computing domains, such as dynamic vision sensors [61].# 4.1 Comparison to the original FF algorithm\n\nIn addressing the limitations of the original FF algorithm, our method introduces several key improvements. Firstly, we have developed an approach for generating negative examples that can be applied to any database. This approach is also biologically plausible, since it operates by aggregating two similar or different images at the input, very much like our eyes do. This innovation directly addresses the criticisms highlighted in [8], which pointed out the biological implausibility of the negative examples used in the original FF algorithm.\n\nFurthermore, we have expanded the applicability of FF to complex unsupervised tasks beyond the MNIST dataset. The SCFF method achieves state-of-the-art (SOTA) accuracy for local methods on challenging datasets such as CIFAR-10 and STL-10.# 4.2 Comparison to SOTA self-supervised learning (SSL)\n\nOur SCFF method draws significant inspiration from self-supervised contrastive learning techniques[49, 68]. While the accuracy of SCFF may be lower compared to these methods, primarily due to its layer-wise learning in a purely local manner, it offers unique advantages. Unlike global self-supervised methods, SCFF operates without requiring auxiliary heads (multi-layer nonlinear projector) or complex regularization techniques, which simplifies its implementation and makes it more suitable for neuromorphic computing applications.\n\nRecent developments in local versions of contrastive self-supervised learning have shown promising results[62, 63]. For instance, Laydevant et al.[62] empirically demonstrated that layer-wise SSL objectives can be optimized rather than a single global one, achieving performance comparable to global optimization on datasets such as MNIST and CIFAR-10 (see Table 1). However, the layer-wise training methods involving multi-layer MLP as projector heads might offer better performance in certain tasks, but at the cost of increased computational complexity. Illing et al.[64] have shown that local plasticity rules, when applied through the CLAPP model, can successfully build deep hierarchical representations without the need for backpropagation. However, this method introduces additional processing along the time axis, which may add complexity when dealing with data that lacks temporal dynamics.# 4.3 Comparison to other forward-only methods including non-Hebbian and Hebbian based\n\nRecently, other purely forward learning techniques have been developed, driven by their potential for biologically plausible and neuromorphic computing applications[7, 8]. Similar to Forward-Forward (FF), Pepita[8] processes data samples in two forward passes. The first pass is identical to FF, while the input of the second pass is modulated by incorporating information about the error from the first forward pass through top-down feedback. Activation Learning[7] builds on Hebb\u2019s well-known proposal, discovering unsupervised features through local competitions among neurons. However, these methods do not yet scale to more complex tasks, limiting their potential applications.\n\nRecent advances in Hebbian deep learning have also shown remarkable progress[9, 11, 12, 69]. These methods are purely local in space and can be applied purely.# 4.4 Comparison to energy-based learning methods\n\nEnergy-based learning methods, such as Equilibrium Propagation (EP), Dual Propagation (DP) and Latent Equilibrium (LE) [66, 70, 71], also offer locality in space and time. These methods have a significant advantage over SCFF due to their strong mathematical foundations, closely approximating gradients from BP and backpropagation through time (BPTT). This theoretical rigor allows them to be applied to a wide range of physical systems, making them particularly appealing for neuromorphic computing applications. EP, for instance, can operate in an unsupervised manner, while recent advancements in Genralized Latent Equilibrium (GLE) [72] have extended these models to handle sequential data effectively.\n\nHowever, the implementation of energy-based methods poses certain challenges. Specifically, the backward pass in these methods requires either bidirectional neural networks or dedicated backward circuits [73, 74]. These requirements can be complex to design and build in a manner that is both energy-efficient and compact. In contrast, the simplicity and versatility of SCFF in supporting both supervised and unsupervised learning, without the need for complex backward circuitry, make it a practical alternative for many applications [3]. This balance of accuracy, ease of implementation, and versatility underscores the potential of SCFF in advancing neuromorphic computing and biologically inspired learning systems.# 5 Conclusion\n\nIn conclusion, the Forward-Forward (FF) algorithm has sparked significant advancements in both biologically-inspired deep learning and hardware-efficient computation. However, its original form faced challenges in handling complex datasets and time-varying sequential data. Our method, Self Contrastive Forward-Forward (SCFF),# 6 Methods\n\nSCFF learns representations by maximizing agreement (increasing activations/goodness) between concatenated pairs of identical data examples while minimizing agreement (reducing activations/goodness) between concatenated pairs of different data examples using a cross-entropy-like loss function at each layer. The network is trained layer by layer, with each layer\u2019s weights being frozen before moving on to the next. Unlike the original FF framework, this approach incorporates several key components that contribute to achieving high accuracy across various tasks.# Normalization and Standardization\n\nFor vision tasks, the data is typically normalized by subtracting the mean and dividing by the standard deviation for each channel. These mean and standard deviation values are computed across the entire training dataset, separately for each of the three color channels. This dataset-wide normalization centers the data, ensuring that each channel has a mean of 0 and is on a comparable scale.\n\nIn addition to dataset-wide normalization, we also applied per-image standardization, which plays an important role in unsupervised feature learning [75]. Standardizing the images involves scaling the pixel values of each image such that the resultant pixel values of the image have a mean of 0 and a standard deviation of 1. This is done before each layer during processing [11, 47], ensuring that each sample is centered, which improves learning stability and helps the network handle varying illumination or contrast between images.# Concatenation\n\nThe positive and negative examples (e.g. xi,pos and xj,neg) are generated by concatenating two identical images for the positive examples and two different images for the negative examples. After being processed by the first layer, the output vectors yi,pos(0) and yj,neg(0) are obtained. There are two approaches for generating the inputs to the next layer. The first approach is to directly use the first layer\u2019s output of the positive example yi,pos(0) as the positive input xi,pos, and the first layer output of the negative example yj,neg(0) as the negative input xj,neg for the next layer (refer to the highlighted blue section in Algorithm 1 in Appendix C). The second approach involves re-concatenating to form new positive and negative inputs for the next layer. This is done by treating the first layer\u2019s positive outputs as a new dataset and recreating.# Appendix C\n\nthe corresponding positive and negative examples, similar to how the original dataset was processed to generate the initial positive and negative examples (refer to the highlighted blue section in Algorithm 2 in Appendix C).\n\nAppendix C details the workflows of Algorithm 1 and Algorithm 2, focusing on their different approaches to generating positive and negative examples after the first layer. In practice, Algorithm 1 tends to be more effective for training the lower layers immediately following the first layer, while Algorithm 2 shows better performance in training deeper layers. Specifically, for the CIFAR-10 dataset, Algorithm 1 is utilized to train the second layer, while Algorithm 2 is applied to train the third layer. Similarly, for the STL-10 dataset, Algorithm 1 is employed for training the second and third layers, and Algorithm 2 is used for the fourth layer.# Triangle method of transmitting the information\n\n\u201cTriangle\u201d method was firstly introduced by Coates et al. [47] to compute the activations of the learned features by K-means clustering. This method was later found to be effective in other Hebbian-based algorithms [9, 11] for transmitting the information from one layer to the next. The method involves subtracting the mean activation (computed across all channels at a given position) from each channel, and then rectifying any negative values to zero before the pooling layer. This approach to feature mapping can be viewed as a simple form of \u201dcompetition\u201d between features while also promoting sparsity.\n\nImportantly, the \u201dTriangle\u201d activation only determines the responses passed to the next layer; it does not influence the plasticity. The output used for plasticity at each position is given by yi,pos = f(l)(xi,pos) and y(l) = f(l)(xj,neg), where f(l) refers to the convolutional operations followed by ReLU activation at layer l.# Penalty term\n\nTraining with the FF loss can lead to excessively high output activations for positive examples, which significantly drives positive gradients and encourages unchecked growth in their activation. To mitigate this, we introduce a small penalty term\u2014the Frobenius Norm of the Goodness vector\u2014into the training loss function. For outputs from a CNN layer, the goodness vector Gi,h,w,pos(l) is a two-dimensional matrix where each element represents the goodness calculated over the channel outputs processed by all filters under the same receptive field. In the case of Bi-RNN outputs, the goodness vector G(l)i,t is a one-dimensional matrix, with each element representing the goodness at each time step. When a large goodness value is computed for a positive example, it generates a negative gradient that reduces the activation, thereby preventing excessive growth. The impact of this penalty term on training performance is further analyzed in Appendix F.# Additional pooling operation to retrieve the features\n\nTo assess the performance of the intermediate layers in image classification tasks, we apply an additional pooling operation (average or max pooling) to the output of the# Pooling Layer\n\nThis reduces the dimensionality of the features and helps in selecting relevant neuron activities. This approach is inspired by the \"four quadrant\" method used in previous work [11, 47], where local regions extracted from the convolutional layer are divided into four equal-sized quadrants, and the sum of neuron activations in each quadrant is computed for downstream linear classification tasks.\n\nAppendix D provides detailed information on the specific architecture of this additional pooling layer for various tasks.",
        "context_id": 13,
        "question": "What is the recent accuracy percentage achieved by the Forward-Forward algorithm in supervised training on image classification tasks?",
        "answer": [
            "88.2%"
        ],
        "context_length": 43432
    },
    {
        "context": "# Community Shaping in the Digital Age: A Temporal Fusion Framework for Analyzing Discourse Fragmentation in Online Social Networks# 1. Introduction\n\nWhile the term community is often linked to a group sharing a common name or common trait, it more broadly signifies a unified group with shared values or experiences, irrespective of geographical, historical, or locality-based influences. The rise of the internet, especially social media, has transcended geographical boundaries, allowing the formation of diverse online communities based on shared interests.\n\nThe effects of this digital interconnectedness are profound. Research has shown that social media platforms can be instrumental in shaping individual identity and self-discovery, particularly for marginalized groups (Mandel 2019; Villa-Nicholas 2019). For example, students have found validation and connection through online platforms (Tavakolian 2019) and those on the autism spectrum have experienced reduced social isolation (Miller 2019).\n\nHowever, this same anonymity and reach can be weaponized. Social networks have been used to mobilize and disseminate extremist ideologies (Tvardina and Vakizadeh 2019). Online spaces like alt-right communities can normalize violent rhetoric and fuel counter-hegemonic movements (Stodle and Salimsdottir 2019). Understanding the mechanisms behind the formation and evolution of online communities is crucial for policymakers to mitigate potential risks while harnessing the positive potential of these digital spaces.\n\nBuilding on the background implications of online communities in shaping societal discourse, our research delves into the algorithmic identification and analysis of these digital assemblies. By leveraging advancements in machine learning and social network theory, we aim to unravel the intricate web of interactions and shared interests that bind individuals across the vast expanse of cyberspace. This study not only underscores the importance of understanding the dynamics of online communities for both social scientists and policymakers but also provides a methodological blueprint for future research in this burgeoning field.\n\nOur approach's methodological rigor and analytical depth reflect recent advancements in computational social systems, as evidenced by related works in the field (Tabpej et al. 2019). Besides the novel framework we propose, our approach incorporates several novel features:\n\n- Not reliant on predefined metrics; instead, it depends on redefining metrics to centrality measures from social network analysis theories.# Current Research Framework\n\nand shooinl an abstrant remresentation ok ohat is hammeninl in our nommleq sjstem, oe adomt a more naipe but inkormatipe ammroanh to unnoper deemer insilhtsr# Key Components\n\n- uranularitj\u00b8 \u2018ur method alloos kor a kine-lrained pisualization ok nommunitj behapiors, namturinl the nuannes ok interantions and enlalementr\n- Four-lajer ueneralizable Frameoork\u00b8 ge mromose a robust krameoork nonsistinl ok kour lajers that nan be leneralized anross dikkerent sonial media mlatkorms and nommunitj tjmesr\n- Visualization Formation oith Inkormation Fusion\u00b8 ge nombine parious data sournes and pisualization tenhniques to nreate nommrehensipe pisual rempresentations ok nommunitj djnaminsr# Analysis Framework\n\nIn this mamer, oe mromose a neo analjsis krameoork based on thematin analjsis tunderstandinl the nontent and themes ok disnussions{, interantion djnamins tkonusinl on the strunture ok user interantions and inkormation kloo{, ominion djnamins tinpolpinl the kormation, epolution, and molarization ok ominions{, and struntural insilhts tkonusinl on the operarnhinl nharanteristins ok sonial media disnussions and their broader immlinations{r# Critical Assessments\n\n- \u2019sers\u201e enlalement oith tomins\n- \u2019sers\u201e enlalement oith inkluential antors\n- \u2019sers\u201e reantions to tomins\n- \u00a8irth, lrooth, and death ok dominant tomins in sonial media\n- }he ratio ok nhanles tenlalement or reantion or lrooth or denline ok tomins in sonial media{\n- iohesipeness ok tomins\n- Polarization thoo diperlent pieos benome more eqtreme as like-minded indipiduals reinkorne eanh other\u2019s belieks{\n- velmentation tkormation ok dikkerent lroums around smenikin tomins{# Novel Framework Introduction\n\n\u2018ur researnh introdunes a nopel krameoork kor identikjinl online nommunities on sonial media mlatkormsr }his krameoork leperales manhine learninl nlassikination models to analjze user mosts and interantions, repealinl underljinl lroum strunturesr }hese lroums are then pisualized throulh sonial netoork analjsis, mropidinl insilhts into their nommosition and djnaminsr }he innopatipe intelration ok teqt nlassikination and sonial netoork analjsis in this krameoork remresents a silnikinant nontribution to the kield, enablinl more ekkentipe identikination and understandinl ok online nommunitiesr# Temporal Visualization Method\n\nAdditionallj, oe hape depelomed a temmoral pisualization method to remresent the nhanles in these nommunities oper time, mropidinl a djnamin pieo ok their epolutionr ge introdune kourteen kej elements deriped krom the strunture and djnamins ok online sonial media, ohinh ennamsulate the insilhts eqmented krom analjzinl sonial struntures and djnaminsr# Analysis of Results\n\nThese elements are used to analyze the results of combining text and network data. Furthermore, we developed a case study using real-like data to demonstrate the maintainable examination of our framework in analyzing the formation and sharing of online communities.# 1.1. Toxic Classification\n\nWithin the field of Natural Language Processing (NLP), a core task lies in text classification \u2013 the process of assigning predetermined labels to textual data, enabling machines to interpret and organize unstructured information. This process has become increasingly pivotal with the growth of social media content, which presents unique challenges due to its informal language, brevity, and context-sensitive nuances. Recent studies have focused on enhancing classification accuracy by adapting models to understand social media texts' informal nature better. Jotikabukkana et al. proposed an improved trained text model specifically for social media content, which shows promising results in accurately classifying such texts.\n\nJotikabukkana et al. also explored sentiment analysis in social media, evolving from simple opinion mining to complex examinations that can gauge public sentiment on a broad spectrum of topics. Employing deep neural networks for sentiment analysis has significantly improved the ability to understand and interpret the past and present data generated on social platforms. This advancement allows for a more nuanced understanding of public sentiment, enabling analyses ranging from market analysis to political sentiment tracking.\n\nMoreover, integrating sentiment analysis with systematic literature reviews in social media has provided a new dimension to understanding public opinion and its evolution over time. This integration allows researchers to sift through massive datasets to identify prevailing sentiments and their shifts, offering invaluable insights into public sentiment and reaction to various stimuli (TDaoei et al.). Notably, the evolution from basic models to sophisticated architectures like Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks has been instrumental in addressing complex linguistic structures and context-dependent interpretations inherent in textual data (Tahn, Plaisant, and Shneiderman).\n\nIn recent years, deep learning models have emerged as the preferred automated methods for text classification, owing to their exceptional performance and the elimination of complex manual feature extraction. These models leverage embedding techniques to transform text into meaningful, continuous feature vectors, which are learned through neural networks (Minaee et al.). Among the prominent contextual embedding models, ELMo (Peters et al.) utilizes a three-layer bidirectional long short-term memory (LSTM) network.# 1. Introduction\n\nFollowing this, recent developments in the field of natural language processing (NLP) have led to the emergence of a bidirectional transformer model, trained on diverse datasets for various natural language processing tasks, including text classification. These advancements underscore a paradigm shift towards models that encapsulate both semantic and syntactic nuances of language, thereby enhancing the granularity and accuracy of text classification.# 2. Social Networks\n\nThe dynamics of social networks present a rich landscape for analyzing the multifaceted interactions and behaviors within digital communities. The advent of sophisticated visualization and analytical tools has significantly augmented our understanding of these networks, transcending beyond mere structural analysis to unravel temporal patterns and evolutionary trajectories. In this realm, the integration of temporal analysis with network visualization opens new avenues for examining organizing roots, community formation, and the ebb and flow of influence amongst network entities. Such insights are invaluable for deciphering the underlying mechanisms driving social interactions and the dissemination of information across digital platforms. A crucial assessment of network analysis lies in the ability to visually represent the network's structure and identify key elements like hubs or communities.# 3. Research on Twitter Dynamics\n\nThe inherent social dynamics of Twitter have consistently attracted researchers seeking to analyze the conversations and perceptions that arise on the social media platform. The majority of these researches focus on the complex interplay between online discussions and real-world circumstances. For instance, Morelli et al. examined how controversial online circumstances can spark collective mobilization - the extent to which communities express dislike towards each other - in online social networks, and how counter-narratives by influential actors might mitigate this phenomenon. Additionally, other researchers proposed a framework for mining community-specific influence in microblogging networks, aiding in the extraction and analysis of prevalent themes. Furthermore, some utilized social network analysis to delve into political polarization on Twitter, examining the relationships and interactions among users with differing political views.# 4. Frameworks for Analyzing Social Media Data\n\nRecent advancements in social media research have increasingly focused on integrating both user-generated content and user interactions to capture the complex dynamics of online communities. Darlin et al. introduced a framework that examines social media use during disasters, highlighting the role of socioeconomic factors and geographical location in shaping user interactions.# Social Media Usage Patterns During Hurricanes\n\nDarlin, Fan, and Mostakapi their framework reveals significant digital divides that affect information access and reliability mentions, emphasizing the need for equitable resilience in disaster response efforts. Similarly, Fan et al. developed a framework to investigate the discussion patterns of situational information on social media, distinguishing between the roles of regular users and influential users in spreading information. Fan et al. their study on hurricane surge demonstrated that early intervention by hub users significantly boosts the speed and magnitude of information dissemination, providing essential strategies for improving emergency response through social media.# Mobile Content Frameworks\n\nIn the realm of mobile content, frameworks such as JoiFl, presented by Antelmi et al., offer real-time monitoring and analysis of mobile discourse on platforms like Twitter. JoiFl captures and visualizes data related to followers, tweets, retweets, and mentions, providing insights into mobile strategies and public sentiment during elections. Antelmi et al. their framework has been applied to analyze mobile trends and election results, demonstrating its utility in mobile analysis and forecasting. Tabpej et al. emphasized the role of social media in political polarization and discourse fragmentation, highlighting the need for comprehensive frameworks that can capture the nuances of online political interactions and their implications for public opinion. Tabpej et al. their work underscores the importance of understanding how social media shapes political discourse and influences voter behavior.# Disaster Response Frameworks\n\nFrameworks for disaster response have also been developed to utilize social media for real-time situational awareness and infrastructure resilience. Fan et al. proposed a systematic framework for detecting infrastructure-related topics during disasters using social sensing and text mining. Their framework integrates keyword search, text lemmatization, P@v talling, TF-IDF vectorization, LDA topic modeling, and K-means clustering to analyze tweets related to infrastructure. Applied to hurricane surge, the framework effectively summarized and ranked infrastructure-related topics, providing actionable insights for improving infrastructure resilience and disaster response strategies. Fan et al. Rajmut et al. modeled inter-organizational communication networks on social media during disasters, using hurricane surge as a case study. Rajmut et al. identified the roles of different organizations in disseminating situational information and highlighted the importance of government and non-government collaboration in disaster response efforts.# Interdisciplinary Approaches\n\nMoreover, Zhan et al. reviewed the use of social media for mobilizing information and warning during disasters, proposing an interdisciplinary approach that leverages the marginalization.# Nature of Social Media to Enhance Disaster Management Strategies\n\nThanal et al. identify key functions such as acquiring situational awareness, supporting peer-to-peer help activities, and enabling disaster management agencies to hear from the public. This comprehensive approach underscores the potential of social media to improve communication and coordination during emergencies. Additionally, the work of T'imizzi et al. on integrating machine learning and social network analysis to understand social media dynamics further highlights the need for frameworks that can generalize across different domains and provide deeper insights into the impact of real-world circumstances on social media interactions.# Systems Thinking\n\nAccording to T'Arnold and Gade, systems thinking is defined as a set of generalist analytical skills used to enhance the ability to identify and comprehend systems, predict their behaviors, and devise modifications to achieve desired outcomes. This definition highlights the significance of understanding the interconnections within a system and the dynamic behaviors that result from these interactions. Numerous studies in the literature have employed this methodology and measurement to address various problems.\n\nFor instance, T'Kalantari et al. emphasize the importance of modeling human interactions and information diffusion within different collaboration networks. By employing a dynamic operational community detection algorithm, researchers can identify and track communities and their leaders over time, allowing for a more comprehensive understanding of network dynamics and information diffusion, leading to better decision-making and optimization strategies.\n\nIn management systems, T'Maleki, Yalhoubi, and Fander focus on analyzing the supply chain not as isolated components but as a cohesive system. Their methodology involves developing mathematical models to simulate real-world scenarios and examining the effects of different variables such as tier, organizing levels, and sales efforts on the supply chain's performance.\n\nIn engineering systems, T'Amini, Marsooli, and Neshat address the complex problem of calibrating pelletization drill mechanisms in coastal environments. Similarly, T'Neshat et al. adopt a holistic, interconnected, adaptive, and multi-objective approach to optimize hybrid power generation systems. Even in cutting-edge research utilizing quantum computing-inspired optimization, the same systems thinking approach is evident.\n\nT'Amani and Karlarian, along with Amani, Mahroo, and Karlarian, indicate that there is more research with this methodology. T'Khameneh et al., Perez-Pereda, Krstikj, and Ramirez-Marquez show that this methodology enables researchers to analyze the system in detail and understand the behavior of its constituent components while also taking a holistic view to comprehend the system's behavior over time within the context of larger systems. This approach has been instrumental in shaping our framework.# Summary of Research Contributions\n\nIn summary, our literature review reveals that recent frameworks increasingly integrate the analysis of both user-generated content and interactions networks of replies, retweets, users, etc. These frameworks vary in focus, with some emphasizing specific domains like political tweets or disaster-related tweets, others examining user interactions based on shared or divergent ideas, and some analyzing non-conversation networks and interactions in response to controversial tweets. Additionally, certain studies employ redesigned network measures to represent the dynamics of complex systems. However, there remains a clear need for a generalizable framework that leverages fundamental social science theories to present the dynamics of discourse interactions and elements and to elucidate the impact of real-world circumstances on social media, as well as the influence of social interactions on societal behavior. Our research aims to fill these gaps by introducing a comprehensive framework focused on major discourses and discourse element analysis. This approach offers insights into the direction and intensity of the impact of real-life circumstances on society, reveals how people react to different discourse elements, and uncovers patterns of growth, decline, cohesion, and polarization within these elements.\n\nThis study aims to contribute to urban sociology and community studies by examining the relationship between online discourse, particularly hate speech, and urban social dynamics. By analyzing social media data from New York City, San Francisco, and Seattle during significant events, we explore how online communities interact with offline urban social structures. Our approach, combining text classification and social network analysis, offers a comprehensive view of community formation in the digital age. By focusing on online hate speech in urban contexts, we provide insights into how digital interactions may reflect and potentially influence urban environments. This research may have implications for understanding contemporary urban communities and could inform discussions on urban decision-making and community engagement in increasingly digitally-connected cities.# Key Contributions of Our Research\n\n- Designing an Integrated Framework: We designed a novel framework that combines text classification with social network analysis to effectively identify and understand online communities.\n- Temporal Visualization of Community Dynamics: We created a method for temporal visualization to capture and represent the evolution of online communities over time.\n- Introducing Key Analytical Elements: Our research introduced fourteen critical elements derived from the structure and dynamics of online social media, providing comprehensive insights into social interactions and discourse patterns.# Ftci ittting Discourss Frtgmsntttion Ant ysis\n\nge established a krameoork that enables denision-makers to identikj mrimarj disnourses and their kralments, enhanninl the analjsis ok disnourse kralmentation and its sonial immlinationsr\n\nzonducting t ust - ifs ztss Study: ge ammlied our krameoork to a nase studj usinl real-oorld data, demonstratinl its ekkentipeness in analjzinl the kormation and depelomment ok online nommunitiesr\n\n\u2018ur oork nontributes silnikinantlj to the literature bj mropidinl a robust tool kor analjzinl the nommleq djnamins ok online disnourses, therebj okkerinl paluable insilhts into the intermlaj betoeen sonial media interantions and sonietal behapiorsr# 3. Msthodo ogict Frtmswork\n\nThis sention details the methodololinal krameoork emmlojed to inpestilate the kormation and epolution ok nommunities oithin sonial netoorksr }he krameoork, illustrated in Figure 1, nonsists ok kour internonnented nommonents, eanh ennommassinl distinnt mronesses that oill be elaborated umon in the subsequent subsentionsr\n\n|DATA COLECTION|DATA CLASSIFICATION|DATA VISUALIZATION|\n|---|---|---|\n|Define dataset scope: time range, mediageolocation;|Create labeled dataset with desired ciass \u20ac 5 similar kind text|Set network specifications: nodes-users, edges- interactions; colors-Iabels|\n|Create automatic scraper for the online platform Collect datal|Train classification model using Iabeled dataset and machine earning Igorithm|Identify connected networks and organize datasets by period|\n|Evaluate model and optimize parameters|Predict labels newdatasetl|Plot network t0 visualize communities emerging online|\n| | |Plot over time understand community bchavior with time|\n\nFigure 1. Conceptual framework developed for understanding community formation# APPLICATION\n\n- Thematic Analysi5\n- Interaciion Dynamics\n- Opinion Dynamics\n- Structural Insights# 3.1.Dttt zo sction\n\nRobust data nollention korms the nornerstone ok data-dripen researnhr In this studj, the konus is on understandinl the temmoral epolution ok online nommunities, ohinh nenessitates data that namtures nhanle oper timer }herekore, the nollented dataset must innormorate timestamms indinatinl the mrenise moment ok data nreationr# A nommunitj, oithin this nonteqt, is dekined as an allrelate ok indipidual sonial media users and their nollentipe antionsr\n\nDemartinl krom mrior studies that emmhasized lroum behapior, this researnh aims to identikj indipidual users oithin their smenikin leolramhin nonteqtsr ionsequentlj, lonation inkormation is a nritinal nommonent ok the datasetr }his konus alloos kor the inpestilation ok hoo online nommunities korm and depelom around silnikinant nirnumstannes, oith nities serpinl as the mrimarj lens kor obserpationr# Follooinl the dekinition ok the dataset\u201es kej attributes and the establishment ok the studj\u201es snome, an automated mronedure kor data nollention is immlementedr\n\nThis mroness is depelomed to lather }oitter toeets based on the kollooinl nriteria\u00b8\n\n- vtartinl oith a list ok leolramhinal areas \ud835\udc34 , eanh area \ud835\udc4e! is dekined bj querjinl motential neilhborhoods and aliases oithin that arear For instanne, ik \ud835\udc4e\" = \u201c\u00b4os Anleles\u201d, the norresmondinl querj kor \ud835\udc4e\" oould be\u00b8 t\u201c\u00b4os Anleles\u201d{ AND tt\u201c\u00b4os Anleles iountj\u201d{ \u2018R t\u201c\u2018ranle iountj\u201d{ \u2018R t\u201cRiperside iountj\u201d{ \u2018R t\u201cVentura iountj\u201d{ \u2018R t\u201c\u00b4onl \u00a8eanh\u201d{ \u2018R etnr{r\n- ghen an nirnumstanne \ud835\udc60# takes mlane on date \ud835\udc61, the data nollention meriod smans krom \ud835\udc61\u2212 \u0394$ to \ud835\udc61 + \u0394%, ohere \u2206 ranles betoeen y and zx dajsr\n- Additional kilters nan be ammlied to minimize irrelepant data; kor eqammle, ik all leolramhinal areas in \ud835\udc34 are in the \u2019nited vtates, the querj oould innclude\u00b8 AND tnountrj\u00b8 \u201c\u2019v\u201d{ AND tlanluale\u00b8 \u201cen\u201d{r\n- Eanh data entrj nenessitates the innlusion ok essential attributes sunh as hnreated_ath to denote the mrenise time ok toeet mublination in \u2019niq kormat, huser_idh to identikj the author ok the toeet, the htoeeth itselk to namture the teqtual nontent ok the most, and a unique hidh to serpe as a distinntipe identikierr\n\nDue to the sheer polume ok dailj sonial media antipitj, namturinl all mosts kor a lipen nitj oithin a dekined timekrame is inkeasible, martinularlj kor denselj momulated areasr vammle size selention must nonsider the lepel ok enlalement oithin the nitj durinl the nirnumstanne under inpestilationr\n\nPrepious studies hape okten utilized sammles ok ammroqimatelj wx,xxx mosts mer oeek, ohinh alilns oith the doonload limitations immosed bj manj momular sonial media APIsr\n\n}he immortanne ok sammle size is ammlikied in the \u201cData Visualization\u201d mhase ok this researnh, as it direntlj immants the nonstruntion ok the sonial netoork, ohinh is demendent on user interantionsr Ik the sammle size is too small, resultinl in insukkinient interantions, it maj not adequatelj rempresent the djnamin nhanles onnurrinl oithin the nommunitj, nenessitatinl kurther data nollention iterationsr\n\nA more in-demth disnussion ok netoork nreation is mresented in subsequent sentionsr# 3. Classification\n\nClassification, a cornerstone of Natural Language Processing (NLP), aims to categorize textual data into predefined classes or labels. This supervised learning process necessitates a carefully constructed training dataset that accurately represents the data to be classified. Ideally, this dataset mirrors the structure and content of the target data, with each observation meticulously labeled by human experts or through crowdsourced efforts. However, advancements in machine learning have led to the availability of numerous labeled datasets, selecting appropriate training data is paramount. Utilizing unsuitable training sets can result in inconsistent, misleading, or biased outcomes.\n\nMachine learning models are not without limitations, and researchers must carefully consider these constraints. The discussion section will address the ethical implications associated with these limitations. In the present context, several crucial factors must be considered when selecting or constructing training datasets for NLP tasks:\n\n- Structural Alignment: The textual structure of the training data should closely resemble the structure of the data to be classified.\n- Linguistic Consistency: Maintaining a consistent language throughout the process is essential. While automating translation tools may be necessary for less common languages, they can introduce errors, particularly when dealing with slang and ephemeral expressions.\n- Contextual Relevance: The context in which the data was generated should be thoroughly understood and evaluated for its applicability to the current task.\n- Source Reliability: When utilizing external training datasets, researchers must ensure the reliability of the source and have a clear understanding of the mechanisms employed for the labeling process.\n\nWhile not exhaustive, these considerations form a foundation for data reliability. In the context of this framework, classes should represent distinct groups identifiable within an online community, such as political affiliations or topics of interest.\n\nOnce a suitable training dataset is obtained, the next step is to select an optimal classification model. The literature highlights the superior performance of deep learning models over traditional machine learning methods in numerous text classification tasks. Summesnu and Romero (year) suggest that even within the realm of deep learning, model performance can significantly depend on the specific task and the nature of the data. To determine the most suitable model, an empirical approach is adopted, involving...# Evaluating Several Models and Selecting the One with the Best Performance According to the Metrics Used Based on the Tasks\n\nThis study addresses a multi-class text classification problem, for which neural network models like \"Long Short-Term Memory\" (LSTM) (Hochreiter and Schmidhuber, 1997; Summers and Romero, 2018) and transformer-based models like \"BERT\" (Devlin et al., 2018; Gonz\u00e1lez-Ib\u00e1\u00f1ez and Garrido-Mernh\u00e1n, 2020) have demonstrated remarkable effectiveness.\n\nBefore fitting the data to a chosen model, preprocessing is necessary. For both LSTM and BERT, this involves cleaning the data, removing links, punctuation, numbers, and stop words, lemmatizing the text, and creating a matrix of word embeddings. The labeled dataset is then partitioned into an 80% training set and a 20% testing set, with classes serving as target variables and word embeddings as features. Models are trained on the training set and then evaluated on the testing set to generate predictions. The accuracy of the model is assessed by comparing the predicted classes to the original classes. This process is iterative, with model parameters adjusted and the model retrained until optimal accuracy is achieved. The model with the highest accuracy is then selected to classify the unlabeled data, assigning a class to each social media post. The final output is the original dataset augmented with the predicted class target variables. The set of assigned classes in the dataset is denoted as \ud835\udc36 = {\ud835\udc50\u2081, \ud835\udc50\u2082, \ud835\udc50\u2083, . . . , \ud835\udc50\u2096}.# 3.3. Visualization Methodology\n\nTo visually represent the dynamics of user interactions, a subset of the dataset, denoted as \ud835\udc4a{\ud835\udc4e, \ud835\udc60\u2081}, is transformed into a graph structure. In this graphical representation, users are represented as nodes, and the interactions between them (replies, retweets, and mentions) are denoted as edges. Each post's assigned class remains associated with the user node that originated the post, and a distinct color is used to visually distinguish each class.\n\nThe graph \ud835\udc3a, representing a specific area during a particular circumstance, is formally defined as \ud835\udc3a{\ud835\udc4e, \ud835\udc60\u2081} = (\ud835\udc48, \ud835\udc41, \ud835\udc36), where:\n\n- \ud835\udc41 denotes the set of interactions, \ud835\udc41 = {\ud835\udc5b\u2081, \ud835\udc5b\u2082, . . . , \ud835\udc5b\u2096}, with \ud835\udc56 = (\ud835\udc62\u2081, \ud835\udc62\u2082) \u2208 \ud835\udc4a{\ud835\udc4e, \ud835\udc60\u2081};\n- \ud835\udc36 signifies the set of classes for each \"sender\" node \ud835\udc62\u2081 and the corresponding target class, \ud835\udc36 = (\ud835\udc62\u2081, \ud835\udc50), where \ud835\udc62\u2081 \u2208 \ud835\udc48 and \ud835\udc50 \u2208 \ud835\udc36.\n\nNodes acting as \"receivers\" are not assigned classes. If a node appears as a \"sender\" in multiple interactions, it...# Online Sentiment Analysis\n\nThe expression of emotions and opinions on social media platforms, often referred to as \u201conline sentiment,\u201d is a widespread phenomenon. For these individual expressions to significantly impact a community, they typically require engagement or interaction from other users. In this context, it is noted that most posts that fail to generate any interaction are unlikely to play a substantial role in shaping community dynamics.\n\nTo enhance the clarity and interpretability of the network visualization, we exclude nodes that lack labels, as these represent users who have not actively participated in the discourse. This filtering process reduces noise and focuses the analysis on those users who are actively contributing to the formation and evolution of online communities.\n\nFigure 2 illustrates a social network graph where distinct communities are clustered and differentiated by color. While larger communities are readily apparent, smaller groups and even isolated individuals can be observed within the broader network structure. This is due to the fact that classification is based on post content rather than solely on interaction patterns. Notably, Figure 2 reveals the presence of a small blue group nested within a larger green group, illustrating the concept of a community (small hub) existing within a larger community (bigger hub).\n\nSocial media is often a platform for individuals to express emotions and opinions. To influence the community, posts should generate interactions. Thus, posts without interactions are filtered out to enhance visualization and reduce noise. Figure 2 illustrates a social network with clustered communities represented by different colors. Larger groups represent distinct communities, but individuals or small groups are also visible, classified by post content rather than group interaction. This is seen in the small blue group within the larger green group at the bottom right of Figure 2. The blue group (small hub) is a community within a community (larger hub).# Communities\n\n- Community A\n- Community B\n- Community C\n- Community D\n- Community E\n- Community F# Dataset Subset\n\nThe dataset subset \ud835\udc4a{\ud835\udc4e, \ud835\udc60#} is drawn from the data gathered for each area \ud835\udc4e over a specified time frame \ud835\udc61, where \ud835\udc61 falls within the range (\ud835\udc61start, \ud835\udc61end) indicating the period \ud835\udc51 days before and after the circumstance \ud835\udc61r. This method is applied consistently across all circumstances and areas, resulting in around 2\ud835\udc51 + 1 spatial visual representations of these communities over time, arranging these visualizations chronologically and justamosinl them according to circumstance, area, and time, to construct a comprehensive understanding of the dynamic formation and evolution of these communities within a broader context.# 3.4. Discours Fragmentation Analysis\n\nSeparating Twitter data offers insights into the dynamics and characteristics of social media. This analysis identifies fourteen key elements that can highlight broader societal trends and insights to provide a structured and theoretically grounded understanding. These elements are categorized into four main groups: Thematic Analysis, Interaction Dynamics, Opinion Dynamics, and Structural Insights.# 3.4.1. Thematic Analysis\n\nThe thematic analysis involves understanding the content and themes of discussions, reflecting the central topics driving engagement and societal concern.\n\n- Content Focus: Identifying the central themes or topics of discussions allows us to understand the primary drivers of engagement and attention on Twitter. This element focuses on what people are talking about and how these topics gain traction, serving as a mirror to contemporary societal interests.\n- Sociocultural Discourse Themes: Identifying overarching themes and topics driving public discourse provides insights into the focal points of societal concern and interest. This element helps to map out the larger conversations shaping public opinion and societal priorities.\n- Cultural and Societal Trends: Examining how cultural shifts and societal trends manifest in social media discussions provides a reflection of changing societal values and norms. This analysis can reveal deeper cultural transformations and emerging societal narratives.# 3.4.2. Interaction Dynamics\n\nInteraction dynamics focus on the structure of user interactions, the flow of information, and the relational aspects of social media networks.\n\n- Relational Dynamics: Exploring the structure of user interactions reveals how users are connected and how information flows between them. This element is crucial for understanding the network properties of social media, including patterns of influence and communication.\n- Influential Participants: Pinpointing key users who initiate and influence discussions helps understand their impact on shaping the narrative. This element focuses on the role of influential actors in driving and moderating social media conversations.# 3.4.3. Opinion Dynamics\n\nOpinion dynamics involve the formation, evolution, and polarization of opinions within the social media landscape.\n\n- Opinion Formation and Evolution: Ranking how opinions on specific topics develop and change over time provides a dynamic picture of shifting sentiments and trends. This element highlights the processes through which public opinions are formed and transformed.\n- Polarization: Analyzing how different opinions become more extreme as like-minded individuals reinforce each other\u2019s beliefs sheds light on the polarization of opinions. This element examines the social mechanisms that lead to increased ideological divides.\n- Fragmentation: Observing the formation of different groups around specific topics or beliefs reveals how limited exposure to alternative opinions creates segmented communities. This element focuses on the fragmentation of public discourse and the creation of echo chambers.\n- Ephemerality: Noting the short lifespan of topics, which are quickly replaced by the next trending issue without deeper engagement or resolution, highlights the transient nature of social media discussions. This element emphasizes the rapid and often superficial nature of online discourse.\n- Historical Comparisons: Comparing current social media trends with past data helps identify historical patterns in public opinion and societal priorities. This temporal analysis offers a dynamic view of how social norms and discussions evolve over time.# 3.4.4. Structural Insights\n\nStructural insights focus on the overarching characteristics of social media discussions and their broader implications.\n\n- Reaction to Social and Political Issues: Exploring how users react to significant circumstances, changes, or societal movements provides a snapshot of public sentiment. This element captures the immediate and collective response of the social media landscape to external stimuli.\n- Dominance: Recognizing how certain points or topics overshadow others, often due to algorithmic amplification or sheer volume, highlights the asymmetries in social media influence. This element examines the structural inequalities in the visibility and impact of different participants and topics.# 1. Introduction\n\nDipersitj and Enho ihambers, Assessinl the eqtent ok nross-nuttinl dialolues that bridle dikkerent lroums persus nonpersations nonkined mredominantlj oithin nlosed netoorks repeals the delree ok ideololinal dipersitj and insularitjr. This element eqmlores the balanne betoeen eqmosure to diperse mersmentipes and the reinkornement ok eqistinl belieks.# 2. Discussion\n\nDisnussion iohesipeness, Identikjinl tomins that koster nohesipe persus kralmented disnussions okkers insilhts into the stabilitj and unikormitj ok nonpersations. This element assesses the eqtent to ohinh disnussions are intelrated or dipided, reklentinl the noherenne ok mublin dialoluer.# 3. Methodology\n\nThese kourteen elements are metinulouslj nrakted to okker a nommrehensipe mersmentipe on hoo sonial media data nan reklent broader sonietal nhanles and kunntion as a barometer kor mublin sentiment anross a oide smentrum ok tomins. Natelorizinl these elements into thematin, interantional djnamins, ominion-based djnamins, and struntural insilhts, oe mresent a theoretinallj robust krameoork kor nommrehendinl the intrinate djnamins ok sonial mediar.# 4. Discourse Fragmentation\n\nDisnourse kralmentation is a term that namtures the smlinterinl ok mublin nonpersation into smaller, okten isolated sub-disnussions or disnourses. It is nharanterized bj the lank ok a nentral narratipe or shared understandinl, leadinl to a snenario onde multimle nonpersations onnur in marallel, oith limited interantion or intelration betoeen them.# 5. Conclusion\n\nIn modern sonietj, esmeniallj online, this is okten ammlikied bj allorithmin nuration, selentipe eqmosure to media, and the enho nhamber ekkentr. Fonusinl on our mroblem in sonial media, oe nan see that disnourse kralmentation is understandable throulh ominion djnamins and struntural insilhts, as dekined mrepiouslj. Therefore, our kinal ekkort is to mresent our ammlination and analjsis lajer as a disnourse kralmentation analjsis tool, ohinh nan be inpaluable kor major mlajers in molitins, sonial sniennes, and lopernanner.# 6. Future Work\n\nThe idea ok introduninl seperal kej elements that nan hilhlilght broader sonietal trends and insilhts krom rao data and then usinl these elements to obtain a hilher lepel ok understandinl ok the theme ok our nommleq sjstem, and in its interantion djnamin, ominion djnamin, and strunture, and kinallj usinl these hilher lepels ok understandinl to analjze the disnourse kralmentation and its immant on nommunitj shaminl roots krom sjstems thinkinl mrinnimles and best mrantines ohinh oe disnussed in literature.# Figure 3\n\nShoos the underljinl idea kor our analjsis.# Discussion Coherence\n\nFigure 3. Workflow of Discourse Fragmentation Analysis for Twitter Data# 4. Experiment: On Online Hate Groups\n\nThis paper demonstrates the utilization of the suggested methodology to examine dynamic social networks. Tabpej et al. explored the consequences of lockdown measures on children's encounters with harmful content, emphasizing the significant influence online platforms can have on people's well-being. Tabpej et al. also underscored the intangible nature of online communities, particularly for authorities trying to monitor individuals responsible for spreading harmful online behavior like the dissemination of false information, online harassment, prejudiced language, or the encouragement of violent acts. From an ethical standpoint, classifying a user as a possible perpetrator of online offenses solely based on a single social media tweet, especially when utilizing artificial intelligence, raises significant concerns. This research refrains from singling out specific individuals, and usernames will be kept private, focusing instead on illustrating the temporal behavior of users engaging in various forms of online hate speech within the context of real-life circumstances.# 4.1. Data Acquisition\n\nTwitter, a widely used platform for textual data and social network studies, was chosen due to its readily available metadata, encompassing user identifiers, location information, timestamps, retweets, images, and references to other users. For this investigation, tweets originating from New York City, Seattle, and San Francisco were gathered in real-time.# Current National Circumstances\n\n- The original designation of coronavirus as a global pandemic on March 11, 2020, leading to widespread shutdowns and restrictions on mobility. The data collection period for this circumstance spanned from March 11 to March 31, 2020.\n- The trailing killing of George Floyd by a police officer on May 25, 2020, which ignited the \u201cBlack Lives Matter\u201d movement protesting police brutality. The data collection period for this circumstance spanned from May 28 to June 24, 2020.\n- The highly contested presidential election on November 3, 2020, occurring after a four-year term marked by Donald Trump's presidency and widespread critique of his handling of the pandemic. The data collection period for this circumstance spanned from October 19 to November 4, 2020.# 4. Classification\n\nTo identify groups disseminating harmful language online, tweets need to be classified based on the nature of the language employed. Since some encompass a variety of intentions and expressions, this research categorizes tweets by the type of harmful language used, thereby recognizing communities as groups engaging in such behavior. The annotated dataset utilized for training the model capable of assigning multiple classes was derived from prior research (Tollas et al., 2020), which utilized online comments and integrated machine learning techniques, human labeling, and validation through crowdsourcing.\n\nThe eight categories of harmful language used are:\n\n- Verbal abuse for harmful language targeting gender\n- Racism for language expressing racial bias\n- Xenophobia for language expressing prejudice against individuals based on their country of origin\n- Ableism for language discriminating against individuals with disabilities\n- Homophobia for harmful language directed at individuals based on their sexual orientation\n- Religious Intolerance for harmful language targeting religious beliefs\n\nTwo distinct models underwent training and assessment using this dataset (W). The \"ER\" model attained an accuracy of 75%, whereas the \"vM\" model reached 86%, upon its superior performance. An \"vM\" model capable of handling multiple classes was then utilized to assign each tweet within the collection a category from the predefined set of eight. Each category was assigned a score ranging from x to y, calculated as a normalized score.# 4.3. Visualization\n\nThe original dataset, \ud835\udc4a, is partitioned into nine distinct subsets denoted as \ud835\udc4a{\ud835\udc4e, \ud835\udc60#}, where \ud835\udc4e represents the metropolitan areas New York City, San Francisco, Seattle, and \ud835\udc60# signifies the circumstances surrounding the coronavirus pandemic, Black Lives Matter, and elections. For each of these subsets, a corresponding graph \ud835\udc3a{\ud835\udc4e, \ud835\udc60#} is generated to enhance visual clarity and reduce noise caused by an abundance of nodes and edges. Nodes disconnected from the primary graph structure are removed, retaining only the most extensive connected subgraph, referred to as \ud835\udc3a{\ud835\udc4e, \ud835\udc60#}. Within the timeframe of each circumstance, the graph for a specific day incorporates tweets from that particular day.\n\nFigure 5 and Figure 6 depict the outcomes for New York City and Seattle during the coronavirus pandemic, whereas Figure 7 and Figure 8 showcase the results for San Francisco during the Black Lives Matter movement and the recent presidential election. Supplementary information regarding community behavior in San Francisco at the onset of the pandemic, and in New York City and Seattle during the Black Lives Matter movement and the presidential election, can be found in the appendix.\n\nThese sequences illustrate the emergence of communities formulating various forms of harmful language at a frequent manner. Nevertheless, the transient nature of Twitter interactions often leads to these groups lasting for only a brief period, typically two to three days. For instance, in Figure 8, the initial quadrant type x/y9 displays a blue community associated with a phenomenon. In the subsequent day, this group diminishes in prominence as a new, nonmarable group takes shape. By the third day, the original group has dissipated, while the second group gains strength. By the fourth day, both groups have vanished entirely.# Figure 4. Users engaging in hate speech tweets\n\n|City|Tweets|Users|Multi-hate users|\n|---|---|---|---|\n|New York City|1,139,236|457,503|105,904|\n|San Francisco|1,305,772|567,557|131,700|\n|Seattle|1,286,526|530,365|128,086|# 7. The Black Lives Matter Movement in San Francisco\n\nThe numerous red dots scattered across the graph, particularly during the initial period,\nsignify the presence of activist language throughout the platform, not limited to specific communities.# 6. The Coronavirus Pandemic in Seattle\n\nA similar trend is observed in Seattle during the coronavirus pandemic, where phenomena are\nwidely distributed among users.# 4.4. Discourse Fragmentation and Analysis\n\nThe goal of the examination and analysis layer of our framework is to help decision-makers gain a deeper understanding of how a primary discourse, such as discrimination and hate speech, and its related elements\u2014sexism, racism, xenophobia, ableism, homophobia, and religious intolerance\u2014operate over time. It also examines the impacts of real-world circumstances on discourse fragmentation and the respective elements of different discourse fragments on the social system as a complex, living system. To achieve this goal, we introduced fourteen key elements that assist in thematic analysis, interaction dynamics analysis, opinion dynamics analysis, and structural insights.# Key Elements\n\n- Opinion Formation and Evolution: The proposed visualization methodology exemplifies the dynamic shifts in themes and trends; when utilized in real-time snapshots, it rapidly illustrates the transitions and developments in opinion formation and evolving trends.\n- Polarization: This visualization methodology reveals non-linear discourses and their interaction patterns, especially within inherently polarized discussions such as political debates. It elucidates the social mechanisms that contribute to increasing ideological divides.\n- Segmentation: The methodology highlights the emergence of distinct groups centered around specific discourses, emphasizing the fragmentation of public dialogue and the formation of echo chambers.\n- Exhaustivity: By providing comprehensive insights into the historical evolution of public opinion and societal priorities, the methodology showcases the duration and lifespan of dominant discourses and the rapid emergence of neo-trending issues.\n- Historical Comparisons: This methodology offers detailed insights into historical shifts in public opinion and societal priorities by comparing current and past social media trends. Analyzing multiple snapshots taken at different intervals highlights changes and developments over time.\n- Instruction to Social and Political Issues: The methodology provides valuable insights into user responses to significant societal changes or movements, acting as a barometer that captures public sentiment over periods ranging from a few days to several months.\n- Dominance: It illustrates how certain points or themes overshadow others, often due to algorithmic amplification or significant popularity. This highlights the...# Imbalances in Social Media Influence by Examining Structural Inequalities# 1. Diversity and Echo Chambers\n\nThe methodology adeptly highlights the nature of cross-cutting dialogues that bridge different forums, training connections between diverse social or political orientations.# 2. Discussion on Sensitivity\n\nWhen supplemented with other measures from NLP tools, this methodology distinctly discerns domains that lead to non-sensitized or fragmented discussions. It provides valuable insights into the stability and uniformity of conversations.\n\nFrom the analyses presented, we gain a deeper understanding of the reasons behind discourse fragmentation and its intricate patterns; analyzing discussion non-sensitivity reveals the mechanisms through which systemic discourses cluster together, enhancing engagement among participants. Consequently, echo chambers become evident when snapshots of the system indicate roots in particular discourses, as illustrated in Figure 5.\n\nDuring the Black Lives Matter movements, echo chambers from June 8th contributed significantly to tweets centered around #BlackLivesMatter. The dominance of certain discourses, as clearly demonstrated in the plots, underscores the power of each fragmented discourse and how they can absorb attention. This necessitates the contextualization of real-world circumstances with visual data to interpret public reactions, exemplified by the responses to the Black Lives Matter movement.\n\nMoreover, most fragmented discourses exhibit ephemerality, typically persisting for less than a day unless directly correlated with a real-world circumstance, an insight that our framework adeptly analyzes to elucidate the implications of discourse fragmentation. The segmentation of discussions by discourse emerges as a crucial strategy for dissenting discourse fragmentation, playing an indispensable role in shaping and evolving public opinion.\n\nAdditionally, a nuanced analysis of political discourses enables a comprehensive examination of polarization, thereby yielding valuable insights into the broader phenomenon of discourse fragmentation. To further quantify discourse fragmentation, we calculated the number of distinct communities identified in each timeframe and assessed the degree of overlap between them. Our analysis revealed a significant increase in the number of distinct communities during periods of heightened social tension, suggesting a fragmentation of the discourse landscape.\n\nThis fragmentation may contribute to the creation of echo chambers where harmful narratives are amplified and reinforced, hindering productive dialogue and exacerbating societal polarization.# 5. Discussion\n\nFigure 9 illustrates the distribution of classes across each class in the datasets. The three urban areas display similar mentalities of tweets, potentially reflecting online behavior in the United States during 2018. It\u2019s worth noting that \u201cXenophobia\u201d tweets were predominant, making up almost 60% of all hate speech found in New York City. The second most common type was \u201cHomophobia.\u201d\n\n|New York City|San Francisco|Seattle|Training Labels|USA 2018|\n|---|---|---|---|---|\n|100%|100%|100%|100%|100%|\n|90%|90%|90%|90%|90%|\n|80%|80%|80%|80%|80%|\n|70%|70%|70%|70%|70%|\n|60%|60%|60%|60%|60%|\n|50%|50%|50%|50%|50%|\n|40%|40%|40%|40%|40%|\n|30%|30%|30%|30%|30%|\n|20%|20%|20%|20%|20%|\n|10%|10%|10%|10%|10%|\n|0%|0%|0%|0%|0%|\n\nThese results differ significantly from a collection of tweets gathered from identical areas in 2018. In 2018, less than 40% of tweets contained \u201cXenophobia,\u201d and the second most common category was \u201cRacism.\u201d This variation between the two years may be attributed to the distinct socio-political contexts. In 2018, the global #MeToo movement against sexual harassment gained momentum, leading to increased online discussions and backlash against the movement. Conversely, 2020 was marked by the devastating impact of the coronavirus pandemic, during which the use of terms like \u201cChina virus\u201d by prominent figures fueled xenophobia and anti-Asian sentiment. While overall crime rates have declined, a report from the United States Commission on Civil Rights documented a significant rise in anti-Asian hate crimes from 2019 through 2020. This highlights the need for contextual analysis.# Current Page Title\n\nWhen interpreting crime statistics, as certain communities may experience disproportionate increases in targeted offenses despite broader trends.\n\nClassification models often grapple with mislabeling, a challenge exacerbated by the simplified assumptions of traditional machine learning approaches (Tabdidizaji et al., 2024). While these models excel at optimizing metrics like precision or recall, they often struggle to capture the nuances of non-meanings contextual systems, particularly when dealing with elements like fairness and discrimination. This limitation, as highlighted by Tarona and Velbst (2026), stems from an oversimplified understanding of the relationship between social context and technology. To address this, a more comprehensive approach is needed, one that considers the intricate interplay between social factors and technological systems.\n\nFurther, research by Vuresh and Uuttal (2029) reveals that many feature importance quantification methods in machine learning inadvertently perpetuate biases against marginalized groups. This is due to an implicit reliance on a singular, dominant measurement that overlooks the pluralistic, contextual, and international viewpoints essential to understanding fairness and discrimination. Empirical evidence from studies by Tuolamoini and Uebru (2028) supports this claim, demonstrating that machine learning models frequently rely on features that may be spuriously correlated with classes during training.\n\nFeminist epistemology critiques machine learning's feature importance quantification techniques, arguing they often embed biases that misrepresent the nuanced, situated, and interconnected measurements valued by marginalized groups (Tsannoq-\u00b4i and Kumar, 2023). This resonates with the broader critique within feminist technoscience, which emphasizes the ways in which technology can reinforce existing social inequalities (Turrell, 2026). Furthermore, empirical investigations reveal a tendency for machine learning models to seize upon features that might exhibit only a spurious correlation with target classes during the training process (Tuolamoini and Uebru, 2028). A spurious correlation, in this context, refers to a statistical association between a feature and a class that is not based on a causal relationship, but rather on incidental co-occurrences in the training data.\n\nFor instance, a model might associate the frequent use of the word \"humanitarian\" with negative sentiment due to its prevalence in negative contexts within the training data, even though the word itself is neutral. These spurious correlations can contribute to questionable classifications, as observed in some tweets classified as sexist or religiously intolerant in this study.# Conclusion\n\nIn conclusion, our study provides a novel framework for analyzing the dynamics of online communities and discourse fragmentation. By integrating text classification and social network analysis, we are able to identify and track the evolution of hate speech communities on Twitter. Our findings highlight the ephemeral nature of these communities, the impact of real-world events on their formation and dissolution, and the potential for discourse fragmentation to contribute to societal polarization. These insights have important implications for understanding the role of social media in shaping public discourse and for developing strategies to promote more inclusive and constructive online interactions.",
        "context_id": 14,
        "question": "What framework does the research propose for analyzing discourse fragmentation in online social networks?",
        "answer": [
            "Temporal Fusion Framework"
        ],
        "context_length": 56216
    },
    {
        "context": "# 1 Introduction\n\nIn the last few years, new AI systems have solved incredible tasks. These tasks include real-world games, such as chess [1] and Go [2\u20134], videogames such as Atari [5], Dota [6], and different robotics tasks [7\u201310]. These results have been mostly achieved through the intensive use of Reinforcement Learning (RL, [11]) with the rediscovered technology of neural networks and deep learning [12]. Usually, \u201cstandard\u201d RL focuses on acquiring policies that maximise the achievement of fixed assigned tasks (through reward maximisation) with a predefined collection of skills. New approaches have been proposed to enrich RL, allowing the agent to extend its initial capabilities over time inspired by neuroscience and psychology. Indeed, studies on animals [13\u201315] and humans [16\u201318] have explored the inherent inclination towards novelty, which is further supported by neuroscience experiments [19\u201321]. The field of intrinsically motivated open-ended learning (IMOL [22]) tackles the problem of developing agents that aim at improving their capabilities to interact with the environment without any specific assigned task. More precisely, Intrinsic Motivations (IMs [23, 24]) are a class of self-generated signals that have been used to provide robots with autonomous guidance for several different processes, from state-and-action space exploration [25, 26], to the autonomous discovery, selection and learning of multiple goals [27\u201329]. In general, IMs guide the agent in acquiring new knowledge independently (or even in the absence) of any assigned task to support open-ended learning processes [30]. This knowledge will then be available to the system to solve user-assigned tasks [31] or as a scaffolding to acquire new knowledge cumulatively [32\u201334] (similarly to what has been called curriculum learning [35]).\n\nThe option framework has been combined with IMs and \u201ccuriosity-driven\u201d approaches to drive option learning [32] and option discovery [36\u201339]. In the hierarchical RL setting [40], where agents must chunk together different options to properly achieve complex tasks, IMs have been used to foster sub-task discovery and learning [41\u201343], and exploration [26]. Autonomously learning and combining different skills is a crucial problem for agents acting in complex environments, where task solving consists of achieving several (possibly unknown) intermediate sub-tasks that are dependent on each other. An increasing number of works are tackling this problem [29, 44, 45], most focused on low-level, sub-symbolic policy learning [46], in turn combined in a hierarchical manner using some sort of meta-policy [47]. While promising, these approaches necessarily face the problem of exploration, which becomes slower and less efficient as the space of states and actions increases.# 2 Background\n\nTo reach a high level of autonomy, an agent acting in the low-level space, sensing the environment with its sensors and modifying it through its actuators must implement a series of layers of abstraction over its state and action spaces. As human beings reason over both simple and complex concepts to perform their activities, so robots should be able to build their own abstract representation of the world to deal with the increased complexity, using labels to refer to actions and events to be recognized and reasoned upon. In this paper, two levels of abstractions are applied: the first one, from primitive actions to options [11, 62] and the second one, from options to classical planning [49].# 2.1 From primitives to options\n\nAs discussed before, at the lowest level, the agent sees the world with its sensor\u2019s values and changes it through the movement of its actuators. The most common formalism at this stage to deal with this type of representation is the Markov Decision Process (MDP), which models the environment as the tuple:\n\n(S, A, R, T, \u03c9), (1)\n\nin which S represents the set of possible high-dimensional states where each s \u2192 S is described by a vector of real values returned by the agent\u2019s sensors, A describes the set of low-level actions a \u2192 A in some cases also called primitives, R the reward function where R(s, a, s\u2192) is a real value returned executing a from state s achieving s\u2192, T the transition function describing for T (s\u2192 |s, a) the probability of reaching the state s\u2192 executing a from s, and the discount factor \u03c9 \u2192 (0, 1] describing the agent\u2019s preference for immediate over future rewards. Usually, in this setting, the goal is to maximize return, defined as\n\nR = \u2211\u03c9i R(si, ai, si+1).\u2191 (2)\n\nHowever, dealing with the state and action spaces of the formulation (1) is, in certain cases, impractical due to the high dimensional spaces considered. An effective formalism introduced to reduce the complexity of the problems is the option framework [62]. The option is a temporally-extended action definition which employs the following abstracted representation:\n\no = (Io, \u03b5o, \u03d1o), (3)\n\nwhere the option o is defined by an initiation set Io = {s|o \u2192 O(s)} representing the set of states in which o can be executed, a termination condition \u03d1o(s) \u2191 [0, 1] returning the probability of termination upon reaching s by o, and a policy \u03b5o which can be run from a state s \u2192 Io and terminated reaching s\u2192 such that the probability \u03d1o(s\u2192) is sufficiently high. A policy is a function defining the behavior of the agent, mapping the perceived state of the environment to the action to be taken [11]. It is worth noting that options create a temporally-extended definition of the actions [62].# 2.3 Intrinsic Motivation\n\nThe impulse to drive the agent away from the monotony of its usual activities, which psychologists and cognitive scientists have studied under the name of intrinsic motivation, is one of the most important elements enabling Open Ended Learning (OEL). The research in the field of Intrinsic Motivation (IM) concerns the study of human behaviors not influenced by external factors but characterized by internal stimuli (i.e. curiosity, exploration, novelty, surprise). In the case of artificial agents, we can summarize such aspect as anything that can drive the agent\u2019s behavior which is not directly dependent on its assigned task.\n\nThe insights provided by the IMs gave the researchers new ideas to model the stimuli of the agent (e.g. curiosity). Indeed, some models have been implemented using the prediction error (PE) in anticipating the effect of agent\u2019s actions (and more precisely the improvement of the prediction error [64, 65]) as an IM signal. A formal definition of agent driven by its curiosity has been formulated by Schmidhuber [64] assimply maximizing its future success or utility, which is the conditional expectation\n\n[  \u2211T          \u2223h(\u21d0 t)],\n\nu(t) = E \u03bc     \u03c9 =t+1r(\u03d6 )\u2223\u2223                                            (10)\n\nover   t = 1, 2, . . . , T time steps, receiving in input a vector x(t), executing the action y(t), returning the reward r(t) at time t, taking into consideration the triplets h(t) = [x(t), y(t), r(t)] as the previous data experienced until time step t (also called history).\n\nThe conditional expectation E \u03bc (\u00b7|\u00b7) assumes an unknown probability distribution \u03bc from M representing possible probabilistic reactions of the environment. To maximize (10), the agent also has to build a predictor p(t) of the environment to anticipate the effects of its actions. The reward signal is defined as follows\n\nr(t) = g(r ext (t), rint (t)),                                     (11)\n\nwhich is a certain combination g of an external reward r ext (t) and an intrinsic reward r int (t). In particular, r int (t + 1) is seen as surprise or novelty in assessing the improvements in the results of p at time t + 1\n\nr int (t + 1) = f |C(p(t), h(\u21d0 t + 1)), C(p(t + 1), h(\u21d0 t + 1))|,                         (12)\n\nwhere C(p, h) is a function evaluating the performance of p on a history h and f is a function combining its two parameters (e.g. in this case, it could be simply the improvement f (a, b) = a \u2198 b). It is important to notice that, as a baby does, an intrinsically motivated agent needs to find regularities in the environment to learn something. Consequently, if something does not present a pattern, there is nothing to learn and this becomes boring for both an agent and a human being.\n\nIn literature, IMs have also been categorized into different typologies [66\u201368]. An important discriminant aspect is the kind of signal received by the agent which can be of two types: knowledge-based (KB-IMs), which depends on the prediction model of the world (e.g. [64]), and competence-based (CB-IMs), which depends on the improvement of the agent\u2019s skills (e.g. [27]). In the framework presented in the next section, both these typologies are employed. CB-IM is used at a lower level to learn new skills and KB-IM at a higher level to push the system to focus on the frontier of the visited states, from which it is more likely to discover novel information (e.g., find new states and learn new actions).# 3 System Overview\n\nThis section presents a new framework of an open-ended learning agent which, starting from a set of action primitives, is able to (i) discover options, (ii) explore the environment using them, (iii) create a PPDDL representation of the collected knowledge and (iv) plan to improve its exploration while reaching a high-level objective set by the game.# The aim of this study is to assess the potential of abstraction in autonomous systems\n\nThe aim of this study is to assess the potential of abstraction in autonomous systems and propose a new approach for planning systems, extending them with learning capabilities and behaviors driven by IMs. IMs are employed for discovering new options in a surprise-based manner at low-level and continuously exploring new states driven by curiosity at high-level. By the term abstraction, we simply mean mapping a certain problem space into a simpler one (e.g. converting a continuous domain into a discrete domain). In the proposed architecture, the abstraction is applied at two levels: a) passing from the primitive action space to the options action space and b) converting low-level data collected during the exploration into a high-level domain representation suitable for high-level planning, thus from raw sensors\u2019 data to a PPDDL representation.\n\n|LTA|Planner|KB|\n|---|---|---|\n|High-level system|Abstraction/dispatching|EX|\n|Dispatcher|Abstraction| |\n|Low-level system|Option|Controller|\n|Exploration|ID, TD| |\n| |ID; ID'|KBo|\n\nFig. 2: The architecture of the system. The system, equipped with five primitive actions, iteratively (1) learns some options from scratch, (2) uses the options to explore the environment and gather low-level states data, (3) creates a PPDDL representation of the collected experience, (4) plan to solve the game and to improve its exploration.\n\nPerforming the pipeline depicted in Figure 2, the system creates different layers of abstraction, enriching the agent\u2019s knowledge with causal correlations between options and enabling more efficient reasoning (i.e. using classical planning). Symbols can be seen as knowledge building blocks that can be used to search for interesting states and find new knowledge in a virtuous loop.# 3.1 Architecture description\n\nAs depicted in Figure 2, the system can be seen as a three-layered architecture: (i) the higher level contains the explicit agent\u2019s knowledge, (ii) the middle layer maps the high-level actions to their controllers and convert the raw sensors data into an explicit representation, and (iii) the lower level containing the components to sense the environment and interact with it. Mainly, the system executes the following pipeline:\n\n9# 1. Option Discovery\n\nusing a set of primitives A = {a1, a2, ...} belonging to the agent, the system combines them to create higher level actions, in this case, the options;# 2. Exploration\n\nthe set of options O = {o1, o2, ...} discovered in the previous step is used to explore the environment. In the meantime, the visited low-level states data are collected into two datasets: the initiation data ID and transition data TD containing, respectively, the samples of states in which an option can be run and the transition between states executing it;# 3. Abstraction\n\ndatasets ID and TD of the visited low-level states until that moment are processed by the algorithm of abstraction, generating a PPDDL domain D of the knowledge collected;# 4. Planning\n\nthe PPDDL representation D is used to assess whether the final goal of the task sg can be reached with the currently synthesized knowledge, and to generate a plan \u03c2EX to explore interesting areas of the domain. This plan is suggested by the Goal Selector, function included in the Long-Term Autonomy (LTA) module.# 5. Execution Loop\n\nThe system, coordinated by the LTA, will execute again the loop from step 1, exploiting \u03c2EX to improve the Option Discovery and Exploration phases.# Algorithm 1: Discover-Plan-Act algorithm\n\n1:  procedure DISCOVER_PLAN_ACT(cycles, dpa_eps, dpa_steps, d_eps, d_steps)\n2:       c \u2192 0 //Cycle initialization\n3:       O \u2192 {} //Option set initialization\n4:       ID \u2192 {} //Initiation Data initialization\n5:       TD \u2192 {} //Transition Data initialization\n6:       \u03c9EX \u2192 {} //Initially, the high-level plan is empty\n7:       while c < cycles do //For each cycle\n8:            Onew \u2192 DISCOVER(d_eps, d_steps, \u03c9EX) //Learning the available options\n9:            O \u2192 O \u2191 Onew\n10:             IDnew ID \u2191 IDnew, TDnew \u2192 Collect_Data(dpa_eps, dpa_steps, O, \u03c9EX)\n11:             ID \u2192 TD \u2191 TDnew\n12:             TD \u2192\n13:             D \u2192 Create_PPDDL(ID, TD)\n14:             starget \u2192 Get_Target_State()\n15:             Ptarget \u2192 Generate_PPDDL_Problem(starget)\n16:             \u03c9EX \u2192 Plan(D, Ptarget)\n17:             Check_PPDDL_Validity(D)\n18:             c \u2192 c + 1\n19:        end while\n20:   end procedure\n\nIn this setting, the agent is initially only endowed with a set of primitive movements A = {a0, ..., am}, and the world s \u2192 S is represented in terms of a vector (v0, ..., vn) of low-level variables vi \u2192 R, whose values as retrieved by the agent\u2019s sensors.\n\nThe iterative utilization of this framework allows the synthesis of an emerging abstract representation of the world from the raw data collected by the agent, which continuously undergoes a refinement process over time, as it gets enriched with new.# 3.1.2 Exploration\n\nAfter discovering a set of valid options O as explained in the previous section, the system exploits them to explore the environment, collecting data about the reached low-level states (Algorithm 1, line 10). Considering that the abstracted representation of the world does not change significantly with a small amount of new data, the function Collect_Data() is in charge of executing d_steps options for d_eps episodes, in which the agent starts its exploration from the initial configuration of the environment.\n\nAt each timestep, the agent attempts to perform an option o \u2192 O from a certain low-level state s \u2192 S. The selection of the action o during the exploration can follow different strategies, which are described in the subsection 3.1.4. In case the execution of\n\n1 Link to the original implementation of [59]: https://github.com/sd-james/skills-to-symbolsthe option changes the low-level variables of the state s and, consequently, the mask 2 m is not null, the system registers two types of data tuple (Algorithm 1, line 10): the initiation data tuple id and the transition data tuple td. The multiple instances of these tuples are stored, respectively, in the datasets ID, for the initiation data, and T D, for transition data. A single initiation data tuple idi has the following structure\n\nidi = (s, o, f (s, o)), (15)\n\nwhere the function f (s, o) returns the feasibility of executing o from s (True if s \u2192 I o and False otherwise). The transition data tuple tdj takes the following structure\n\ntdi = (s, o, r, s\u2192, g, m, O), (16)\n\nwhere s\u2192 is the state reached after executing option o from the state s, g is a flag stating whether the final objective of the task has been reached, m is the mask of the option and O\u2192 is a list defining the options that can be executed from s\u2192. When all the steps of the episode have been executed, the environment is reset and the next episode is started until reaching the maximum number of allowed episodes deps, where the Collect_Data() procedure terminates and the stored data are added to the existing datasets ID and T D (line 11-12).# 3.1.3 Abstraction\n\nThe datasets collected in the previous step are then used as input for the function Create_P P DDL() (Algorithm 1, line 13), returning a symbolic representation D of the agent\u2019s current knowledge expressed in PPDDL formalism (PPDDL domain). The main advantage of the obtained PPDDL representation is that it makes explicit the causal correlations between operators that would have remained implicit at the option level. In the following, we provide a summary description of the abstraction procedure; for further details, the reader is referred to the original article [59].\n\nThe abstraction procedure executes the following five steps:\n\n1. Options partition: this step is dedicated to partitioning the learned options into abstract subgoal options, a necessary assumption of the abstraction procedure. Abstract subgoal options are characterized by a single precondition and effect set. However, given the uncertainty of the actions\u2019 effects in the environment, the operators\u2019 effects will be modeled as mixture distributions over states. This phase utilizes the transition dataset T D collected before, as it captures the information about the domain segment the option modifies. Basically, the transition dataset is divided into sets of transition tuples presenting the same option o and mask m. Then, for each set, the partitions are ultimately obtained by performing clustering on the final states s\u2192 through the DBSCAN algorithm [69]. If some clusters overlap in their\n\n2 The mask is the list of all the state variables that are changed by the execution of a specific option. See details in [59]. 3 An abstract subgoal option o is characterized by a list of indices of the low-level variables, called mask, which are changed with the execution of o, without modifying other variables. In addition, the changing variables\u2019 values do not depend on their initial value.\n\n13# 2. Precondition estimation\n\nThis step is dedicated to learning the classifiers that will identify the preconditions of the PPDDL operators. In order to have negative examples of the initiation set classifier for each operator, this operation utilizes the initiation dataset ID considering all the samples with option o and f(s, o) = False. The positive examples comprise instead the initial states s taken from TD tuples belonging to the same partition. The initiation set classifier of the option is computed using the Support Vector Machines (SVM) [70]. The output of this phase is the set of all the initiation set classifiers of all the operators.# 3. Effect estimation\n\nAnalogously, this step is dedicated to learning the symbols that will constitute the effects of the PPDDL operators. The effects distribution is modelled through the Kernel density estimation [71, 72], taking in input the final states s\u2192 of each partition.# 4. PPDDL Domain synthesis\n\nFinally, this step is dedicated to synthesising the PPDDL domain, characterized by the complete definition of all the operators associated with the learned options in terms of preconditions and effect symbols. This step entails the simple mapping of all the data structures generated during the previous steps in terms of symbolic predicates to be used as preconditions and/or effects for every operator.\n\nThe produced PPDDL domain can be potentially used to reach any subgoal that can be expressed in terms of the available generated symbols at any point during the Discovery-Plan-Act (DPA) loop. One interesting aspect of the proposed DPA framework is that the semantic precision of the abstract representation and its expressiveness increase as the DPA cycles proceed, as will be described in the experimental section.# 3.1.4 Goal Selector\n\nThis module aims at simulating the intrinsic motivations driving the agent towards interesting areas to satisfy its curiosity and optimize its exploration. In particular, one of the most fascinating aspects of this system is the capability of setting its high-level goals, which potentially could be a combination of symbols defining a state that the agent has never experienced before. In other words, abstract reasoning could be the driving criterion for using the imagination to explore an unknown environment.\n\nDespite the previous goal is rather ambitious and still the object of future work, we will demonstrate in this work that the abstract reasoning can indeed be used for the more \u201cdown-to-earth\u201d task of devising rational criteria to make more efficient the exploration of unexplored parts of the environment.\n\nThe selection of the target state starget \u2192 S to be reached in the next exploration cycle is performed at line 14 of Algorithm 1, calling the procedure Get_Target_State(). The Goal Selector suggests such state to the system, following an internal strategy which can be, in this case, Action Babbling, Goal Babbling and Distance-based Goal Babbling.\n\nAction Babbling is the simplest strategy of the system for the exploration, consisting of a pure random walking of the agent. This strategy returns starget = NULL, soThe curiosity level for a state is defined as\n\n\u21bc(s) = \u21d1sinit \u2198 s + Z\u21d1, (17)\n\nconsisting of the norm of the difference between the state vector of the agent at the beginning of each episode sinit and the visited state s. The low-level state starget is selected between the farthest visited states, as follows:\n\nstarget = max\u21bc(s), \u21d3s \u2192 Svisited. s (18)\n\nMore precisely, the low-level state starget of the exploration, is the state that maximizes the distance \u21bc(s). In the computation of starget, a Gaussian noise Z \u21d4 N(0, 1) is added, to facilitate the reaching of different states. The Goal Selector driven by \u21bc continuously pushes the agent towards the border of the already acquired knowledge (similarly to the idea presented in [74, 75], but with a different implementation) and is thus the main responsible for the knowledge increase of the agent. Moving towards the frontier states, the agent is more likely to encounter novel states thus maximizing the probability to acquire new symbolic knowledge, which can in its turn be used to synthesize new (e.g., more expressive) high-level goals to be eventually reached through planning, ultimately creating a virtuous loop in which the agent\u2019s capabilities of increasing its knowledge through environment exploration are iteratively enhanced.# 3.1.5 Translating low-level states into symbols\n\nThe peculiarity and, simultaneously, the biggest challenge of this software architecture is thus to use an abstract symbolic representation to manage the evolution of the agent\u2019s knowledge. Using a symbolic representation to describe a desired environment configuration is a powerful tool for an efficient exploration of the world.\n\nAfter selecting starget (line 14), the purpose of the planning module is twofold. On the one hand, it can be employed as usual to verify the reachability of the goal sg of the environment (i.e. get the treasure and bring it \"home\" in the Treasure Game, in line 17) and, on the other hand, it can be exploited to generate a plan driving the agent towards states, in our case starget, relevant to extend the knowledge of the system (line 16). In both cases, to take advantage of planning it is necessary to transform a low-level state into a high-level one. This operation requires finding the combination of propositional symbols\n\n!\u02c6target = {\u03c61, ..., \u03c6k} (19)\n\nthat best represent the portion of state space including starget. In other words, we look for a subset of symbols !\u02c6target whose grounding is starget. These symbols make it possible to generate the definition of a planning problem (line 15) which, together with the domain definition, can be used to perform planning and solve the problem (line 16).\n\nIn order to select the right symbols conjunction, the system creates the classifier Cltarget \u21d4 p(starget) approximating a distribution over starget. Cltarget is a SVM# 4 Experiment and Results\n\nThis section, dedicated to the experimental analysis, will first describe the dynamics of the environment, followed by an example of the system\u2019s cycle execution with its outputs and, finally, the overall results collected over different environment configurations.# 4.1 Environment Setup\n\nThe implemented system has been tested in the so-called Treasure Game domain [59]. In such an environment, an agent can explore the maze-like space by moving through\n\n4 \"Sets of low-level state variables that, if changed by an option execution, are always changed simultaneously\" [59].The system is tested in five different configurations of the environment, of increasing complexity: two small-sized (Figure 4a and 4b), two medium-sized (Figure 4c and 4d) and one complete instance (Figure 4e). For example, in the setting depicted in Figure 4e the agent has to: (i) pull the two levers on the top of the maze to open the doors, (ii) get the key which is used to (iii) unlock the bolt, (iv) get the treasure and (v) bring it back on top of the environment. The obstacles that pertain to the other configurations are shown in Table 1.\n\n|Domain|Levers|Keys|Bolts|Notes|\n|---|---|---|---|---|\n|domain1|1|0|0|None.|\n|domain2|0|1|1|None.|\n|domain3|1|1|1|None.|\n|domain4|1|1|1|Shortcut available.|\n|domain5|3|1|1|2 levers going to the treasure, 1 going home.|\n\nTable 1: Obstacles to be solved to end the game.# 4.2 The cycle\n\nFor exemplificatory purposes, a complete execution cycle of the system is briefly described in the following, showing the output of each phase of an intermediate cycle (cycle n. 10) performed on domain3 (see Figure 4c) in the Goal Babbling strategy case.# Option Generation\n\nAt the beginning of each cycle, the agent executes Algorithm 2 to collect some options exploiting the agent\u2019s primitives, before the exploration can commence. In the Treasure Game environment selected for this work, the agent executes d_eps = 1 episodes, composed by d_steps = 200 primitive actions. After the execution of the plan \u03c2 EX, the random exploration is reprised using the primitives contained in A (see Section 2.1). The result is the following set of learned options (11 in total) following the formalization (14):\n\nO = { (go_up,{}), (go_down,{}), (go_left,{}), (go_left,go_up), (go_left, go_down), (go_left,interact), (go_right,{}), (go_right,go_up), (go_right,go_down), (go_right,interact), (interact,{}) }.\n\nIt is important to note that, in general, the discovered options are not all the options that may be possibly discovered in the environment, but only those experienced by the agent during the exploration. This procedure is incremental, adding options to the set O each iteration of the Algorithm 1. As described in section 3.1.1, this procedure leverages IMs at low level, capturing the curiosity of the agent when it discovers to have new available primitives to exploit.# Treasure Game Domain Definition\n\n(define (domain TreasureGame)(:requirements :strips :probabilistic-effects :rewards)\n(:predicates(notfailed)\n(symbol_0)\n(symbol_1)\n(symbol_2)\n...\n(symbol_25)\n)\n(:action option-0-partition-0-0:parameters ()\n:precondition (and (notfailed) (symbol_5) (symbol_22)(symbol_14))\n:effect (and (symbol_6) (not (symbol_5)) (decrease(reward) 36.00))\n)\n...\n(:action option-10-partition-3-715:parameters ()\n:precondition (and (notfailed) (symbol_20) (symbol_22)(symbol_14) (symbol_15) (symbol_0))\n:effect (and (symbol_13) (not (symbol_20)) (decrease(reward) 90.00))\n)\n)\n\nFig. 5: An extract of the PPDDL generated by the abstraction procedure. Notice that option-0-partition-0-0 can be explained by looking at the symbols of Figure 6. In fact, the effect is to change the x position replacing symbol_5 with symbol_6, maintaining invariant the presence of symbol_14 and symbol_22. Consequently, it is the option moving the agent on the right on the last floor.# Exploration\n\nIn this step, the plan \u03c2 EX is again executed before starting the random walk over the available options O. In this particular example, 600 data entries have been collected in the ID dataset and 6600 in the TD dataset. It is important to remember that the number of collected data over the cycles is not constant because when an option does not produce effects on the environment, no data is collected.# Abstraction\n\nFigure 5 shows a part of the output of the PPDDL domain obtained from the abstraction procedure. As visible, the figure does present a valid PPDDL domain description, upon which the planner may reason. Moreover, any subset of the produced domain predicates (symbols) can be used to define high-level goals the planner may try to plan for. In this specific case, the system generated 26 symbols and 716 operators.\n\n20# Planning\n\nThe produced PPDDL domain is used by the system to (1) solve the entire game, or task, and (2) improve the exploration. Figure 7 illustrates two PDDL problems: the problem on top refers to the general goal of solving the game (Pg) while the problem at the bottom refers to the goal dynamically synthesized by the Goal Selector module, which in this example relates to reaching the left corner of the middle floor (Ptarget). The solution for both problems is presented in Figure 8. The first solution, \u03c2g, solves the game problem in 21 moves and the second one, \u03c2EX, reaches the Goal Selector goal in 6 moves.# 4.3 Results\n\nIn this section, the overall results of the system over different domain instances are described. First, the setting of the environment is discussed, then the adopted baseline, as well as the other strategies enabling the planning exploration. Subsequently, some charts are presented, that highlight the system\u2019s performance in terms of success ratio on the planning task. Finally, some issues worth being underscored are commented, and the limitations of the employed technologies are discussed.\n\nFor our purposes, the Treasure Game5 has been used with five different mazes (see Figure 4), to focus on the performances of the system on smaller domains, highlighting pros and cons of the symbolic approach proposed. The system has been executed, following the workflow described in Algorithm 1, in the cited five mazes configurations using parameters suitable to solve the tasks, which are described later. To summarize,\n\n5 Github repository: https://github.com/sd-james/gym-treasure-game# Problem Definitions\n\n(define (problem task_goal)(:domain TreasureGame)\n(:init (notfailed) (symbol_0) (symbol_1)(symbol_2) (symbol_3)\n(symbol_4) (symbol_5) )\n(:goal (and (notfailed) (symbol_4)(symbol_17) (symbol_18)) )\n)\n\n(define (problem im_goal)(:domain TreasureGame)\n(:init (notfailed) (symbol_0)(symbol_1) (symbol_2)\n(symbol_3) (symbol_4)\n(symbol_5) )\n(:goal (and (symbol_13) (symbol_24)(symbol_14) (symbol_2)\n(notfailed)) )\n)# Figure 7\n\nOn the top, the problem of solving the task is synthesized by the system Pg and, on the bottom, the problem generated by the Goal Selector module Ptarget.\n\nThe system iteratively (i) looks for new options Onew performing dsteps primitives for deps episodes, (ii) explores for dpa_eps episodes the maze executing dpa_steps, (iii) creates a symbolic abstraction of the domain D, (iv) creates a plan \u03c2EX to optimize the exploration of the successive cycle c + 1. It is important to note that the autonomously discovered options successfully produced a valid high-level model, usable to build correct plans. To assess the accuracy of the current abstraction D, we evaluate its ability to reach the goal sg at the end of each cycle. This is done by requesting the planner to solve the problem Pg using the currently produced symbolic domain description D.\n\nThe first strategy used in the experiments is the random walk, which is called Action Babbling [50] (see Section 3.1.4). This strategy simply executes the Algorithm 1 without using planning because the Goal Selector returns starget = N U LL and no plan \u03c2EX is generated. This simple strategy is used as a baseline for the experimental analysis, and it is necessary to fully appreciate the advantages of using the symbolic approach. The exploration is equivalent to the one used by Konidaris [59], and we use it to observe the development of the symbolic model over time.\n\nTo support the use of symbolic planning, two other strategies have been considered: Goal Babbling and Distance-based Goal Babbling (see Section 3.1.4). These three strategies could be seen as having an increasing complexity and effectiveness in the exploration:\n\n- Action Babbling selects completely random actions;\n- Goal Babbling selects random goals, reaching them with a planned sequence of actions;# PLAN TASK GOAL:\n\n|1|(go_down,{})|; climb down the stairs|\n|---|---|---|\n|2|(go_left,go_down)|; go left until it can go down|\n|3|(interact,{})|; pull the lever|\n|4|(go_right,go_down)|; go right up to the stairs to go down|\n|5|(go_down,{})|; climb down the stairs|\n|6|(go_right,{})|; go right up to the end of the corridor|\n|7|(go_left,{})|; go left up to the end of the corridor|\n|8|(interact,{})|; take the key|\n|9|(go_right,go_down)|; go right up to the stairs|\n|10|(go_down,{})|; climb down the stairs|\n|11|(go_right,interact)|; go right up to the bolt|\n|12|(interact,{})|; unlock the bolt|\n|13|(go_left,go_down)|; go left until it is possible|\n|14|(interact,{})|; get the treasure|\n|15|(go_right,go_up)|; go right up to the stairs|\n|16|(go_up,{})|; go upstairs|\n|17|(go_right,{})|; go right up to the end of the corridor|\n|18|(go_left,go_up)|; go left up to the stairs|\n|19|(go_up,{})|; go upstairs|\n|20|(go_left,go_up)|; go left up to the stairs|\n|21|(go_up,{})|; go up to home|# (e) domain 5\n\nFig. 9: The figure depicts the success rate of \u03c2 g using different exploration strategies over time. In figure 9a, 9b, 9c, 9d and 9e, we have respectively the results of the domain of figures 4a, 4b, 4c, 4d, 4e over 15 cycles of exploration.\n\n24# Table 2: Main parameters used in the different environment configurations.\n\n|Domain|dpa_eps|dpa_steps|minimal solution steps|\n|---|---|---|---|\n|domain1|4|50|11|\n|domain2|4|50|13|\n|domain3|4|150|19|\n|domain4|4|200|15|\n|domain5|4|800|31|\n\nThe charts of Figure 9 are depicted cycles on the x-axis, and the percentage of trials that have succeeded in solving the game in the specific cycle on the y-axis.\n\nIn the smaller domains domain1, domain2, domain3 and domain4, the planning contribution is limited and sometimes not visible at all. Especially in the simplest domain domain1 the three strategies present similar performances, solving in the last cycles 80-90% of the trials (Figure 9a). In domains domain2 and domain3, presenting slightly higher complexities, the advantages of executing plans start being visible (Figure 9b and 9c). It is evident that Goal Babbling behaves almost perfectly in the last cycles, demonstrating the usefulness of collecting transition data with reasonable sequences of actions. The combination of randomness in selecting the goal to be reached and the reasoned transitions speed up the exploration and the consequent maturity of the PPDDL representation of the environment. Instead, the Distance-based Goal Babbling seems better than a completely random search but less effective than the precedent. This result entails that applying an exploration focused on the frontier\u2019s goals is not convenient in smaller domains. In fact, in the conclusive cycles of domain2 and domain3, Goal Babbling maintains 90-100% of success ratio, Distance-based Goal Babbling around 80% and Action Babbling between 60-70%.\n\nAlthough the problem faced by domain3 seems easier than domain4, presenting respectively minimal solutions of 19 and 15 steps (see Table 2), the system needs more steps per episode to solve domain4 using the same amount of episodes used by domain3. The reason of this behaviour is due to the higher complexity of synthesizing a PPDDL domain D for this scenario. In fact, the scenario domain4 presents a sort of \"shortcut\" to reach the treasure, which does not require collecting the key and opening the door. From the point of view of the abstraction procedure, the shortcut represents a branch in the possible choices of the agent, thus presenting the agent with additional concepts to be abstracted in order to synthesize a complete representation. Consequently, the system generates additional symbols and operators, requiring more experience to strengthen the transition model captured by the PPDDL and provide satisfying plans.\n\nThe significance of the frontier exploration emerges in the domain5 configuration, where planning evidently boosts the results of the agent. Indeed, being bigger than other domains, domain5 is more difficult to be solved and planning results to be efficient in driving the exploration of the agent immediately towards the borders of its knowledge. The main result highlighted by the charts is that in bigger domains (Figure 9e), the impact of using planning is evident. Planning is able to easily drive the agent towards interesting visited states, where it can continue exploring. Statistically, after collecting the transition data for 15 cycles, the system struggles to solve the problem.# 5 Conclusions\n\nIn this paper, a novel approach for open-ended learning in autonomous agents based on intrinsically motivated planning is presented. This approach integrates two powerful paradigms, intrinsic motivation and classical planning, to enable agents to continuously learn and improve their knowledge and skills without relying on external supervision or rewards.\n\nThis work suggests an alternative or complementary approach to the advanced and popular sub-symbolic methods, demonstrating interesting features. First, it allows agents to explore and learn in a self-directed and open-ended manner without being limited to a predefined set of goals or tasks. Second, it enables agents to represent and reason about their knowledge and skills in a structured and formal way, which can facilitate planning and generalization to new situations. Third, it can incorporate intrinsic motivations that drive the agent to explore and learn beyond extrinsic goals.",
        "context_id": 15,
        "question": "What does the symbol \u03c9 represent in the Markov Decision Process (MDP) tuple?",
        "answer": [
            "discount factor"
        ],
        "context_length": 38492
    },
    {
        "context": "# 1 Introduction\n\nSince the earliest times, our ancestors gazed upon the luminous moon, a symbol of mystery and wonder. Its bright facade, an elegant sphere in the cosmos, has always made us think about what remains hidden: the moon\u2019s obscure and elusive dark side. This curiosity, as ancient as human history itself, represents our innate desire to uncover the concealed dimensions that exist beyond the visible.\n\nThis quest, once purely philosophical, has now ventured into the realm of practicality, propelled by the advancements in 3D generative model [29,34,42,45,48]. These technologies enable a broad range of applications, especially in gaming and virtual reality, allowing for the creation of rich, detailed environments and objects without extensive modeling.\n\nNevertheless, the development of robust large-scale 3D generative models remains a formidable challenge, predominantly due to the limited availability of 3D data. Numerous attempts [1,13,27] have been made to train 3D diffusion models on relatively small 3D datasets, condition on textual or visual prompts; Yet, \u03c9 Corresponding Author.# Shen et al.\n\n|A grey Volvo XC90|A grey BMW X5|\n|---|---|\n|A white skin horse with flowing mane|A sleek silver horse with flowing mane|\n|A pale cream-color bear pillow|A pale cream-color panda pillow|\n|Reference|A muscle dog with capital S|\n| |A muscle dog wearing a superman cape|\n\nFig. 1: 3D Darkside of Single Image. By employing various text prompts, Vista3D is capable of unveiling the diversity of unseen views while retaining 3D consistency and detail. Two novel views and the normal map are visualized for each text prompt.\n\nThese endeavors often fall short in creating 3D objects with structural integrity and textural consistency. This challenge is further compounded in the context of reconstructing 3D objects from single images. In this context, two primary approaches emerge. The first considers the task as a problem of sparse-view reconstruction. However, this often leads to blurred 3D outputs due to the neglect of unseen elements, resulting in excessively blurred 3D objects [8, 52] as most views remain unseen.\n\nOn the other hand, the generative approach, which leverages large-scale 2D diffusion models [29, 42], introduces its own set of challenges. Efforts to develop 3D-aware 2D diffusion models [19, 21, 30, 32, 34, 39, 40, 51] involve fine-tuning 2D models with camera transformation modeling on 3D datasets [5,6]. Nevertheless, the prevalence of synthetic objects in these datasets can lead to a compromise in 2D diversity. This often results in the generation of oversimplified geometries and textures.\n\nIn this paper, we present Vista3D, a framework designed for reconstructing the unseen view (or \"darkside\") from a single image. Central to Vista3D is a dual-phase strategy: a coarse phase followed by a fine phase.\n\nIn the coarse phase, we leverage 3D Gaussian splatting [14] to swiftly create basic geometry and textures. To stabilize Gaussian Splatting optimization, we employ a gradient-based Top-K densification strategy, focusing on Gaussian points with the highest gradients. Additionally, we introduce two novel regularization terms targeting the Gaussian scale and transmittance values, significantly enhancing the convergence speed.\n\nThe fine phase then transforms this initial geometry into signed distance fields (SDF) for further optimization. Here, we employ FlexiCubes [38], an advanced differentiable isosurface technique, to refine the geometry. This refine-# Vista3D\n\nment aids in learning the signed distance fields (SDFs), deformation, and interpolation weights. The parameters are optimized by ensuring fidelity to the original image and guided by a score function derived from diffusion priors.\n\nDespite these advancements, a unified representation and supervision across all views, both seen and unseen, prove insufficient for capturing the unique characteristics of different viewpoints and generating diverse, consistent 3D objects. To address this, we enhance the representation by implementing Disentangled Texture Representation, using two angularly disentangled networks for accurate texture prediction. Furthermore, our Angular-based Composition method amalgamates different diffusion priors, adjusting their gradients within specific angular bounds according to their gradient magnitudes. This strategic adjustment assures 3D consistency while promoting diversity in the unseen views.\n\nVista3D excels in efficiently generating diverse and consistent 3D objects from a single image within five minutes. Our extensive evaluations demonstrate its ability to maintain a flexible balance between the consistency and diversity of the generated 3D objects.# We summarize our contribution as follows:\n\n- We present Vista3D, a framework for revealing the 3D darkside of single images, efficiently generating diverse 3D objects using 2D priors.\n- We develop a transition from Gaussian Splatting to isosurface 3D representations, refining coarse geometry with a differentiable isosurface method and disentangled texture for textured mesh creation.\n- We propose an angular composition approach for diffusion priors, constraining their gradient magnitudes to achieve diversity on the 3D darkside without sacrificing 3D consistency.# 2.1 3D Generation Conditioned on a Single Image\n\nThe objective of image-to-3D generation is to create 3D objects from a single reference image. Initial methods [8, 52] approached this challenge as a variant of sparse view 3D reconstruction. However, these methods often resulted in blurred object outputs due to insufficient priors. Recently, drawing inspiration from text-to-3D initiatives that utilize Score Distillation Sampling (SDS) to elevate 2D diffusion priors into 3D generative models, image-to-3D works [24, 33, 34, 40, 42] have adopted a similar approach for 3D object generation based on a single image. However, 2D diffusion priors alone cannot ensure 3D consistency, as they are typically trained solely on image datasets. To address this, several studies [19\u201321, 39] have attempted to refine 2D diffusion priors with 3D data [5, 6], enhancing their ability to model 3D consistency. A notable example is Zero-1-to-3, which can generate novel views condition on single image and camera position. Integrating this refined model with SDS [30, 41] allows for the reconstruction of coherent 3D objects. Moreover, another stream of works [9, 17, 36, 46, 47, 50, 55] pretrained on large-scale 3D dataset [5] directly predicting the representation.# 4 Shen et al.\n\nof a 3D object from a single image. Diverging from previous works, our work does not solely view this as a 3D reconstruction issue. We redefine it as a 3D generation task aimed at uncovering the unseen 3D aspects behind a single image. Through a meticulously crafted framework, our method efficiently generates diverse and consistent 3D objects.# 2.2 3D Representations for Generation\n\nPresently, most zero-shot text-to-3D and image-to-3D models utilize an optimization based pipeline, parameterizing the 3D object as a differentiable representation, which varies among different methods. The most prevalent representation in groundbreaking works like dreamfields [12], dreamfusion [29], and SJC [43] is Neural Radiance Fields (NeRF) [25]. However, training a NeRF is computationally intensive and takes long time to convergence. Magic3D [16] introduced a two-stage representation, initially learning a coarse NeRF, followed by refining the polygon mesh using a differentiable isosurface method, DMTet [37]. Fantasia3D [2] suggested directly optimizing DMTet [37] in separate phases for geometry and texture, but this often leads to mode collapse in the geometry phase and extends training time beyond NeRF. Gaussian Splatting [10, 14, 35, 44, 53] has gained attention for its efficiency in various 3D tasks, with several 3D generative models [3, 4, 41, 49] incorporating it for effective generation. However, as a point-based representation, it cannot yield high-fidelity meshes. In our approach, we employ Gaussian Splatting exclusively to create coarse geometry. This coarse geometry is then transformed into SDF, optimized with a hybrid isosurface representation, FlexiCubes [38], to produce high-fidelity meshes. Additionally, we propose an angular disentangled texture representation, tailored to the specifics of this task.# 3 Methodology\n\nIn this section, we outline our framework to generate detailed 3D object from single image with 2D diffusion priors. As depicted in Figure 2, our exploration of the 3D darkside of a single image commences with the efficient generation of basic geometry (Section 3.1), represented through 3D Gaussian Splatting. In refinement stage (Section 3.2), we devise a method for transforming the rudimentary 3D Gaussian geometry into signed distance fields, and thereafter, we introduce a differentiable isosurface representation to further enhance the geometry and textures. To enable diverse 3D darkside of given single image, we present a novel approach to constrain two diffusion priors (Section 3.3), enabling the creation of varied yet coherent darkside textures by bounding gradient magnitude. With these approaches, our method can efficiently generate diverse, high-fidelity meshes from a single image.# Stage I\n\nGaussian Splatting\n\nTop-K Densification\n\nRegularization\n\nInput Image# 3D Prior\n\nDisentangled Texture\n\nFlexiCubes\n\nRepresentation\n\nAngular-based Composition\n\nSDS# Fig. 2: Overview of Vista3D.\n\nWe generate high-fidelity mesh from single image input in a coarse-to-fine manner. In the coarse stage, we utilize Gaussian Splatting to learn a coarse geometry with a 3D-aware 2D diffusion prior. We further extract sign distance fields from Gaussian Splatting for refinement. Another 2D diffusion prior is enabled with an angular-based composition to explore diverse darkside while retain 3D consistency in refinement stage.# 3.1 Coarse geometry from Gaussian Splatting\n\nIn the coarse stage of our framework, we focus on constructing a basic object geometry using Gaussian Splatting. This technique, as described in [14], represents 3D scenes as set of anisotropic 3D Gaussians. Compared to other neural inverse rendering methods, such as NeRF [25, 26], Gaussian Splatting demonstrates a notably faster convergence speed in inverse rendering tasks.\n\nSome works [3, 41, 49] has attempted to introduce Gaussian Splatting into 3D generative models. In these methods, we found that directly using Gaussian splatting to generate detailed 3D objects requires optimizing a large number of 3D Gaussians, necessitating significant time for optimization and densification, which is still time-consuming. However, Gaussian Splatting can quickly create a coarse geometry from a single image using a limited number of 3D Gaussians within just one minute. Therefore, in our approach, we utilize Gaussian Splatting solely for the initial coarse geometry generation.\n\nSpecifically, each 3D Gaussians is parameterized by its central position, scaling, rotation quaternion, opacity, and spherical harmonics to represent color. To generate a coarse 3D object, we optimize a set of these Gaussian parameters \u03b5 = {\u03d1}, where i \u03d1 i = {x, r, q, \u03c9 i, c i}. To render 3D Gaussians to 2D images, we utilized the highly-optimized tile based rasterization implementation [14].\n\nTo generate the coarse geometry of given single image I ref, we adopt Zero-1-to-3 XL [5, 19] as 2D diffusion priors \u03d6 \u03b5 with pretrained parameters \u03f1. This prior enables denoising of novel views based on the given image I ref and relative camera pose \u03c2\u03c6. Accordingly, we optimize the 3D Gaussians \u03b5 with SDS [29]:\n\n\u2191 \u03d1 L SDS = E t,\u03d6 (\u03d6 \u03b5 (I R \u21bc\u03b5)# 6 Shen et al.\n\nwhere \u03c6 denotes the camera pose sampled around the object with fixed camera radius and F oV, I\u03f1 R is the rendered image from 3D Gaussian set \u03b5 with camera pose \u03c6, timestep t is annealed to weight the gaussian noise \u03d6 added to the rendered image. Beyond this basic approach, we introduce a Top-K Gradient-based Densification strategy to accelerate convergence and add two regularization terms to enhance the reconstructed geometry.# Top-K Gradient-based Densification.\n\nIn the optimization process, we find the periodical densification [14] with naive gradient threshold is hard to tune due to the nature randomness of SDS. So we instead use a more robust densification strategy. Only gaussians points with top-k gradients will be densified during each interval, this simple strategy can stablize training cross various given images.# 3.2 Mesh refinement and texture disentanglement\n\nIn the refinement stage, our focus shifts to transforming the coarse geometry, produced via Gaussian splatting, into signed distance fields (SDF) and refining its parameters using a hybrid representation. This stage is crucial for overcoming the challenges presented in the coarse stage, notably the surface artifacts frequently introduced by Gaussian splatting. Due to the inability of Gaussian splatting to provide direct estimates of surface normals, we cannot employ traditional smoothing methods to alleviate these artifacts. To counter this, our method incorporates a hybrid mesh representation, which entails modeling the 3D object\u2019s geometry as a differentiable isosurface and learning the texture using two distinct, disentangled networks. This dual approach not only smooths out the surface irregularities but also significantly improves the fidelity and overall quality of the 3D model.# 3.3 Darkside Diversity via Prior Composition\n\nIn implementing our pipeline, we encountered a key challenge related to the lack of diversity in unseen views. This issue largely stems from the reliance on the Zero-1-to-3 XL prior, a model trained on synthetic 3D objects from Objaverse-XL [5]. While this prior is adept at handling 3D-aware generation based on reference images and relative camera poses, it tends to produce oversimplified or overly smooth results in unseen views. This limitation becomes especially pronounced when dealing with objects captured in the real world.\n\nTo address this, we integrate an additional prior from Stable-Di!usion, known for its ability to synthesize diverse images.# Darkside diversification with 2D di!usion.\n\nWe introduce a second prior, \u03d6 \u03c6 with pretrained parameters 3, leading to two Score Distillation Sampling (SDS) loss terms \u2191L SDS \u03b5 and \u2191L SDS \u03c6 (Equation 1) for optimization. The optimal balance between these two priors remains relatively unexplored. While Magic123 [30] uses an empirical loss weight of 1/40 for the latter term, this approach may not fully harness the potential of the 2D prior. The key objective in introducing this 2D prior is to introduce greater diversity in unseen view. A small weight \u03c6 with \u2191L SDS may largely limit its e!ect.\n\nTo enhance the diversity in the unseen aspects of the given image, we employ a gradient constrain method to merge these two priors. We reformulate the SDS loss as a score function [29], \u2191 \u03c2 L SDS (\u03f1, x) = \u2193E t,z t |x \u2191 \u03c2 logp \u03b5 (z t |y), where t is the timestep and z t \u03b5 is noise latent.\n\nHere \u03c6 \u2191L SDS is a 3D-aware term conditioned on y = {\u03c2\u03c6, Iref}, while \u2191L SDS is a diverse text-to-image term conditioned on text prompt y = P T. With different condition y, the score function of these two SDS term varies. To retain 3D consistency of unseen views, the magnitude of \u2191 \u03c2 logp \u03c6 (z |y) need tot be constrained with respect to the 3D-aware term \u2191 \u03c2 logp \u03b5 (z t |y). And to avoid the texture to be over-smoothed by the 3D-aware di!usion model, the magnitude of \u2191 \u03c2 logp \u03b5 (z |y) t is indeed to be constrained with the \u2191 \u03c2 logp \u03c6 (z t |y) term.# 4.1 Implementation Details\n\nCoarse geometry learning. In this phase, the input image undergoes pre-processing with SAM [15, 23, 34], where the object is extracted and recentered. We initialize all 3D Gaussians with an opacity of 0.1 and a grey color, confined within a sphere of radius 0.5. The rendering resolution is progressively increased from 64 to 512. This stage involves a total of 500 optimization steps, with the densification and pruning of 3D Gaussians occurring every 100 iterations. The top-K densification starts at a ratio of 0.5 and gradually anneals to 0.1, while the pruning opacity remains constant at 0.1. After the first densification, transmittance regularization is activated and selectively applied to the top-80% opacity values of 3D Gaussians to avoid affecting transparent Gaussians. Scale regularization is enforced using L1 norm. The weights of \u21bd scale and \u21bd tr are maintained at 0.01 and 1, respectively, throughout the optimization, whereas \u21bd rgb and \u21bd mask are gradually increased from 0 to 10000 and 1000, respectively. The timestep for SDS is linearly annealed from 980 to 20. For camera pose sampling, the azimuth is sampled in the range of [\u2212180, 180] and elevation in [\u221245, 45], with a fixed radius of r = 2. This phase of optimizing the coarse geometry takes about 30 s.\n\nMesh refinement. In the refinement phase, we configure the grid size of FlexiCubes to 803 within the space [\u22121, 1]3. The coarse geometry obtained from the initial stage is recentered and rescaled to initialize the Signed Distance Field (SDF) for the vertices of this grid. Interpolation weights are set to 1, and all deformations start at 0. For texture, we use two hash encodings with a two-layer Multilayer Perceptron (MLP). The batch size is maintained at 4. The learning rate for deformation and interpolation weights is 0.005, while it\u2019s 0.001 for SDF, and 0.01 for texture parameters. The rendering resolution is gradually increased from 64 to 512. In Equation 5, the loss weights are set as follows: \u21bd rgb = 1500, \u21bd mask = 5000, \u21bd sdf = 1, and \u21bd SDS = 1. We develop two versions for optimization: Vista3D-S and Vista3D-L. Vista3D-S performs 1000 steps of optimization solely with the 3D-aware prior, aiming to generate 3D mesh within 5 minutes. Vista3D-L undergoes 2000 steps of optimization with two diffusion priors to create more detailed 3D objects. The entire optimization process for Vista3D ranges from 15 to 20 minutes. In this stage, camera poses are sampled using a 3D-aware Gaussian unsampling strategy to expedite convergence (additional details are provided in the supplementary material). All experiments are conducted on an RTX3090 GPU.\n\nScore distillation sampling. In SDS optimization, the practice of linearly annealing the timestep t to adjust the noise level has been established as effective for producing higher-quality 3D objects [11]. However, in our experiments, we observed that linear annealing may not be the optimal strategy. Consequently, we...# Qualitative Comparison\n\nIn Figure 3, we show our efficient Vista3D-S is capable of generating competitive 3D objects with a 20% speedup compared to existing coarse-to-fine methods. For Vista3D-L, as depicted in Figure 1 and Figure 4, we highlight our angular gradient constraint which distinguishes our framework from previous image-to-3D methods, as it can explore the diversity of the backside of single images without sacrificing 3D consistency. In Figure 3, we primarily compare our Vista3D-S with two baselines, Magic123 and DreamGaussian, for generating 3D objects from a single reference view. Regarding the quality of generated 3D objects, our method outperforms these two methods in terms of both geometry and texture. Regarding Vista3D-L, we compare it with two inference-only single view reconstruction models, specifically One-2-3-45 and Wonder3D. As shown in Fig. 4, One-2-3-45 tends to produce blurred texture and may result in incomplete geometry for more complex objects, while our Vista3D-L achieves more refined textures, particularly on the backside of 3D objects, using user-specified text prompts. And Wonder3D often resorts to simpler textures due to its primary training on synthetic datasets, which occasionally leads to out-of-# 12 Shen et al.\n\ndistribution issues for certain objects. In contrast, Vista3D-L offers zero-shot 3D object reconstruction by controlling two diffusion priors, enabling more detailed and consistent textural. Moreover, given that only a single reference view of the object is provided, we posit that the object should be amenable to editing during optimization with user-specified prompts. To illustrate this, we display several results in Figure 1 that emphasize the potential for editing.\n\n|Type|CLIP-Similarity \u2191|Time Cost \u2193|\n|---|---|---|\n|One-2-3-45 [18]|0.594|45 s|\n|Point-E [27]|0.587|78 s|\n|Shape-E [13]|0.591|27 s|\n|Zero-1-to-3 [19]|0.778|30 min|\n|DreamGaussian [41]|0.738|2 min|\n|Magic123 [30]|0.802|2 h|\n|DreamCraft3D [40]|0.842|3.5 h|\n|Vista3D-S|0.831|5 min|\n|Vista3D-L|0.868|15 min|\n\nTable 1: Quantitative Comparisons on generation quality in terms of CLIP-Similarity for image-to-3D task. Average generation time is reported.# 4.3 Quantitative Comparison\n\nIn our evaluation, we employ the CLIP-similarity metric [19, 24, 30] to assess the performance of our method in 3D reconstruction using the RealFusion [24] dataset, which comprises 15 diverse images. Consistent with the settings used in previous studies, we sample 8 views evenly across an azimuth range of [\u2212180, 180] degrees at zero elevation for each object. The cosine similarity is then calculated using the CLIP features of these rendered views and the reference view. Table 1 highlights that Vista3D-S attains a CLIP-similarity score of 0.831, with an average generation time of just 5 minutes, thereby surpassing the performance of the Magic123 [30]. Furthermore, when compared to another optimization-based method, DreamGaussian [41], Vista3D-S may take longer at 5 minutes, but it significantly improves consistency, as evidenced by the higher CLIP-Similarity score. For Vista3D-L, we apply an enhancement-only setting. By employing angular diffusion prior composition, our method achieves a higher CLIP-Similarity of 0.868. The capabilities of Vista3D-L, especially in generating objects with more detailed and realistic textures through prior composition, are demonstrated in Figure 4. Additionally, we conduct quantitative experiments on the Google Scanned Object (GSO) [7] Dataset, following the setting in SyncDreamer [20]. We evaluate each method using 30 objects and computed PSNR, SSIM, and LPIPS [54] between the rendered views of the 3D object and 16 ground-truth anchor views. The results, as shown in Tab. 2, reveal that our Vista3D-L achieves SOTA performance among these methods with a large margin. Vista3D-S also demonstrates competitive performance, albeit with a single diffusion prior.# Vista3D\n\n|Method|PSNR \u2191|SSIM \u2191|LPIPS \u2193|\n|---|---|---|---|\n|RealFusion [24]|15.26|0.722|0.283|\n|Make-it-3D [42]|15.79|0.741|0.245|\n|Zero-1-to-3 [19]|18.93|0.779|0.166|\n|One-2-3-45 [18]|17.47|0.768|0.184|\n|SyncDreamer [20]|20.05|0.798|0.146|\n|DreamGaussian [41]|23.43|0.832|0.092|\n|Magic123 [30]|24.89|0.875|0.084|\n|Vista3D-S|25.42|0.912|0.073|\n|Vista3D-L|26.31|0.929|0.062|\n\nTable 2: Quantitative Comparison on the GSO [7] dataset# 4.4 User study\n\nIn our user study, we evaluate reference view consistency and overall 3D model quality [41]. The evaluation encompasses four methods: DreamGaussian [41], Magic123 [30], and our own Vista3D-S and Vista3D-L. We recruited 10 participants for this user study. Each was asked to sort generated 3D object from different methods in terms of view consistency and overall quality respectively. Thus, the scores presented for each metric range from 1 to 4. The results, presented in Table 3, reveal that our Vista3D-S outperforms the previous methods in both view consistency and overall quality. Furthermore, the adoption of the angular prior composition in Vista3D-L leads to additional improvements in both the consistency and quality of the generated 3D objects.\n\n|Method|View Consistency \u2191|Overall Quality \u2191|\n|---|---|---|\n|DreamGaussian [41]|1.78|2.02|\n|Magic123 [30]|2.11|1.83|\n|Vista3D-S|2.87|2.81|\n|Vista3D-L|3.24|3.33|\n\nTable 3: User study of Vista3D. We conduct user study in terms of view consistency and overall quality, the score ranges from 1 to 4, the higher the better.# 4.5 Ablation Study\n\nCoarse-to-fine framework. Our framework integrates a coarse stage to learn initial geometry then a fine stage to refine geometry and shade textures. We validate the necessity of such a coarse-to-fine pipeline in Figure 5 (a). We first commence with isosurface representation to learn geometry directly, finding the geometry optimization is prone to collapse without preliminary geometry initialization. Thus, a coarse initialization becomes imperative. Beside, we present the normal map of a rough mesh extracted from 3DGS from the coarse stage. It is observed that the coarse stage tends to generate rough even non-watertight geometry, both difficult to mitigate. These findings demonstrate that combining both stages is crucial for the optimal performance of Vista3D.\n\nDisentangled Texture. For validating the effectiveness of the disentangled texture, we compare adopting both hash encodings with single hash encoding.# 14 Shen et al.\n\nRef\nFlexiCubes\n3DGS\nOurs\nSingle hash-encoding\nBoth hash-encoding# b) Ablation study of angular hash-encoding\n\nFig. 5: Ablation study of overall framework and disentangled texture.\n\nin Figure 5 (b). With both hash-encodings, the artifacts on the reconstructed robot are notably reduced, especially at the backside. Further, we visualize the disentangled texture in supplementary Figure 6(b). Specifically, when visualizing Href, Hback is set as 0 in Equation 4, and vice versa. From the shown visualization, we can clearly find that the facing-forward hash encoding Href mainly encodes the detail features consistent with the given reference view. While the back hash encoding Hback mainly encodes the features in the unseen views. The textures of the facing-forward view and back views are disentangled and learned in two separate hash encodings, which can facilitate learning better textures near the reference view and in unseen views.# 5 Conclusion\n\nIn this paper, we present a coarse-to-fine framework Vista3D to delve into the 3D darkside of a single input image. This framework facilitates user-driven editing through text prompts or enhances generation quality using image captions. The generation process begins with a coarse geometry obtained through Gaussian Splatting, which is subsequently refined using an isosurface representation complemented by disentangled textures. The design of these 3D representations enables the generation of textured meshes within a mere 5 minutes. Additionally, the angular composition of diffusion priors empowers our framework to reveal the diversity of unseen views while maintaining 3D consistency. Our approach surpasses previous methods in terms of realism and detail, striking an optimal balance between generation time and the quality of the textured mesh. We hope our contributions will inspire future advancements and foster future exploration into the 3D darkside of single images.",
        "context_id": 16,
        "question": "What technique does Vista3D use to refine geometry in the fine phase?",
        "answer": [
            "FlexiCubes"
        ],
        "context_length": 27005
    },
    {
        "context": "# Biological arrow of time: Emergence of tangled information hierarchies and self-modelling dynamics# 2.1. Major evolutionary transitions\n\nThe Second law of thermodynamics states the impossibility of fully transforming disordered heat energy into coherent work without the expense of some additional resource. Importantly, the Second law implies that physical systems naturally decay towards less structured arrangements. When seen against this background, the spectacular evolution of living systems on Earth is even more remarkable, requiring explanations that are compatible with these limiting physical principles.\n\nWhen explaining the progress of biological evolution, two main perspectives have been adopted: approaches that emphasise that evolution happens slowly and continuously, and approaches that state that evolution is mainly driven by abrupt transitions [30, 91]. Within the second family of approaches, it has been argued that evolution displays major transitions in terms of biological complexity [100]. In general, these transitions produce important changes in the way organisms exchange information, cooperate and coordinate with each other, and how they survive and replicate. These collectives can become so tightly arranged \u2014 via functional specialisation, which enhances the efficacy of cooperation while inducing mutual dependency [116] \u2014 that they start acting as \u2018higher-order\u2019 units that effectively drive the selection process [88]. In effect, by tying each gene\u2019s replication to the survival of higher-order structures, selection favours genes that promote survival of the higher-order structure, which in turn enables greater division of labour and specialisation.\n\nWhen trying to characterise what these major evolutionary transitions have in common, one can see that they involve profound changes in what an individual is and how it preserves its properties to new generations. This is usually supported by the emergence of novel inheritance systems, involving new modes of storing, transmitting, and processing information \u2014 sometimes referred to as new \u201ccodes of life\u201d [3, 57]. Examples of this include replicating molecules which form cells as compartmentalised \u201cpopulations\u201d of molecules; independent replicators (hypothesised to be RNA) which may have formed chromosomes to reduce information loss during replication; emergence of the genetic code and translation machinery (with DNA used as genes and proteins as enzymes); prokaryotes evolving into eukaryotes with a membrane-bound nucleus which stores their genetic information, and so on, towards the evolution of multicellularity and eusociality, as well as language and sociocultural evolution [100, 116, 57].\n\nIn this way, natural evolution is thought to give rise to more elaborate arrangements of higher-order organisms, from eukaryotic cells to multicellular organisms and even collectives such as swarms and hives, supported by novel information-processing modes. However, there are more \u201ccodes of life\u201d than major evolutionary transitions [57], and several crucial questions remain unanswered: Are there computational principles that drive these evolutionary transitions? What are the computational trade-offs that are being negotiated during a transition?# Gene transfer\n\nTransmission of genes from one generation to the next, i.e., from parent to offspring (as a result of sexual or asexual reproduction), is referred to as vertical gene transfer (VGT) [13]. Horizontal gene transfer (HGT) moves partial genetic information laterally across distantly related organisms in the same generation [51].# Genotype and phenotype\n\nThe genotype contains the specific genetic information an organism carries in its DNA, encoded in sequences of nucleotides. The phenotype comprises the set of observable traits (physical, biochemical and behavioural) of the organism. Genetic instructions are used in development and functioning of a living organism: they are involved in construction of other components and copying itself.# Extended phenotype\n\nThe extended phenotype encompasses all the effects that a genotype can have on the environment beyond the immediate organism itself. This includes modifications or structures an organism creates that affect its survival and reproduction, influencing other organisms in the ecosystem [26]. Extended phenotype includes:\n\n- (i) behavioural innovations, e.g., a bird\u2019s ability to construct complex nests;\n- (ii) interactions with environment and other organisms, indirectly influenced by traits encoded in the genome, e.g., pheromone production in insects affects their mating, foraging and social behaviours;\n- (iii) physical modifications of the organism\u2019s environment, e.g., beaver dams, spider webs, or ant pheromone trails arise from behaviours guided by genetic predispositions and environmental factors.# Gene expression\n\nTranscription and translation are two fundamental processes of gene expression through which the information encoded in DNA is used to produce functional proteins that contribute to an organism\u2019s phenotype. Transcription is the process of making RNA copies of the genetic protein information encoded in DNA, and translation is the decoding of instructions for making proteins.# 2.2. Coding thresholds and phase space expansions\n\nIt has been argued that major evolutionary transitions overcome specific \u201ccoding thresholds\u201d, with the saltations opening a novel evolutionary phase-space with qualitatively new possibilities to handle information [123, 57]. For example, when discussing the emergence of DNA, Woese argued that:\n\n\u201cSomewhere along the line there had to have occurred a saltation that we could call the \u201ccoding threshold,\u201d where the capacity to represent nucleic acid sequence symbolically in terms of a (colinear) amino acid sequence developed, a development that would generate a truly enormous new, totally unique evolutionary phase space.\u201d [123]\n\nFurthermore, Woese [123] suggested that another such saltation \u2014 which he described as \u201cDarwinian threshold\u201d \u2014 may have been crossed when cells went from an initial arrangement where their evolutionary dynamics were dominated by horizontal gene transfer (HGT) to a new arrangement dominated by vertical inheritance. This turned evolution from a communal process (in which evolving cells maintained no stable genealogical information) to one where stable organismal lineages could develop, preserve their genetic makeup, and coexist [123] (see subsection 2.3).\n\nAnalogously, a \u201clanguage threshold\u201d may have been crossed when early human groups developed language (another \u201ccodes of life\u201d [3, 57]), which enabled a new way to inherit knowledge and significantly speed-up adaptation. Woese [123] elaborated on this conjecture, noting that one common feature of higher-order arrangements \u2014 such as multicellularity or human language \u2014 is their enhanced communication ability, which enables a kind of \u2018interaction at a distance\u2019. This insight reinforces the idea that higher-order structures are able to synergistically access an extended problem-solving space, making such structures non reducible to a mere aggregation of information-processing subunits.\n\nUnfortunately, however, the investigation of major evolutionary transitions is hindered by a chicken-or-egg causal dilemma, highlighted by various error threshold paradoxes (e.g., Eigen paradox [29]), as it is often difficult to explain how a more complex structure can evolve if the evolutionary benefits can be realised only at the higher level. This challenge was eloquently expressed in 1904 by a pioneer of genetics Hugo de Vries: \u201cnatural selection may explain the survival of the fittest, but it cannot explain the arrival of the fittest\u201d, and remains a subject of active research.\n\nFor example, Wagner [109] explored how innovation and adaptability drive evolutionary success, arguing that the ability of organisms to generate new traits and capabilities is crucial for their long-term survival and thriving, and highlighting that constraints, rather than being purely limiting, can actually drive innovation. An intriguing conjecture implicated genetic parasites, e.g., plasmids or viruses, as major catalysts for evolutionary transitions. Such parasites may (i) necessitate the formation of more complex structures helping the host to combat the parasitic threats and win in the parasite-host arms race, while (ii) providing building blocks \u2014 via a parasite-enabled gene transfer \u2014 for constructing the higher levels of organisation:\n\n\u201conly increasing organizational complexity, e.g. emergence of multicellular aggregates, can prevent the collapse of the host\u2013parasite system under the pressure of parasites\u201d [55].# 2.3. Innovation-sharing at the coding threshold\n\nVetsigian et al. [108] considered evolution of the genetic code during early life, that is, during the stages preceding the emergence of vertical genealogical descent. They argued that early evolutionary dynamics of cell-like entities involved communal descent mediated by HGT.\n\nA key insight of their analysis is that the genetic code emerging in this communal world was not only needed to encode amino acid sequences in the genome, but also served as an innovation-sharing protocol. They conjectured that \u201cearly life did not require a refined level of tolerance\u201d, and hence, the emerging innovation-sharing protocol allowed for imprecise copies created by an ambiguous translation without a unique mapping between codons and amino acids. Nevertheless, this dynamic produced a (nearly) universally common mechanism for encoding information, the universal genetic code:\n\n\u201cHGT of protein coding regions and HGT of translational components ensures the emergence of clusters of similar codes and compatible translational machineries. Different clusters compete for niches, and because of the benefits of the communal evolution, the only stable solution of the cluster dynamics is universality\u201d [108].\n\nThe study concluded that once the universal genetic code emerged, the genetic complexity is likely to grow exponentially. This, in turn, leads \u2014 via another (Darwinian) transition \u2014 to dominance of the vertical (individual) descent, and hence the Darwinian evolution [108].# 2.4. Division of labour increases fitness and the dimensionality of phenotype space\n\nGenerally, biological fitness consists of two components \u2013 fecundity (reproduction) and viability (survival), and trade-offs between the two attributes shape diverse life-histories. In the context of clonal multicellularity, or a colony of genetically related cells, such trade-offs provide a selective pressure towards cell specialisation since group fitness can potentially be greater than the average of individual cells [72]. For example, a microtubule organising centre (MTOC) can typically be utilised to either support flagella for motility or cell division, but not both simultaneously. Hence, single-celled organisms typically separate reproduction and motility phases temporally. In multicellular Volvox this trade-off is thought to have led to the evolution of a spatial distinction between two groups of cells, corresponding to a germ/soma separation [73], and a similar scenario may have occurred during animal evolution [53].# Darwinian fitness\n\nDarwinian fitness is a quantitative measure of reproductive success. More specifically it measures the average contribution to the gene pool of the next generation from an individual, classified by genotype. It depends on, for a given environment, the relative probability of survival and rate of reproduction.# Inclusive fitness\n\nInclusive fitness is similar to Darwinian fitness but is helpful in explaining social interactions since it explicitly recognises that an individual can influence their genetic contribution to the next generation via their influence on individuals other than themselves. It takes into account the influence of such interactions on both the individual and the individual being influenced as well as the degree of shared genetic information between the two.# Multicellular organisms\n\nMulticellular organisms consist of more than one cell. Most multicellular organisms develop clonally, that is from the division of a single cell. This in turn means that most cells in such organisms are genetically identical (exceptions are caused by mutations during development or by genetic recombination in the case of germ cells). Less commonly, multicellular organisms form through aggregation, where single cells may come together to form the organism in response to environmental cues. In this case the cells involved are usually more genetically heterogeneous.\n\nThere are many other trade-offs beyond the general categories of survival vs reproduction that are relevant to a division of labour. For example, oxygen sensitive processes such as nitrogen fixation [33] are separated spatially from oxygenic photosynthesis in filamentous cyanobacteria while the exchange of metabolites between cells benefits the whole [129]. Considering extant complex organisms and their many distinct cell types, the concept of division of labour can obviously be extended to a great variety of cellular functions. In general, a division of labour within multicellular organisms can be viewed as an increase in the dimensionality of phenotype space, since spatial separation enables otherwise incompatible processes to occur:\n\n\u201cMathematically, the evolutionary advantage of the division of labour in aggregate forms can be viewed as the emergence of new, higher fitness maxima when the dimensionality of phenotype space is increased. The new fitness maxima are not a direct consequence of aggregation, but are based on the interaction between aggregated individuals that engage in the division of labour.\u201d [47].# 2.5. Higher-level agents operate with extended patterns of information\n\nA higher-level aggregate organisation emerging out of interactions among the lower-level components may influence its constituent parts within a complex feedback loop [43]. McMillen and Levin [71] investigated the competition between collective and individual biological levels, in which \u201cthe behavior of subunits percolates up toward adaptive processes at higher levels\u201d, while \u201chigher levels of organization constrain and facilitate the behavior of their parts\u201d [71]. In general, the notion of a biological individual, the question of group selection and the nature of competition between collective and individual levels should be interpreted from a temporal, \u201cdiachronic\u201d, perspective [76]. This view allows us to consider intermediate stages in evolutionary transitions which create the potential for \u201cconflict between levels of selection, for selection between the smaller units may disrupt the well-being of the collective\u201d [76].\n\nIn adopting this approach, McMillen and Levin [71] argued that one of the advantages of multiscale organisation is the capacity of collective agents to create a novel problem-space and modify the energy landscape available at the higher level, which allows the lower-level components to operate more efficiently [71]. In this interpretation an energy function represents an optimisation problem \u2014 an approach which traces back to the study of neural circuitry by Hopfield and Tank [46]. This seminal work demonstrated that interconnected networks of analog neurons can be computationally effective in solving hard optimisation problems, with the optimal solution corresponding to the lowest energy state of the network\u2019s energy function (defined in terms of neuronal voltages), i.e., its most stable state. In general, as has been pointed out by Watson et al. [114],\n\n\u201c...in systems built out of the superposition of many low-order constraints, low-energy (high-utility) attractors necessarily have large basins of attraction... So, the better the attractor, the more it is visited, thus the more it is enlarged by learning, and the more it is visited in the future, and so on.\u201d [114].\n\nAdopting a broader definition of \u201ccollective intelligence\u201d for hierarchical biological systems which consist of many interconnected parts (e.g., gene regulatory networks, cells, etc.) utilises the analogy with neural networks, suggesting that evolution involves altering those connections in a way similar to the training of a neural network (see also subsections 2.6 and 5.6). When the higher-order system attains a lower energy by modifying the energy landscape, it allows the collective to achieve a higher utility.\n\nIn information-processing terms, the access to a novel problem-space means that \u201ccollective intelligence of competent parts\u201d is able to propagate information across scales, exhibiting an integrated problem-solving capacity, so that the higher-level agents are able to make decisions based on extended patterns of information [71]. For example, gene expression of the frog embryo is influenced by the spatial voltage differences across its brain regions (i.e., by a group-level pattern), rather than by the absolute values of individual cells [78, 77, 71]. As a result, the dynamics is able to visit larger spatial areas, approaching the higher-utility (i.e., lower-energy) attractors with larger basins of attraction.# 2.6. Tensions between individual and group interests\n\nIt has been argued that the emergence of higher-level organisation generates a tension between levels. As noted by Szathm\u00e1ry and Maynard Smith [100]: \u201centities that were capable of independent replication before the transition can replicate only as part of a larger whole after the transition\u201d. This notion has been further developed by Watson and Szathm\u00e1ry [115] (see also [113]), who pointed out that \u201cin evo-ego, correlations change the evolutionary unit (such that multiple, previously separate units become a new single unit at a higher level of organisation)\u201d. This analysis is closely related to the study of \u201ccollective intelligence\u201d by McMillen and Levin [71], and reinforces the observations that a higher-level organisation is qualitatively different and expands the phase-space of possibilities.\n\nImportantly, Watson and Szathm\u00e1ry [115] explored the \u201cevo-ego\u201d relationship, highlighting a tension between individual and group interests: \u201cindividual-level selection will oppose the creation and maintenance of adaptations that enforce selection at the group level\u201d. They emphasised that the transitions in individuality essentially create new evolutionary units and may contribute to the evolution of evolvability [27], by evolving new mechanisms of inheritance or reproductive codispersal. Crucially, this approach identified a potential tension:\n\n\u201c...if individual and group interests are aligned then selection applied at the group level does not alter evolutionary outcomes, and if individual and group interests are not aligned then individual-level selection will oppose the creation and maintenance of adaptations that enforce selection at the group level [76].\u201d [115].\n\nWatson and Szathm\u00e1ry [115], as well as Watson et al. [113], examined this tension, aiming to explain how evolution at one level of biological organisation (e.g., individual cells) can systematically generate reproductive structures non-trivially adapting at a higher level of organisation (e.g., multicellular organisms), even \u201cbefore that level of adaptation exists?\u201d [115]. The proposed approach \u2014 evolutionary connectionism \u2014 is discussed in sections 5.6\u20135.8.# 2.7. Ecological scaffolding\n\nAn alternative approach to explaining evolutionary transitions in individuality is offered by Black et al. [9], who pointed out that these transitions are immediately related to the emergence of biological complexity. In trying to analyse and clarify the conditions favouring emergence of collective-level reproduction (e.g., multicellular life), they proposed the concept of \u201cecological scaffolding\u201d. Specifically, they modelled that, given an ecological structure of distributed and dispersing resources, a division of labour would allow individual cells to \u201cparticipate directly in the process of evolution by natural selection as if they were members of multicellular collectives\u201d [9].# 3. Beyond computational undecidability: \u201cbreaking\u201d the limits of computation\n\nIn this section, we turn our attention to computation-theoretic approaches that can provide formal means to analyse an open-ended process of complexification. This computation-theoretic background will be used in subsequent sections to describe novelty generation during open-ended evolution (discussed in Section 2).\n\nIn general, the problems or functions that cannot be solved or computed by any algorithm are referred to as incomputable. For example, the Halting Problem, a classic problem in the theory of computation, asks whether it is possible to create an algorithm that can determine, for any arbitrary program and input, whether that program will eventually halt (terminate) or continue running indefinitely [106]. The halting problem is undecidable: no general algorithm exists that solves the halting problem for all possible program\u2013input pairs. Likewise, G\u00f6del showed that in any sufficiently powerful formal system, there are true statements that cannot be proven within the system [37].# 3.1. Self-reference and diagonalisation arguments\n\nA self-referential expression is one that refers to itself literally, e.g., the phrase \u201cthis sentence\u201d in the liar sentence \u201cthis sentence is false\u201d or via its referents i.e., distinct expressions that denote, name, or encode the original expression [12]. A notable example of self-reference by referents is G\u00f6del numbering, where statements about natural number arithmetic are uniquely assigned a natural number themselves \u2014 so self-reference arises when statements about natural number arithmetic are applied to their own G\u00f6del number [56].# Algorithms\n\nComputation is defined as a process of executing a series of well-defined instructions such that given an input state, its execution may satisfy a termination condition, which produces a corresponding output state.\n\nAn algorithm is a finite sequence of mathematically rigorous instructions, typically used to perform a computation.# Turing machines\n\nA model of computation, e.g., Turing machine, is an abstract model which describes how an output of a mathematical function is computed given an input. Specifically, Turing machines model a mechanism that operates on an infinitely long input tape of discrete symbols, using a head that can read or write a symbol in a given position, store a state and move in either direction of the tape. Turing Machines include transition rules with respect to the state and symbol at its head, and terminating transitions known as halting conditions (see Fig. 1). The final state of the tape corresponds to the computation\u2019s output.\n\nTuring machines have become synonymous with general-purpose computers because Universal Turing Machines (UTM) can be constructed such that the specifications of any Turing Machine (any algorithm) and input can be encoded as a UTM input, whereby the UTM\u2019s execution produces the output that matching the original Turing Machine applied to the original input (see Fig. 2).# Information processing\n\nIt is useful to distinguish computation from \u201cinformation processing\u201d, the latter referring to the manipulation or transformation of data or information by a system, such as encoding, compression, storage, transmission, decoding, search, pattern recognition, retrieval, etc. While information processing typically transforms input data into an output, in general, it does not have to produce a meaningful output and may continue without a termination condition. Hence, information processing differs from computation, which is a process with well-defined algorithm and termination (output) states. (See [74, ch. 12] for further discussion).\n\nSelf-referential statements have historically been employed to prove results related to incomputability, using what are known as diagonalisation arguments. A canonical setting for diagonalisation arguments are expressions Ei, referents \u231cEj\u231d and properties Pij arising out of the application of expressions Ei to referents \u231cEj\u231d, which we can# Readlwwrite head\n\n|Program| |ORL| |LRR|\n|---|---|---|---|---|\n|start| |ORR|IPL| |\n| | | |OPL| |\n| | | |MRR| |\n\nFigure 1: An example finite-state specification of a Turing Machine. States include A, B, C and halting state H. An arrow label (e.g., 0/P,R) specifies the tape symbol (e.g., symbol 0) that upon reading triggers a particular transition to another state, followed by the action, e.g., print (P) and move tape to the right (R) [122].# Program M\n\nInput /\n\nprogram as data\n\nencoding of Turing Machine M\n\nencoding of input /# Universal Turing Machine\n\n(UTM)\n\nFigure 2: Universal Turing Machine (UTM) simulating Turing Machine M on input I. An encoding of program P converts it into input data which is separated on the tape from the input I by suitably chosen separator symbol(s) \u00d7.# 3.1. Cantor\u2019s Theorem\n\nLet us consider Cantor\u2019s Theorem. This result states that the subsets of the natural numbers (e.g., {2, 5, 1}) are uncountable, i.e., that any list of subsets E0, E1, E2, . . . (indexed by the natural numbers) is incomplete [15]. The argument begins by assuming that an arbitrary such list is complete. For such a list, one builds an expression-referent grid as follows: expressions Ei correspond to the subsets, referents \u231cEi\u231d indicate the corresponding index (i.e. \u231cE\u231d := i), and properties Pij are defined as Pij = 1i if i is an element of Ej and 0 otherwise. The second step of the argument to prove incompleteness is to find i and j such that Pij cannot be determined. To do this, one uses the diagonal to construct the set {k \u2208 N | Pkk = 0}, i.e., each number k when considered as a referent is not a member of the expression referenced by it. Because this is a subset of the natural numbers, it must be an expression within our list, say Ex. However, any attempt to evaluate Pxx leads to a contradiction: assuming the referent x is not in Ex leads to the conclusion that x does belong to Ex, and vice versa. Therefore, we use the incomputability of Pxx to conclude that our initial list could not have been complete.# 3.2. Discrepancy between expressions and referents\n\nVisualise as the following grid:\n\n| |\u231cE0\u231d|\u231cE1\u231d|\u231cE2\u231d|\u00b7 \u00b7 \u00b7|\u231cEk\u231d|\u00b7 \u00b7 \u00b7|\n|---|---|---|---|---|---|---|\n|E0|P00|P01|P02|\u00b7 \u00b7 \u00b7|P0k|\u00b7 \u00b7 \u00b7|\n|E1|P10|P11|P12|\u00b7 \u00b7 \u00b7|P1k|\u00b7 \u00b7 \u00b7|\n|E2|P20|P21|P22|\u00b7 \u00b7 \u00b7|P2k|\u00b7 \u00b7 \u00b7|\n|...|...|...|...|\u00b7 \u00b7 \u00b7|...|...|\n|Ek|Pk0|Pk1|Pk2|\u00b7 \u00b7 \u00b7|Pkk|\u00b7 \u00b7 \u00b7|\n\nIncomputability is then represented by finding combinations of expressions Ei and referents \u231cEj\u231d whose property Pi,j cannot be determined without a contradiction. These combinations typically arise out of constructions on properties of the form Pkk, which are considered to be self-referential as they represent the application of an expression to its own referent.# 3.3. Incomputability and oracle machines\n\nAs somewhat ironically noted by Soare [97], \u201cthe field of computability theory deals mostly with incomputable, not computable, objects\u201d. Crucially, the incomputability of a given problem is considered relative to a given computational system, and in certain cases it can be overcome.# 3.4. Open-ended computational meta-simulation\n\nIn general, the idea of resolving undecidability by extending bounds of the computational system is developed within the field of recursion theory. In particular, the G\u00f6del\u2013Turing\u2013Post recursion-theoretic framework proposed various methods of constructing extensible ordinal or recursively generated logics [37, 105, 106, 107, 81, 97].\n\nTuring [107] proposed a way of overcoming the halting problem by providing a means to source an answer beyond the system boundary, that is, by considering an \u03b1-order oracle machine specified for a particular level. In general, one may consider a sequence of \u03b1-order oracles machines with increasing orders \u03b1, each of which (for \u03b1 > 0) is capable of resolving undecidable halting problems of lower levels [107, 80], as illustrated in Fig. 4.\n\nThe \u03b1-order oracles machines can be related by an operation known as the \u201cTuring jump\u201d. Formally, the \u201cTuring jump\u201d is an operation that assigns to each decision problem X a successively harder decision problem X\u2032 such that X\u2032 is not decidable by an \u03b1-order oracle machine with an oracle for X. Crucially, the Turing jump of X generates an (\u03b1 + 1)-order oracle to the halting problem for \u03b1-order oracle machines with an oracle for X.\n\nThe boundaries of lower-order systems are expanded by adding specific \u201cnovelties\u201d generated by an interaction with the corresponding oracle. In describing this continual iterative process, Turing [107] drew an analogy with ordinal logics of G\u00f6del [37]. Ordinal logic is associated with an ordinal number by recursively adding elements to a sequence of previous logics:\n\n\u201cThe well-known theorem of G\u00f6del (1931) shows that every system of logic is in a certain sense incomplete, but at the same time it indicates means whereby from a system L of logic a more complete system L\u2032 may be obtained... A logic L\u03c9 may then be constructed in which the provable theorems are the totality of theorems provable with the help of the logics L, L1, L2,...\u201d [107].\n\nThis view was further developed by Post [81] using the recursively enumerable sets. In demonstrating that every recursively generated logic may be extended, Post [81] proposed the concept of relative computability: degree of unsolvability, or the Turing degree, of a set of natural numbers measures the level of algorithmic unsolvability of the set. Consequently, Post [81] proposed a hierarchy of degrees of unsolvability, where each degree represents the extent to which a set is unsolvable or incomputable. Each level of the hierarchy corresponds to a different degree of computational complexity, with sets at higher levels being more incomputable than those at lower levels.\n\nImportantly, each extended system X(\u03b1) comprising an \u03b1-order oracle machine (\u03b1 > 0) is able to simulate a lower-level X(\u03b1\u22121) system. Arguably, one may consider a higher-order \u03b1-order oracle machine as a meta-simulator, i.e., a simulator of nested, lower-order, simulations (down to the 0-order oracle machine that can simulate any Turing).# 1.0\n\nmachine). Thus, we suggest that when a higher-order system simulates the lower-order computation, providing answers to lower-level undecidable problems, it performs meta-simulation of the lower-order systems. We shall refer to the successive construction of extensible computational systems and ordinal or recursively generated logics (within the G\u00f6del\u2013Turing\u2013Post recursion-theoretic framework) as open-ended meta-simulation.# 4. Undecidability and open-endedness in dynamical systems\n\nThe third primary domain, considered as part of our background, is physical dynamical systems. The concept of undecidable dynamics has been defined in physical systems by some analogy with computational undecidability, and so we can expect to see emerging parallels between unpredictable dynamics, expanding phase-spaces and extensible computational systems.# 4.1. Undecidability in physical dynamical systems\n\nPhysical dynamical systems are inherently nonlinear and time\u2013continuous. According to Moore [75], physical dynamical systems with at least three degrees of freedom can be Turing equivalent. They exhibit a type of unpredictability that is qualitatively stronger than low-dimensional chaos, with undecidable long-term dynamics, even if the initial conditions are known exactly. Bennett [4] qualifies this as follows:\n\n\u201cFor a dynamical system to be chaotic means that it exponentially amplifies ignorance of its initial condition; for it to be undecidable means that essential aspects of its long-term behaviour \u2014 such as whether a trajectory ever enters a certain region \u2014 though determined, are unpredictable even from total knowledge of the initial condition.\u201d\n\nThus, undecidability is a more extreme kind of unpredictability than chaos [5]. One implication of this is that if the halting problem is viewed through the lens of dynamical systems, then its undecidability means that the long\u2013term behaviour of the dynamics corresponding to a universal Turing machine is unpredictable, even if the initial conditions are known exactly.\n\nA relatively simple example offered by Moore is the motion of a particle in a 3D potential. He suggests, however, that the three-body problem (n-body more generally) presents more realistically complex dynamics for testing these ideas, in particular by demonstrating lack of scaling behaviour, irregular spectra of periodic points and attractor states, which are beyond the features that typically characterise chaotic dynamics in non-physical dynamical systems (e.g. the Lorenz 63 system, a mathematical model that only loosely approximates the complex dynamics of real weather systems). As shown in Fig. 5 for the classic 3-body problem in a Newtonian gravitational system, it is impossible to predict if or when one of the orbiting objects becomes gravitationally unbound (i.e. the halting condition), even if the initial conditions (position and velocity of each object) are known.\n\nPhysical dynamical systems (e.g. fluid flows, propagating waves), as described by space\u2013time partial differential equations, may have universal computing abilities if they have sufficient dynamical variables to maintain logically distinct trajectories in the presence of external noise. Bennett [5] argued that computationally universal dynamical systems can be used to define physical complexity, the quantity that increases when a self-organising system organises itself, producing novel features beyond those resulting from long-term evolution. Wolpert [127] derived specific impossibility results associated with physical computation. This work developed analogues of Chomsky hierarchy results about universal Turing Machines and the Halting theorem, showing, in particular, the impossibility of certain kinds of error-correcting codes. These observations not only open a way to connect computation-theoretic and dynamical-systems views on complexification processes, but also suggest that novelty generation during an open-ended biological evolution may also be seen through the lens of expanding computational spaces. These cross-disciplinary perspectives are explored in more detail in Section 5.# 4.2. Expanding phase-space\n\nIn contrast to non-physical dynamical systems, physical dynamical systems are constrained by the laws of physics, which could be argued as restricting phase space expansion.\n\n| |0.0|0.2|0.4|0.6|0.8|1.0|\n|---|---|---|---|---|---|---|\n| |0.2| | | | | |\n| |0.00| | | | | |However, physical systems may also exhibit stochastic (non-deterministic) dynamics, which, while also obeying laws of physics (e.g. thermodynamics), introduces extra degrees of freedom (i.e. higher-order complexity) in addition to any deterministic dynamics, provided the stochasticity is not purely random, without any underlying structure or information content.\n\nIn general, dynamical systems (both physical and non-physical) may encounter different dynamical phase-space regimes, e.g. ordered, chaotic and edge-of-chaos, with different computational representations. Chaotic dynamics, associated with an increase in phase space exploration, continuously generates information and the amount of information needed for prediction grows with time. As discussed above, chaotic dynamics are not necessarily computationally undecidable. Edge-of-chaos dynamics, on the other hand, has been proposed as optimal for information processing and novelty generation [58]. In physical systems, edge-of-chaos is associated with the spontaneous emergence of long-range spatio-temporal correlations, effectively expanding dynamical phase-space, with recent studies (e.g. [11, 42]) demonstrating enhancement in information processing, but only in terms of general properties or for relatively complex computational tasks rather than necessarily for any specific task. Prokopenko et al. [84] examined the link between edge-of-chaos and undecidable dynamics in the context of cellular automata. This is briefly introduced in section 4.3 and discussed in more detail in Section 5.1.# 4.3. Game of Life\n\nTo illustrate how undecidable dynamics can be generated in a concrete system, let us introduce Conway\u2019s Game of Life \u2014 a well-known example of a discrete dynamical system [35]. It is a two-dimensional cellular automaton (CA) (see Appendix A.3), specified with a binary alphabet and a local update rule defined for the Moore neighbourhood with 9 cells, incorporating:\n\n1. Deaths. Any live cell with fewer than two or more than three live neighbours dies.\n2. Survivals. Any live cell with two or three live neighbours lives on to the next generation.\n3. Births. Any dead cell with exactly three live neighbours becomes a live cell.\n\nGame of Life dynamics produce coherent spatial patterns, including oscillators that repeat themselves after a fixed number of generations (e.g., see Fig. 6), and gliders that move across the grid replicating their structure. It has been demonstrated that CA carry out distributed computation, with oscillators representing information storage (i.e., memory) [64], gliders capturing information transfer (i.e., communications) [61], and glider collisions corresponding to information modification (i.e., processing) [62], with the computational processes forming coherent information structures [63].# Box 4: Arrows of time\n\nThere are several conceptually distinct arrows of time which are often interrelated in forming an understanding of the \u201casymmetry\u201d of time, explaining why processes in nature appear irreversible and why we perceive time as moving forward from the past to the future.# Thermodynamic arrow of time\n\nThermodynamic arrow of time is defined by the second law of thermodynamics, which states that entropy in a closed system always increases over time, leading to the irreversibility of natural processes.# Thermodynamic arrow of time and evolution\n\nBlum [10] discussed \u201cthe relationship between time\u2019s arrow (the second law of thermodynamics) and organic evolution, exploring irreversibility and direction in evolution, and arguing that evolutionary patterns may be predetermined by thermodynamic processes. Another hypothesis, Dollo\u2019s law, suggests that once a complex trait or structure has been lost by an organism in the evolutionary process, it is unlikely to be regained in exactly the same form [20]. In other words, Dollo\u2019s law posits that evolution is not generally reversible, although reversible evolution has been observed on relatively short evolutionary timescales [19].# Cosmological arrow of time\n\nCosmological arrow of time refers to the direction of time in which the universe is expanding, and is based on the observation that the universe is growing larger over time, as opposed to contracting.# Radiative arrow of time\n\nRadiative arrow of time aligns the direction of time with the way radiative processes, such as retarded electromagnetic radiation, the emission of light and sound waves, expand outward from their source, from a higher energy state to a lower one.# Causal arrow of time\n\nCausal arrow of time reflects the principle that cause precedes effect: perceived events are ordered in a way that causes come before their effects.# Quantum arrow of time\n\nQuantum arrow of time as defined in quantum mechanics relates time\u2019s direction to the collapse of the wave function. While the direction of time in a quantum system may be blurred due to uncertainty, when the system is measured, it transitions from a state of superposition to a definite state, which defines a temporal direction. An insightful perspective which may characterise other time arrows was offered by Seth Lloyd: the arrow of time is an arrow of increasing correlations [65].It has been shown that computational power of Game of Life is equivalent to that of a universal Turing machine [6], and thus, the Game of Life dynamics may be undecidable under some conditions [84]. In other words, it cannot be determined, for all initial configurations of the Game of Life, whether or not they will reach some predefined final (termination) configurations (see Appendix A.3).\n\nFigure 6: Two configurations of the oscillating polyomino pattern known in Conway\u2019s Game of Life as \u201ctoad\u201d.# 5. Cross-disciplinary perspectives on novelty generation\n\nIn this section, we draw insights from several background studies across the three examined domains \u2014 biological, computational and physical \u2014 and explore possible connections among (a) open-ended biological complexity, including major evolutionary transitions (Section 2); (b) open-ended computational meta-simulation, including successive resolution of lower-level undecidable problems (Section 3); and (c) complex dynamical systems, including chaotic and strange attractors (Section 4). In doing so, we will try to examine how the \u201cevolving evolvability\u201d is related to emergence of hierarchical structures comprising individual and collective information-processing elements, e.g., (a) formation of new units of selection, comprising ensembles of pre-existing organisms, (b) computational novelty generation, and (c) emergence of self-replicating dynamic behaviours at the \u201cedge of chaos\u201d.# 5.1. Chaos, incomputability, and undecidable dynamics\n\nCasti posited a link between the existence of chaotic dynamical systems and G\u00f6del\u2019s incompleteness theorem [16, p.317] [17, p.148]. Casti argued that\n\n\u201c...the theorems of a formal system, the output of a UTM [Universal Turing Machine], and the attractor set of a dynamical process (e.g., a 1-dimensional cellular automaton) are completely equivalent; given one, it can be faithfully translated into either of the others\u201d [16, p.317].\n\nBy this correspondence Casti investigated dynamical systems that compute Kolmogorov complexity of numbers / programs, which is the length of the shortest programs that can compute them. Casti then applied Chaitin\u2019s (Incompleteness) Theorem [18] on Kolmogorov complexity to prove such universal computing dynamical systems have strange attractors unreachable from any initial state, which Casti argued is equivalent to G\u00f6del\u2019s (first) incompleteness theorem. Analogously, Adams et al. [1] pointed out that simple one-dimensional cellular automata possess many states that cannot be reached from any initial state by repeated application of any of the complete set of 256 evolution rules. Such states may, however, become accessible if the evolution rules themselves evolve with time.\n\nSeveral fundamental relations between (i) formal systems, (ii) algorithms, and (iii) dynamical systems were identified by Prokopenko et al. [84]. The comparative analysis identified three common factors implicated in the generation of undecidable dynamics within the three examined computational frameworks: (i) the program-data duality (e.g., encoding of Turing machines as input data); (ii) access to an infinite computational medium (e.g., an infinite tape used by a Turing machine); and (iii) the ability to implement negation (e.g., the ability to flip accept and reject states).\n\nConsidering \u201cundecidable dynamics\u201d in a broad context, Prokopenko et al. [84] generalised the concept of incomputability across several computational frameworks. For example, in formal systems, a dynamic process can be identified with a proof: a sequence of well-formed formulas derived by inference rules and starting from an axiom. Correspondingly, G\u00f6del Incompleteness Theorem applicable to formal systems establishes the limits of provability in axiomatic theories, capitalising on the observation that some formal sentences can neither be proved nor disproved (e.g., the famous G\u00f6del sentence which encapsulated the Liar paradox).\n\nSimilarly, for a Turing machine, a dynamic computational process is provided by a sequence of machine states and tape patterns, starting from some input. The halting Problem is the canonical example of computational undecidability, capturing the fact that it cannot be established, for all program-input pairs, whether the computation reaches a predefined halting state or runs forever.\n\nFinally, one can consider an evolution of configurations \u2014 a dynamic trajectory \u2014 within a dynamical system such as a Cellular Automaton, starting from an initial state. Given suitably defined termination conditions (e.g., testing for fixed points or limit cycles), one may attempt to establish whether the trajectory is reaching the predefined attractors or continues to unfold along the \u201cedge of chaos\u201d, i.e., class IV Cellular Automata [125]. The latter scenario can be identified with undecidable dynamics; see also Section 4.1.\n\nIn summary, formal systems (e.g., logical systems), models of computation (e.g., Turing machines), and dynamical systems (e.g., Cellular Automata) are deeply related: these frameworks can produce universal computation and generate undecidable dynamics. This undecidability is manifested through three factors: self-reference, infinite computation, and negation [84].# 5.2. Computational novelty generation\n\nThe self-referential basis of undecidable dynamics is fundamentally related to novelty generation [84]. As pointed out by Markose [67, 68], computational novelty production and \u201cthinking outside the box\u201d by digital agents is underpinned by their capacity to encode self-referential statements with negation (e.g., Liar paradox or a G\u00f6del sentence). Crucially, this capacity allows the agents to exit from known listable sets (e.g., actions, technologies, phenotypes) and produce new structured objects. As discussed earlier (Section 3.2), this is a consequence of the fundamental discrepancy between (the spaces of) expressions and referents which is exploited by various diagonalisation arguments.\n\nSvahn and Prokopenko [99] explored how undecidability which places computational limits on a formal system can manifest itself in biological RNA automata. In particular, they considered Turing-equivalent RNA automata, i.e., the ones that can simulate, and be simulated by, a universal Turing machine. Importantly, they argued that the evolutionary space for these RNA automata can be expanded in a continual process analogous to a hierarchical resolution of computational undecidability by a sequence of Turing\u2019s oracles (and hence, by a sequence of Turing\u2019s ordinal logics and Post\u2019s extensible recursively generated logics). The proposed ansatz hypothesised that the resolution of possible undecidable configurations in biological RNA automata may represent a novelty generation mechanism, in context of interactions between the automata and their environment [99].\n\nThus, computational novelty can be seen as the problem-space expansion, created by agents that use the diagonalisation argument in exploiting the expression-referent discrepancy. In other words, novelty can be generated by self-modelling agents that have access to results of meta-level computation \u2014 for example, meta-simulation by Turing oracles and extensible logics (see Sections 3.3 and 3.4), or receive the corresponding information from external environment.# 5.3. Tangled hierarchies and strange loops\n\nIn his seminal book, Hofstadter [44] explored how self-reference leads to emergent complexity in systems, by examining recursive structures which appear in a self-similar way at different levels or scales. Importantly, Hofstadter discussed G\u00f6del\u2019s Incompleteness theorems, which show that in any formal mathematical system that is expressive enough to describe basic arithmetic, there will be statements that are true but cannot be proven within the system.\n\nHofstadter [44] proposed and extensively discussed the notion of tangled hierarchies (see Box 5) emerging in a wide range of phenomena, from language and cognition to artificial intelligence and consciousness. Self-reference plays a key role in this process, as it allows for feedback loops and recursive interactions between different levels of the hierarchy. In particular, Hofstadter argued that tangled hierarchies are fundamental to human cognition, which relies heavily on analogical thinking and self-referential loops. The concept of the strange loop encapsulated this idea, showing that patterns of thought may loop back on themselves to generate new levels of understanding.# 5.4. Causative efficacy of biological information\n\nWalker and Davies [111] argued that a hierarchy does not just encapsulate some quantity of information, but offers a specific information-processing arrangement describing its organisation, e.g., feedback loops, active information, and integrated information. Such an arrangement gives the higher levels in the hierarchy more functional \u201cpower\u201d, i.e. causal efficacy. In other words, information hierarchies may emerge (or self-organise) because they capture the causative efficacy of information:\n\n\u201c...biological information has an additional quality which may roughly be called \u2018functionality\u2019 \u2014 or \u2018contextuality\u2019 \u2014 that sets it apart from a collection of mere bits as characterized by its Shannon information content. ...DNA is a (mostly) passive repository for transcription of stored data into RNA, some (but by no means all) of which goes on to be translated into proteins. ...It is the functionality of the expressed RNAs and proteins \u2014 not the bits \u2014 that is biologically important\u201d [111].\n\nWalker and Davies [111] further pointed out that biological information (i.e., functionality) is actively involved in information processing, e.g., control and feedback. Thus, the functionality is dynamic, depending on both the current state and the history of the organism. This perspective \u2014 the algorithmic origins of life \u2014 suggests that in order to delineate the phases of non-life and life one needs to analyse dynamical information and identify causal architecture:\n\n\u201c...the emergence of life may correspond to a physical transition associated with a shift in the causal structure, where information gains direct and context-dependent causal efficacy over the matter in which it is instantiated\u201d [111].\n\nFollowing Davies [25], this work interpreted the phenotype-vs-genotype relationship as hardware-vs-software, identifying chemistry in living systems with hardware, and information (e.g., genetic and epigenetic) with software: \u201cthe chicken-or-egg problem, as traditionally posed, thus amounts to a debate of whether analogue or digital hardware came first\u201d [111]. In a related study, Walker et al. [110] discussed how the information efficacy and top-down causation can also be applied to the major evolutionary transitions [100].# 5.5. Evolution as collective information-processing dynamics\n\nThe emergence of genetic code can be modelled as an information-preservation phenomenon in the presence of noise [85]. In particular, the capacity to symbolically represent nucleic acid sequences was argued to emerge, overcoming the \u201ccoding threshold\u201d [123], in response to a change in environmental conditions. In modelling the emergence of universal coding, Prokopenko et al. [85] proposed the concept of \u201cstigmergic gene transfer\u201d, and considered interacting proto-cells as a dynamical system, within which \u201cproto-symbols\u201d encoding features of individual cells are stigmergically shared.\n\nThe model demonstrated that a joint encoding can emerge as a symbolic representation of the shared dynamics, in order to preserve information about attractors of the dynamics in a noisy environment: \u201c...the pressure to develop a distinctive \u201csymbolic\u201d encoding only develops if the noise in the original system is in a particular range, not too small and not too large\u201d [85].\n\nGoldenfeld and Woese [38] described evolution from the standpoint of non-equilibrium statistical mechanics, as a problem in which the key dynamical modes are collective. Taking a provocative stance \u2014 \u201cLife is Physics\u201d \u2014 they argued that unifying principles of collective behaviour which arise from physical interactions are applicable to biology, especially in context of the interplay between evolution and environmental fluctuations.\n\nIn exploring the principles underlying collective behaviour, Goldenfeld and Woese [38] posed a key question: \u201cHow is it that matter self-organizes into hierarchies that are capable of generating feedback loops which connect multiple levels of organization and are evolvable?\u201d [38]. Importantly, the study related co-evolutionary processes to far-from-equilibrium dynamics and pointed out that the corresponding physical laws remain unknown. Nevertheless, their review highlighted a salient connection between co-evolutionary/ecological dynamics and game-theoretic interactions which may generate paradoxical collective outcomes (e.g., Nash equilibria) and even chaotic dynamics (in dynamic game-theoretic models [92]).# 5.6. Evolving self-modelling dynamical systems\n\nAs discussed in Section 2.6, Watson et al. [112] studied the \u201cevo-ego\u201d relationship, describing a tension between individual and group interests. While not framing their analysis in terms of \u201ctangled hierarchies\u201d, the study nevertheless related the organisation of individual self-interested entities to their long-term collective interest, posing two questions:\n\n- \u201c(a) what kind of functional relationships between components are needed to make a new individual, and how they need to be organised; and\n- (b) how the organisation of these relationships arises \u2018bottom-up,\u2019 i.e., without presupposing the higher-level individual we are trying to explain\u201d [112].\n\nThe study pointed out that the chicken-and-egg dilemma (i.e., a strange loop), encapsulating these two questions, is typical across the major evolutionary transitions in individuality, as it is challenging to answer whether the higher-level unit of selection (needed for complex adaptations) emerges before or after the complex adaptations (needed to generate the higher-level unit of selection).\n\nIn attempting to resolve this conundrum, Watson et al. [112] drew an analogy with unsupervised deep learning, arguing that when individual reinforcement or selection modifies the strength of inter-unit relationships (analogously to updating the neural network weights during reinforcement learning), \u201cthe system becomes a self-modelling dynamical system\u201d with predictable system dynamics. In other words, based on interactions with the environment but without an explicit external guidance, a self-modelling system is able to discover some structure in its own constraints and dynamics \u2014 that is, construct its own model. This is analogous to unsupervised learning algorithms which, given the unlabelled input data, can analyse and learn patterns by clustering similar data points or reducing dimensionality.# 5.7. Information integration within collective action\n\nIn formulating the approach of evolutionary connectionism based on the analogy with machine learning, Watson et al. [112] identified the conditions required to evolve a novel organisation \u2014 a new level of individuality \u2014 comprising \u201cshort-sighted, self-interested entities\u201d, with the conditions necessary to learn non-decomposable functions. Put simply, the relationships between evolutionary units must be organised bottom-up in such a way that their interactions are synergistic, producing \u201cmore than the sum of the parts\u201d. Crucially, this distributed (unsupervised) learning can occur without an explicit system-level feedback, by exploiting regularities encountered at localised interactions.\n\nIn other words, interactions of bottom-level entities produce distributed learning dynamics and integrate information by computing a non-decomposable (i.e., non-linearly separable) function of input states, for example, between some \u201cembryonic\u201d collections of particles and the corresponding \u201cadult\u201d collective phenotypes [112]. In turn, this integrated information facilitates collective action, for example, generating \u201cspecific coordinated responses in multiple downstream variables\u201d \u2014 thus, signifying a downward causation. In terms of tangled hierarchies, the bottom-up process, generating a new step in evolutionary individuality, involves synergistic computation of non-decomposable functions which confer collective fitness, thus constraining interactions of the constituent units in a top-down way.# 5.8. Evolutionary connectionism vs ecological scaffolding\n\nWatson et al. [112] contrasted their \u201cevolutionary connectionism\u201d approach with \u201cecological scaffolding\u201d [9].# 2.7 Ecological Scaffolding and Evolutionary Transitions\n\nSection 2.7), emphasising that in the latter one has to assume existence of some \u201cfortuitous extrinsic conditions\u201d that trigger the population structure to generate specific selective pressures. In other words, they argued that ecological scaffolding resolves the chicken-and-egg problem of the evolutionary transitions in individuality by (temporarily) allowing the environment to assume the role of a \u201cchicken\u201d. Nevertheless, Watson et al. [112] pointed out that, even under these conditions,\n\n\u201c...individual selection at the lower level supports the evolution of characters that access synergistic fitness interactions, changing the relationships among the particles, and given that synergistic fitness interactions among particles have evolved, it is subsequently advantageous for particles to evolve traits that actively support this grouped population structure\u201d [112].\n\nIt can be argued that the ecological scaffolding is another way to shape a tangled hierarchy \u2014 albeit, without self-modelling. Once/if the scaffolding becomes redundant, the evolved \u201cstrange loop\u201d endogenously captures the collective dynamics, comprising both (i) the top-down distribution and dispersal of resources and (ii) the bottom-up computation shaped by the division of labour [9]:\n\n\u201cNow the original extrinsic ecological conditions might change or cease, but the population structure necessary to support higher-level selection is nonetheless maintained, supported by the adaptations of the particles\u201d [112].\n\nIn subsequent analysis, we will argue that developing the self-modelling capacity is a crucial step in biological complexification, improving the organism\u2019s capacity to adapt, replicate and function autonomously.# Top level: emergent macro-scale regularity\n\ndownward causation\n\nExternal observer (environment) operating at different scales\n\nbottom-up emergence# Tangled hierarchies\n\nTangled hierarchies are systems in which different levels of organisation or abstraction are intertwined in a way that defies simple hierarchical categorisation: \u201can interaction between levels in which the top level reaches back down towards the bottom level and influences it, while at the same time being itself determined by the bottom level\u201d [44]. In other words, tangled hierarchies interleave bottom-up emergence and top-down (downward) causation [31]. Fig. 7 illustrates the concept of tangled hierarchy, and Fig. 8 shows an example: shortest path formation during ant foraging.# Strange loops\n\nAs described by Hofstadter [44], \u201ca strange loop is a hierarchy of levels, each of which is linked to at least one other by some type of relationship. A strange loop is self-referential, meaning that it loops back on itself in some way. Despite the appearance of movement or progression, a strange loop ultimately returns to its starting point, creating a sense of paradox or contradiction.\u201d A strange loop involving ant foraging and optimal path formation, shown in Fig. 8 (see the example described in Section 6.2.1), lacks a preferred causal direction: are the ants driven by the pheromone gradient of the path (downward causation), or is the shortest path itself being generated by the ants movement (bottom-up emergence)?# Top level: emergent shortest path\n\nPheromone field stigmergy pheromone gradient\n\nNest I Food pheromone deposits# Figure 8:\n\nAn example of tangled hierarchy (a strange loop): ants foraging for food deposit pheromones which diffuse and evaporate in the environment; ants direct their movement towards locations with higher pheromone concentration, utilising stigmergy (indirect coordination through the environment); a shortest path emerges out of these interactions (bottom-up emergence), and influences the ant foraging (downward causation).# Figure 7:\n\nA tangled hierarchy (strange loop) with an external interpreter (e.g., observer, environment, interacting particle, etc.) which may operate and respond to regularities detected at different scales.\n\n15# Scales vs levels\n\nThe difference between scales and levels: \u201cscales\u201d imply a (continuous) progression along a single dimension (e.g., from fine-grained to coarse-grained), while \u201clevels\u201d typically suggest discrete distinct stages within a hierarchical structure.# Meta-levels and interpretation\n\nThere may be an external observer interpreting the interaction between the levels of a (tangled) hierarchy. This external interpreter is not part of the hierarchy, but may exchange matter and information with both interacting levels. For example, the environment may serve as an external, meta-level observer (or meta-simulator), determining the organism\u2019s fitness, and thus providing an \u201cinterpretation\u201d of the organism behaviour/dynamics. While hierarchies have \u201clevels\u201d, observer may employ different \u201cscales\u201d of observation: high-resolution (e.g., statistical mechanics), and low-resolution (thermodynamics) [83], distinguishing emergent phenomena by detecting and responding to regularities at a particular scale of observation, see Fig. 7.# 6. Tangled hierarchies and self-modelling\n\nHaving examined cross-disciplinary views on novelty generation in biological, computational and physical dynamical systems, we point out a common feature of complexification processes: emergence of hierarchical structures comprising lower-level (e.g., individual) and higher-level (e.g., collective) information-processing elements. These tangled structures exhibit varying degrees of self-reference, self-modelling and self-replication.\n\nTo explore the role played by self-modelling in the emergence of replicating information-processing hierarchies, in this section we propose a distinction between two types of tangled hierarchies: type I (strange loop without self-modelling) and type II (strange loop with self-modelling). We exemplify these two types in several contexts (subsection 6.2), and argue that different mechanisms are employed to replicate their dynamics (subsection 6.3).# 6.1. Two types of tangled hierarchies\n\nWe define a tangled hierarchy of type I (TH-I) as one where macro-scale information patterns which emerge from the micro-scale interactions within the environment are not compressed, encoded or decoded. In other words, no emergent macro-scale regularity is explicitly modelled in its entirety by micro-scale objects. Despite presence of a recursive flow between levels which \u201cloops back on itself\u201d [44], this loop does not involve compression \u2014 and hence, there is no object in TH-I that is explicitly referred to as \u2018self\u2019.\n\nIn contrast, a tangled hierarchy of type II (TH-II) is one where micro-scale objects operating at the bottom level contain an encoded, compressed representation of some information pattern emerging at the macro-scale. By utilising the compressed information, the micro-scale objects are capable of representing \u2014 that is, modelling \u2014 some of the macro-scale regularities. This modelling is used within a self-referential loop by alternating encoding and decoding processes. As a result, the compressed representation (i.e., a model) of a higher-level pattern, encoded at a lower level, augments the entire hierarchy with self-modelling, due to the tangled nature of the inter-level relationship (see examples of TH-II in subsections 6.2.2 and 6.2.4).\n\nIn general, there may be a spectrum of hierarchies between types I and II, with some tangled hierarchies utilising compression/modelling to a partial extent, and the general dichotomy we proposed essentially shows the limit cases.# 6.2. Examples of different TH types\n\nIn order to illustrate the two TH types and build a better intuition, we present several examples, mostly drawn from biology.# 6.2.1. Ant foraging, stigmergy and optimal path formation\n\nAnt foraging and optimal path formation in a pheromone field is a tangled hierarchy without self-modelling (TH-I). In this case, the tangled hierarchy is constituted by the ants foraging behaviour at the bottom level and the shortest path emerging at the top level (see Fig. 8). Each ant foraging for food deposits pheromone in response to only local information, without reference to the global (optimal) path. The path itself emerges as a result of stigmergy [101]: indirect interactions and coordination of the ants through the environment (Fig. 8). Importantly, ants perceive only local differences in the pheromone field (the pheromone gradient), and make only local field updates. The optimal path is a regularity within the pheromone field \u2014 however, this regularity is not compressed, and the information patterns underpinning stigmergy remain distributed through the environment, without any encoding, decoding or self-modelling.\n\nIt can be argued that some features of the emergent path (e.g., the likelihood of it being a shortest path) are attributable to (i.e., partially and indirectly encoded within) the ants genome. In other words, while the ant foraging behaviours belong to the ants immediate phenotype (being directly influenced by their genetic makeup), the class of optimal paths can be seen as a part of the ants extended phenotype [26]. That is, the species-dependent pheromone paths, shaped by the interactions between the ants and the environment, contribute to the genome evolution. In this process, the path optimality provides evolutionary rewards resulting in an eventual spread of the beneficial, fitness-increasing genotype. The encoded features are specifically the ones which are likely to produce shortest paths, given the environmental factors.\n\nHowever, any actual physical path emerging in a given environment \u2014 a specific regularity in the environment-dependent pheromone field \u2014 is not directly encoded in the# 6.2.2. Genotype\u2013phenotype relationship\n\nThe relationship between genotype and immediate phenotype involves a complex self-referential biological dynamics between the encoded genetic information and the organism itself in presence of environmental influences, as illustrated in Fig. 9. Genotype-phenotype relationship can be interpreted as a strange loop between the \u201cself\u201d (phenotype) and another object (genotype): the genotype \u201cencodes\u201d (models) the phenotype and is also \u201cdecoded\u201d by the phenotype (see Box 1). With sexual reproduction, the fitness of a genotype is manifested through its phenotype. In turn, the fitness of a given phenotype, i.e., the organism\u2019s chances of survival and reproduction, varies across different selective environments [54]. Hence, we interpret genotype-phenotype relationship as a tangled hierarchy with self-modelling (TH-II).# Top level: phenotype (decoded information)\n\nenvironmental influences\n\nepigenetic changes\n\nselective pressure inheritance\n\ngene expression:\nDNA\nRNA\nProtein# Bottom level: genotype (encoded information)\n\nFigure 9: Genotype-phenotype relationship. Gene expression involving transcription and translation of encoded genetic information leads to the decoding of an organism with observable phenotypic traits (bottom-up construction). Given the phenotype, selective pressure and inheritance influence genotype over generations, updating the encoded genetic information which encodes the phenotype itself (top-down causation). Genotype-phenotype relationship is realised in context of environmental influences, epigenetic changes and other factors. DNA image: Wikipedia [118].# 6.2.3. Phyllotactic patterning in plants\n\nAnother example of TH-I is the process of leaf positioning in plants. Leaves typically form at regular intervals at the plant apex, often creating recognisable spiral patterns. These patterns, however, are not directly encoded by the genome, but instead arise due to cell-cell interactions involving a feedback loop. The hormone auxin which triggers leaf outgrowth [89] is distributed by directional cellular transport [90]. Each cell assesses the concentration of auxin in neighbouring cells and transports auxin towards those neighbours in proportion to their auxin concentrations [49, 7]. This feedback, between auxin and its own transport, means that cells that happen to start with high concentrations receive even more auxin from neighbouring regions. However, at a certain distance defined by the relative strength of diffusion, another auxin peak forms and then another, as new space is created through cell divisions [49]. The resultant auxin distribution patterns are not explicitly encoded or compressed, and hence, cannot be replicated without replicating the entire system.# 6.2.4. Evolution of self-modelling collective dynamics\n\nThe \u201cevo-ego\u201d relationship between the \u201cembryonic\u201d evolutionary units which synergistically interact and integrate information in producing their \u201cadult\u201d collective phenotypes [112] is an example of TH-II. As pointed out by Watson et al. [112], the relationship between the lower-level (individual) and higher-level (collective) units of selection is a self-modelling dynamical system in which integrated information patterns encode (compress) non-decomposable functions of input states (see Section 5.7). Thus, we again encounter a self-referential loop with self-modelling: \u201cthe key problem is that evolution is self-referential, i.e. the products of evolution change the parameters of the evolutionary process\u201d [113].# 6.2.5. Ecological scaffolding without self-modelling\n\nAs noted in Section 5.8, a tangled hierarchy comprising collective interests may shape without self-modelling, using ecological scaffolding, i.e., information about the evolutionary niche [9, 112]. In this case, there is no self-modelling, as the division of labour does not use any compressed or encoded information about distributed and dispersing resources, and hence, this is an example of TH-I.# 6.2.6. Visual paradoxes\n\nWe now turn our attention to two well-known visual paradoxes, considering whether they may illustrate tangled hierarchies of different types.\n\nThe first paradox is an impossible staircase pattern (Fig. 10.Left), devised in 1937 by Oscar Reutersv\u00e4rd [104], rediscovered in 1958 by Lionel Penrose and Roger Penrose [79], and artistically implemented in Escher\u2019s lithograph print \u201cAscending and Descending\u201d (1960). In this pattern, the stairs are connected in an impossible way, i.e., the recursion is used within an impossible spatial configuration. Despite the appearance of a continuous staircase, there is an infinite loop of ascent and descent, so that the staircase appears to loop back on itself, creating an optical illusion of infinite repetition. However, there is no compression of any information pattern within a self-encoding object, and no self-modelling, suggesting that this is a tangled hierarchy of type I.Figure 10: Left: \u201cImpossible staircase\u201d (1958) [79, 119]. Right: \u201cDrawing Hands\u201d (Escher, 1948) [120].\n\nIn \u201cDrawing Hands\u201d (1948), Escher created a visual paradox by depicting two intertwined hands drawing each other into existence (Fig. 10.Right). Each hand is in the process of perpetually drawing the other, creating a recursive loop of drawing an object which draws itself. This lithograph not only explicitly captures the idea of self-reference and the infinite recursive loop, but arguably depicts how one object \u201cmodels\u201d (constructs) the other.\n\nYet, there is no discernible compression of the model in \u201cDrawing Hands\u201d per se, and so this example can be called a tangled hierarchy of type II only with a stretch. Nevertheless, the notion of \u201cself\u201d is pronounced in this lithograph much more than in \u201cImpossible staircase\u201d and \u201cAscending and Descending\u201d, showing a transient stage towards fully-formed TH-II, in which self-encoding may be achieved by compression. In short, \u201cDrawing Hands\u201d lithograph may illustrate a tangled hierarchy positioned between the extremes of TH-I and TH-II.# 6.3. Replication in tangled hierarchies\n\nThe distinction between the two types is also evident in different replication mechanisms available to dynamics within TH-I and TH-II.\n\nIn order to replicate information patterns and regularities emerging within TH-I (e.g., a pheromone path), the entire environmental state or complete dynamics needs to be reproduced (e.g., the entire pheromone field or the complete history of ant interactions from the initial state). Without compression and encoding, the information-processing is distributed throughout the entire system (e.g., stigmergy), and hence, needs to be replicated in every detail.\n\nComputation-theoretically, in order to simulate dynamics unfolding within TH-I, one would require a trivial universal simulator which is defined as a simulator describing the collection of all particular solutions \u2014 in other words, it \u201cneeds access\u201d to all corresponding Turing machines computing possible dynamic trajectories [39]. In this case, one may recall an insight of Brooks [14]: \u201cthe world is its own best model \u2014 always exactly up to date and complete in every detail\u201d.\n\nThe impossibility to replicate an optimal pheromone path emerging during ant foraging without reproducing the entire pheromone field (including the full pheromone gradient) reinforces our classification of this case as TH-I. If an optimal pheromone path was symbolically (i.e., digitally) encoded somewhere in the system, then it would have been possible to replicate just this encoding and expect that ants would access, \u201cread-out\u201d the encoded information, and rapidly rediscover the optimal connectivity.\n\nAn interesting but fatal phenomenon \u2014 \u201cant mills\u201d \u2014 is observed when an environmental effect causes ants to lose the pheromone path and start following each other instead, potentially driving them in \u201cdeath spirals\u201d to exhaustion and death [93]. As pointed out by Delsuc [28], \u201cthe occasional but deadly formation of circular mills seems to be the evolutionary price that army ants pay to maintain such an ecologically successful and stable strategy of collective foraging\u201d. Specifically, this pathological behaviour can be explained by the ability of army ants to collectively select a raid direction [24]. Arguably, this behaviour arises when the tangled hierarchy lacks adequate means to explicitly encode and replicate additional information (e.g., a preferred raid direction). In effect, in the absence of an explicit pattern to follow, ants are reduced to follow the only regularity that remains (a circular path).\n\nIn contrast, information patterns and regularities emerging within TH-II can be replicated more efficiently. For example, to replicate a phenotype sufficiently well, one would need to copy/clone a genotype, and place it in a similar environment. This is ensured by the compression and encoding of relevant genetic information within DNA. Computationally, to replicate dynamics of TH-II, one needs a non-trivial \u201csingleton\u201d universal simulator which provides a universal solution via a single universal Turing machine [39]. This solution is more efficient than the trivial universal simulator needed to describe the collection of all trajectories generated by TH-I dynamics.# 7. Emergence of self-modelling in tangled hierarchies\n\nIn principle, patterns emerging in TH-I (e.g., pheromone paths, hormone distribution gradients, ecological scaffolding) are compressible due to the presence of regularities. Once such compressed information becomes available to the system, it can then begin to process this information about itself \u2014 hence becoming a tangled hierarchy of type II. In this section we investigate the functional advantages enabled by such compression (i.e., the dimensionality reduction), which allows a self-referential system to be more predictive about its dynamics, exploiting the regularities more efficiently.# 7.1. Emergence of functional self-descriptions\n\nHaving discussed the difference between tangled hierarchies of two types, we now consider the questions of how and why a functional self-description may emerge, and what functional advantages it could provide to TH-II relative to TH-I. It is worth pointing out that TH-I which# 7.2. Synergistic fitness interactions exploit the discrepancy between \u201creferents\u201d and \u201cexpressions\u201d\n\nOur conjecture is that the evolutionary tension between individual and group interests [113, 115, 112, 71] is an example of a principled and generic \u201cinconsistency\u201d, directly related to the expression-referent discrepancy discussed in Sections 3.2 and 5.2. Thus, we argue that the major evolutionary transitions in individuality exploited a divergence between the sizes of the expression and referent phase-spaces (i.e., problem-spaces).\n\nWe argue that the tangled hierarchy linking the individual and group levels exhibits the expression-referent discrepancy \u2014 the reason is that not all collective interests are reducible to a (sum of) individual preferences. This creates inconsistencies between individual and group interests (\u201cindividual-level selection will oppose the creation and maintenance of adaptations that enforce selection atthe group level\u201d [115 ]). Computation-theoretically, our con-\n\njecture is that while individual interests are encodable via\n\na given set of referents, the non-decomposable synergistic\n\ngroup interests are not necessarily expressible via the given\n\nreferents. Thus, a disagreement between the individual\n\nand group interests can lead to either of two cases:\n\n- (i) in the case when \u201cmaximising the utility of the com-\n- ponents rather than the collective\u201d [ 113 ] dominates,\n- then robustness and stability of the current setup is\n- preferred (there is no transition);\n\n- (ii) otherwise, the tension is resolved by a transition\n- where \u201ccoordinated phenotypic differentiation is then\n- favoured and can thereby maximise collective fit-\n- ness\u201d [113], which in turn expands the phase-space.# simulation\n\nIn section 6 we introduced the distinction between tan-\n\ngled hierarchies with and without self-modelling. Then, in\n\nsection 7 we argued that a functional self-description helps\n\nthe TH-II system to innovate in two ways: by capturing\n\nand encoding (i.e., compressing) beneficial regularities and\n\npatterns in its dynamics, and preserving the resultant ex-\n\ntended information patterns. This, in turn, facilitates a\n\nmore efficient replication process.\n\nIn this section, we build on this premise and formulate\n\nour central conjecture, hypothesising that tangled hier-\n\narchies that emerge at various stages during biological\n\nevolution develop and expand in a continual, open-ended,\n\nprocess of self-referential dynamics and meta-simulation.\n\nThis process encounters and then resolves undecidability by\n\nexpanding the computational problem-space (\u201cjumping out\n\nof the system\u201d) during transitions, proceeding according\n\nto the following steps:\n\n1. Emergence of tangled hierarchy without self-modelling.\n2. The emergent TH-I (see Sec. 6) enables distributed\n\ncoordination (e.g., pheromone paths emerging as a\n\nresult of ant foraging and stigmergy) and imprecise\n\nreplication (e.g., \u201cstatistical proteins\u201d), but offers\n\nlimited stability.\n\nEncapsulation of compressed functional self-descrip-\n3. tion. This transitional step identifies some regularity,\n\ni.e., information pattern, which brings evolutionary\n\nadvantages (e.g., replication robustness), and then\n\nencodes this pattern within the lower-level of TH-I,\n\nproviding it with a functional self-description. In\n\ngeneral, this can be driven by a combination of ex-\n\nogenous and endogenous factors such as symmetry\n\nbreaking (e.g., division of labour) and information\n\npreservation in presence of external noise.\n\nFormation of tangled hierarchy with self-modelling.\n4. The formed TH-II (see Sec. 6) is capable of using\n\nencoded and decoded self-descriptions (e.g., a fully\n\nformed genotype-phenotype map). This has the fol-\n\nlowing consequences:\n\n- Self-reference. Given the encoding and decod-\n- ing capabilities, the formed expression-referent\n- relationship exhibits duality, with expressions\n- potentially becoming referents and vice versa,\n- e.g., program-data duality where programs can\n- be encoded as data and used as inputs (see Ap-\n- pendix A.1).\n\n- Undecidability. The expression-referent duality\n- inevitably brings undecidability for a given sys-\n- tem due to the expression-referent discrepancy:\n- there are always more expressions (objects) than\n- referents (encodings), and some expressions (pos-\n- sible objects) are inaccessible by the system \u2014\n- essentially, this is a diagonalisation argument.\n- In the context of genotype-phenotype relation-\n- ship, there are always more possible phenotypes\n- than the genotypes available under a particular\n- encoding scheme.\n\nExtension of the problem-space, due to the expression-\n\nIn general, the extensions described in step (4) are not\n\nmeant to be abrupt (notwithstanding the computation-\n\ntheoretic analogy of Turing jumps), and may develop by a\n\nslow accumulation of lower-level changes within subunits\n\nadapting over time, until a tipping point.\n\nThe entire process involves bottom-up emergence, self-\n\nmodelling, non-decomposable collective fitness (i.e., infor-\n\nmation integration) [112 ], and \u201cinformation self-creation\u201d [41 ].\n\nThese key features shape the tangled information hierar-\n\nchies in a way that generates computational novelty. Infor-\n\nmally, at a major transition, the collective dynamics may\n\nneed different \u201csymbols\u201d (a new \u201ccode\u201d) to efficiently de-\n\nscribe non-decomposable dynamics, relative to the symbols\n\ncurrently available to the individual subunits.\n\nIn general, this process follows a directional spiral, re-\n\nturning after step (4) to step (1) and forming a new TH-I,\n\ninitially without self-modelling. Resolving a tension ex-\n\npands the problem-space which allows the extended system\n\nto access a new, more complex, landscape of novel (col-\n\nlective) possibilities. However, the new landscape brings\n\nabout a new discrepancy (e.g., new biological contradiction,\n\nthat is, undecidability), thus continuing the process in an\n\nopen-ended irreversible way, represented by the biological\n\narrow of time.\n\nWe emphasise that the open-ended computational meta-\n\nsimulation, described in subsection 3.4, provides a computa-\n\ntion-theoretic interpretation \u2014 a kind of semantics \u2014 rather# 9. Discussion and conclusion\n\nIn this study we interpreted the phenomenon of open-ended biological complexity as a dynamic computational process and proposed a computation-theoretic argument for the biological arrow of time. Our argument follows G\u00f6del\u2013Turing\u2013Post recursion-theoretic framework which formalises the construction of extensible computational systems such as Turing \u03b1-oracle machines. We proposed that this open-ended generation of computational novelty involves meta-simulation performed by higher-order systems that successively simulate the computation carried out by lower-order systems. Essentially, this open-ended meta-simulation provides solutions to the undecidable problems encountered by the lower-order \u201csubunits\u201d and hence, expands the effective phase-space of possibilities.\n\nBefore concluding, here we briefly reflect on several studies which proposed fundamental principles for open-ended evolution and biological complexification, and point out connections with our approach.# 9.1. Evolutionary role of expanded genomes\n\nHeng and Heng [41] discussed (species-specific) karyotype or chromosomal coding, arguing that\n\n\u201cIf a change in karyotype coding generates new information at the system level, then an altered karyotype contains the necessary information for the emergence of a new genome system \u2013 the necessary information for macroevolution\u201c [41].\n\nWhile some changes in karyotype may promote macroevolution, larger genome or karyotype changes are unlikely to be sufficient for it on their own. Nevertheless, these changes may provide a pre-condition: a large random jump in the fitness landscape, followed by a period where the organism is able to explore locally within the landscape more easily than before.\n\nThe evolutionary role of an expanded genome is also emphasised by Bingham and Ratcliff [8] who reported an association between eukaryotic genome duplication and the evolution of multicellularity. This was contrasted with ancestral prokaryotes which tended to lose rather than accumulate DNA, and this may have prevented a transition of prokaryotes to multicellularity. We may interpret this difference between eukaryotic and prokaryotic genomes as the difference in their abilities to form a larger phase-space by adding more \u201cletters\u201d to their encoding schemes \u2014 essentially, as the difference in their self-modelling abilities.\n\nInterestingly, Bingham and Ratcliff [8] also highlighted that eukaryotic cells have a well-developed ability to deal with parasitic elements which form a significant part of eukaryotic genomes. This ability exemplifies an inconsistency-resolving element which may have also facilitated a transition to multicellularity.\n\nOne may draw a parallel between parasitic elements (invaders) and the \u201ccontrarian\u201d agents (i.e., Liar agents, in the sense of Liar paradox) which exploit the gaps in self-descriptions, e.g., computer viruses that change the host code to generate the outcome opposite to the intended computation [66], or novel antigens disrupting the \u201cself vs non-self\u201d distinction within autoimmune system and causing autoimmune diseases [36]. Markose [69] insightfully pointed out that the knockout of specific auto-immune regulators leads to the loss of self-representation for certain self-gene codes within the autoimmune system, thus generating autoimmune pathologies. These examples illustrate how the ability to implement negation contributes to generation of inconsistencies within a tangled information hierarchy with self-modelling.# 9.2. Increasing \u201cdynamic kinetic stability\u201d\n\nIn an attempt to reformulate Darwinian theory of evolution and extend it to inanimate matter, Pross [86] introduced the principle of Dynamic Kinetic Stability (DKS). The DKS principle, expressed in physicochemical terms, favours stable kinetic patterns balancing the rates of replication and decay (e.g., during autocatalytic reactions). This approach emphasised that certain relatively simple self-replicating systems (e.g., single molecules, oligomeric sequences comprising more than one subunit, minimal molecular networks) may have used imperfect replication while evolving toward replicating systems of greater DKS. Crucially, autocatalysis and cooperative behaviours were identified as common features of both abiogenesis (i.e., life emerging from nonliving matter) and biological evolution:\n\n\u201c...cooperative behavior can emerge and manifest itself at the molecular level, that the drive toward more complex replicating systems appears to underlie chemical, and not just biological, replicators. ...life\u2019s emergence began with the chance appearance of some relatively simple replicating chemical system, which then began the long road toward increasingly complex replicating entities.\u201d [86].\n\nThese elements \u2014 higher-order regularities (i.e., collective or cooperative behaviours) emerging out of interactions of imperfectly replicating subunits; self-referential (autocatalytic) reactions; and the stability of replication dynamics \u2014 can also be interpreted in terms of tangled hierarchies which continually expand their problem-spaces.# 9.3. Increasing \u201cfunctional information\u201d\n\nWong et al. [128] studied the roles of function and selection in evolving systems, and identified three universal evolutionary mechanisms utilising information about the system\u2013environment interactions: static persistence, dynamic persistence, and novelty generation. Information was interpreted as \u201cpatterns of data in a system that encodes about itself, its environment, or about its relation with its environment\u201d, while functions were interpreted as processes that have \u201ccausal efficacy over the internal state of a system or its external environment\u201d [128]. The study proposed a general law of increasing functional information:\n\n\u201cThe functional information of a system will increase (i.e., the system will evolve) if many different configurations of the system undergo selection for one or more functions\u201d [128].\n\nAmong core functions capable of perpetuating themselves, such as dissipation and homeostasis, this approach also highlighted that self-replicating systems, including living systems, are necessarily autocatalytic, and drew an analogy with the DKS framework. Going beyond the DKS approach, Wong et al. [128] identified information-processing as another self-perpetuating core function. The described information-centric account distinguished different kinds of dynamical persistence with respect to the distinct levels at which information patterns contribute to persistence. In particular, the study considered (i) information storage: \u201cmemory\u201d which allows for encoding associations, (ii) information inference of future states based on encoded memory: \u201cmemory-based prediction\u201d which provides a causal model and improves persistence, and (iii) counterfactual reasoning: \u201cprediction outside of memory\u201d which generates novelty through imagining previously nonexistent versions of reality [128].\n\nThe increasing dynamic kinetic stability and the increasing functional information reflect the arrow of time. We contend that both these principles can be subsumed by the computation-theoretic G\u00f6del\u2013Turing\u2013Post characterisation of the open-ended meta-simulation by systems that expand their problem-spaces in the search of ways to resolve specific contradictions and tensions. In particular, \u201ccounterfactuals\u201d which are required to generate novel functional information [128] can be formed only by considering negation, typically in context of the Liar paradox.# 9.4. Assembly theory and the \u201cadjacent possible\u201d\n\nA recent proposal on \u201cassembly theory\u201d (AT) also attempted to explain and quantify selection and evolution in the context of novelty generation:\n\n\u201cThis approach enables us to incorporate novelty generation and selection into the physics of complex objects. It explains how these objects can be characterized through a forward dynamical process considering their assembly. By reimagining the concept of matter within assembly spaces, AT provides a powerful interface between physics and biology. It discloses a new aspect of physics emerging at the chemical scale, whereby history and causal contingency influence what exists\u201d [94].\n\nIn assembly theory, an object is defined through its possible formation histories in an \u201cassembly space\u201d, so that \u201cobjects are made by joining elementary building blocks together recursively to form new structures\u201d [32]. Essentially, given the building blocks available at the time, the assembly space is a problem-space that comprises possible pathways for assembling an object.\n\nThis view can be compared with the concept of the \u201cadjacent possible\u201d proposed by Kauffman [50]: the set of all potential configurations that are just one step away from the current state of a system. These possibilities are constrained by the existing components, structures, or knowledge of the system. Thus, the innovations arise from the existing system\u2019s state and the adjacent possibilities that are immediately accessible from that state.\n\nThe assembly theory and the \u201cadjacent possible\u201d concept identified a discrepancy between the space of objects that can be constructed using actually accessible blocks, and the space of objects that are conceivable. Computation-theoretically, this discrepancy can be interpreted as the expression-referent discrepancy which generates contradictions, and forces an expansion of the problem-space by meta-simulating and discovering new (assembly) pathways.",
        "context_id": 17,
        "question": "Which law states the impossibility of fully transforming disordered heat energy into coherent work without the expense of an additional resource?",
        "answer": [
            "Second law"
        ],
        "context_length": 92174
    },
    {
        "context": "# I. INTRODUCTION\n\nQuantum computers are poised to deliver algorithmic speedups for a broad range of application in science and industry[1\u20133]. However, realizing these speedups requires overcoming the challenge presented by the noise which limits the computational power of today\u2019s quantum devices. Error correction[4] provides a scalable path to fault-tolerance and has shown significant progress in hardware recently[5\u201314]. Nonetheless, quantum error-correction imposes large overheads, making it challenging to execute even small-scale applications fully fault-tolerantly. As a result, fully fault-tolerant demonstrations of quantum algorithms for practical applications have been out of reach of experiment, despite immense progress in implementation and benchmarking of algorithmic components, such as preparation of magic states[15], one-bit addition[16] and quantum Fourier transform[17].\n\nQuantum error detection (QED) codes provide an opportunity for partially fault-tolerant implementation of algorithms in the near term[9, 10, 18\u201322]. While non-scalable, they can still deliver improved algorithmic performance beyond what is possible without protecting against noise[23\u201326]. The protection offered by QED codes opens an opportunity to use quantum computers to study the performance of quantum algorithms for sizes and noise rates beyond classical simulation.\n\nThe recently proposed [[k + 2, k, 2]] \u201cIceberg\u201d QED code[18] is particularly suitable for near-term algorithms due to its ability to encode expressive circuits, using a universal set of local and global logical rotations, with a low overhead. The Iceberg code has been demonstrated to improve the fidelity of random circuits with up to 8 logical qubits and 1323 physical two-qubit gates[18], the performance of quantum phase estimation with up to 4 logical qubits and 920 two-qubit gates[25], and the fidelity of ground state preparation with probabilistic imaginary-time evolution with 4 logical qubits and up to 906 two-qubit gates[26]. While not fully fault-tolerant, these experiments provided preliminary evidence that for circuits with small numbers of qubits the Iceberg code can improve algorithmic performance.\n\nQuantum Approximate Optimization Algorithm (QAOA)[27, 28] is a quantum optimization heuristic applicable to a broad range of combinatorial optimization problems in finance and other industries[29\u201332]. QAOA has been shown to provide a quantum algorithmic speedup over state-of-the-art solvers for some problems[33, 34], motivating its implementation on hardware. While relatively low resource requirements enabled QAOA execution on non-error-corrected hardware[35\u201342], realizing the speedup offered by QAOA is widely believed to require fault tolerance[43, 44]. We remark that both quantum hardware performance and the impact of quantum noise on QAOA have been subject of extensive interest[45\u201352].\n\nWe demonstrate a partially fault-tolerant implementation of QAOA applied to the MaxCut problem on Quantinuum H2-1 trapped-ion quantum computer[54] with the Iceberg code. At the time of experiments, the H2-1 device had 32 all-to-all connected qubits and 99.8% two-qubit gate fidelity[38]. We execute circuits with up to 24 logical qubits encoded into up to 26 physical.# A. Quantum Approximate Optimization Algorithm\n\nQAOA is a quantum algorithm for combinatorial optimization. It solves optimization problems by preparing quantum state using a sequence of \u03c9 layers of alternating cost Hamiltonian and mixing Hamiltonian operators, parameterized by vectors \u03c9 and \u03b5 respectively.\n\n|\u03d1\u2191 = e \u2193i\u03c9 M e \u2193i\u03b5 \u03c9 H \u00b7 \u00b7 \u00b7 e \u2193i\u03c9 1 M e \u2193i\u03b5 1 H |\u03d1 0 \u2191 \u03c9 (2)\n\nThe parameters \u03c9 and \u03b5 are chosen such that the measurement outcomes of |\u03d1\u2191 correspond to high-quality solutions of the optimization problem with high probability. In this paper, we take the initial state |\u03d1 0 \u2191 = |+\u2191 \u2194k as the equal superposition of all possible candidate solutions, and the mixing Hamiltonian as a summation of all single-qubit Pauli-X operator M = \u2211 n=1 X . i i\n\nDenoting the value of optimal cut by f max, we can quantify how well QAOA with state |\u03d1\u2191 solves the Max-Cut problem by computing the approximation ratio:\n\n\u03b5(\u03d1) = |E| \u2192 \u2194\u03d1|H|\u03d1\u2191. (3)\n\nRecent progress in parameter setting heuristics has considerably advanced the execution of QAOA in the early fault-tolerant era [42, 58], with good parameter choices available for many problems. A set of parameters that leads to good approximation ratios was proposed in [59] for the MaxCut problems on regular graphs that we solve in this work. Throughout our paper we use these \u201cfixed angles\u201d to set QAOA parameters in the experiments.# B. Iceberg Code\n\nThe Iceberg code protects k (even) logical qubits with n = k + 2 physical qubits and two ancillary qubits. We label the physical qubits as {t, 1, 2, . . . , k, b}, where the two additional qubits are called top t and bottom b for convenience. The two code stabilizers and the logical gates of the QAOA circuit are implemented as the physical gates:\n\nexp(\u2192i\u03d1 Xi \u00af ) = exp(\u2192i\u03d1X X i ), t (8)\n\nexp(\u2192i\u03d6 Zi \u00af\u00af Z j ) = exp(\u2192i\u03d6Z Z j ). i (9)\n\nIn Quantinuum devices, these physical gates are implemented by just one native two-qubit gate exp(\u2192i\u03f1Z Z j )i and various single-qubit Clifford gates.\n\nAs depicted in Fig. 1A, the Iceberg code employs an initialization block to prepare the initial QAOA state | + \u00af\u2191 \u2194n in the common +1 eigenspace of the stabilizers. The logical QAOA gates in Eqs. (8) and (9) are then implemented in blocks, interleaved with syndrome measurement blocks until the QAOA circuit is complete. These syndrome measurement blocks measure the stabilizers regularly across the circuit to prevent the accumulation of noise. The final measurement block measures the stabilizers as well as the k data qubits. The precise form of these blocks is depicted in Appendix A. Accepted samples can be decoded by a classical post-processing and serve as candidate solutions for the problem.\n\nTo detect these errors, the fault-tolerant initialization, syndrome measurement and final measurement blocks employ two ancillas. In the absence of noise, the state remains purely in the +1 eigenspace of the stabilizers during the entire circuit execution and ancillas always output a +1 when measured. The final measurement block additionally measures the stabilizer S Z, which is also expected to be measured as +1 in the absence of noise. Therefore, a \u21921 output in any of them signals the presence of an error caused by noise, and the circuit execution is discarded.\n\nThe fault-tolerant design of the initialization, syndrome measurement and final measurement blocks ensures that no single faulty component in these blocks (like a two-qubit gate) can cause a logical error. In contrast, our logical gates, despite being natural for the hardware, are not fault-tolerant, as some errors in their physical implementation cannot be detected. We nevertheless show in Sec. III B 2 that undetectable errors are rare, rendering the QAOA protection of the Iceberg code effectively fully fault-tolerant.# III. RESULTS\n\nWe now present our results. First, we summarize the results obtained on the hardware. Then, we discuss the model fitting results and the performance predictions on future hardware. The H-series emulators [8, 55, 56] we use perform a state-vector simulation where noise is randomly sampled following realistic noise models and then inserted into the circuit. Currently, the most influential noise channels are gate errors and single-qubit coherent dephasing from memory errors. The performance gap between the hardware and emulator experiments is discussed in Appendix B for completeness.# A. Iceberg code protection of QAOA on hardware\n\nThe performance of the Iceberg code with QAOA on 3-regular graph MaxCut on the Quantinuum H2-1 quantum computer [54] is shown in Fig. 1C. The logical fidelity reported in this figure is estimated from the average energy measured experimentally by assuming a global white noise model distribution, as described in Sec. V C. We fix the QAOA depth to \u03c9 = 10 and vary the number of logical qubits k, randomly selecting one MaxCut graph instance per k. For each problem we run QAOA unencoded and QAOA protected by the Iceberg code with three intermediate syndrome measurements. Throughout this paper, the final measurement is counted as a syndrome measurement, so in the previous experiment we say that four syndrome measurements are used. The Iceberg data has larger error bars due to the smaller number of post-selected samples.\n\nThe histogram in Fig. 1D reports the hardware shots of several Iceberg and unencoded QAOA circuits for k = 16. After post-selection, the output distribution has higher weight on bitstrings with higher approximation ratio, as expected from a better protection against noise.\n\nWe compare the performance with that of the Pauli check sandwiching (PCS) [53, 60], an error detection scheme with a similar motivation to that of the Iceberg code. PCS uses pairs of parity checks to detect some but not necessarily all errors that occur in a given part of the circuit. The parity checks are chosen based on the symmetries already present in the circuit. For QAOA circuits considered in this work, the problem Hamiltonian X\u2194k Z\u2194k, so we use them as the checks of our PCS experiments. To unify the notation and checks is denoted as one syndrome measurement. For example, one syndrome measurement in PCS means that we select one cost Hamiltonian layer e\u2193i\u03b5H and sandwich it with two parity checks. The overhead of one syndrome measurement in PCS includes an additional 4k two-qubit gates, along with two ancillas. The comparison with PCS on a k = 18, \u03c9 = 11 QAOA circuit is shown in Fig. 1B and E, where all data are from the H2-1 emulator (H2-1E). We observe that PCS leads to a lower logical fidelity that does not increase with the number of syndrome measurements. At this scale, the large overhead and the non-fault tolerant design of the PCS method decreases the circuit performance. At the same time, we observe that the Iceberg code can effectively improve the QAOA performance and obtain a higher logical fidelity than the unencoded circuit with four syndrome measurements.# B. Estimated performance from our model\n\nTo understand the protection capability of Iceberg code, we propose the performance model of Sec. V B for the unencoded and Iceberg code circuits. The model outputs analytical functions of the logical fidelity Fune for the unencoded circuits, the logical fidelity Fice for the Iceberg code circuits, and the post-selection rate 1 \u2192 D for the Iceberg code (D is the discard rate). Inputs from the circuit are the numbers of logical qubits k, logical single-qubit gates g1, logical two-qubit gates g2, and syndrome measurements s. From the hardware, the model for the unencoded and the Iceberg code respectively inputs only one and three error rates related to the noise produced by two-qubit physical gates. These are motivated and described in more detail in the next section. In this work we leave the error rates as fitting parameters so that, when fitted to data from the H2-1 emulator the fitted values incorporate corrections from other noise sources.\n\nSince QAOA experiments on hardware or emulators output the approximation ratio instead of the logical fidelity, we extend the performance model by approximating the noise distribution as that of a global white noise. Details are discussed in Sec. V C.# 1. Dataset and model validation\n\nWe use the emulator of the Quantinuum H2-1 quantum computer to generate a dataset with varying number of logical qubits in the range k \u2198 [8, 26], QAOA layers in the range \u03c9 \u2198 [1, 11], and syndrome measurements in the range s \u2198 [1, 8]. This dataset contains 115 circuits for the Iceberg code and 56 for the unencoded circuits. We take 1000 shots for the unencoded circuits and 3000 shots for the Iceberg code circuits before post-selection.\n\nFrom this dataset we select partial data that have relatively stable logical fidelity and large numbers of two-qubit gates to fit the model. For the Iceberg code model,\n\n|Parameter|pcx|pc|pa|p\u03c9|\n|---|---|---|---|---|\n|Emulator|1.28e-3|[1.3e-05, 3.2e-05]|[4.3e-4, 1.1e-3]|[4.7e-4, 1.0e-3]|\n|Model|5.5e-3|7.0e-5|2.2e-3|4.4e-4|\n|CI|[5.0e-3, 6.2e-3]|[4.3e-5, 9.5e-4]|[1.9e-3, 2.5e-3]|[4.0e-4, 5.0e-4]|\n\nTABLE I. Error rates from the H2-1 emulator [8, 55, 56] and the performance model.# 3. Frontiers of the Iceberg Code performance.\n\nNext, we analyze the performance of the Iceberg QAOA circuits based on the fitted model. We report the difference Fice \u2192 Fune in logical fidelity between Iceberg and unencoded circuits in Fig. 3A, and the post-selection rate of Iceberg circuits 1\u2192D in Fig. 3C, for varying numbers of syndrome measurements.\n\nAs observed from the shift of the breakeven frontiers in (red lines) in Fig. 3A, the QAOA logical fidelity stabilizes as the number of syndrome measurements increases, even though the post-selection rate, indicated by the orange.# 4. Predicting performance on future hardware\n\nWe now use our model to predict the performance of QAOA with the Iceberg code on future quantum hardware. To study this, we extrapolate the model performance by scaling all the model parameters in Table I by a varying factor. A smaller factor corresponds to smaller effective error rates, indicating higher fidelity of the quantum hardware. Scaling all error rates down by the same factor is clearly an additional simplification, as hardware development will not necessarily reduce all noise sources homogeneously and at the same pace. Nevertheless, this analysis provides a valuable qualitative perspective on the potential performance in a foreseeable scenario.\n\nAs shown in Fig. 3B and D, as the factor decreases, we observe a significant shift of the performance frontier to larger number k of logical qubits while the post-selection rate improves dramatically. This indicates that with higher-quality quantum hardware, we can push the breakeven frontier of logical fidelity to deeper circuits on larger problem instances with less post-selection overhead.\n\nTo elucidate the conditions for QAOA to become competitive with classical solvers, we use our model to answer the question of when a QAOA hardware experiment can outperform the Goemans-Williamson (GW) algorithm [57] in terms of approximation ratio. As reported in the literature [59], a noiseless QAOA with fixed parameters has been able to surpass the GW algorithm on small graphs. In Fig. 4A, we show an example of solving k = 16-node 3-regular graphs with noiseless QAOA, GW, and Iceberg QAOA with four syndrome measurements as well as unencoded QAOA emulated on the H2-1 emulator. The noiseless QAOA is able to outperform GW for \u03c9 \u21d2 6 layers. However, both the Iceberg and unencoded QAOA have not yet surpassed GW.\n\nSpecifically, at \u03c9 = 10, the approximation ratio of noiseless QAOA is 0.9810 . . ., while the average approximation ratio for GW is 0.9554 . . .. This implies that the logical fidelity of a noisy QAOA must be of at least 0.9554/0.9810 \u21d0 0.974 to outperform GW, assuming our white noise model approximation. In Fig. 4B, we vary the scaling factor of the model parameters to determine when this breakeven logical fidelity can be achieved. The results indicate that the Iceberg and unencoded circuits require scaling factors of approximately 0.81 and 0.60, respectively.# Post-selection rate\n\n| |A|B|\n|---|---|---|\n| |0.97|Iceberg QAOA|\n| |0.95|Unencoded QAOA|\n|0.90|0.95|Goemans-Williamson|\n|0.85|Noiseless QAOA|Iceberg QAOA|\n|0.80|Unencoded QAOA|0.94|\n| |Goemans-Williamson| |\n\n1.0 0.9 0.8 0.7 0.6 0.5 Scaling factor\n\nNoise decreasing\n\n2 4 6 8 10# FIG. 4.\n\nHardware improvement necessary for QAOA to become competitive with the Goemans-Williamson algorithm. A Solve k = 16 MaxCut using different solvers. Each data is reported as the mean of approximation ratio over 100 k = 16 3-regular graphs. The standard errors are too small to be seen. B Scaling of model parameter to beat Goemans-Williamson (GW) algorithm for k = 16 graphs. The Iceberg code helps the QAOA to beat GW earlier than an unencoded one.# Comparison of Models\n\n||E|=27||E|=45|\n|---|---|\n|0.9|0.9|\n|0.8 Iceberg model|Iceberg H2-1E|\n|0.7 Unencoded model|Unencoded H2-1E|\n|0.6| |\n|0.3| |\n|0.2| |\n|0.1| |\n\n1 2 3 4 5 6 7 8 # syndrome measurements\n\n1 2 3 4 5 6 7 8 # syndrome measurements# FIG. 5.\n\nComparison between experimental data and model predictions on random Erd\u02ddos\u2013R\u00b4enyi graphs with different numbers of edges. The prediction of Iceberg logical fidelity is less accurate compared to testing on 3-regular graphs, while the prediction of unencoded logical fidelity and the prediction of post-selection rate remain accurate.# Sherrington-Kirkpatrick model\n\nThis is because the Iceberg code can execute two-qubit logical rotations with no overhead, whereas one-qubit logical rotations require a noisier two-qubit physical gate. Iceberg QAOA could be more advantageous for dense graph problems where the number of two-qubit Z \u21942 terms in the problem Hamiltonian is much larger than the number of single-qubit X terms in the mixing Hamiltonian.\n\nSecond, the compilation of Iceberg circuits could be further optimized. We speculate that the high deviations of the error rates observed between the emulator and fitted error rates in Table I could be explained by a larger amount of memory noise accumulated in the highly sequential syndrome measurement blocks and the QAOA mixing layer than in the optimized unencoded circuits. Currently, we are using pytket [61] to compile the logical gates alone, but a better strategy that jointly compiles the logical gates and the error detection blocks of the Iceberg code could potentially improve hardware performance while reducing execution time.\n\nWhile performance extrapolation indicates promising results with improved hardware fidelity, we observe that, with fixed model parameters, increasing the number of syndrome measurements marginally diminishes the performance gains in the extrapolation heatmaps. Additionally, the overhead of post-selecting samples grows rapidly. This observation is consistent with the experimental results on the H2-1 emulator, indicating that the protective power of the Iceberg error detection code is limited. This reinforces the need for quantum error correction to achieve error rates low enough to run large-scale circuits.# Performance model\n\nThis section introduces the detailed model to predict the performance of the unencoded and Iceberg code circuits. To build the model outputs efficiently, we consider the following noise channels:\n\n- Uniform two-qubit depolarizing channel with error rate p \u03d1 after every two-qubit logical gate of the unencoded circuit: insert a random Pauli error from the set P \u21942 \\ {I \u21942 } on the gate support.\n- Uniform two-qubit depolarizing channel with error rate p cx after every two-qubit cnot gate in the error detection blocks of the Iceberg code circuit: insert a random Pauli error from the set P \u21942 \\{I \u21942 } on the gate support.\n- A noise channel with error rate p c that introduces a random error that commutes with both stabilizers after every logical gate of the Iceberg code: insert a random Pauli error from the set {X \u21942 , Y \u21942 , Z \u21942 } on the gate support.\n- A noise channel with error rate p a that introduces a random error that anti-commutes with both stabilizers after every logical gate of the Iceberg code: insert a random Pauli error from the set P \u21942 \\ {I \u21942 , X \u21942 , Y \u21942 , Z \u21942 } on the gate support.# Location of syndrome measurements\n\nAs depicted in Fig.1A, in this work we place syndrome measurements evenly spaced in the circuit so that every block of logical gates has roughly the same number of logical gates.\n\nIn Fig. 6, we provide evidence supporting this strategy. The k = 16, \u03c9 = 11 logical QAOA circuit is evenly partitioned into eight blocks of logical gates and we introduce a single syndrome measurement in the seven intermediate positions between them. By comparing the logical fidelity obtained from these seven circuits, we find that the circuit with the syndrome measurement inserted in the middle (labeled 4) presents the highest fidelity, at the cost of the smallest post-selection rate.# Update rules\n\nL \u2194 H(1 \u2191 P0 (pc, g))P0 (pa, g) + LP0 (pa, g) + 3/4 (H + L + E)P2 (pa, g)1/3 EP1 (pa, g) + 1/4 (H + L + E)P2 (pa, g)\n\nE \u2194 EP0 (pa, g) + (H + L + 2/3 E)P1 (pa, g) + D \u2194 D",
        "context_id": 18,
        "question": "What is the name of the trapped-ion quantum computer used in the experiments with the Iceberg code?",
        "answer": [
            "H2-1"
        ],
        "context_length": 20412
    },
    {
        "context": "# 1 Introduction\n\nThe main goal of machine condition monitoring is, as the name implies, to monitor the condition of industrial applications, and one of the crucial parts in these applications is rotating machinery. When considering this type of machinery, it has been determined that the main cause of failure is attributed to Rolling Element Bearings (REB) (Nabhan et al, 2015). Currently, the most common method to monitor these REB is currently by capturing and analyzing the vibrations produced by REB (Hoang and Kang, 2019). However, acoustic signals have also been used as a promising alternative (Pacheco-Ch\u00e9rrez et al, 2022), and allow for easier data collection.\n\nThe work regarding this condition monitoring can be mainly split into two branches. The first branch considers a diagnostic problem, where a signal is either classified as normal or anomalous, otherwise called Anomaly Detection (AD), or it can be further categorized into specific fault types, depending on the problem and considered method (Jiang et al, 2019). The other branch considers a prognostic problem, where a signal is given a condition score, or Condition Indicator (CI), that assesses the condition of a machine (She et al, 2020). This CI can then be used as a basis to predict the Remaining Useful Life (RUL) of a machine (component) or can be used to determine if the state of the machine is considered normal or anomalous based on some predefined threshold.\n\nFor both AD and CI estimation research has been done using signal processing, e.g. traditional clustering (Knorr et al, 2000) or shallow Machine Learning (ML) (Widodo and Yang, 2007), and data-driven, mainly Deep Learning (DL) (Ionescu et al, 2017; Said Elsayed et al, 2020), methods, with the latter receiving more attention in recent years (Zhou et al, 2022; Zhang et al, 2020). This work will focus on using data-driven DL methods.\n\nAD is an important task in a broad range of domains, such as, network intrusion detection (Yang et al, 2022), finance (Ahmed et al, 2016), and machine condition monitoring (Kamat and Sugandhi, 2020). The most common DL approach used for AD is an AutoEncoder (AE) or an AE-based variant (Chalapathy and Chawla, 2019). These methods work by encoding data into a compact latent representation in a manner that allows the original data to be reconstructed as well as possible (Hinton and Salakhutdinov, 2006). By applying this on normal data, it is expected that the reconstruction of anomalous data is worse, as there are differences between both types of data.\n\nThis can be seen in (Oh and Yun, 2018), where an AE was used to perform AD on acoustic data from a surface-mount device assembly machine. A similar methodology was used in (Givnan et al, 2022), where a stacked autoencoder was used to perform AD on sensor data from industrial motors. A downside of this type of approach is the inability to use anomalous data during training, which can provide valuable information, hence, approaches that allow the use of anomalous data are of interest (Primus et al, 2020; Liu and Gryllias, 2020). This additional information can be used by either semi- or fully supervised methods. However, it is indicated in (Arunraj et al, 2017) that fully# 3\n\nSupervised methods only have a better performance in case all types of anomalies are known during training. A similar conclusion is made in (Zhang et al, 2021), where it was indicated that incorrect labeling reduces the performance of a supervised method, while keeping these label uncertainties unlabeled in a semi-supervised approach mitigates this issue. Since it is unlikely that all types of anomalies are known during training and label uncertainties are possible, semi-supervised methods are likely to, in general, outperform fully supervised methods.\n\nA commonly used semi-supervised method is Deep Support Vector Data Description (DSVDD) (Ru\u21b5 et al, 2018, 2019), which attempts to map normal data close to a center and abnormal data further away from the same center, and is an adaptation of the original Support Vector Data Description (SVDD) method (Tax and Duin, 2004) into the DL framework. However, this adaptation requires to restrict the DL network used by the method, so that trivial solutions are avoided (Ru\u21b5 et al, 2018, 2019): (i) the center used by DSVDD cannot be initialised as 0, (ii) the network cannot use biases, and (iii) the network cannot have bounded activation functions.\n\nIn (Liu and Gryllias, 2021) a DSVDD model was used to perform AD on vibration data from helicopters. They made a comparison between the data-driven DSVDD method and a traditional SVDD method with engineered features, showing an improved performance for the DSVDD method. A similar approach was used in (Peng et al, 2023) to monitor the condition of wind turbines. The DSVDD method was also compared to traditional SVDD as well as to a regular AE, again showing an improved performance, although the difference with the AE was not large.\n\nThese restrictions were alleviated in a combined AE and DSVDD approach, where the method will still learn to map normal data close to a center, as per the DSVDD objective. However, a trivial solution, where all the normal data is mapped to a single point, will be prevented by the added reconstruction objective of the AE (Zhang and Deng, 2021; Zhou et al, 2021; Hojjati and Armanfard, 2024).\n\nAs discussed earlier, next to the diagnostic AD problem, a prognostic CI estimation problem can also be considered. DL methods have already been applied to this problem in previous works. In (Su et al, 2020) a Variational AutoEncoder (VAE) is combined with different types of Recurrent Neural Networks (RNN) to estimate the RUL of aircraft engines. A pretrained VGG16 model was used in combination with SVDD in (Mao et al, 2020), by first adapting the VGG16 model to the bearing data, after which the features obtained from this model were passed to the SVDD algorithm to estimate the CI.\n\nA Multi-Task DSVDD approach was used in (Shi et al, 2021) to estimate the CI for different bearings. This was done by creating a hypersphere around a center for each different bearing and introducing a hypersphere similarity loss that attempts to overlap these hyperspheres as much as possible. A similar approach was used in (Kou et al, 2022). However, instead of attempting to create overlapping hyperspheres, a set of centers is chosen using prototype clustering and these are used to determine the probability of a point to be assigned to one of these centers. In (Wang et al, 2023) a combination of a CNN and RNN was used to perform CI estimation based on the vibration data and an enhanced empirical mode decomposition algorithm.\n\nThere are also methods that detect anomalies when the CI goes above some predefined threshold (assuming a low CI corresponds to normal behavior while a high CI corresponds to deviating behavior) (Liu and Gryllias, 2020). However, determining# Threshold Setting in Machine Learning\n\nThis threshold is a nontrivial problem as it often depends on the specific application, e.g. false negatives might be undesirable, which evidently influences the threshold. Different methods to set this threshold exist. They can be split into three groups:\n\n1. Methods that rely on statistics from the training data, such as the mean and standard deviation or the maximum (Wang et al, 2023; Kou et al, 2022).\n2. Methods that use additional data, e.g. the test set, to set a threshold to optimize a metric (Garg et al, 2022).\n3. Methods that dynamically determine the threshold based on the data that is being evaluated (Jia et al, 2022).\n\nIn this work, it is opted to focus on methods belonging to the first group, as these do not require additional data or analysis. It has been empirically observed that choosing a threshold based on these methods, as is expected, shows a strong dependency on the training and initialization of a model, especially when DL models are used. A more robust thresholding method, called Constraint-Guided AutoEncoders (CGAE) was proposed in (Meire et al, 2023a), where a fixed threshold was defined and constraints were applied to the training so that an encoder is enforced to map normal and anomalous data to the correct side of this threshold. It was indicated that this method of training with constraints resulted in a robust threshold that is less dependent of the distribution of the training data in the latent space.# Monotonic Behavior in Machine Condition Monitoring\n\nThe aim of CGAE is to separate normal and anomalous data based on a fixed, predefined threshold. However, when considering machine condition monitoring as a prognostic problem, it is expected that this CI shows a monotonically increasing behavior, which is not directly linked to the objective of CGAE. Previous works have already applied different methods to learn or enforce this monotonic behavior. In (Wu et al, 2019) 4 different monotonic curves, that emulate an expected RUL curve, were used as target for a learning objective. A similar approach was used in (Hu and Li, 2020), where features were selected based on both a correlation metric between the feature and the running time of an asset and a monotonicity metric. The selected features were then used to construct a single CI with the normalized life of an asset as target. While these methods allow for the construction of a monotonic CI, they require a target or label that matches the degradation pattern of the bearing, which might not be trivial, as each bearing can show a different degradation pattern (Sikorska et al, 2011).\n\nA two step approach was used in (Qi et al, 2024), where a CI was selected by first splitting a raw vibration signal into multiple frequency bands, then calculating multiple CIs per frequency band, and finally selecting the CI that attains the highest score based on the monotonicity, trendability, and prognosability. This CI is then provided to a SVDD model to perform AD, and when an anomaly is detected the RUL is estimated using a statistical model. A two step approach was also used in (Nieves Avendano et al, 2022) to first estimate a CI using a stochastic gradient descent regressor, and second, optimize an isotonic regression to fit this CI onto a monotonic curve. As these methods use two steps, a change in one of the steps results in the other step needing to be reoptimized, based on that change.# Proposed Method: Monotonically Constraint Guided AutoEncoder (MCGAE)\n\nThis work proposes an extension to CGAE, which will be called Monotonically Constraint Guided AutoEncoder (MCGAE), where an additional monotonicity constraint is included to enforce the monotonic behavior of the CI estimations over time. In this way the proposed method causes an AE to learn to both map normal and# Current Page Title\n\nAnomalous data to the correct side of some threshold, as per the CGAE, objective, as well as to map data from earlier in the life time of a bearing closer to the origin than data from later in the life time of a bearing. Since the CI is defined as the distance of the latent representation of the current measurement to the latent space origin the latter will cause the desired monotonic behavior. Using this approach, our proposed method does not require a target that matches the expected degradation pattern of a bearing, as the monotonicity is enforced by the constraint. The optimization of both the AD and CI estimation is also integrated into a joint optimization, meaning that a change in either will automatically also optimize the other.# The main contributions of this work are:\n\n- An extension of the CGAE algorithm that retains the ability to separate normal and anomalous data, while having an improved underlying CI by enforcing it to have a monotonic trend.\n- A joint modelling of CI estimation and AD which allows the use of both labeled and unlabeled data, without needing to assume that the latter can be considered (mostly) as normal data.\n- A comparison of the proposed method with existing methods using the data from two sets of life time tests of bearings: one where the health is monitored via an accelerometer and another where a microphone was used.\n\nThe rest of this paper is structured as follows. The methods that will be used for comparison are described in Section 2. This is followed by detailed discussion of the proposed MCGAE method in Section 3. Then, the experimental setup, including the used datasets, preprocessing, model architectures, and evaluation metrics are explained in Section 4. The performed experiments and corresponding results are discussed in Section 5. An ablation study is presented in Section 6. Finally, a conclusion to this work is given in Section 7 as well as possible directions for future work.# 2 Methods\n\nThis section will introduce the methods that will be used for comparison with the proposed MCGAE method, more specifically, AutoEncoder Deep Support Vector Data Description (AE-DSVDD) and Constraint Guided AutoEncoders (CGAE). The data used in this work consists of normal, unlabeled, and anomalous points. The normal points correspond to measurements taken when the machine was in a healthy condition, the unlabeled points correspond to a situation where the machine condition is considered to be unknown, and anomalous points correspond to faulty machine conditions, this will be explained in more detail in Section 4.1.# 2.1 AutoEncoder Deep Support Vector Data Description\n\nThe learning objectives of both AE and DSVDD are combined in AutoEncoder Deep Support Vector Data Description (Zhang and Deng, 2021). The resulting objective learns to both reconstruct normal points as good as possible, as per the AE objective, and also to map the normal points as close as possible to a center point c \u2208 RDl, with Dl the amount of dimensions in a latent space, and anomalous points as far away as.# 1. Introduction\n\nThis is done by learning both the weights of an encoder E(\u00b7|\u2713E), where \u2713E are the weights of the encoder of an AE, and those of a decoder D(\u00b7|\u2713D), where \u2713D are the weights of the decoder of an AE, by solving the following objective:\n\nargmin \u2713E,\u2713D 1 X \u27e8kx D (E (x | \u2713E ) | \u2713D )k2 + kE(x|\u2713E ) ck2\u27e9 N x\u2208N\n\n+ Ax2A1 X \u27e8kE(x|\u2713E ) ck2\u27e91,\n\nwhere N=1 and N A are the amount of normal and anomalous points, respectively, N = {n}i is the set of normal points, A = {a}i=1 is the set of anomalous points, i A > 0 is a hyperparameter that balances the weight of anomalous data in comparison with normal data, and || \u00b7 || is the L2-norm. Do note that the objective in (Zhang and Deng, 2021) only uses normal points, whereas (1) also incorporates anomalous data. This addition of anomalous data has already been used in (Meire et al, 2023a).\n\nDetermining the CI for an input point x can then be done by using the distance of the encoding of x to the center c as follows:\n\nfCI(x) = kE(x|\u2713E ) ck2.\n\nThis CI can then be used to determine if x should be considered as normal or anomalous by comparing it to a threshold T using the following classification rule:\n\n\u02c6(x) =\ny 1, fCI(x) > T\n1, fCI(x) \u2264 T,\n\nwhere \u02c6 = 1 and \u02c6 = 1 corresponds to a normal and anomalous point, respectively.\n\nAs already mentioned in Section 1, there are different methods to determine T, and in this work three methods, that were described in (Meire et al, 2023a), will be used. The first method determines an \"optimal\" threshold Topt. The value is chosen such that a chosen metric is optimized for the considered test set. As this is the best possible threshold, the performance can be regarded as an upper bound. The second method determines a threshold Ttrain by computing the mean \u03bcn and standard deviation \u03c3n of the estimated CI for the normal training data and then sets Ttrain to \u03bcn + 3\u03c3n. This is one of the more simple and basic methods to determine a threshold. The third and final method attempts to fit a sigmoid on the estimated CI of the normal training data, such that the median and 99-th percentile are receiving values 0.25 and 0.5 at the sigmoid\u2019s output, respectively. The supremum, the maximum value of the sigmoid, is set to 1. By setting the threshold to 0.6 on this sigmoid and then inverting the function, the value of this threshold Tsigmoid can be determined. Note that the latter has similarities with the output layer of DL models as they typically employ sigmoid activation functions.\n\n6# 2.2 Constraint Guided Autoencoder\n\nIn the Constraint Guided AutoEncoder (CGAE) (Meire et al, 2023a) the learning objective of an AE is subjected to a set of constraints: (i) normal data should be mapped inside of a sphere, with radius R1, around the origin, and (ii) anomalous data should be mapped outside of a larger sphere, with radius R2, around the origin. This learning objective is made with the assumption that at least some anomalous data is available, so that the model can learn to distinguish between normal and anomalous data, and it is also assumed that R1 + < R2 with some positive constant, so that there is a separation between the mapping of the normal and anomalous data, which is expected to lead to a better generalization. The resulting constrained learning objective is as follows:\n\nargmin\u03b8E,\u03b8D 1/N \u03a3x\u2208N ||x - D(E(x | \u03b8E) | \u03b8D)||2,\n\nwhere B[0, R1] and B[0, R2] denote the closed balls, with radius R1 and R2, respectively, around the origin. The learning objective is optimized using CGGD (Van Baelen and Karsmakers, 2023).\n\nTo determine the CI for an input point x a similar procedure as for AE-DSVDD can be followed, with the difference being that for CGAE the origin is considered as the center instead of a point c:\n\nfCI(x) = ||E(x | \u03b8E) - 0||2.\n\nBy using the constrained learning objective, it is expected that a (near) \u201coptimal\u201d threshold can be found somewhere in the interval [R1, R2], as this lies between the normal and anomalous data. Following this expectation, the threshold associated with CGAE is defined as:\n\nT := R1 + (R2 - R1) N + A .A\n\nDo note that this threshold only depends on the amount of known normal and anomalous points that are available at training time and the choice of R1 and R2. Furthermore, it is not influenced by the actual training of the model, as is the case for Ttrain and Tsigmoid. This threshold can then be used in (3) to determine if a point should be considered normal or anomalous.# 3 Monotonically Constraint Guided Autoencoder\n\nIn the application of condition monitoring of machines, it is natural to assume that the health of a machine cannot improve over time. Equivalently, the machine can over time only become less healthy or, equivalently, the CI can only increase over time. Therefore, it is desirable to predict a monotonically increasing sequence of norms for the encodings of the life time of a single machine, hereafter called a run. Morespecifically, for a given time series of data {xj}j (with the data chronologically ordered from left to right) consisting of normal and unlabeled data points of a single recording, it should hold that\n\nj1 &lt; j2 ) kE(xj1 |\u2713E)k &lt; kE(xj2 |\u2713E)k.\n\nTo this end we propose Monotonically Constraint Guided AutoEncoder (MCGAE), which extends CGAE with a constraint that enforces this monotonous behavior. This results in the extension of the optimization problem (4) to\n\nargmin\u2713E,\u2713D Xx\u2208N D (E (x | \u2713E) | \u2713D)2,\n\nwhere the constraints are:\n\n- s.t. \u2200x \u2208 N : E(x|\u2713E) \u2208 B[0, R1],\n- \u2200x \u2208 A : E(x|\u2713E) / B[0, R2],\n- \u2200r, \u2200xt1, xt2 \u2208 Sr : t1 &lt; t2 &lt; tr kE(xt1 |\u2713E)k &lt; kE(xt2 |\u2713E)k.\n\nwhere r denotes the considered run, tar is the time at which the first anomalous point occurs for run r, and Sr = Nr \u222a Ur | [ Ur, with Nr the set of normal points of run r, Ur = {ui | i=1} being the set of unlabeled points of run r, with Ur the amount of unlabeled points of run r, and xti is the sample on time ti of run r. For simplicity, we omit the time index if it is not required. Note that only points in a single run can be compared to each other for the monotonicity constraint, because, there is, in general, no relation between the encodings of certain time points from different runs.\n\nThis optimization problem can be optimized using CGGD (Van Baelen and Karsmakers, 2023). The latter framework enables training a neural network by minimizing a loss function while satisfying constraints. This is realized by prioritizing satisfying the constraints over minimizing the objective function during every step of the optimization. In order to do so, it is required to define the direction by which a certain model variable needs to be adjusted when it violates a corresponding constraint. This direction needs to be chosen such that when following it during the update the resulting updated model at least comes closer to the feasible region (that contains model solutions that satisfy the constraint). In case a constraint is satisfied the direction is considered to be 0, otherwise, the different directions can be computed as follows.\n\nSimilar to CGAE, in case (8) is not satisfied, the direction to update the model when normal training points are considered (dirN) is computed so that these points will be mapped inside of B[0, R1]\n\ndirN(x, E) := kE (x | \u2713E)k .E (x | \u2713E)\n\nConsidering (11) it can be observed that a scaled version of direction (dirN) is added to the considered weight \u2713j which will cause the solution to move towards the inside of the ball B[0, R1]. The latter occurs as the computed direction will be followed in the opposite direction, due to the update procedure. In this case, the computed direction is the vector from the origin to E (x | \u2713E), and hence, following this vector in the opposite direction causes the solution to move inwards.\n\nIn the previous equation a direction was calculated for a single point x. To evaluate a set of points, this direction is aggregated, by summation. As mentioned earlier,dirN (x, E) is considered to be 0 if the constraint is satisfied for point x. In case E (x | \u2713E ) = 0, thus when the encoding of x is the origin, the direction to the origin is not computed, instead a random direction is chosen, that is, by sampling from a uniform distribution on the unit sphere.\n\nIn a similar manner, in case needed, to satisfy (9), the direction for the anomalous training points (dirA) is computed so that these points will be mapped outside of B[0, R2]\n\ndirA (x, E) := kE (x | \u2713E )). E (x | \u2713E k\n\nTo satisfy (10), the direction that causes the normal and unlabeled points to be mapped so that the norm of their corresponding encodings monotonically increases is computed as follows. For a given run, a function argsort(\u00b7) is used that takes in a set of points and returns a vector of indices that are ranked in ascending order according to the norm of their corresponding encodings.\n\nvector \u21e50 ... This vector of indices can then be compared to the expected order expressed by\u21e4 by performing an element-wise subtraction to get the difference |Sr |  1 between the rankings. The resulting difference can then be normalized, using the L2 norm, to obtain the direction. More formally this becomes:\n\nargsort\u21e3nkE(x|\u2713E )k2 | x 2 Sro\u2318 \u21e50 ... \u21e3n |Sr |  1\u21e4\n\ndirmono (Sr , E) := argsort kE(x|\u2713E )k2 | x 2 Sro\u2318 \u21e50 ... |Sr |  1\u21e4.\n\nNote that, if the ranking for a point is far from the desired ranking, this point will more strongly influence the direction than if the ranking of a point is only 1 position different from the desired ranking.\n\nThe weights of a model can then be updated using the objective function and the directions for the different constraints\n\n\u2713j+1 = \u2713j \u2318 [rL (N , (N ))\n\n+ R dirN (N , E) max {kreL (N , (N ))k , \u21e3}\n\n+ R dirA (A, E) max {kreL (N , (N ))k , \u21e3}\n\n+R dirmono (Sr , E) max {kreL (N , (N ))k , \u21e3}] ,\n\nwhere (N ) = D (E (N | \u2713E ) | \u2713D ), \u2318 is the learning rate, R > 1 a fixed real number called the rescale factor, \u21e3 > 0 a fixed real number, which is chosen small, L is the objective function from (7), reL is the gradient of the loss function with respect to the encodings, and \u2713 is the union of \u2713E and \u2713D . (N ) denotes the evaluation of the set of normal points N by the model. This evaluation is done separately for each point in the set, and results in a new set that contains these evaluations. The objective function from (7) only uses normal data, this is denoted in (11) by the use of the set of normal points N . As it is not feasible to use an entire run at once during training,\n\n9this process will be performed on batches. While this direction can be computed with only 2 points, it is expected that this will not perform well, and that more points are needed for this constraint to be used to compute a better direction. Therefore, batches are used. Note that these are created in a stratified way, where it is guaranteed that, if a run is present in a batch, sufficient, e.g. 10 or more, points of that run are present in that batch.\n\nAs MCGAE is an extension of CGAE, the CI for an input point x can be determined using (7), and the associated threshold can also be acquired using (6). This threshold can then also be used in (3) to distinguish between normal and anomalous points.# 4 Experimental setup\n\nThis section will discuss the datasets used in this work as well as the associated preprocessing, the model architectures and hyperparameter settings, and the relevant metrics.# 4.1 Datasets\n\nThe experiments in this work will be performed on two datasets. Firstly, a dataset containing vibration data from accelerated run-to-failure tests of bearings, hereafter called the Smart Maintenance (SM) dataset. Secondly, a dataset that contains acoustic data from accelerated run-to-failure tests of bearings, hereafter called Acoustic Bearing Monitoring (ABM) dataset.# 4.1.1 Smart Maintenance\n\nThe Smart Maintenance (SM) dataset contains vibration data of a total of 70 accelerated run-to-failure tests, hereafter called runs, of bearings and was already used in (Meire et al, 2023a) and (Meire et al, 2022). One of the 7 bearing rigs used to perform the tests is shown in Figure 1.\n\nThe runs were performed under fixed operation conditions, more specifically a high radial load of 9 kN and with the shaft rotating at 2000 rpm, and the stopping condition was set to 20g peak acceleration, which is the assumed end-of-life. The high radial load and a very small indentation in the inner race of the bearing were used to significantly accelerate the bearing lifetime. Radial accelerations were measured by an accelerometer, attached to the bearing housing, at a sampling frequency of 50 kHz. As the aim is to work with a run-to-failure test under fixed conditions, data collected at a rotational speed lower than 2000 rpm during the initial run-up or during short measurement interruptions was omitted during the experiments.\n\nOnly data from 1 of the 7 bearing test rigs was selected to reduce the computing time needed for the ablation studies in this work. The data of this single setup contains 6 runs that start with a small initial indentation in the inner race of the bearing and end with a severe fatigue fault. The length of these runs ranges from roughly 6600 to 14500 seconds, and the fault occurrence ranges from roughly 100 to 3000 seconds prior to the end of the run.# 4.1.2 Acoustic Bearing Monitoring\n\nThe Acoustic Bearing Monitoring (ABM) dataset contains both vibration and acoustic data of a total of 64 accelerated run-to-failure tests, hereafter called runs, of bearings and was collected on the same bearing test rig as shown in Figure 1. The placement of the internal microphone can be seen in Figure 2. This dataset was already used in (Meire et al, 2023b). In this work the data collected by the internal microphone, from runs with a fixed speed of 2000 rpm, and a fixed load were selected, resulting in 5 runs being retained. The length of these runs ranges from roughly 5000 to 18500 seconds, and the fault occurrence ranges from roughly 700 to 4300 seconds prior to the end of the run.\n\nData was captured with a sampling frequency of 50 kHz for each sensor. Each run was accelerated in the same manner as the SM dataset, by creating a small indent on the inner race of the bearings in addition to a high radial load and the same stopping criteria of 20g peak accelerations as for the SM dataset was used.\n\nNext to the raw data, a ground truth was also provided. An important note is that this labelling was not based on the physical state of the bearings, but on analysis of the data collected by the internal microphone. This dataset was also split into 3 segments along the time axis, similar to the SM dataset, by using two cuto\u21b5 points ph and pf. ph was determined in the same way as for the SM dataset and pf was determined as the point in time where the faulty behavior was first detected.# 4.2 Preprocessing\n\nThe acceleration and acoustic signals are first transformed into a time-frequency representation, more specifically log mel spectra, as was done in (Meire et al, 2022, 2023a). This transformation is performed by first acquiring time-frequency spectra from the raw signals, using a window and hop size of 1 second. From these spectra mel bands are then extracted, 64 and 512 bands for the ABM and SM datasets, respectively, and these are then log scaled. Finally, 8 consecutive seconds were concatenated into an input frame, with shapes (64,8) and (512,8) for the ABM and SM dataset, respectively, which was then used as an input to the models. It was noticed that the distribution of the values of the mel spectra is different between runs. Therefore, each run was separately standardized to have zero mean and unit variance prior to ph.# 4.3 Experimental details\n\nThe models used in the experiments all use a similar AE architecture, consisting of an encoder and decoder. The encoder uses convolutional blocks, consisting of a convolutional layer, followed by batch normalisation (Ioffe and Szegedy, 2015), an activation function and finally a max pooling with stride and size of 2. The decoder uses\n\nFig. 2: The microphone setup used in the bearing run-to-failure tests.# 4.4 Metrics\n\nThe alternative methods studied in this paper will be compared in two ways: (i) the ability to discriminate between normal and anomalous behavior, and (ii) the quality of the models for the SM model were trained for 300 epochs, and the models for the ABM dataset for 500 epochs, both with the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e3. The best performing model was saved during training, and if the performance did not improve for 30 epochs, the learning rate was halved, up to a limit of 1e6. The training performance is determined by the learning objective associated with the algorithms, as described in Sections 2 and 3. For CGAE and MCGAE the satisfaction ratio of the constraints, the amount of points for which the constraints are satisfied with respect to the total amount of points, is also considered. An improvement in model performance for AE-DSVDD corresponds to the lowering of the learning objective on the validation set. When considering CGAE or MCGAE, the improvement corresponds to both the lowering of the learning objective on the validation set and either an increase of the satisfaction ratio on the associated constraints or the satisfaction ratio being above 95%.\n\nIt was previously mentioned that the monotonicity constraint can only be applied to data from the same runs, and hence it is crucial that the runs in each batch are represented by multiple points. To ensure this is the case, the batches were created in a structured manner. This is done by first setting a number of runs that should be present in a batch and a number of points that should be selected from each run. Based on these settings a batch is created by first randomly selecting runs and then selecting points from each of these runs. For the latter, a stratified sampling was used that first selects a fixed number of points of the normal and, if applicable, anomalous data in a run and then another fixed number of points from the unlabeled data. For the SM dataset each epoch a total of 80 batches were created in this way, with 5 runs in each batch, with 25 normal or anomalous points and an additional 10 unlabeled points for each of these runs. For the ABM dataset the amount of batches was increased to 100. However, each batch only contained 4 runs, with the same amounts of points as the SM dataset. Do note that this procedure will cause some points to be included in multiple batches in a single epoch. However, this is not expected to have much impact on the training of the models.the CI in terms of its monotonicity. The performance with regards to the discrimination between normal and anomalous data is evaluated using the balanced accuracy (BA) (12), which is the mean of the recall obtained on each class. While it is common to use the F1 score to evaluate discrimative performance, it was opted to use the BA instead in this work for two reasons. Firstly, both the F1 score and BA were used in (Meire et al, 2023a), where it could be seen that both showed similar results. Secondly, the different runs show a varying amount of normal and anomalous data, which is not ideal for the F1 score as it is biased towards correctly classifying the positive class. In this work anomalous data is considered as the positive class, leading to\n\nBA = 1 - (T N + F P) + (T P + F N) T N T P\n\n2\n\nwhere T P, F P, T N, and F N correspond to the amount of true positives, false positives, true negatives, and false negatives, respectively. To evaluate the BA, a threshold is required. Three thresholds, Topt, Ttrain, and Tsigmoid were described in Section 2.1 and will be used in the evaluation.\n\nThe second evaluation looks into the quality of the CI. A high quality CI is expected to show an increasing trend up to a point where a fault occurs. In (Kim et al, 2016) and (Meire et al, 2022) this was done by using the Spearman\u2019s \u03c1 (Kokoska and Zwillinger, 2000), which correlates the rank of two variables, A and B, and is calculated as follows,\n\n\u03c1 = cov(r(A), r(B))\n\nr(A)r(B)\n\nwith r(A) and r(B) being the ranking of A and B, in ascending order, and cov and \u03c3 being the covariance and the standard deviation, respectively. In the context of this work, A and B correspond to the CI and the time, respectively.# 5 Experiments and results\n\nIn this section a comparison is made between CGAE, AE-DSVDD, and our proposed method, MCGAE.# 5.1 Experiments\n\nTo compare alternatives, a leave-one-run-out scheme using both the SM and ABM dataset was employed. All experiments were repeated 3 times, using different seeds for model initialisation and batch generation. As indicated earlier the selected subset of the SM dataset has 6 faulty runs. This results in 6 folds, as per the leave-one-run-out scheme, with each fold using 5 runs for training and validation, split 75% and 25%, respectively, and the remaining run for testing. The amount of anomalous data was incrementally increased by selecting the anomalous data from either 1, 2, 3, 4, or 5 runs used in the training and validation sets. For each of these results the mean and standard deviation over the different folds was computed.# 5.2 Results\n\nThis section will discuss the results of the performed experiments, as described in Section 5.1. Firstly, the discriminative performance with regards to the BA was evaluated. Secondly, the quality of the generated CI was evaluated using the Spearman\u2019s \u03c1. When evaluating the discriminative performance, a trivial predictor, that predicts all data as anomalous, is also provided for reference. Next to the fixed thresholds and the trivial predictor, an \u201coptimal\u201d threshold is also considered. Its corresponding discrimination performance can be considered as an upper bound. As MCGAE extends CGAE, it is expected that the former outperforms the latter, mainly with regards to the quality of the CI, as this is the focus of the additional monotonicity constraint.# 5.2.1 Discriminative performance\n\nThe results of the comparison of the different methods in terms of the BA are shown in Figure 3.\n\nWhen considering the results that are obtained when an optimal threshold was used it can be observed that MCGAE has equal or slightly improved performance compared to the alternatives. For the SM dataset this better performance is only noticeable when using anomalous data from 1 run, while for the ABM dataset MCGAE shows a consistent (near) perfect performance, and only when anomalous data from all 4 runs is used the performance of CGAE is similar. CGAE does perform slightly worse than AE-DSVDD when anomalous data from 1 or 2 runs is used on the ABM dataset. However, when more anomalous data is used it performs slightly better. However, do remember that these thresholds are determined using the test data, and should be considered as the upper bound for the performance.\n\nWhen not using an optimal threshold, both MCGAE and CGAE attain a performance that is significantly better than AE-DSVDD using a threshold based on training statistics, except for the case where the anomalous data from 1 run was used on the SM dataset. Upon closer inspection of the generated CI, it was noticed that this lower score is attributed to 2 test runs where the CI for anomalous data is estimated to be lower than the threshold, whereas the other test runs perform closer to the behavior.# Acoustic Bearing Monitoring\n\n|MCGAE|CGAE|AE-DSVDD optima|AE-DSVDD sigmoid|\n|---|---|---|---|\n|MCGAE optimal|CGAE optimal|AE-DSVDD train|Trivial predictor|# Performance Metrics\n\n|1.0|1.0|\n|---|---|\n|0.9| |\n|0.8| |\n|0.6|0.200|\n|0.20|0.175|\n|0.15|0.150|\n|0.10|0.100|\n|0.075| |\n|0.05|0.050|\n|0.025| |\n|0.00|0.000|\n\nAnomalous data from number of runs\n\nFig. 3: The mean (a) and standard deviation (c) of the BA on the SM dataset, and the mean (b) and standard deviation (d) on the ABM dataset, for the different methods and different thresholds. The thresholds are Ttrain, Tsigmoid, and Topt, as described in Section 4.4. The x-axis indicates from how many runs the anomalous data is used.\n\nthat is observed when more anomalous data is used. This lower performance might be due to a combination of 2 reasons. The first being that the model has only seen data from 1 type of anomaly, as only 1 run was used to select anomalous data. The second being that it is possible that the model was trained with a limited amount of anomalous data for 1 or more folds, as anomalous data is selected from only 1 run and some runs only contain a limited amount of anomalous data. When using anomalous data from more runs this is no longer the case, and the BA remains relatively constant.# 5.2.2 Condition indicator\n\nNext to discrimination performance also the quality of the CI estimation is assessed. More specifically, the quality of the CI is evaluated using the Spearman\u2019s \u21e2, described in Section 4.4, for which the results can be found in Table 1.# Table 1: The Spearman\u2019s \u21e2 results for the both datasets on the test set. The best results for each amount of anomalous data are shown in bold.\n\n|Anomalous data from number of runs|SM data set|SM data set|SM data set| | |\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.658 \u00b1 0.304|0.439 \u00b1 0.229|0.284 \u00b1 0.285|\n|2|0.667 \u00b1 0.233|0.439 \u00b1 0.419|0.205 \u00b1 0.597|\n|3|0.588 \u00b1 0.369|0.538 \u00b1 0.432|0.238 \u00b1 0.415|\n|4|0.582 \u00b1 0.377|0.515 \u00b1 0.452|0.475 \u00b1 0.319|\n|5|0.558 \u00b1 0.403|0.475 \u00b1 0.467|0.518 \u00b1 0.350|\n\n|Anomalous data from number of runs|ABM data set|ABM data set|ABM data set| | |\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.228 \u00b1 0.308|0.157 \u00b1 0.247|0.184 \u00b1 0.456|\n|2|0.249 \u00b1 0.285|0.099 \u00b1 0.251|0.116 \u00b1 0.428|\n|3|0.213 \u00b1 0.451|0.034 \u00b1 0.255|-0.052 \u00b1 0.516|\n|4|0.174 \u00b1 0.438|0.007 \u00b1 0.319|0.053 \u00b1 0.400|\n\nIt can be seen that MCGAE shows the highest Spearman\u2019s \u21e2 overall for both datasets. When looking at the results for MCGAE for the SM dataset more closely, a better performance is noticed when anomalous data from only 1 or 2 is used. The lower performance when using more anomalous data is caused by 2 test runs performing poorer while the other test runs show a more constant performance. This slight decrease in the Spearman\u2019s \u21e2 when anomalous data from more runs is used can also be seen on the ABM dataset, albeit to a lesser extent.While the behavior of CGAE and AE-DSVDD is similar, it can be seen that the former does perform slightly better when considering the SM dataset. However, as was also the case for MCGAE, the performance difference is not always clear when considering the ABM dataset. This different behavior between the datasets could be due to the ABM dataset being more noisy, as it contains acoustic data, in comparison to the SM dataset, which contains vibration data.\n\nWhile MCGAE does show a better performance, it also shows a decreasing trend when using anomalous data from more runs. As mentioned earlier, this is mainly due to 2 test runs showing a strong decrease in Spearman\u2019s \u03c1. However, when observing the other methods for the SM dataset, an increasing trend can be seen. This could be due to the nature of the constraint not being simple to generalize to different unseen test runs, as an increasing trend in a CI over time is strongly tied to the run itself, and bearings can show different degradation patterns, even when operating under the same conditions (Sikorska et al, 2011). The overall lower Spearman\u2019s \u03c1 on the ABM dataset is likely also due to poor generalization, which is further worsened due to increased noise in the dataset.\n\nTo investigate the effect of the added constraint without the difficulties of the generalization, the Spearman\u2019s \u03c1 is also evaluated on the training set, and the corresponding results are shown in Table 2.# Table 2: The Spearman\u2019s \u03c1 results for the both datasets on the training set. The best results for each amount of anomalous data are shown in bold.\n\n|Anomalous data from number of runs|Smart Maintenance|Smart Maintenance|Smart Maintenance| | |\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.891 \u00b1 0.062|0.235 \u00b1 0.554|0.396 \u00b1 0.437|\n|2|0.926 \u00b1 0.040|0.536 \u00b1 0.445|0.547 \u00b1 0.305|\n|3|0.931 \u00b1 0.037|0.626 \u00b1 0.336|0.579 \u00b1 0.343|\n|4|0.943 \u00b1 0.029|0.677 \u00b1 0.341|0.608 \u00b1 0.254|\n|5|0.945 \u00b1 0.030|0.631 \u00b1 0.361|0.658 \u00b1 0.211|# Acoustic Bearing Monitoring\n\n|Anomalous data from number of runs|MCGAE|CGAE|AE-DSVDD|\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.860 \u00b1 0.089|0.216 \u00b1 0.341|0.463 \u00b1 0.318|\n|2|0.872 \u00b1 0.091|0.266 \u00b1 0.354|0.473 \u00b1 0.301|\n|3|0.883 \u00b1 0.096|0.355 \u00b1 0.274|0.553 \u00b1 0.253|\n|4|0.898 \u00b1 0.080|0.433 \u00b1 0.221|0.555 \u00b1 0.251|\n\nIt is clear that MCGAE attains the highest Spearman\u2019s \u03c1 when considering the runs used during training, and that the difference with the other methods is significantly larger than on the test set. This indicates the effectiveness of the constraint and that the generalization to an unseen test run is indeed not simple. When comparing the difference in performance for MCGAE on the training and test set between the SM and ABM dataset, it is clear that this difference is noticeably larger for the latter. This indicates that the generalization is indeed more difficult on the ABM dataset, likely due to the increased acoustic noise in the dataset, as mentioned earlier.\n\n18# 6 Ablation study\n\nThis section discusses the results obtained when reconstructing additional data next to only the normal data. Experiments were performed with 4 different combinations of reconstructed data: (i) only the normal data (MCGAE-n), these are the same as in Section 5.2, (ii) additionally reconstructing unlabeled data (MCGAE-nu), (iii) additionally reconstructing anomalous data (MCGAE-na), and (iv) additionally reconstructing both unlabeled and anomalous data (MCGAE-nua).# 6.1 Discriminative performance\n\nThe results of the comparison of the different reconstruction based on the BA is shown in Figure 4. It can be seen that the results on the SM and ABM datasets are different. When considering a chosen threshold on the SM dataset, it is clear that there are no large differences in performance for the different reconstructions. However, MCGAE-n seems to perform slightly worse, mainly when using anomalous data from up to 3 runs. This is due to 1 or 2 test runs, across the different folds and seeds, performing worse, while the other test runs perform the same. It also seems that MCGAE-na and MCGAE-nua perform slightly better overall. However, the difference is very minimal. When considering an optimal threshold this performance difference is even further reduced, as all reconstructions attain a near perfect BA, except for when anomalous data from 1 run is used. This indicates that the lower performance of MCGAE-n using the chosen threshold is due to a mismatch between the chosen and optimal threshold, and not due to the model performing worse.\n\nWhen considering a chosen threshold on the ABM dataset, it is indicated that MCGAE-n and MCGAE-nu perform better overall, except when anomalous data from 1 or 2 runs is used. This behavior is slightly different when an optimal threshold is considered, as MCGAE-n outperforms MCGAE-nu when anomalous data from less than 3 runs is used. It is indicated that MCGAE-na performs the worst, when considering a chosen threshold, although the difference with MCGAE-nua is minimal when.# Acoustic Bearing Monitoring\n\n|MCGAE-n|MCGAE-na|MCGAE-n optimal|MCGAE-na optimal|\n|---|---|---|---|\n|1.00| |0.95| |\n|0.90| |0.90| |\n|0.85| |0.85| |\n|0.80| |0.80| |\n|0.75| |0.75| |\n|0.20| |0.175| |\n|0.15| |0.150| |\n|0.10| |0.100| |\n|0.05| |0.050| |\n|0.00| |0.025| |\n\nFig. 4: The mean (a) and standard deviation (c) of the BA on the SM dataset, and the mean (b) and standard deviation (d) on the ABM dataset, for the different reconstructions. The x-axis indicates the number of runs from which anomalous data is used.\n\nAnomalous data from more than 2 runs is used. When selecting an optimal threshold both MCGAE-na and MCGAE-nua also perform worse than MCGAE-n and MCGAE-nu. This could be due to the CI estimated by either MCGAE-na or MCGAE-nua, and to a lesser extent MCGAE-nu, being less distinct for the normal and anomalous data in comparison to MCGAE-n. The cause of this lesser distinction is mainly split into 2 parts. Firstly, it seems that reconstructing unlabeled data, MCGAE-nu and# 6.2 Condition indicator\n\n|Anomalous data from number of runs| | |Smart Maintenance| | |\n|---|---|---|---|---|---|\n|MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua| | |\n|1| |0.658 \u00b1 0.304|0.601 \u00b1 0.343|0.516 \u00b1 0.401|0.569 \u00b1 0.364|\n|2| |0.677 \u00b1 0.233|0.622 \u00b1 0.283|0.463 \u00b1 0.035|0.479 \u00b1 0.347|\n|3| |0.588 \u00b1 0.369|0.631 \u00b1 0.330|0.513 \u00b1 0.377|0.476 \u00b1 0.347|\n|4| |0.582 \u00b1 0.377|0.620 \u00b1 0.315|0.497 \u00b1 0.389|0.539 \u00b1 0.346|\n|5| |0.558 \u00b1 0.403|0.553 \u00b1 0.357|0.540 \u00b1 0.343|0.569 \u00b1 0.306|\n\n|Anomalous data from number of runs|Acoustic Bearing Monitoring|Acoustic Bearing Monitoring|Acoustic Bearing Monitoring|Acoustic Bearing Monitoring| | | |\n|---|---|---|---|---|\n|MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua| |\n|1|0.228 \u00b1 0.308|0.160 \u00b1 0.325|0.175 \u00b1 0.280|0.066 \u00b1 0.254|\n|2|0.249 \u00b1 0.285|0.162 \u00b1 0.315|0.153 \u00b1 0.255|0.112 \u00b1 0.332|\n|3|0.213 \u00b1 0.451|0.175 \u00b1 0.280|0.100 \u00b1 0.366|0.086 \u00b1 0.352|\n|4|0.174 \u00b1 0.438|0.212 \u00b1 0.408|0.162 \u00b1 0.320|0.151 \u00b1 0.382|\n\nThe evaluation of the quality of the CI using the Spearman\u2019s \u21e2 for the different reconstructions can be found in Table 3. Overall, it can be seen that MCGAE-n and MCGAE-nu show a higher Spearman\u2019s \u21e2 in comparison to MCGAE-na and MCGAE-nua, except for the SM dataset when using anomalous data from all runs. As mentioned in Section 5.2 the decreasing trend for MCGAE-n on the SM dataset is mostly due to 2 test runs. For MCGAE-nu the Spearman\u2019s \u21e2 shows a more increasing trend, expect for the SM dataset when anomalous data from all runs is used, where a noticeable decrease in performance can be seen. This is mainly due to a significantly lower performance of 4 test runs, across all folds and seeds, while the rest of the runs show a more stagnant Spearman\u2019s \u21e2. A small difference between the results for both datasets can be seen for MCGAE-na and MCGAE-nua. When considering the SM dataset, both reconstructions perform quite similarly. However, on the ABM dataset MCGAE-nua performs worse than MCGAE-na. As was also done when comparing the different methods, the quality of the CI on the training set will also be evaluated to omit the potentially difficult generalisation. The results are shown in Table 4.# Table 4: The Spearman\u2019s \u21e2 results for the both datasets on the training set. The best results for each amount of anomalous data are shown in bold.\n\n|Anomalous data from number of runs|Smart Maintenance|Smart Maintenance|Smart Maintenance|Smart Maintenance| | | |\n|---|---|---|---|---|\n| |MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua|\n|1|0.891 \u00b1 0.062|0.906 \u00b1 0.086|0.870 \u00b1 0.088|0.877 \u00b1 0.100|\n|2|0.926 \u00b1 0.040|0.939 \u00b1 0.044|0.899 \u00b1 0.066|0.910 \u00b1 0.082|\n|3|0.931 \u00b1 0.037|0.952 \u00b1 0.026|0.927 \u00b1 0.049|0.931 \u00b1 0.052|\n|4|0.943 \u00b1 0.029|0.955 \u00b1 0.035|0.944 \u00b1 0.036|0.922 \u00b1 0.129|\n|5|0.945 \u00b1 0.030|0.965 \u00b1 0.018|0.954 \u00b1 0.024|0.956 \u00b1 0.032|# Acoustic Bearing Monitoring\n\n|Anomalous data from number of runs|MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua| | | |\n|---|---|---|---|---|---|---|---|\n| | | | |1|2|3|4|\n|1|0.860 \u00b1 0.089|0.938 \u00b1 0.052|0.886 \u00b1 0.082|0.951 \u00b1 0.037| | | |\n|2|0.872 \u00b1 0.091|0.946 \u00b1 0.052|0.907 \u00b1 0.066|0.951 \u00b1 0.050| | | |\n|3|0.883 \u00b1 0.096|0.963 \u00b1 0.039|0.925 \u00b1 0.061|0.955 \u00b1 0.049| | | |\n|4|0.898 \u00b1 0.080|0.963 \u00b1 0.035|0.936 \u00b1 0.053|0.968 \u00b1 0.024| | | |\n\nIt can be seen that these results show a different behavior than those on the test set. For the SM dataset, MCGAE-nu shows the overall highest Spearman\u2019s \u21e2 instead of MCGAE-n. Both MCGAE-na and MCGAE-nua still show a slightly lower Spearman\u2019s \u21e2, except for when anomalous data from all runs is used, although the difference is not always significant. For the ABM dataset, MCGAE-n shows the lowest Spearman\u2019s \u21e2 out of the different reconstructions, and MCGAE-nu and MCGAE-nua show the highest Spearman\u2019s \u21e2. This indicates that additionally reconstructing the unlabeled data seems to improve the effect on the constraint on the training set, likely due to a similarity between the normal data and most of the unlabeled data. The influence of reconstructing anomalous data is not as clear, for the SM dataset there is no real benefit, while for the ABM dataset there is some gain in Spearman\u2019s \u21e2 in comparison to MCGAE-n. However, MCGAE-nua performs similar to MCGAE-nu, indicating that the reconstruction of the unlabeled dataset has a stronger influence on the constraint.\n\nAs was done in Section 5.2, the evaluation with regards to how similar the fixed threshold are to an \"optimal\" threshold was also performed for this ablation study, and the detailed results are shown and discussed in Appendix B.# 7 Conclusion and future work\n\nIn this work an extension to the CGAE algorithm, MCGAE, was proposed to jointly optimize a model for CI estimation and AD. This extension adds a constraint that enforces a monotonic behavior in the estimated CI, while retaining the ability of CGAE to discriminate between normal and abnormal data. The proposed MCGAE algorithm was evaluated and compared to AE-DSVDD and CGAE in terms of the discriminative performance and the quality of the CI. This comparison was performed on two datasets, the SM dataset containing vibration data from run-to-failure tests of bearings, and the ABM dataset containing acoustic data from similar run-to-failure.# Tests\n\nNext to the comparison with AE-DSVDD and CGAE, an ablation study was also performed to evaluate the reconstruction of unlabeled and/or anomalous data. The results show that MCGAE attains a similar or slightly better discriminative performance than CGAE, depending on the considered dataset, while also providing a higher quality CI. However, it should be noted that, while increasing the amount of anomalous data improves the discriminative performance, it does result in a decrease in the quality of the CI. It is possible that this is due to the monotonicity constraint not being simple to generalize to unseen runs, as the increasing trend in CI over the life of a bearing is strongly tied to the bearing itself. Additionally, as the runs are highly accelerated, the variation in the degradation trends is also higher. Specifically for the ABM dataset, as acoustic data is more prone to noise than vibration data, this could result in the lower monotonicity. When evaluating the quality of the CI on the training set, to investigate the effect of the monotonicity constraint without the generalization, MCGAE showed a noticeably higher quality of the CI, and increasing the amount of anomalous data also improved the CI.\n\nThe results of the ablation study show that, depending on the dataset, the discriminative performance is affected differently based on what data is reconstructed. On the SM dataset, reconstructing additional data shows a slight improvement, while on the ABM dataset it noticeably lowers the performance. The quality of the CI on an unseen run is highest when reconstructing either only normal data, or normal and unlabeled data. However, when evaluating the quality of the CI on the training set, additionally reconstructing unlabeled data does show a clear increase in performance.\n\nA possibility for future research is the improvement of the function (6) that determines the threshold for (M)CGAE, either through including some (limited) test data for calibration, a dynamic thresholding mechanism, a more advanced function, or incorporating insights from e.g. industry, such as considering false negatives to be more important than false positives, or vice versa.\n\nAnother possibility could be investigating whether a MCGAE model could be fine-tuned to an unseen test set by using some data from this set. This could be seen as a more advanced calibration in comparison to what was mentioned in the previous point.",
        "context_id": 19,
        "question": "What type of neural network is combined with a Variational AutoEncoder (VAE) to estimate the Remaining Useful Life (RUL) of aircraft engines according to Su et al, 2020?",
        "answer": [
            "Recurrent Neural Networks"
        ],
        "context_length": 53774
    },
    {
        "context": "# Adaptive Selection of Sampling-Reconstruction in Fourier Compressed Sensing# 1 Introduction\n\nCompressed sensing (CS) has revolutionized the field of image acquisition, enabling the reconstruction of high-quality images from a reduced number of measurements. This remarkable feat is achieved by exploiting the sparsity of natural images (or medical images) in certain transform domains. The CS theory [7, 13]\n\n\u22c6 Corresponding authors.# Data\n\n|Mask M|Mask Backpropagation|\n|---|---|\n|Input k|Output I\u2032|\n|\u2a00|Recon. Network \u03b8|# (a) Joint optimization of sampling-reconstruction (H1 [3, 8, 9, 19, 38])\n\n|Mask M0| | |\n|---|---|---|\n|\u2a00|\u03c0\u03d5 : M0K \u2192 M| |\n|Input k| |Output I\u2032|\n|\u2a00|Recon. Network \u03b8| |# (b) Adaptive sampling (H2 [4, 5, 35, 36])\n\n|Mask M0|Output I\u2032|\n|---|---|\n|\u2a00|e\u03c8|\n|M1|\u2a00|\n|\u03b81|MJ|\n|\u22ee|\u2a00|\n|\u03b8J| |# (c) Adaptive selection of sampling-reconstruction (H1.5, ours)\n\nFig. 1: We propose the adaptive selection of sampling-reconstruction (H1.5, c). In Fourier compressed sensing, there were two classes of methods for finding the optimal sampling: joint optimization of sampling-reconstruction (H1, a) and adaptive sampling (H2, b). H1 has low potential as its mask M is not adaptive to each data point. H2 poses a challenge in optimizing the mask generator (\u03c0\u03d5) and then exhibits Pareto suboptimality, where a single \u03b8 is not optimal for multiple masks M \u2208 R(\u03c0\u03d5). In contrast, H1.5 is adaptive for input k (e\u03c8 in c selects the best M -\u03b8 pair), avoids the challenge of backpropagation to discrete space (red lines in a, b, c), and achieves Pareto optimality by dedicating each network \u03b8j exclusively to Mj. \u2299 denotes the componentwise multiplication.\n\nguarantees that an image can be accurately recovered from a non-adaptive random sampling pattern, with much fewer samples than the Nyquist-Shannon sampling theorem requires, if the image has a sparse representation in that domain. Before the era of deep learning, CS used to refer to obtaining the final image through l1-regularized reconstruction, i.e., solving Lasso [7]. Recently, reconstruction has# Adaptive Selection of Sampling-Reconstruction in FCS# 3\n\n|Methods|Adaptive to input k|Backprop to a continuous space|Pareto optimal \u03b8|\n|---|---|---|---|\n|H1 [3, 8, 9, 19, 33, 38]|%|%(! [33])|!|\n|H2 [4, 5, 29, 35, 36]|!|%(! [29])|%|\n|H1.5 (ours)|!|!|!|\n\nOften been performed using deep neural networks trained on the data. In this paper, we focus on deep learning-based reconstructions.\n\nFourier compressed sensing (Fourier CS) refers to CS where the measurement is in the discrete Fourier transform (DFT) of an image. As electromagnetic waves are inherently wave-like, obtaining spatial information such as pixel values directly is not feasible. Instead, spatial information is acquired through DFT. Obtaining samples for every Fourier-transformed element can be costly. Unlike the CS theory that suggests random sampling is sufficient, the sampling results of Fourier CS, similar to the prior in natural images, concentrate a significant amount of energy in the low-frequency (LF) components [25,34]. Despite deviating from the random sampling principle of CS theory, Fourier CS achieves excellent image quality in many domains by extensively sampling LF components; hence it has been successfully applied to various electromagnetic imaging applications, including magnetic resonance imaging (MRI) [25, 26] or radar [10, 14].\n\nHowever, finding an optimal sampling method that is both efficient and effective remains a challenge in Fourier CS. One approach is the joint optimization of sampling-reconstruction (denoted by H1) [3,8,9,19,33,38], where the parameters of both the sampling and reconstruction networks are jointly trained using the dataset, as depicted in Fig. 1a. But this approach has two drawbacks, as described in Tab. 1. Firstly, it is not adaptive to each data point, i.e., the optimized sampling and the reconstruction parameter would not be the best pair for a specific input. Moreover, the reconstruction network is usually trained by backpropagation. To employ backpropagation, the parameters are expected to be defined within continuous spaces. This makes training the sampling mask not trivial, as it is defined in a discrete space. Most methods [3, 8, 9, 19, 38] just perform discrete optimization anyway using the straight-through estimator [6, 38].\n\nThe other approach is adaptive sampling (denoted by H2) [4,5,27,32,35,36,41], which aims to generate the best sampling mask for each data point (or each image) based on the fact that a predetermined sampling mask may not be optimal for every situation. Most adaptive sampling studies generate the optimal sampling mask based on the information from the initially measured LF components of each data point, which has the potential to achieve excellent results. Then, they usually have a single reconstruction network that is responsible for many optimal masks for all data points. Unfortunately, there are a couple of major issues in# 4 S. Hong et al.\n\ncurrent adaptive sampling, as in Tab. 1. Similar to the joint optimization of sampling-reconstruction models, optimizing the mask generator is challenging due to the broad and discrete mask space, as depicted in Fig. 1b. Secondly, the single reconstruction network for diverse sampling masks may not be Pareto optimal, which we called Pareto suboptimal reconstruction network. Note that a similar issue can arise in the task of restoring various degradations (e.g., the performance of a blind denoising network trained on multiple noise levels is usually lower than that of an identical network trained only on the specific noise level used as the actual input [39]).\n\nIn this paper, we propose a novel adaptive selection of the sampling-reconstruction framework for Fourier CS that alleviates the drawbacks of joint optimization of sampling-reconstruction and adaptive sampling. It is adaptive to each data point, avoids backpropagation to discrete spaces, and its reconstruction network is Pareto optimal. In the adaptive selection, we first sample LF components quickly and then leverage a super-resolution (SR) space generation model, to quantify the high-frequency (HF) Bayesian uncertainty. This approach ensures that HF components, which contain crucial details, are sampled more effectively, leading to improved reconstruction quality. The main contributions of our paper are as follows:\n\n- Proposing a novel adaptive selection of sampling-reconstruction framework for Fourier CS that alleviates the drawbacks of joint optimization of sampling-reconstruction and adaptive sampling with theoretical justification.\n- Designing the adaptive selection to efficiently quantify HF Bayesian uncertainty by leveraging an SR space generation model for determining sampling masks.\n- Demonstrating that our adaptive selection improves performance in multiple Fourier CS problems such as facial image restoration (up to 0.04 average gain in SSIM) and multi-coil MR reconstruction (up to 0.004 average gain in SSIM).# 2.1 Fourier compressed sensing\n\nFourier CS can be defined as the following regression problem. Let us define the dataset D = {(ki, Ii)}i=1 , . . . , IN \u2208 I \u2286 RL are the corresponding images, sampled k-space data and I1, I2 such that k1, k2, . . . , kN \u2208 K \u2286 CL are fully-N respectively. Let us define the mask space by M \u2286 {0, 1}L\u00d7L whose element is a diagonal binary matrix (indicating acquired (1) and unacquired (0) grid points). Let h(k; M, \u03b8) : K \u00d7 M \u00d7 \u0398 \u2192 I be a reconstruction function of k for a sampling mask M \u2208 M and a reconstruction network (e.g., U-Net [28] or E2E-VarNet [31]) parameterized by \u03b8 \u2208 \u0398. Then, this function h can be used as a joint sampling-reconstruction model that optimizes both the sampling mask M and the reconstruction network \u03b8. Specifically, for the given dataset D, the model is optimized to minimize the following empirical risk:\n\nL\u02c6[h(K; M, \u03b8)] = N-1 \u2211i=1N l(Ii, h(k; M, \u03b8))# Adaptive Selection of Sampling-Reconstruction in FCS\n\nwhere l is the loss function (e.g., l(I, I\u02c6) = 1 \u2212 SSIM(I, I)).# 2.2 Joint optimization of sampling-reconstruction\n\nOne of the recent approaches to finding a good sampling mask is joint optimization of sampling-reconstruction [3, 8, 9, 19, 33, 38], which reconstructs the image with a non-adaptive mask M \u2208 M. They are defined as follows:\n\nH1 = {h(\u00b7; M, \u03b8)|M \u2208 M, \u03b8 \u2208 \u0398}. (1)\n\nThey jointly optimize M and \u03b8 for a dataset; however, M is not adaptive to each data point. Whether using a tailored M or not, the fundamental limitation of H1 is that the sampling mask is not optimal for each data point. Moreover, they exhibit highly varying results across different settings, as they require discrete optimization [3, 8, 9, 19, 38] or virtual data [33], as shown in Fig. 1a and Tab. 1.# 2.3 Adaptive sampling\n\nSome recent works [4, 5, 27, 35, 35, 36] employ adaptive mask, using a mask generator \u03c0\u03d5 : M0K \u2192 M, parameterized by \u03d5 \u2208 \u03a6, as shown in Fig. 1b. Here, M0 \u2208 M denotes a mask that samples only LF components. Adaptive sampling approaches minimize L\u02c6[h] on\n\nH2 = {h(\u00b7; \u03c0\u03d5(\u00b7), \u03b8)|\u03c0\u03d5 : M0K \u2192 M, \u03b8 \u2208 \u0398, \u03d5 \u2208 \u03a6}. (2)\n\nObviously, H1 \u2286 H2. That is, H2 has the greatest potential but is hard to train because of its complexity. Specifically, H2 faces two main issues: the difficulty of the mask generator (\u03c0\u03d5) optimization, and Pareto suboptimality of \u03b8, due to the fact that a single reconstruction network is responsible for multiple masks.\n\nPrevious studies on H2 have used reinforcement learning [4, 27, 35] or back-propagation [4, 5, 35, 36] using straight-through estimator [6, 38] to optimize mask generator \u03c0\u03d5, but this is a complicated problem because the action space M is too broad and discrete. \u03b8 is Pareto suboptimal in adaptive sampling studies since there are multiple sampling masks M while only one \u03b8 exists at inference time. Due to these difficulties, most adaptive sampling studies in CS-MRI have been conducted in a clinically less relevant simple setting of single-coil [4, 27, 32, 35, 36, 41]. There was only one study conducted on realistic multi-coil setting [5], but the final models of [5] turned out to be non-adaptive, which is an unintended consequence.\n\nTo avoid optimization in discrete space, [29] proposed adaptive sampling using a conditional Generative Adversarial Network (cGAN). During sampling, cGAN assesses the uncertainty of samples yet to be acquired. Subsequently, the user selects a sample with the highest uncertainty for acquisition (i.e., greedy algorithm). This process of quantifying and sampling is iteratively repeated. While this method benefits from backpropagation occurring only in continuous space (Tab. 1) it still faces the challenge of a single reconstruction network, having to perform reconstruction for all masks. Consequently, CS-MRI experiments were conducted using a simple single-coil setup.# 2.4 Super-resolution space generation\n\nSuper-resolution (SR) space generation [23, 24] aims to create diverse high-resolution (HR) images that can be downsampled to the same low-resolution (LR) image (i.e., q\u03c8 (IHR|ILR) \u2208 {\u03b4I |I \u2208 I}). For this purpose, a stochastic approach is used rather than a deterministic one. Conditional normalizing flow-based SR space generation methods [15, 22, 30] explicitly obtain q\u03c8 (IHR|ILR) using a diffeomorphic mapping f\u03c8 : I \u2192 Z and a simple base distribution qz (e.g., standard Gaussian), as q\u03c8 (IHR|ILR) = qz (f\u03c8 (IHR; ILR))|det \u2202f\u03c8 (IHR; ILR)|.\n\nSince f\u03c8 is invertible, qz and IHR can be used to directly sample IHR from q\u03c8 (i.e., z \u223c qz =\u21d2 f\u03c8\u22121(z; ILR) \u223c q\u03c8 (\u00b7|ILR)). In this work, we trained and exploited a recent robust flow-based SR space generation method [30], with tuned hyperparameters [15] for stability, to generate HR images from the corresponding LR image that is reconstructed from undersampled k-space data with mask M0.# 3 Proposed methods\n\nIn Sec. 2.2 and 2.3, we investigated the difficulty of optimizing the mask generator \u03c0\u03d5, and Pareto optimal \u03b8 (for all masks). This section proposes a novel scheme, adaptive selection of sampling-reconstruction, which does not encounter these problems. Using two Theorems 1 and 2, we explain our adaptive selection (H1.5) is better than the joint optimization (H1) and the adaptive sampling (H2).# 3.1 Adaptive selection of sampling-reconstruction\n\nOur adaptive selection model H1.5 is defined as follows:\n\nH1.5 =\n\nXe\u03c8 (\u00b7)j h(\u00b7; M, \u03b8j )\n\ne\u03c8 : M0K \u2192 {ej }J=1, Mj \u2208 M, \u03b8j \u2208 \u0398, \u2200j\n\nwhere ej is the j-th standard unit vector (i.e., one-hot vector). Each submodel h(\u00b7; Mj , \u03b8j ) contains mask Mj and reconstruction network \u03b8j as a pair, which is Pareto optimal. At inference time, each data selects an appropriate submodel through the mask selector e\u03c8 (\u00b7)j, which takes input M0k. This scheme is similar to a segmented regression problem that ensembles multiple submodels using one-hot encoding.\n\nRemark 1. If H1 is a linear regression, then H1.5 is a segmented linear regression.\n\nWe propose the following Theorems 1 and 2. Theorem 1 shows that H1.5 is better than H1 due to its adaptivity, and Theorem 2 demonstrates that H1.5 is superior to H2 because H2 has poor Pareto optimality.# Theorem 1 (Adaptive selection is better than non-adaptive).\n\nFor a true risk L,   h\u2208Hinf1.5L[h] \u2264 inf1L[h].h\u2208H# Adaptive Selection of Sampling-Reconstruction in FCS# Theorem 2 (Adaptive selection is Pareto optimal).\n\nFor a true risk L, |\u03c0\u03d5(M0K)| \u2264 J \u21d2 h\u2208Hinf1.5L[h] \u2264 inf2L[h].h\u2208H\n\nPlease see the supplementary material for the proofs. Theorem 2 requires an assumption that optimizing \u03c0\u03d5 is difficult (i.e., |\u03c0\u03d5(M0K)| \u2264 J), which is justified in Section 2.2 (e.g., The final model in [5] converged to |\u03c0\u03d5(M0K)| \u2192 1, which means non-adaptive). Theorems 1 and 2 suggest that the proposed adaptive selection scheme (H1.5) may outperform both non-adaptive methods (H1) and adaptive sampling (H2). In Section 3.2, we describe the implementation of the scheme using the HF Bayesian uncertainty quantified by an SR space generation method [30].# (How to) proposed mask selector e\u03c8:\n\nThe sample variance of a generative model to produce diverse samples can be utilized to quantify uncertainty for adaptive sampling [29]. Specifically in Fourier CS, inspired by the idea of initially sampling LF components, we employ an SR space generation model [15, 30] as a HF uncertainty quantifier.\n\nThe sample variance v(M0k) := (Vardq\u03c8 [ks\u2032])s=1 is an estimator of the mean square error in k-space domain, where S is the number of the SR samples and k\u2032s is the Fourier transform of the s-th sample. We make up the mask selector e\u03c8 using v(M0k). Specifically, at train time, we normalize v(M0k) so that u(v) := v/\u2225v\u22252 and then use the k-means++ clustering algorithm [2] to {u(v(M0ki))}i=1 to N create centroids (cj )J=1. At inference time, we select adaptive mask index j by calculating the distance u(v(M0k)) and (cj )J=1.# We also need to determine the number of the sampling-reconstruction pairs J.\n\nThinking of Remark 1, increasing J doesn\u2019t always mean better average performance; while increasing J can help in robustly handling outliers. This trade-off can be organized as Remark 2:# Remark 2 (Trade-off with the number of segments J).\n\nAs J increases, despite more training resources, the average performance reaches a plateau at some point, but it becomes more robust against outliers. The choice of J depends on the user\u2019s needs; we defaulted to J = 3. We delve into and validate Remark 2 in Sec. 5.# (What to) constructed sampling-reconstruction pairs (Mj , \u03b8j )J=1:\n\nOne might try to create Mj from cj using just sorting, i.e.,\n\nMj = arg max\u2225M cj \u2225,\n\nbased on the following proposition.# Proposition 1 (simplified version).\n\nWith mild assumptions, the sorted sample variance (4) is the PSNR-maximizing mask.# SR image samples\n\n|LF k-space data|Mask M1|Recon \u03b81|\n|---|---|---|\n|Sample variance in k-space|Mask MJ|Recon \u03b8J|# (3) HF Bayesian uncertainty quantification (e\u03c8)\n\n(6.25% samples)# (4) Additionally scan masked k-space region\n\nFig. 2: Using the sample variance of SR space generation [15,30] results, we can quantify HF Bayesian uncertainty (highlighted in magenta dotted box). Then, we can adaptively select a sampling-reconstruction (M \u2212 \u03b8) pair (highlighted in cyan dotted box). Here, we illustrate how our adaptive selection of sampling-reconstruction (Algorithm 2) is employed in CS-MRI. In (1)&(2) in the figure, MRI scanner scans LF k-space region and adaptively selects mask (Mj\u22c6 , \u03b8j\u22c6 ) pair using HF Bayesian uncertainty quantification. (3)&(4) After additionally scanned masked k-space region from Mj\u22c6 , reconstructed images are generated from masked k-space data using \u03b8j\u22c6 . See Algorithm 2 for details.# Algorithm 1 Training\n\nInput: Training set {ki}i=1, initial sampling mask M0 \u2208N M, trained SR space generation model f\u03c8 : I \u2192 Z, the number of segments J, the number of SR generated images S, the number of total sampling points NM, and Output: Masks (Mj )j the empirical risk L\u02c6.J=1, reconstruction parameters (\u03b8j )J=1, and centroids of uncertainty (cj )J=1# for i = 1 to N do\n\nfor s = 1 to S do\n\ni \u2190 SPS\u2032=1 (\u03c8 1(z\u20322)M0ki) \u2212 m)\u25e62\n\nmi \u2190 \u22121Ps=1f\u22121(zss; M0ki)\n\nv S11 sS f\u2212 \u03c8 ; i\n\n(cj )uJ=1 \u2190 k-means++({ui}i=1, J)i \u2190 vi/\u2225vi\u22252# for j = 1 to J do\n\nMj \u2190 M0 + RejectionSampling(cj , NM \u2212 Tr(M0))\n\nTrain \u03b8j to minimize L\u02c6[h(\u00b7; Mj , \u03b8j )]# Algorithm 2 Inference\n\nInput: k-space input k, initial sampling mask M0 \u2208 M, trained SR space generation model f\u03c8 : I \u2192 Z, the number of segments J, masks (Mj )J=1, reconstruction network parameters (\u03b8j )J=1, and the centroids of uncertainty (cj )J=1# Output: Reconstructed image I\u2032\n\nfor s = 1 to S do\n\nSample zs \u223c N (0, \u03c3s s\u20322)0k)\n\nm \u2190 SPS\u2032=1 f \u03c8 1 11PS=1(f \u03c8 ; Ms s\u22121(z\u22121(zs; M0k) \u2212 m)\u25e62\n\nu \u2190 v/\u2225v\u22252\n\nj\u2032 \u2190 h(k; Mj\u22c6 , \u03b8j\u22c6 )\n\nI# Adaptive Selection of Sampling-Reconstruction in FCS\n\nHowever, this method is not the optimal approach for maximizing SSIM. In general, it is known that introducing randomness to the mask is effective in maximizing SSIM [34]. Therefore, we generate Mj using rejection sampling proportional to cj. Then, we train the dedicated \u03b8j for the corresponding Mj. Figure 2 shows the overview of adaptive selection, clearly showing why the proposed adaptive selection is adaptive. Algorithms 1 and 2 provide detailed descriptions of the training and inference processes of our adaptive selection method, respectively.# 4 Experiments\n\nWe proposed the adaptive selection of sampling-reconstruction scheme in Sec. 3 to address the issues of H1 and H2. This section experimentally demonstrates that the proposed method performs well in various settings of Fourier CS.# 4.1 Fourier CS face reconstruction\n\nWe performed Fourier compressed sensing on the CelebA dataset [21], which consists of 160 \u00d7 160 RGB human face images. Similar to LOUPE [3], the reconstruction network used a U-Net [28] architecture, with 6 input channels and 3 output channels, because the input, zero-filling reconstruction, is complex.\n\nFigure 3 shows a qualitative comparison of our method (H1.5) and other H1, H2 methods [3, 5, 34] at the acceleration rate 16\u00d7 with the metrics of SSIM and PSNR. Our final reconstruction results are superior to those of other methods, which supports Theorems 1 and 2. In detail, our method adeptly selects sampling-reconstruction pairs using HF Bayesian uncertainty. Looking at the first column of Fig. 3, in the case of A, the presence of horizontal stripes in the background results in a high uncertainty in the vertical direction, whereas in B, the elongated blonde hair leads to a high uncertainty in the horizontal HF components. In Fig. 4, which shows the sampling masks (with the selection) and the corresponding reconstruction results of H1.5, our e\u03c8 selected M2, which has a shape similar to the uncertainty of A in the second column, obtained the highest SSIM for A. M2 emphasizes in red in the error map, indicating effective suppression of artifacts caused by horizontal high-frequency components in the background, achieved by sampling more in the vertical direction. Similarly, M3 in the third column, which has a shape similar to the uncertainty of B, achieved the highest SSIM for B. M3 is highlighted in the error map, revealing reduced errors in the hair region of the subject due to increased sampling in the horizontal direction.# 4.2 Multi-coil CS-MRI reconstruction\n\nWe also performed Fourier compressed sensing on the fastMRI multi-coil brain dataset [37]. We resized all slices to a size of 320 \u00d7 320. The number of coils was 16. Most implementations of H1 and H2 methods were based on the official.# Ground truth\n\nH1: VD [34]\nH1: LOUPE [3]\nH2: Policy [5]\nH1.5 (ours)\n\nFig. 3: Our adaptive selection of sampling-reconstruction (H1.5) shows the strongest reconstruction performance (emphasized in red). Here, we show a qualitative comparison of reconstruction and error map at acceleration rate 16\u00d7 in the CelebA dataset [21]. For comparison, we also show the results of the variable density (VD) [34], LOUPE [3], and policy-based adaptive sampling [5]. SSIMs and PSNRs are included in the reconstructions and the error maps, respectively.\n\nfastMRI repository1. We experimented not only with 2D undersampling patterns, as described in Sec. 5, but also with 1D line subsampling used in actual MRI. For the latter, we modified the SR space generation model [30] to achieve a 16\u00d7 SR only in the horizontal direction.\n\nFigure 5 shows a qualitative comparison of our method (H1.5) and other H1, H2 methods [3, 5, 34] at 4\u00d7 1D undersampling with the metrics of SSIM and PSNR. Our final reconstructions outperform other methods, supporting Theorems 1 and 2. In the right half of Fig. 4, our e\u03c8 selected (M1, \u03b81), which samples more of the low-frequency components, obtained the best reconstruction result for A (second row). Besides, for input B (fourth row), (M3, \u03b83) generated the best reconstruction result (highlighted in red). Since M3 samples the high-frequency components more, it made the clearest imaging of the longitudinal.\n\n1 https://github.com/facebookresearch/fastMRI# Adaptive Selection of Sampling-Reconstruction in FCS\n\n|0.942|0.928|0.940|\n|---|---|---|\n|34.19|32.55|33.97|\n|0.962|0.961|0.963|\n|39.25|38.59|39.31|\n\nFig. 4: Our adaptive selection of sampling-reconstruction (H1.5) adaptively selects the best sampling-reconstruction pair based on the HF uncertainty of the input, leading to strong reconstruction performance. Here, we show a qualitative comparison of reconstruction and error map obtained using the mask-reconstruction pairs ((Mj , \u03b8j )3=1)j generated from Algorithm 1 at acceleration rate 16\u00d7 in the CelebA dataset [21] and at acceleration rate 4\u00d7 in the fastMRI dataset [37]. Our Algorithm 2 estimated the uncertainty of each image as in Figs. 3 and 5, and then selected the appropriate mask Mj (with \u03b8j) as emphasized in red. For all images in this case, the selected (M, \u03b8j)j resulted in the best reconstruction outcomes. SSIMs and PSNRs are included in the reconstructions and the error maps, respectively.\n\nfissure, indicating the effectiveness of the proposed method (i.e., Algorithms 1 and 2).\n\nIn Tab. 2, we present the average SSIM of the proposed scheme in various accelerations and datasets. For comparison, LOUPE [3] and policy-based method [5] are evaluated. Two non-adaptive methods, uniformly random mask and sampling from VD [34], are also evaluated. For a 1D line sampling CS-MRI, equispaced masks are additionally evaluated. As shown in Tab. 2, our adaptive selection approach consistently achieves higher SSIM compared to other methods in all scenarios. For example in CelebA dataset at acceleration rate 8\u00d7, SSIM of our method (0.9405) is about 0.04 higher than the best of H1 (0.9073) and# 12 S. Hong et al.\n\n|Ground truth|H1: VD [34]|H1: LOUPE [3]|H2: Policy [5]|H1.5 (ours)|\n|---|---|---|---|---|\n|A|0.933|0.923|0.923|0.942|\n|Uncertainty of A|32.89|32.44|32.44|34.19|\n|B|0.960|0.956|0.956|0.963|\n|Uncertainty of B|38.59| |37.90|39.31|\n\nFig. 5: In a practical multi-coil CS-MRI 1D line sampling scenario, our adaptive selection of sampling-reconstruction shows the highest SSIM (highlighted in red). Here, we show a qualitative comparison of reconstruction and error map at acceleration rate 4\u00d7 in the fastMRI dataset [37]. For comparison, we also show the results of the variable density (VD) [34], LOUPE [3], and policy-based adaptive sampling [5]. SSIMs and PSNRs are included in the reconstructions and the error maps, respectively.\n\nH2 (0.8501). In addition in a realistic setting, CS-MRI 1D at acceleration rate 8\u00d7, SSIM of our method (0.9407) is about 0.004 higher than the best of H1 (0.9367) and H2 (0.9240), which is a significant difference in MRI reconstruction problem [40].# 5 Discussion\n\nDoes the SR space generation model quantify the HF uncertainty well? Since sample variance estimates MSE, evaluating SR space generation can be done by sorting the sample variance, as in Proposition 2. After adaptive sampling and zero-filling for reconstruction, PSNR can be used as a metric for assessment. We qualitatively (in the supplementary material) and quantitatively (in Tab. 3) compare the adaptive sampling results in MRI [37] at acceleration.# Adaptive Selection of Sampling-Reconstruction in FCS# Table 2: Our adaptive selection of sampling-reconstruction (H1.5, Algorithms 1 and 2)\n\nshows the highest SSIM in Fourier CS in various settings (CelebA dataset [21] w/ 2D sampling, fastMRI multi-coil dataset [37] w/ 1D, 2D sampling).\n\n|SSIM\u2191|CelebA|CelebA|CelebA|CS-MRI|CS-MRI|CS-MRI|\n|---|---|---|\n|Method \\ Accel.|8\u00d7|16\u00d7|4\u00d7|8\u00d7|4\u00d7|8\u00d7|\n|Random|0.8378|0.8684|0.9663|0.9506|0.9533|0.9255|\n|H1 VD [34]|0.9073|0.8734|0.9698|0.9578|0.9603|0.9367|\n|LOUPE [3]|0.8742|0.8673|0.9671|0.9525|0.9541|0.9218|\n|Equispace (1D)|-|-|-|-|0.9603|0.9258|\n|H2 Policy [5]|0.8501|0.8394|0.9698|0.9572|0.9569|0.9240|\n|H1.5 Adaptive selection (ours)|0.9405|0.8952|0.9704|0.9585|0.9624|0.9407|# Table 3: Quantitative comparison of PSNR and SSIM with zero-filling\n\n\u2018Sorted-Self\u2019 achieved the highest PSNR, which supports our Proposition 2. Thus, we can assert that the SR space generation model effectively quantifies the HF uncertainty.\n\n|PSNR / SSIM|Sorted-Self|Sorted-Another|VD [34]|\n|---|---|---|---|\n|4\u00d7|37.15 / 0.939|36.36 / 0.922|33.33 / 0.854|\n|8\u00d7|34.79 / 0.910|34.24 / 0.894|32.16 / 0.834|\n\nrate 8\u00d7. Table 3 presents the average PSNR and SSIM of the proposed methods on the validation dataset. \u2018Sorted-Self\u2019 refers to the zero-filling reconstruction results obtained when sorting its own HF Bayesian uncertainty to create a mask, while \u2018Sorted-Another\u2019 randomly shuffles the masks among the data points. We additionally generate a mask sampled from VD [34] for comparison. As a result, the \u2018Sorted-Self\u2019 approach consistently achieves the highest PSNR and SSIM. These results show that the SR space generation effectively quantifies HF Bayesian uncertainty.# Effect of the number of segments J\n\nHere, we validate Remark 2 by conducting an ablation study using the dataset employed in our experiments. Figure 6low, shows the average of the lowest 5%, 10%, and 100% of SSIM values (SSIM 5% SSIM 10%low, and SSIM) for all J = 1, . . . , 4 in the CS-MRI 8\u00d7 experiments (2D and 1D). The fact that the increase in SSIM is less noticeable when transitioning from J = 2 to J = 3 or J = 4 compared to the transition from J = 1 to J = 2 supports the first part of Remark 2, \u201cAs J increases, despite more training resources, the average performance reaches a plateau at some point.\u201d Additionally, the relatively low SSIM 5% low when transitioning from J = 2 to J = 3 or from J = 3 to J = 4 supports the latter part of Remark 2, \u201cAs J increases, it becomes more robust against outliers.\u201d Therefore, users can choose J considering this trade-off.# SSIM margin w.r.t. J = 13.5\n\n| |J, number of segments| |J, number of segments|\n|---|---|---|---|\n|SSIMlow|0.5|1|1.5|\n|2|2.5|3|4|# (b) CS-MRI 1D 8\u00d7\n\nFig. 6: Trade-off with the number of segments J. The average of the lowest {5%, 10%, 100%} of SSIM values according to the number of segments J in CS-MRI (a) 2D 8\u00d7 and (b) 1D 8\u00d7 experiment, respectively. All SSIM values were shown as margin with respect to J = 1. SSIM reaches a plateau after J = 2, but SSIM 10% more higher in J = 2, 3 or 4. These results support our Remark 2.# Table 4: Comparison of rejection sampling and \u2018kmeans-Sorted\u2019.\n\n|SSIM in CS-MRI J = 3|2D 4\u00d7|2D 8\u00d7|1D 4\u00d7|1D 8\u00d7|\n|---|---|---|---|---|\n|Rejection sampling (ours)|0.9704|0.9585|0.9624|0.9407|\n|kmeans-Sorted|0.9612|0.9478|0.9493|0.9167|\n\nOther sampling methods We employed the rejection sampling to introduce randomness to the mask [34]. To check the effectiveness of the rejection sampling, we compare it with \u2018kmeans-Sorted\u2019 method (i.e., applying \u2018Self-Sorted\u2019 in Tab. 3 to the k-means centroids). In Tab. 4, SSIM in rejection sampling is much higher than \u2018kmeans-Sorted\u2019 in MRI datasets, supporting the effectiveness of rejection sampling. For more discussions such as runtimes, see the supplementary material.# 6 Conclusion\n\nWe have presented an adaptive selection of sampling-reconstruction framework for Fourier CS. Our method uses an SR space generation model to quantify the high-frequency Bayesian uncertainty of each input; hence is adaptive compared to the existing joint optimization of sampling-reconstruction (Theorem 1). Since our method has a dedicated reconstruction network for each sampling mask, unlike adaptive sampling, our method does not suffer from the Pareto suboptimality (Theorem 2). The proposed method improved SSIM in various Fourier CS experiments, such as CS of facial images and CS-MRI in a practical multi-coil setting.",
        "context_id": 20,
        "question": "What field has compressed sensing revolutionized by allowing the reconstruction of high-quality images from fewer measurements?",
        "answer": [
            "image acquisition"
        ],
        "context_length": 29549
    },
    {
        "context": "# 1 Introduction\n\nThe last decade has experienced a more and more intense research activity in the field of quantum computing as well as an increased interest of companies in its use within their businesses. Indeed, it gave rise to the experimental realization of different physical platforms for the realization of Quantum Processing Units (QPUs). Some of the most promising ones include: superconductors,# Trapped Ion and Neutral Atom Devices\n\nTrapped ion, spins in semiconductors, NV centers in diamond, photons, and neutral atoms trapped in optical tweezers [1\u20136]. However, all these systems share the fact that they are afflicted by noise introducing errors during the computation. In fact, we refer to today\u2019s quantum processor as Noisy Intermediate-Scale Quantum (NISQ) devices [7]. In addition to noise, the number of operations performed and their duration must also be kept under control so that good results can be achieved. The number of qubits, which is relatively low, is also a problem because as their number increases, noise does increase. For this reason, numerous studies are being carried out to find methods to reduce the effects of noise: these may be based on the addition of extra qubits to realize quantum error correction, still unfeasible today, [8\u201310] or on ML protocols [11\u201313].# Growth of Neutral Atom Devices\n\nIn this context, neutral atom devices have experienced a significant growth in interest for both the academic and non-academic community. This intense growth culminated in the realization of error corrected logical qubits [14], placing neutral atom devices among the most promising systems at the moment. The 48 logical qubits are obtained from 280 physical qubits (atoms) grouped into logical blocks. Leveraging this, the authors show how it is possible to perform operations while maintaining high fidelity (99.8%).# Advantages of Neutral Atom Devices\n\nIn addition to this, atoms are naturally identical, hence free from manufacturing errors. Moreover, their scalability is obtained via the realization of spatially extended potentials being able to create multiple optical tweezers. In these devices, excited and fundamental states of atoms are used for the realization of the two (qubit) computation states. They generally benefit also from relatively long coherence times and allow working in both digital (gate mode) and analog mode (Hamiltonian mode) [15]. In the latter, one directly manipulates the Hamiltonian of the system, for instance by applying global laser pulses on the atoms. This greatly reduces the duration of quantum protocols.# Applications in Optimization Problems\n\nFinally, neutral atom devices are naturally suited for efficiently solving combinatorial optimization problems, which makes them particularly popular in various fields: routing [16\u201318], scheduling [19, 20], energy distribution [21, 22], finance [23\u201325] and also ML applications [26\u201329]. Optimization problems that can be solved naturally with such devices are the Quadratic Unconstrained Binary Optimization (QUBO) problems [30, 31]. Such problems, apart from being very difficult to solve exactly as they belong to the NP-hard class, find many applications in industry. Among these there are graph clustering (quantum community detection problems) [32], traffic-flow optimization [33], vehicle routing problems [34], maximum clique problems [35], and financial portfolio management problems [36]. Therefore, many attempts have been made to find possible ways of solving them.# Quantum and Classical Approaches\n\nGiven their relation with the Ising model, the possibility of solving them in an approximate manner using quantum computers has been explored. Such problems can also be solved in a classically approximate manner by simulated annealing, where temperature fluctuations are exploited to try to reach a solution to the problem. However, in some cases a quantum advantage can be obtained by exploiting quantum fluctuations and the tunneling effect to find the minima of optimization problems [37]. This is the basis of adiabatic.# Computing and Quantum Annealing\n\nFor such problems, Quantum Adiabatic Algorithm (QAA), i.e. a process in which a quantum Hamiltonian is evolved very slowly into the one representing the desired problem, can be exploited. The state of the system, in turn, changes from the ground state of the initial Hamiltonian to the one of the target Hamiltonian, thus obtaining the solution of the problem. Potentially, the high connectivity and flexibility of atom topology seem to make them more advantageous than other quantum annealing platforms [38].\n\nSince it is possible to reformulate the training process of a ML model as a QUBO problem [39], a neutral atom device can be used to train ML models via QAA. Specifically, the model that will be trained is a QUBO-based version of Support Vector Machine (SVM) [40], which will be called QUBO SVM from here on.# Supervised Learning\n\nSupervised Learning, as a branch of ML, is characterised by the presence of ground truths (labels) that are associated with the data. These ground truths play a crucial role in the training and testing phase of the model. For this purpose, the dataset is divided into two parts (at least), one for training and one for performance evaluation, i.e. training and test sets, respectively. Within supervised learning, it is possible to identify classification and regression tasks. The former concerns the training of a model capable of assigning data to two (or more) classes. In the latter, a model is trained to predict a continuous value of one or more parameters.\n\nSVMs are a family of widely used classifier models that are appreciated for their stability [41, 42], meaning that small differences in the training set do not cause significant changes in the results. In general, these models are used when the dataset is small, but there are applications where they are used on top of neural networks with significant gains in performance [43\u201345]. In particular, SVM versions based on digital quantum computing have been developed [46] which, however, suffer from the limited number of qubits, noise, and the fact that the quantum processors must be used for both training and test set.\n\nThis makes them unusable on large datasets because of the cost and access limitations of QPUs. In this manuscript, we will compare the performance of the QUBO SVM models trained via QAA on a dataset of particular relevance: CCF, i.e., a scenario in which someone other than the owners makes an unlawful transaction using a credit card or account details. The problem is of crucial relevance given the exponential growth of electronic payments, resulting in annual losses of billions of dollars. Automated detection of CCFs is therefore a hot topic of research [47].\n\nThis problem has already been addressed in an unsupervised manner by Ref. [48], however, this approach currently restricts its applicability because it cannot be implemented on current QPUs due to its unsupervised nature. In fact, it is necessary to provide the entire dataset to the QPU by encoding it into a single QUBO matrix, whose size scales with the number of qubits. Although the model shows very good performance, current QPUs do not have sufficiently enough number of qubits for application-relevant data sets. Our approach, on the other hand, requires that only the training part exploits a QPU, while the testing phase exploits classical hardware.since the QUBO matrix is obtained only from the training data, it is sufficient that the size of the training set is chosen in such a way as to respect the limits of today\u2019s QPUs. Thus, what is proposed in this manuscript is applicable on today\u2019s NISQ devices.\n\nThe QUBO SVM model is first implemented and simulated on classical hardware, then tested on a real QPU by extending the number of qubits (or similarly atoms) used. For the simulation on classical hardware, we exploit the Python library Pulser [49], which allows ideal and noisy simulations. Pulser is developed by Pasqal [50], a company that builds QPUs based on neutral atoms, which, in some versions, can leverage more than 100 atoms. Via Pulser, it is possible to run simulations either on real machines or on classical hardware. As far as the implementation on QPUs is concerned, the real Fresnel neutral atom QPU developed by Pasqal is used, which, in the experimental configuration available at the time of the experiment, can use up to 25 atoms.\n\nThis manuscript is organised as follows: in Sec. 2, a brief introduction to QUBO problems is given, and in Sec. 3, a quick overview of the phenomenon of Credit Card Frauds (CCFs) and their detection is given. The sections 4 and 5 respectively introduce the SVM model with some of its applications and modified versions. In Sec. 6, the used CCF dataset is presented and described, while Sec. 5 discusses the implementation of the QUBO problem concerning the training of an SVM model on a neutral atom QPU. Finally, in Secs. 8 and 9, more general comments on our results are presented and some final conclusions and outlooks are drawn.# 3 CCF detection\n\nRecent years have seen a growth in electronic payments based on credit cards, both in physical shops and in online payments. With them, the number of identity thefts is also on the rise. This poses a major new problem, as CCFs are responsible for financial losses for credit card holders and credit institutions themselves. Confirming this, in 2018 the value of fraudulent transactions related to cards issued in the eurozone alone amounted to approximately 1.8 billion euros [51]. In 2025, worldwide fraud-related losses are expected to reach approximately 35 billion dollars [52]. For this reason, financial institutions are prioritising the development of algorithms capable of detecting and preventing losses due to such activities. In particular, ML and Deep Learning (DL) algorithms are being applied to decide whether an incoming transaction is related to a fraud attempt or not. However, given its importance, it is a very active field of research, with constant improvements and in which many techniques from different backgrounds are applied.# 4 SVM\n\nBinary classification is one of the most famous and common ML tasks: it involves training a parametric model that becomes able to assign data to two distinct categories via supervised training. SVM is a very famous classical ML model used to perform binary classifications. With mapping data in high-dimensionality spaces, it is possible to classify both linearly separable data and non-linearly separable data. However, this results in computational costs that can grow exponentially when dealing with large amounts of data. This problem can be circumvented by using kernels. Indeed, when data appears in the form of a scalar product, instead of mapping the data into another space and calculating their scalar product, it is possible to replace it directly with the inner product of the data in the new space. The use of kernels leads to an overall less computationally expensive problem and has the advantage of strong mathematical foundations.\n\nIn the field of supervised learning, it is well known that SVM can offer good performance even with small datasets, unlike neural networks requiring a lot of data to be trained. We now consider a dataset\n\nD = {(xn, yn) : xn \u2192 Rd, yn = \u00b11} n=0,...,M \u21971, (3)\n\nwhere xn is a sample (i.e. the feature vector) and yn the associated target. We call the classes yn \u2192 {1, \u21911} \u2193n respectively as \u201cpositive\u201d and \u201cnegative\u201d. The reader interested in more technical details of the SVM model can find them in the appendix 10.1.adopted practices. Initially, the SMOTE oversampling algorithm [54] is applied to increase the minority class data (the frauds), then random undersampling [55] is used to reduce the size of the dataset to balance the classification problem. At the end, a balanced dataset containing 500 samples is obtained. This will then be divided into training and validation sets. The test set, instead, has not been rebalanced and has the same class ratio as the original data set (0.17% fraud). This is because the test set only needs to simulate a scenario that is as realistic as possible, unlike the other splits that can instead be modified to attempts making the model to learn better. Treating the test set separately avoids data leakage that would lead to overestimation of performance.\n\nTo be sure about the performance of the QUBO SVM models, they are compared with the main classical models in the literature: classical SVM and Decision Tree. In addition, various versions of the QUBO SVM models are trained using ideal and noise simulations (denoted respectively with the letter \u201ci\u201d and \u201cN\u201d). Furthermore, two types of models are tested: QUBO SVM and a stacked ensemble configuration in which the QUBO SVM model is used as a meta-model, i.e. it is trained using the outputs of other models as data. After ensuring that the models actually work, both configurations are tested on a real QPU, exploring regimes that are not easily simulated with local clusters.\n\nThe reader interested in all the information on the various model configurations (including the number of measurements used for each training) that have been used can find it in the appendix 10.4.# 7 Neutral atom implementation\n\nWe now describe the model implementation on a neutral atom NISQ device. We distinguish three phases: encoding, training and testing. Let us point out that only the second phase takes place on a QPU, while the first and third ones are pre- and post-processing via traditional CPU. Encoding is achieved by computing the QUBO matrix of Eq. (18): this is done by specifying all the training data together. This means that the linear dimension of the (square) matrix will be K \u2194N. Hence, the parameter K plays an important role in terms of the number of qubits required to implement the algorithm. K \u2194 N are the required atoms. The encoding in this case takes place on the quantum register.\n\nWe recall that for a QPU of neutral atoms the (global) Hamiltonian has usually the form\n\nH = \u228b$(t) \u2211\u03d1 i x \u2191\u228b\u03b5(t)\u2211\u03d1 i z + \u2211U ij n n j . i\n\nHere, z)/2 and with \u03d1 i z representing the component along the z-axis of the pauli$ and \u03b5 are, respectively, the laser amplitude and the detuning, n i = (1 + \u03d1 i vector applied to the i-th atom. By tuning the positions of the atoms, the value of $ and \u03b5, we can make H as close as possible to Q. This is done using the COBYLA optimizer [56] and imposing the physical constraints of relative# Distance Between Atoms\n\nThe distance between atoms that are feasible on the specific real QPU. As far as the QPU is concerned, due to the prototype available at the time of the simulations, the atoms in the quantum register can only be arranged on the vertices of a triangular lattice with a fixed lattice constant. Therefore, now the co-ordinate optimisation no longer takes place on continuous but discrete space, which is why now the best spatial configuration of atoms on a triangular lattice is found by simulated annealing. Although this appears to be a more disadvantageous condition for satisfying HQ \u2197 Q, we will show that from the results it does not appear that the model suffers from such a limiting constraint.# Hamiltonian and Measurement\n\nWe denote with HQ the Hamiltonian obtained after tuning the coordinates and fixing the sequence. In any case, we recall the goal of this procedure is to obtain a Hamiltonian such that HQ \u2197 Q. Finally, we repeat the preparation-pulse-readout cycle N shots times. This yields a distribution of atomic states with the corresponding probabilities: if the problem has been well encoded in the Hamiltonian, the highest probability states are the best solutions of the QUBO problem.# QUBO Problem and SVM Model\n\nBut the problem framed as a QUBO is the training of an SVM model, i.e. finding a hyperplane that best separates the points of the two classes while committing as few errors as possible. Therefore, each quantum state obtained (a string of binary variables) is a possible decision boundary defined by \u03d6n. The state with the largest probability is the best solution to the QUBO problem and it contains the coefficients of the best model. Since a particular SVM model is completely defined by the hyperplane (which is defined).# Figure 2\n\nPulse sequence of the protocol, the height of the smooth green curve depends specifically on the QUBO matrix.# 10\n\nBy the coefficients \u03d6), to test a model it is sufficient to use a particular \u03d6 n in the decision function (12) and provide new data as input to the model. This testing operation is done on a classical computer. At this point, the predicted class can be derived from the decision function. By repeating this it is possible to compute the predictions on a whole test set. After that, one can use the metrics for classification used in classical models: accuracy, precision, recall, f1-score, balanced accuracy, Area Under the Curve of the Receiver Operating Characteristic (AUC ROC) and so on.\n\nHowever, this classifier allows us to experiment different approaches towards the best performing one: (i) using the trained model as it is or (ii) using it as a base model in different kinds of ensemble techniques. The latter is a ML technique that combines the predictions of several trained models together to form a new model that exhibits superior performance, lower bias, robustness, etc. There are various ways of combining models, here we exploit average voting and stacking. In the first case, many models are trained on the same training set and subsequently tested on the same test set. The final predictions are an average of the predictions of the individual models. In stacking, on the other hand, two layers of models are used. In the first layer, several models are trained on the same training set and then tested on the same test set. In this case, however, the outputs of the models in the first layer are used to train and test the model in the second layer, also known as the meta-model. These types of approaches are exploited in CCF detection [57, 58]. More information on ensemble techniques can be found in the appendix 10.3 and in Ref.[41].\n\nIn particular, since a full training of N shots measurements leads to a distribution of states with the relative probabilities, it is possible to set up a full quantum average voting strategy. In fact, remembering that each observed bit-string represents a possible solution of the QUBO problem), one can potentially obtain up to 2N atoms states (and thus solutions of the training problem). It is interesting to see if aggregating multiple models into a single model yields more robust predictions. Then, operationally, the predictions on the test set are computed using each bitstring (state) obtained from the training process. The predictions obtained from the various models are then aggregated by taking an average. In addition, by providing a validation set and specifying with respect to which metric one wants to optimize, it is possible to repeat this procedure by finding the number of aggregate models maximizing that metric (one starts with the highest probability states eventually trying to use all of them). At the end of this procedure, one obtains the classifier ensemble composed of the models that maximize the specified metric.\n\nFurthermore, it is possible to use the proposed QUBO SVM model as a meta-model in a hybrid quantum-classical stacked ensemble learning procedure, in which the quantum model is trained using the outputs of classical models as input.# 8.1 Computational complexity\n\nIn this section we aim to analyze time-qubit complexity of the introduced QUBO SVM model, and compare it with the commonly used classical SVM models. We have to distinguish three different phases in which we will derive the complexity:\n\n1. time required to convert the algorithm into a QUBO problem\n2. time required to embed the QUBO problem on the quantum register\n3. theoretical time complexity of QAA.\n\nThe former can be derived from Eq. (17) and it is estimated by O(N2 \u2194 K2 \u2194 Nfeatures) operations if a linear kernel is exploited. Regarding the number of operations required by the embedding, it is constant, since we specify a maximum number of iterations in the COBYLA optimizer. Therefore, there is no real scaling. Moreover, the number of features Nfeatures of the dataset is fixed and can be reduced via dimensionality reduction techniques. Finally, although the theoretical time complexity of QAA is O(1/%2), where % is the minimum eigenvalue gap between the ground state and the first excited state of the Hamiltonian H. In any case, since in the current neutral atom QPUs the maximum sequence duration is upper bounded, we consider it to be constant and equal to this limit. Therefore, even in this case, there is no real scaling to take into account. Therefore, the time complexity of the proposed quantum algorithm is O(N2 \u2194 K2). In contrast, the time complexity of SVM is O(N3), although recently algorithms have been proposed to lower the complexity to O(N\u03c9), with 2 < \u03f1 < 3. Finally, regarding the qubit footprint, our algorithm shows a O(N \u2194 K) qubit scaling. However, since K is a fixed hyperparameter, the complexity reduces to O(N2) and O(N) for time complexity and qubit footprint respectively.\n\nTherefore, the algorithms proposed in this manuscript have: better scaling (N2 < \u03c9 < 3); better (or at best) performance than their classical counterparts (NQUBO2 vs SVM) and lower error bars; the possibility of being implementable on atom-neutral NISQ devices; little use of the QPU; and the privacy of the data is preserved because only atomic coordinates and laser parameters reach the QPU, the model test is done locally because only training requires a QPU.# Decision Tree\n\n|Recall|0|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1.0|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|(a) 8 training samples, 16 qubits simulations.|1|0.9|0.8|0.7|0.6|0.5| | | | | |\n\nNoisy simulations\n\nIdeal simulations\n\nQPU real data\n\n|Number of qubits|0.3|8|10|12|14|16|18|20|22|24|\n|---|---|---|---|---|---|---|---|---|---|---|\n|(b) Recall scaling with the number of qubits.| | | | | | | | | | |\n\nFigure 5: Comparison of recall between classical and QUBO SVM models using 8 training samples (16 qubits), subfigure (a). Subfigure (b) shows the recall scaling in both ideal, noisy and QPU QUBO SVM stack models. In all subfigures, data with error are intended as mean and standard deviation calculated on 10 different splittings of the dataset. For the complete list of the used models see Tab. 1.# 9 Conclusion\n\nWe have shown a model of SVM whose training is reformulated through in a QUBO problem. These kinds of problems are naturally implementable on neutral atom devices using analog processing, that is, performing only global laser operations on the atoms. The linear classifier model can be easily used in the presence of nonlinear data in a variety of ways: using a nonlinear kernel, via a kernel PCA, or via feature discretization.\n\nThe model was trained and tested on a very unbalanced CCF detection dataset. This required first the application of all the techniques used in the case of highly unbalanced datasets: namely, the use of resampling techniques and the choice of appropriate metrics (other than accuracy, which is definitely obsolete in these cases). The model we considered was tested in several variants considering also ensembling strategies. For each strategy, the trained model was tested by both ideal and noisy simulations. To provide robust estimates with associated measures of error, each configuration was trained and tested on 10 different splits of the dataset finally calculating mean and standard deviation of the chosen metrics.\n\nIn addition, in order to numerically study the scalability of performance, these analyses were performed by running ideal and non-ideal simulations with training sets of 4, 5, 6, 7 and 8 samples, using quantum registers of 8, 10, 12, 14 and 16 atoms, respectively. Finally, the same analysis was then carried out using a real QPU with neutral atoms. In addition to the configurations already used and listed above, quantum registers of 18, 20, 22 and 24 atoms were also used; corresponding to training sets of 9, 10, 11 and 12 samples.\n\nAll QUBO SVM models were compared with the main binary classifier (classical) models in the literature, resulting in performance comparisons. Finally, the same analysis was then carried out using a real QPU with neutral atoms. In addition to the configurations already used and listed above, quantum registers of 18, 20, 22 and 24 atoms were also used; corresponding to training sets of 9, 10, 11 and 12 samples.\n\nWe have faced that our proposed models have performances that do not deviate much from the classical models and are compatible with them within the error bars. However, a stacked ensemble variant we proposed seems to suggest superior performance and smaller error bars, both in terms of recall and balanced accuracy. In particular, focusing only on this type of model and analyzing its scaling of both the ideal, noisy and QPU versions, some interesting conclusions can be drawn.\n\nThe first is that the performance seems to increase as the number of atoms (and thus training samples) increases. Based on this, we believe that by using a few hundred training samples, it may be possible to achieve superior performance to classical models. This means that the model could lead to higher metric values and lower associated errors. This is important because it motivates interest in investigating its performance by increasing the number of atoms even more.\n\nFor this reason, it would be interesting to test it on a neutral-atom QPU with 100-200 qubits, in order to verify the actual robustness under real conditions. Another interesting consideration concerns the fact that both the noisy and QPU model, in addition to having almost identical performances as the ideal one, shows slightly lower error. To confirm this observation, we performed# Quantum Algorithms and Applications\n\nand compared simulations at different noise levels, showing improvements in the presence of noise compared to the ideal case. This could be related to the fact that quantum noise in QAA can help to find better solutions, i.e. global minima of the cost functions. Finally, here there are some remarks on the QUBO SVM model. It is trained adiabatically via analog quantum computing and the data is encoded via the Q matrix. This avoids using the various types of encoding used in digital computation that often result in quantum circuits that are too long to execute, hence not feasible on NISQ machines. Therefore, the increase in the number of qubits is not a problem in this respect. In fact, the part of the circuit responsible for encoding, which is absent here, grows in depth with the number of qubits. Moreover, the fact that the model is trained only on the QPU is advantageous because it reduces the use of the QPU and the associated costs. Since only the training is quantum, the model can be saved and used later or multiple times at the same cost. Such a model is suitable also for contexts where data privacy is a crucial factor: in fact, the training data are never disclosed, but are used locally to compute the Q matrix. In turn, the Q matrix is reproduced as closely as possible from the Hamiltonian of the system through device optimization. Therefore, even in case of data leakage during the connection with the QPU, only the coordinates of the atoms and the intensity and detuning values would be disclosed, or at the limit, the results of the measurement without any information on the data used for the training. The analysis of the test data is again performed locally. This study motivates the development and use of model training protocols based on analog processing in real-world scenarios, given the low requirements compared to gate-based versions, in anticipation of fault-tolerant quantum computers. Others possible developments could include the unsupervised version of SVM known as one-class SVM, a version of SVM with a modified loss so as to assign different weights to misclassifications in cases where the datasets are not balanced and the use of counterdiabatic drivings, which are useful to prevent excitation while preventing the adiabatic protocol from becoming too long. Although noise does not seem to be a problem in the explored regimes, we will study what benefits we can get from logical qubits, and how they can be moved to change the topology of the quantum register, further improving our results.",
        "context_id": 21,
        "question": "How many physical qubits are used to obtain 48 logical qubits in neutral atom devices?",
        "answer": [
            "280"
        ],
        "context_length": 28685
    },
    {
        "context": "# Bridging Domain Gap for Flight-Ready Spaceborne Vision# I. Introduction\n\nThe autonomous Rendezvous, Proximity Operations and Docking (RPOD) capability with non-cooperative Resident Space Objects (RSO) is a core technological requirement for various future space missions. Aimed at sustainable space development, these missions include on-orbit servicing and refueling operations such as now-canceled OSAM missions by NASA [1, 2] and active debris removal such as RemoveDEBRIS [3] by Surrey Space Center, ADRAS-J [4] by Astroscale and ClearSpace-1 [5] by ClearSpace SA. The RPOD capability with such targets\u2014defunct satellites, debris, etc.\u2014requires accurate, real-time knowledge of the position and orientation (i.e., pose) of the target spacecraft with respect to the servicer spacecraft. In these scenarios, the non-cooperative nature of the target implies that it is without active communication links or aiding fiduciary markers on its surface to guide the rendezvous process, such that the servicer must be able to estimate and track the target\u2019s pose using the onboard sensors and computers only. Given such constraints, a monocular camera is an attractive choice of sensor due to its low Size-Weight-and-Power-Cost (SWaP-C), which is suitable for miniaturized systems such as SmallSats and CubeSats. Moreover, low SWaP-C implies additional sensor redundancy and fail-proofing compared to more complex sensor systems such as LIght Detection And Ranging (LIDAR) and stereovision which requires careful calibration.\n\nRecent years have seen a significant breakthrough in Machine Learning (ML)-based monocular pose estimation of a known, non-cooperative spacecraft. Training of these ML models such as Convolutional Neural Networks (CNN) generally requires a large-scale dataset of images and pose labels. However, access to space is difficult and expensive, and a close-range rendezvous of two satellites is in itself a very rare occasion, making in-situ data collection and label annotation a challenging task for spaceborne applications. In response, the aerospace community has adopted a methodology whereby synthetic images are generated with rendering tools and gaming engines such as OpenGL and Unreal Engine in order to train the neural networks [6\u20139]. However, synthetic images have inherently dissimilar visual features compared to real spaceborne images characterized by low Signal-to-Noise Ratio (SNR) and harsh illumination conditions. In addition, the degradation of vision cameras and target RSOs due to operations in the space environment is difficult to model prior to the mission and through computer graphics. This results in the problem known as domain gap, which manifests as a drop in performance when the models are tested on the Out-Of-Distribution (OOD) data sampled from a statistical distribution different from that of the training data [10, 11].# Footnotes\n\n\u2217Ph.D. Candidate, Department of Aeronautics & Astronautics, 496 Lomita Mall; tpark94@stanford.edu. Student Member AIAA.\n\n\u2020Associate Professor, Department of Aeronautics & Astronautics, 496 Lomita Mall. Associate Fellow AIAA.# Viehl boxts\n\nFig. 1 (left) TRON simulation room and its components. Figure from Park et al. [14]. (right) Example HIL images from the lightbox (top) and sunlamp domains (bottom) of the SPEED+ dataset [15].\n\nwith synthetic images and tested on real-life images, it is also known as reality gap [12] or sim2real gap [13] primarily in robotics literature.\n\nThe inaccessibility of space poses a particularly challenging logistics problem to not only overcoming the domain gap but also verifying it during the stringent pre-flight verification processes typically required of space missions. The most prominent approach taken by the aerospace community is to utilize a robotic testbed to simulate the space environment on-ground. The idea is to physically stimulate the vision-based sensors using a mockup model of the target placed in a facility simulating the high-fidelity space-like illumination conditions. A well-calibrated testbed can then be used to re-create various RPOD scenarios, generating pose-annotated images at scale with minimal human intervention for the purpose of validating a NN\u2019s robustness across the sim2real gap. One such example is the Testbed for Rendezvous and Optical Navigation (TRON) facility at the Stanford\u2019s Space Rendezvous Laboratory (SLAB) [14] which was used to create the so-called Hardware-In-the-Loop (HIL) images that constitute datasets such as SPEED+ [15] and SHIRT [16]. Figure 1 shows the TRON facility and a few example images from the two HIL domains of the SPEED+ dataset\u2014lightbox and sunlamp. In contrast to SPEED+ which contains static images at random poses, the SHIRT dataset includes sequential lightbox images obtained during dynamic RPOD scenarios. This allows the testing of navigation filters with ML algorithms in the loop. Overall, these HIL images can then be used as on-ground surrogates of otherwise unavailable spaceborne images for the evaluation of NN and the navigation algorithm robustness across domain gaps. SPEED+ was used as the core dataset of the second Satellite Pose Estimation Competition (SPEC2021) [17] co-organized by SLAB and the Advanced Concepts Team (ACT) of the European Space Agency (ESA), in which the participants were tasked to predict the poses on SPEED+ HIL images while only having access to the labeled synthetic and unlabeled HIL domain images.\n\nGiven such tools and datasets, the authors have explored a strategy to fully close the domain gap between synthetic and spaceborne images. The strategy consists of three steps. The first is to train a NN model using only synthetic images to be as robust as possible to the unknown spaceborne images [18]. The robustness is evaluated on-ground using HIL domain images which are excluded from the training. Then, the NN is integrated as a measurement module into an Adaptive Unscented Kalman Filter (AUKF) which tracks the pose of the target [16]. While it was shown that a well-designed AUKF allows robust tracking of the target\u2019s pose despite the domain gap suffered by its NN measurement module, there is still a remaining gap between the HIL imagery used for on-ground evaluation and the spaceborne flight images. The primary reason is that HIL images use an inexpensive mockup model of the target spacecraft which has different material and surface properties compared to the real satellite. Therefore, Park and D\u2019Amico [19] proposed to additionally fine-tune the NN weights using the flight images collected during in-space RPOD. The fine-tuning is done online in a supervised manner whereby the pose pseudo-labels are obtained from the most up-to-date state estimates of the onboard AUKF. This work has shown that Online Supervised Training (OST) can fully close the domain gap in the orientation prediction even when using a sub-optimally trained NN.\n\n\u2217https://slab.stanford.edu/\n\n\u2020https://kelvins.esa.int/pose-estimation-2021/# Current Research on Onboard Neural Networks for Spacecraft Pose Estimation\n\nDespite the success of OST, there are benefits to further maximizing the robustness of onboard NN during the offline training on synthetic images. The first is that OST requires that the onboard ML-in-the-loop navigation filter be able to first converge to steady-states so that accurate pose pseudo-labels can be generated from the state estimates. The probability of failure to converge is minimized as the OOD robustness of the onboard NN is maximized prior to deployment. The second is that training a NN onboard the limited computing environment of satellite avionics could be expensive both in terms of the computational efficiency and power consumption. This could pose a logistical challenge to schedule the NN training amidst the nominal Guidance, Navigation and Control (GN&C) operations. These two reasons motivate:\n\n1. a training mechanism that renders the NN as robust as possible across domain gap from the offline training alone, which effectively minimizes the required number of OST steps performed in space, and\n2. a NN design that is computationally efficient for both inference and online training while maintaining the maximum OOD robustness.\n\nThese two objectives\u2014OOD robustness and computational efficiency\u2014are at odd, since it is well known that larger NNs with higher capacity tend to perform better on both in-distribution and OOD data [20, 21].\n\nIn response to the above challenge, the main contribution of this work is Spacecraft Pose Network v3 (SPNv3), a NN model based on Vision Transformer (ViT) [22] for monocular pose estimation of a non-cooperative spacecraft. SPNv3 is designed and trained with onboard computational efficiency and robustness across the sim2real gap as top priorities. Extensive ablation studies of SPNv3 on the SPEED+ dataset demonstrate that ViT-based models without convolution operations dominate the computational efficiency front, and that their OOD robustness on HIL domain images can be improved by employing extensive data augmentation, enhanced transfer learning and increased input image resolution with minimal gain of computational overhead for inference. A comprehensive set of experiments reveals that the proposed SPNv3 can achieve state-of-the-art performances on the HIL domain images of the SPEED+ dataset while training exclusively on its synthetic images. Most importantly, a mid-size variant of SPNv3 would rank the first place on the lightbox category of SPEC2021 without any adversarial training or unsupervised domain adaptation, the methods which all winners employed by directly including the unlabeled test domain images into the training process. Such state-of-the-art robustness is achieved while taking no more than 40 ms per inference on an NVIDIA Jetson Nano 4GB [23], a representative, restricted compute environment similar to GPU-powered systems on satellite missions [24, 25].\n\nThis paper is organized as follows. Section II provides a brief overview of literature on ML-based monocular pose estimation and existing methods to bridge domain gap. Section III then goes over three key requirements that must be met by spaceborne ML models in addition to OOD robustness, two of which drive various design and training options for SPNv3 introduced in Section IV. Section V shows extensive experiments on the SPEED+ dataset to identify key elements of the design and training of SPNv3 that contribute the most to its robustness on HIL domain images and computational efficiency. The paper ends with conclusions and future works in Section VI.# A. ML Approaches to Spacecraft Pose Estimation\n\nThe first ML-based approach to pose estimation of a known target spacecraft was Spacecraft Pose Network (SPN) [6] which performs 1) relative attitude determination via a hybrid approach of attitude classification and regression and 2) translation estimation by exploiting the perspective transformation and geometric constraints. Since it is impossible to collect a pose-annotated image dataset from space, the same work introduced the Spacecraft PosE Estimation Dataset (SPEED) [26] which consists of 15,300 images of the Tango spacecraft from the PRISMA mission [27]. Specifically, the dataset comprises 1) 15,000 synthetic images rendered with OpenGL-based Optical Stimulator (OS) camera emulator software [28, 29] of SLAB multi-Satellite Software Simulator (\ud835\udc463) [30] and 2) 300 real images of a mockup of the same target captured from the TRON facility at SLAB. The dataset was made publicly available as part of the first Satellite Pose Estimation Challenge (SPEC2019)\u2021 [31] co-hosted by SLAB and ESA.\n\nMany top-performing entries of SPEC2019 have adopted diverse CNN architectures and pose estimation strategies, such as probabilistic orientation estimation via soft classification [7] or estimation of 2D pixel coordinates of the designated keypoints on the spacecraft surface [32, 33]. Specifically, Chen et al. [33] and Park et al. [32], who respectively ranked first and fourth places in the challenge, independently proposed a three-stage architecture: 1) an\n\n\u2021https://kelvins.esa.int/satellite-pose-estimation-challenge/# Fig. 2\n\nVisualization of an image cropped around the far-away target spacecraft.\n\nObject detection CNN which is used to identify the Region-of-Interest (RoI) around the target, 2) a pose estimation CNN which takes in an image cropped around the detected RoI and outputs the 2D keypoint locations, and 3) a P\ud835\udc5bP module which solves for the full 6D pose based on the known correspondence of detected keypoint 2D locations and 3D model coordinates. Notably, Park et al. [32] directly regresses (\ud835\udc65, \ud835\udc66) coordinates of the keypoints, whereas Chen et al. [33] outputs a set of 2D heatmaps whose peaks correspond to the locations of the keypoints. They also showed that cropping the input image around the target using a detected bounding box is crucial to performing pose estimation of the far-away target. For example, the original resolution of the SPEED images is 1920 \u00d7 1200, while most CNNs for ImageNet [34] classification expect 224 \u00d7 224 inputs. Cropping around the target prior to such dramatic down-scaling helps preserve much of the detailed target features that would otherwise be lost due to large inter-spacecraft separation as visualized in Fig. 2. Many CNN models that followed SPEC2019 also adopt similar strategies as well [8, 35\u201338]. The readers are referred to Pasqualetto Cassinis et al. [39] for a more comprehensive review of monocular spacecraft pose estimation using both conventional and deep learning-based methods.\n\nIn addition to SPEED, a number of datasets for spacecraft pose estimation have been published and made publicly available in the literature. For example, Proen\u00e7a and Gao [7], who also ranked third place in SPEC2019, published the Unreal Rendered Spacecraft On-orbit (URSO) dataset which uses Unreal Engine 4 to render synthetic images of the Soyuz and Dragon spacecraft. Kaki et al. [37] renders synthetic images of the Cygnus spacecraft using Blender and its Cycles rendering engine. Dung et al. [40] also renders about 3,000 synthetic images of different satellites for various computer vision tasks such as bounding box prediction and satellite foreground and parts segmentation. Other authors also created their own synthetic datasets, such as those of the Envisat spacecraft rendered with Cinema 4D [38], the SwissCube dataset for spacecraft pose estimation from wide-depth-range images using the Mitsuba 2 renderer [36], the SPARK [8] dataset which contains images of 11 different spacecraft rendered with the Unity3D game engine, and Synthetic-Minerva II2 [41] which renders images of the Minerva-II2 rover from the Hayabusa2 mission [42] using SolidWorks\u2019 Photoview 360 renderer.# B. Algorithms to Bridge Domain Gap\n\nThe general approaches to tackling domain gaps can be put into two categories: domain adaptation and domain randomization. Specifically, Unsupervised Domain Adaptation (UDA) aims to close the gap by directly incorporating the unlabeled target domain images into the training phase [10, 11, 43\u201349]. Many different approaches have been proposed in the literature to close the said gap. One primary approach is to align the source and the target data distributions in the latent feature space by minimizing a certain divergence criterion representing the domain discrepancy, such as maximum mean discrepancy [45, 49\u201351], H-divergence [10, 44], correlation via statistical moments [46, 52] or Wasserstein distance [53\u201355]. Another approach employs adversarial training [44, 48, 56] so that the predicted features become indistinguishable for both source and target domain inputs from the perspective of an auxiliary domain discriminator network. Note that the aforementioned approaches all aim to promote learning invariant representations by aligning both source and target domain data in the feature space.\n\nOne major shortcoming of UDA is that it requires simultaneous availability of the labeled source and unlabeled target domain data. This assumption violates the operational constraints of space missions as spaceborne images of the target do not become available until on-orbit rendezvous. Moreover, the processor and memory onboard the satellites are extremely limited for the full training session using large-scale synthetic source images and spaceborne target images collected during RPOD. Therefore, the conventional UDA approaches cannot be used for realistic space mission scenarios.As opposed to UDA, domain randomization instead aims to randomize various aspects of the training images such that OOD test images would appear as another randomized instance of the training set [57 \u201361 ]. For example, Sadeghi and Levine [58] and Tobin et al. [59] randomize lighting conditions, object textures and placements at rendering stage to train domain randomized reinforcement learning models. Jackson et al. [62] proposes style augmentation which randomizes the image texture [63 ] via neural style transfer [ 64 ]. While domain randomization does not require target domain images during the training phase, it also makes it difficult to provide any assurance that the CNN trained with domain randomization will work well on the target spaceborne images.# 1. Source-Free Domain Adaptation\n\nA different approach conducive to the operational constraints of space missions is source-free domain adaptation which does not require the availability of large-scale training images during the adaptation phase. Existing source-free algorithms leverage generative models for feature alignment [ 65\u2013 67] or pseudo-labeling and information maximization [68 ]. TENT [69] performs entropy minimization while updating only the affine parameters of the Batch Normalization (BN) layers [ 70 ]. However, methods based on entropy minimization require optimizations to be performed on batches of images in order to avoid trivial solutions, which could become computationally expensive on satellite avionics.\n\nOn the other hand, Test-Time Training (TTT) [71 , 72 ] trains on a secondary Self-Supervised Learning (SSL) task from a shared feature encoder. During test time, the encoder is trained on SSL tasks, such as rotation prediction [ 73 ] or image colorization [74]. However, TTT generally requires a hand-designed task or a large batch of negative sample pairs (e.g., contrastive learning [75 ]). Finally, Lu et al. [76] self-trains an object pose estimator CNN using pseudo-labels from its own predictions. In order to improve the accuracy of pseudo-labels and mitigate outliers, they take a SLAM-based approach and solve the Pose Graph Optimization (PGO) problem to enhance the consistency of pseudo-labels across different images. However, PGO is an offline problem solving for a set of multiple poses satisfying the motion constraints, and collecting many images until pose pseudo-labels can be obtained via PGO could become computationally expensive on satellite avionics.# 2. Spaceborne Applications\n\nThe approaches to bridging the domain gap in spaceborne applications have been explored recently after the introduction of SPEED+ [ 15] and SPEC2021 [ 17 ]. Two winners of the competition on respective HIL domain categories have both employed a generative-adversarial training procedure [ 77 ], introducing a discriminator network to the NN outputs to classify whether the predictions are made on the synthetic or HIL domain images. The winner of the sunlamp category further employed pseudo-labeling and self-training on the unlabeled HIL domain images [78 ]. P\u00e9rez-Villar et al. [79] , who placed second place on both categories, also employed UDA and pseudo-labeling of the HIL domain images.\n\nOn the other hand, SPNv2 [ 18], one of the baseline models developed by the authors, designed a multi-scale, multi-task learning CNN architecture and trained with extensive data augmentations on the SPEED+ synthetic images only. The largest variant of SPNv2 would have ranked 3rd place in lightbox and 6th in sunlamp categories, respectively, without accessing the HIL domain images during training at all. Finally, EagerNet [ 80] performs dense predictions of pixel-wise object coordinates as opposed to heatmap predictions that many other methods adopted. They additionally predict the errors of predicted model coordinates which results in multiple pose hypotheses which are further refined using a probabilistic refinement. EagerNet trained with extensive data augmentation, including those specifically designed to target the HIL domain imagery, achieves state-of-the-art robustness without accessing the HIL domains, winning both HIL categories in the post-mortem competition.\n\nAs shown later, SPNv3 is able to achieve comparable robustness relative to EagerNet without access to the HIL domains as well. However, SPNv3 is a much simpler architecture which outputs heatmaps about known keypoints on the target surfaces, which can be provided directly to the onboard navigation filter [16 ] or be used to solve for the optimal pose solution via P\ud835\udc5bP [81].# III. Requirements of Spaceborne ML Models\n\nBefore introducing SPNv3, this section provides a brief overview of various requirements that must be met by not only SPNv3 but also ML models that are intended to run in space onboard satellite avionics. Some of these requirements, in addition to the OOD robustness across the sim2real gap, drive the design choices of SPNv3 in the next section.# A. Computationally Efficiency\n\nThe first requirement of a spaceborne ML model operating on satellite avionics is its computational efficiency. This is an obvious requirement for real-time operation of NNs in space whose runtime should ideally be within the update frequency of its overarching GN&C system. For example, Roscoe et al. [82] reports that the CubeSat Proximity Operation Demonstration (CPOD) mission launched in 2022 ran its RPO GN&C system at 0.5 Hz frequency with its star sensor and IMU data processed at up to 10 Hz for its Attitude Determination and Control System (ADCS) as part of an Extended Kalman Filter (EKF). However, the image processing algorithms often exceeded the onboard filter measurement update cycles, requiring a robust filter design that can handle out-of-sync and latent measurements. Taking the CPOD mission as a reference, this means the measurement processing of a NN, which includes not only its forward pass but also pre- and post-processing of its inputs and outputs, must be completed well within two seconds on a representative computing hardware with limited processing capabilities. Achieving this could be facilitated by the presence of an onboard GPU that can perform the NN inference and any other parallelizable operations, allowing the inference to be executed asynchronously from other GN&C operations running on Central Processing Units (CPU).\n\nAs explained in Section I, it may be desirable to perform online training of NNs in space to fully close the domain gap by directly incorporating the flight images that only become available during RPOD [19]. In this case, the NN must also be computationally efficient to run not only forward but also backward gradient propagation. Unlike inference, the training most certainly requires a GPU for the operation. However, the bright side is that the training need not run for every acquired image but only when there has been a substantial \u201cchange\u201d in the imagery compared to the previous training round. The motivation is to avoid overfitting the NN to the specific scenery (e.g., Earth background) and view of the target since they do not change so abruptly within the short time frame in space especially at higher altitudes. Therefore, as long as there is a GPU onboard, the training latency is likely to be less of a computational bottleneck than inference which should be running in real time.# 1. Remark on Flight Heritage GPU\n\nEven nowadays, GPUs are still rarely found only in short-term missions which typically are not concerned with long-term radiation effects such as Total Ionising Dose (TID), or as part of a larger spacecraft which provides better protection of the onboard computing systems. Recent state-of-the-art report on small spacecraft technology by NASA has tabulated a number of GPU-based avionics [83, Table 8.1, \u00a78.3], but many Commercially Off-The-Shelf (COTS) systems such as NVIDIA Jetson series are still not flight-proven or are radiation tolerant only against Single Event Effects (SEE) (e.g., bit flips).\n\nHowever, there is an increasing interest in deploying GPUs into deep space or long-term missions in LEO with existing flight heritage [25]. For example, Aitech Systems, Inc., a company that manufactures rugged computers for military and aerospace applications, utilizes the NVIDIA Jetson TX2i System-on-Module (SoM) in their S-A1760 system [24] which is designed for spacecraft and small satellites and has a flight heritage in LEO as part of a larger payload system [84]. The startup company Aethero is also developing radiation-hardened edge computers for on-orbit data processing and autonomous decision-making in space. Its ECM-NxN utilizes an NVIDIA Jetson Orin processor, considered the best GPU edge processor available. While the ECM-NxN module has not directly flown in space, Aethero\u2019s use of this processor in their space computer indicates potential applications in space exploration. Finally, the Ingenuity Mars Helicopter used a GPU-equipped Qualcomm\u00ae Snapdragon\u2122 801 processor with great results, even though it operated in a \u201cterrestrial\u201d environment [85]. These are a few examples of global efforts to deploy edge GPU systems to space, and there is no doubt that more and more powerful space-grade GPUs will become commercially available to endure the harsh space environment for a prolonged period of time.# B. Number of Parameters\n\nThe number of parameters is closely related to the computational efficiency since more weights translate to more operations to be performed for both training and inference. However, the NN size is also tightly connected to the power consumption. For instance, the seminal work of Han et al. [86] in 2015 identified that, under the 45nm Complementary Metal-Oxide Semiconductor (CMOS) technology, 32-bit memory access to a Dynamic Random Access Memory (DRAM) consumes three orders of magnitude more power than accessing a Static Random Access Memory (SRAM) cache or performing a 32-bit floating point arithmetic. Considering miniature satellite systems such as CubeSats with limited power generation capability, it would be favorable to minimize the NN size so that the entire network can be.# Current Page Content\n\nhosted on a local SRAM which is limited in memory size (typically up to 20MB for radiation hardened SRAM [83, Table 8.2, \u00a78.3.4]).\n\nThe reduction in NN memory footprint can be achieved not just by minimizing the number of learnable parameters but also by quantizing the weights to lower-precision values such as IEEE half-precision floating point (FP16) [87] and 8-bit integer (INT8) [88]. More recent innovations in quantized training and inference include brain floating point (bfloat16) [89] for half-precision, 8-bit floating point (FP8) [90] and 4-bit floating point (FP4) [91], and even binary CNN whose weights and activations are constrained to {\u22121, +1} [92]. Unfortunately, working with quantized weights often requires care to prevent over/underflowing of floating point values during gradient backpropagation and to minimize the loss of accuracy due to reduced bit resolution of individual weights. Moreover, many quantized floating point formats require specific hardware support to realize actual computational gain.\n\nWhile power consumption of operating NNs on satellite avionics is an important engineering topic, this work does not immerse into the subject as realizing NN quantization is often an issue of software and hardware support on a specific system. Instead, it focuses on more general approaches to minimizing the total number of parameters while realizing the best OOD robustness on the SPEED+ dataset.# C. Batch-Agnostic Architecture\n\nIf any online training is to be done, it is desirable to have a batch-agnostic NN architecture, i.e., it does not contain any Batch Normalization (BN) layers [70]. The core of normalization (NORM) is to prevent internal covariate shifts of the feature maps and stabilize the training process. This is done by normalizing each feature map with a mean and a variance, for an \ud835\udc56-th layer feature map,\n\n\ud835\udc7f\u02c6\ud835\udc56 = NORM( \ud835\udc7f\ud835\udc56 ) \u225c \u221a\ufe01\ud835\udc7f\ud835\udc56 Var[ \ud835\udc7f]\u2212 E[ \ud835\udc7f]\ud835\udc56 (1)\n\nwhere \ud835\udc7f\ud835\udc56 is the feature map at the \ud835\udc56-th layer. Unlike other methods, BN aims to compute the batch-wise feature map statistics across the entire training set. Since this is infeasible, in practice, the NORM layer statistics are computed across mini-batches, and the \u201cglobal\u201d statistics are updated using a running average as training progresses. Then, during inference, these running averages of the mean and variance are fixed and used for normalization.\n\nThe problem is that these approximations of the training data distribution in BN layers are simply incompatible with those of the test data that are drawn from a different distribution. Therefore, in this work, two methods are considered in search of a robust and batch-agnostic pose estimation NN architecture. One is to simply replace the BN layers in existing pre-trained architectures with batch-agnostic NORM layers such as Layer Normalization (LN) [93] and Group Normalization (GN) [94]. The other is to design or adopt a NN architecture that are inherently without BN layers such as ViT [22].# IV. Searching for Next Spacecraft Pose Network (SPNv3)\n\nThe goal of this section is two-fold. One is to introduce the overall pose estimation architecture given an image of the target at a visible distance in an RPOD scenario (Section IV.A). Then, different NN architectures (Section IV.B) and data augmentation techniques (Section IV.C) for training a pose estimation NN are explained which are combined in various configurations to study and explore which aspects of NN architecture and training algorithm contribute the most to the OOD robustness and reduced latency of a pose estimation NN (Section V). While SPN is the name of the first spacecraft pose estimation CNN introduced by Sharma and D\u2019Amico [6], it is also used throughout this work as a legacy name referring to all NN architectures designed or adopted for spacecraft pose estimation by SLAB. When referring to the first original SPN, it is always accompanied by the reference to the relevant work [6].# A. Operational Scenario\n\nIn all vision-based RPOD scenarios, the servicer spacecraft begins tracking the non-cooperative target at kilometers of separations as shown in Fig. 3. For example, Angles-Only Navigation (AON) [95\u201398] obtains bearing angle measurements of the target from a Narrow Field-Of-View (NFOV) camera such as a star tracker which allows tracking of the target\u2019s relative orbital state via nonlinear filtering. AON continues until the inter-spacecraft distance becomes small enough such that the target appears resolved in the camera view, at which point the pose estimation algorithm kicks in.\n\nGiven the scenario, this work adopts the two-stage approach visualized in Fig. 3 as the common mechanism of the ML-based pose estimation algorithm which was first independently proposed by Park et al. [32] and Chen et al. [33] for# Non-SrcllatObject\n\nFig. 3 Visualization of far-range AON [95] (left); close-range pose estimation on a cropped image (middle); and mechanism of feature/heatmap extraction by a NN (right). In general, the sensors are selected to ensure overlapping working distance to the target but it is not always possible based on the orbit and size of the target.# SPEC2019 [31]\n\n1. The first stage detects the target within the image frame using the state estimates from AON or a separate object detection NN. The latter may become especially useful in the case of the \u201clost-in-space\u201d scenarios in which the navigation filter must reset all its state estimates. The detected bounding box or Region-of-Interest (RoI) from these sources is used to crop the image around the target spacecraft, and the cropped image is resized to lower-resolution inputs expected by the second-stage pose estimation NN (e.g., 224 \u00d7 224).\n2. The second stage is the main pose estimation NN which takes the cropped low-resolution images and outputs \ud835\udc3e heatmaps centered at the 2D locations of known target surface keypoints. Knowing the 3D coordinates of these keypoints, the 2D keypoint locations extracted from the heatmap peaks can be used to solve P\ud835\udc5bP to compute the full 6D pose solution. Moreover, the advantage of dense predictions such as heatmaps is that their spread about the peak can be interpreted as uncertainties associated with each prediction [38]. These uncertainties can be leveraged to better perform state estimation via navigation filter [16] or even incorporated into the uncertainty-aware P\ud835\udc5bP algorithm [38, 99].\n\nThe heatmap-based pose estimation NN architecture is the most prominent approach, having won the first places in both SPEC2019 [31] and SPEC2021 [17]. Therefore, this work adopts the same heatmap detection architectures for NN. The remainder of this section explores various architectural designs and training algorithms to make the NN as robust as possible to OOD data.# B. Pose Estimation Architectures\n\nThis section explores three different heatmap detection architectures\u2014EfficientDet [100], HRNet [101] and ViTPose [102]\u2014visualized in Fig. 4. Note that this is not a comprehensive list of state-of-the-art pose estimation NN architectures. Nonetheless, they are chosen due to their simplicity and in order to keep the number of training sessions tractable while exploring vastly different architectural options.# 1. EfficientDet\n\nEfficientDet was originally proposed by Tan et al. [100] to enable computationally efficient yet high-performance object detection and semantic segmentation. EfficientDet builds upon the EfficientNet [20] backbone designed for the image classification task. At the heart of EfficientDet is the Bidirectional Feature Pyramid Network (BiFPN), a novel FPN architecture [103] which fuses and processes feature outputs from the backbone at multiple scales. By fusing features from low- to high-resolution and vice versa, BiFPN outperforms previous state-of-the-art FPN architectures on various benchmark object detection and semantic segmentation datasets. In the end, the feature outputs from BiFPN are processed by the task-specific prediction heads composed of convolution layers. For tasks that require multi-scale predictions (e.g., object detection), features from all resolutions are processed. In this work, the heatmap prediction head is attached only to the P2 level feature as shown in Fig. 4, where P\ud835\udc5b indicates that the feature size is 2\u2212\ud835\udc5b of the input image resolution. Only the P2 features (i.e., heatmap has 1/4 of the input resolution) are used since the accuracy of heatmap prediction heavily depends on the heatmap pixel resolution.# 2. HRNet\n\nHigh Resolution Network (HRNet) [101] is also designed for performing tasks that require multi-scale predictions. Recall that EfficientDet builds upon a CNN backbone that is typically designed for the image classification task, resulting in an architecture which continuously reduces the feature resolution all the way down to 1/32 of the original input. On the other hand, HRNet is designed to maintain high-resolution features throughout the entire forward propagation, outputting the multi-resolution features by the nature of its design. To facilitate learning at different scales, HRNet first processes the input image to 1/4 resolution, then gradually adds lower-resolution features in parallel, continuously fusing information across different scales as visualized in Fig. 4.\n\nHRNet has won SPEC2019 [31, 33] and the lightbox category of SPEC2021 [17], so it is considered as a candidate benchmark in this paper. Similar to EfficientDet, only the highest resolution P2 level features are processed for heatmap outputs.# 3. ViTPose\n\nCompared to EfficientDet and HRNet which are CNN architectures based on convolution operations, ViTPose [102] leverages the ViT backbone [22] which divides the input image into \ud835\udc43 \u00d7 \ud835\udc43 patches then processes them in parallel through transformer blocks. ViTPose groups the output token embeddings according to their original spatial position in the input image. The resulting features have the resolution of \ud835\udc3b/\ud835\udc43 \u00d7 \ud835\udc4a/\ud835\udc43, where (\ud835\udc3b, \ud835\udc4a) is the input image size. For example, the output features will have 1/16 resolution of the original image for \ud835\udc43 = 16. ViTPose processes them through 2 transposed convolution layers in the prediction head which upsamples the input feature to 1/4 resolution heatmap outputs, same as EfficientDet and HRNet.# C. Extensive Data Augmentation\n\nData augmentation is vital to training NNs that generalize well beyond their training data distribution and to preventing overfitting. As basic sets of augmentation, SPN heavily leverages the Albumentations library [104] for various image processing operations. For each training sample, \ud835\udc41 augmentation operations are randomly chosen from the set of designated operations in a manner similar to RandAugment [105]. Specifically, while RandAugment utilizes a single hyperparameter to control the constant level of magnitude of these operations (e.g., standard deviation of white noise).# List of data augmentations and equivalent commands in Albumentations v1.3.1 [104].\n\n|Augmentations|Commands|\n|---|---|\n|Brightness & Contrast|RandomBrightnessContrast|\n|Blur|OneOf(GaussianBlur, MedianBlur, Defocus)|\n|Noise|OneOf(GaussNoise, ISONoise, MultiplicativeNoise)|\n|Bit Reduction|Posterize|\n|Sharpen|Sharpen|\n|Pixel Inversion|Solarize|\n|Spatter|Spatter|\n|Histogram Equalization|Equalize|\n|Gamma Correction|RandomGamma|\n|Solar Flare\u2020|RandomSunFlare|\n\n\u2020Modified so that the solar flare only appears within the ground-truth bounding box.# Fig. 5\n\nImage augmentations. From top to bottom: SPEED+ synthetic images; StyleAugment [62]; DeepAugment [21]; RandConv [106].\n\nGaussian noise), this work simply picks each operation with a random magnitude within some pre-defined window. Then, these \ud835\udc41 operations are applied to the original image in sequence. The complete list of employed augmentations is provided in Table 1 along with their equivalent Albumentations commands. All commands are run with the default range of magnitudes.\n\nIn addition to the RandAugment-like augmentations which are composed of simple image processing operations such as Gaussian noise and contrast adjustment, three additional operations are explored that were designed specifically to address the domain gap issue.# 1. Style Augmentation\n\nGeirhos et al. [63] has empirically shown in 2019 that many state-of-the-art CNN architectures, such as ResNet [107] and VGG [108], exhibit strong bias toward object\u2019s texture when trained on ImageNet [34]. This finding is very counter-intuitive for humans who would, for example, classify an object or an animal based on its global or a set of local shapes, not based on its texture. To demonstrate such behavior, the authors have created Stylized-ImageNet (SIN) dataset by applying the AdaIN Neural Style Transfer (NST) pipeline [109] to each image in ImageNet with random styles from Kaggle\u2019s Painter by Numbers dataset\u00b6. The result shows that when the same CNN architectures are trained on the SIN dataset, the networks not only exceed the performance of those trained on ImageNet, but they also show human-level robustness to previously unseen image distortions, such as noise, contrast change, and high- or low-pass.\n\n\u00b6https://www.kaggle.com/c/painter-by-numbers/# filtering.\n\nFollowing the same logic, Jackson et al. [62] devised style augmentation using the style transfer pipeline of Ghiasi et al. [110]. Specifically, in order to facilitate the style randomization process without resorting to individual \u201cstyle images,\u201d they embedded different style images into the vector space using the intermediate feature output of an Inception CNN [111]. Then, during training, random style vectors are sampled and passed to the style transfer network to \u201cstylize\u201d the input image. This work adopts the style augmentation pipeline, using the same style embedding statistics as a source of randomization due to its ease of sampling different styles from the vector space embedding.# 2. DeepAugment\n\nDeepAugment [21] is another NN-based data augmentation technique that is done by perturbing the internal representations of a NN. These perturbations are randomly sampled from a set of hand-designed operations which include zeroing, negating, convolving, flipping, etc., of intermediate features and weights of a NN (e.g., an autoencoder) at random layers. For simplicity, this work follows the procedure identical to the one described in Hendrycks et al. [21] and uses the CAE architecture [112] to produce perturbed images.# 3. RandConv\n\nFinally, RandConv [106] applies a convolution operation with a \ud835\udc58 \u00d7 \ud835\udc58 kernel composed of random weights. The motivation is similar to that of style augmentation \u2013 to distort the local texture of an input image. Following the original description, this work applies a convolution at the beginning with kernel size randomly sampled from \ud835\udc58 = {1, 3, 5, 7}, and the perturbed image is blended with the original image with random weights.# V. Trade-Off Analyses: OOD Robustness and Latency\n\nThis section performs the trade-off analyses between robustness across domain gap and training/inference latency for various configurations of pose estimation architectures (Section IV.B), data augmentation techniques (Section IV.C), and other aspects of NN training such as transfer learning and input resolution. The ultimate goal is to find the configuration that is both robust on SPEED+ HIL domain images and computationally efficient on a representative hardware, capable of operating above nominal GN&C update frequency (Section III.A) while also being batch-agnostic for potential online training (Section III.C).# A. Datasets\n\nFollowing SPEC2021 [17], this section performs experiments using the SPEED+ dataset [15, 113] which consists of 59,960 synthetic images split in 80:20 ratio into the training and validation sets. It further includes two HIL image domains acquired from the TRON facility and a mockup model of the Tango spacecraft: 6,740 lightbox and 2,791 sunlamp images which are reserved solely for testing. In this section, all NN models are trained on the synthetic training set and evaluated on the synthetic validation set, lightbox and sunlamp test sets, and 25 labeled flight images from acquired in-space during RPO of the PRISMA mission (prisma25). Recall Fig. 1 for visualization of a few HIL domain images.# B. Implementations\n\nIn the original EfficientDet implementation [100], the input image resolution and the size of the BiFPN and prediction layers differ depending on the EfficientNet scaling parameter \ud835\udf19. In this section, for a fair comparison, the BiFPN layers are fixed to 4 convolution layers with 128 channels. The prediction heads for all pose estimation architectures consist of two convolution layers with 128 channels as well. Note that for most EfficientDet and HRNet implementations, the convolution operations in the prediction heads are depthwise separable convolutions [114] except ViTPose architectures that adopt full transposed 2D convolution operations with \u00d72 upsampling and 128 channels. The NORM and activation layers for BiFPN and prediction heads all follow those used in the respective backbone NN. If the backbone consists of multiple types of NORM layers including BN, the following BiFPN and prediction heads adopt only BN. The 2D keypoint locations are extracted from each heatmap as the location with maximum pixel intensity then refined via Unbiased Data Processing (UDP) [115]. The keypoint coordinates are used to solve for the 6D pose solution via EP\ud835\udc5bP [81].\n\nGiven the ongoing interest in Artificial Intelligence (AI) for space applications, this work assumes that hardware# Support for Edge Computing\n\nSupport for edge computing to run NN onboard satellites will increase in the upcoming years (see Section III.A.1). Therefore, this work adopts an NVIDIA Jetson Nano 4GB which contains a Quad-Core Arm\u00ae Cortex\u00ae A57 CPU and a 128-core NVIDIA Maxwell\u2122 architecture GPU [23]. Nano is the most restrictive version of the commercially available Jetson family, which suits the limited processing power of onboard GPUs as evidenced in the NASA small spacecraft technology report [83]. Given its deprecated GPU architecture and CUDA language support, all NN training and inference on Jetson Nano are performed with full 32-bit Floating Point (FP32) precision using the C++ API of PyTorch 1.10.0.# 1. Default Configuration\n\nThe default training configuration consists of the heatmap prediction head from the P2 level feature output and all data augmentations including style augmentation, DeepAugment and RandConv. The ground-truth heatmaps are Gaussian blobs with the standard deviation \ud835\udf0e = 1 pixel around the 2D keypoint locations. The NN is trained with the pixel-wise Mean Squared Error (MSE) loss defined for the predicted heatmap \u02c6H and the ground-truth H as:\n\nLMSE ( \u02c6H, H) =\n1   \u2211\ufe01\ud835\udc3e  \u2225 H(\ud835\udc56) \u2212 H (\ud835\udc56) \u2225\ud835\udc39\n\ud835\udc3e                         \u02c6                     2\n\ud835\udc56=1\n\nwhere \ud835\udc3e is the number of total heatmaps and \u2225 \u00b7 \u2225\ud835\udc39 is a Frobenius norm of the heatmap matrix.\n\nAll training sessions excluding the experiments on the Jetson Nano are run on an NVIDIA RTX 4090 24GB GPU with bfloat16 precision on PyTorch 2.1. All models are trained with the AdamW optimizer [116] for 30 epochs with the batch size 32, weight decay 1.0 \u00d7 10\u22124 and the initial learning rate 0.001 which linearly warms up during the first epoch then decays according to the cosine annealing schedule [117]. Note that the proposed training algorithm is not optimal for all different models considered in this study; nevertheless, adopting a common training method allows for a fair comparison of different metrics.\n\nThe default input image resolution after cropping is 224 \u00d7 224, identical to the ImageNet classification scenario. The input RoI is cropped using the ground-truth bounding box around the target following the procedure described in Park et al. [32]. Specifically, the ground-truth RoI is first corrected to a square-sized region with a size of max(\u210e, \ud835\udc64), where (\u210e, \ud835\udc64) are the height and width of the original RoI. This correction is implemented to ensure the aspect ratio of the target remains the same when resizing the cropped region. Then, during training, the new square RoI is enlarged and shifted by random factors in order to make the NN robust to object translation and misaligned RoI detection. During testing, the detected RoI is similarly converted into a square-sized region and enlarged by a fixed factor of 20% in order to ensure that the cropped region contains the entirety of the spacecraft.\n\nFinally, all backbone NNs are obtained from the timm library available at HuggingFace [118], and they are all pre-trained on ImageNet-1K (IN-1K) [34] alone via supervised training unless noted otherwise. For simplicity, the architecture and backbone composition of a model is noted Architecture (Backbone) throughout this section.# 2. Metrics\n\nThe performance metrics consist of the mean translation error (\ud835\udc38), mean orientation error (\ud835\udc38q) and the mean pose error (\ud835\udc38pose) defined below for individual samples:\n\n\ud835\udc52t = \u2225 \ud835\udc95\u02dc \u2212 \ud835\udc95\u22252 (3a)\n\n\ud835\udc52\u00aft = \ud835\udc52/\u2225\ud835\udc95\u2225t (3b)\n\n\ud835\udc52q = 2 arccos | < \u02dc, \ud835\udc92 > |\ud835\udc92 (3c)\n\n\ud835\udc52pose = \ud835\udc52q + \u00aft\ud835\udc52 (3d)\n\nAlso note that for the SPEED+ HIL domains, the rotation and translation errors account for the calibration accuracy of the TRON facility. Specifically, for individual samples, the HIL errors are defined as [17]:\n\n\ud835\udc52\u2217( \ud835\udc95, \ud835\udc95) =\n(0               if \u00af( \ud835\udc95\u02dc, \ud835\udc95) < \ud835\udf03t\ud835\udc52t\n\ud835\udc52\u2217( \u02dc, \ud835\udc92) =     (0                 if \ud835\udc52q ( \u02dc, \ud835\udc92) < \ud835\udf03q\ud835\udc92\n\u00aft                 \ud835\udc52t \u00af( \ud835\udc95\u02dc, \ud835\udc95)otherwise                ,      q  \ud835\udc92              \ud835\udc52q ( \u02dc, \ud835\udc92)\ud835\udc92  otherwise                   (4)\n\nThe rotation threshold (\ud835\udf03q) and the normalized translation threshold (\ud835\udf03t) are derived from the calibration accuracy of TRON and are set to \ud835\udf03q = 0.169\u00b0 and \ud835\udf03t = 2.173 \u00d7 10\u22123 [14]. Then, the HIL pose error for a sample is given as:\n\n\ud835\udc52pose = \ud835\udc52q \u2217\u2217( \u02dc, \ud835\udc92) + \u00aft \ud835\udc92\ud835\udc52\u2217( \ud835\udc95, \ud835\udc95)\u02dc (5)# Ablation study on different pose estimation architectures and backbones. Latency and peak memory during training are measured on an NVIDIA RTX 4090 24GB GPU with batch size 1 and FP32 precision. Mean(std. dev.) across all samples for each domain are reported. The boldface denotes the best performance within each group.\n\n|Backbone|Norm.|Num. Param.|Mem. [MB]|Time [ms]|\ud835\udc38q / \ud835\udc38q\u2217 [\u00b0]|\n|---|---|---|---|---|---|\n|MobileNetV3-L (\u00d70.75) [119]|BN|2.3M|149|21.3|6.1|\n|HRFormer-T [120]|BN/LN|2.6M|264|58.5|20.4|\n|HRNet-W18-S [101]|BN|2.9M|65|13.2|4.8|\n|MobileNetV3-L (\u00d71.0) [119]|BN|3.5M|168|20.4|6.1|\n|EfficientFormerV2-S0 [121]|BN/LN|3.7M|197|25.8|7.9|\n|EfficientNet-B0 [20]|BN|4.1M|212|23.1|6.6|\n|MobileViTv2-100 [122]|BN/LN|5.0M|262|21.8|6.9|\n|EfficientNetV2-B0 [123]|BN|6.1M|208|25.9|7.3|\n|ViT-T/16 [124]|LN|6.2M|129|13.2|3.8|\n|EfficientFormerV2-S1 [121]|BN/LN|6.2M|252|31.5|8.7|\n|EfficientNet-B1 [20]|BN|6.6M|274|28.4|7.7|\n|EfficientNetV2-B1 [123]|BN|7.1M|236|29.2|7.7|\n|MobileViTv2-125 [122]|BN/LN|7.5M|324|21.4|6.8|\n|EfficientNet-B2 [20]|BN|7.7M|292|27.6|7.9|\n|HRFormer-S [120]|BN/LN|7.9M|500|69.2|23.7|\n|EfficientNetV2-B2 [123]|BN|8.9M|263|30.3|8.1|\n|MobileViTv2-150 [122]|BN/LN|10.5M|393|21.9|6.8|\n|EfficientNet-B3 [20]|BN|10.6M|369|27.9|8.5|\n|HRNet-W18 [101]|BN|11.0M|238|42.4|13.7|\n|EfficientFormerV2-S2 [121]|BN/LN|12.6M|374|34.3|11.4|\n|EfficientNetV2-B3 [123]|BN|12.9M|335|29.1|8.9|\n|EfficientNet-B4 [20]|BN|17.2M|499|31.4|9.9|\n|EfficientNetV2-S [123]|BN|20.3M|468|32.9|10.3|\n|NFNet-L0 [125]|-|22.3M|497|24.3|8.3|\n|ViT-S/16 [124]|LN|22.7M|377|11.8|3.3|\n|HRNet-W30 [101]|BN|27.4M|493|44.8|13.6|\n|EfficientNet-B5 [20]|BN|27.8M|707|39.3|11.1|\n|ResMLP-S24 [126]|Affine|30.7M|604|16.4|4.2|\n|NFNet-L1 [125]|-|37.6M|799|33.5|12.7|\n|ViT-M/16 [124]|LN|39.6M|647|11.5|3.4|\n|EfficientNet-B6 [20]|BN|39.9M|944|39.9|11.9|\n\nNote that in the overall pose estimation scenario (Section IV.A), the information on the relative position of the target is mostly included in the RoI detected from either the navigation filter or a separate object detection NN. Since the ground-truth RoIs are used for image pre-processing in this section, mean translation errors are expected to be very small throughout the experiments. Therefore, only the mean orientation errors (\ud835\udc38q / \ud835\udc38q \u2217) are reported for the majority of this section for brevity.# 1. Architecture & Backbone\n\nThis section begins with the study on OOD robustness and latency of various pose estimation architectures and NN backbones. First, Table 2 tabulates the latency and pose accuracy of different pose estimation architectures with various.# Fig. 6\n\nVisualization of performance of different backbones tabulated in Table 2. Rotation error (\ud835\udc38q\u2217) is for the sunlamp domain.# Backbone NNs of Different Sizes\n\nThese backbones cover a wide variety of NN architectures, including conventional CNNs with BN layers (e.g., EfficientNet [20], EfficientNetV2 [123], MobileNetV3 [119], HRNet [101]); NFNet [125] which has no separate NORM layers but performs weight standardization of the convolution weights [127]; Vision Transformers (ViT) [22] with self-attention modules [128]; architectures purely based on Multi-Layer Perceptrons (MLP) (e.g., ResMLP [126]); and hybrid architectures which both include convolution operations and self-attention layers for image patches as hinted by the use of both BN/LN layers (e.g., HRFormer [120], EfficientFormerV2 [121], MobileViTv2 [122]).\n\nIn order to facilitate readability, Table 2 sorts different architectures according to their total number of parameters in ascending order and grouped into similar sizes. Overall, larger NNs tend to result in better OOD robustness as measured by the performance on the SPEED+ lightbox and sunlamp domains. Then, by investigating the models within each size group, it becomes immediately clear that the EfficientDet (EfficientNet) and EfficientDet (EfficientNetV2) consistently outperform all other models across different size groups in terms of robustness. They are closely followed by EfficientDet (NFNet) and ViTPose (ViT).\n\nFigure 6 condenses the information in Table 2 by plotting the mean orientation error on the sunlamp domain and the inference latency against the number of parameters. Different families of backbones are plotted with different colors and marker shapes. First, the \ud835\udc38q \u2217plot in Fig. 6 (left) shows that EfficientDet (EfficientNet) and EfficientDet (EfficientNetV2) consistently outperform all other models in terms of OOD robustness. While EfficientDet (NFNet) models are closely behind them, there is no smaller variant of NFNet pre-trained on IN-1K that is publicly available. On the other hand, the latency plot in Fig. 6 (right) shows that ViTPose (ViT) and ViTPose (ResMLP) are clear winners. Both architectures consist solely of MLP layers, enabling a faster throughput compared to those with convolution operations. Unfortunately, ResMLP cannot perform well across domain gaps, which is no surprise since it is a pure MLP-based NN with no inductive bias of convolution operations or global spatial knowledge via self-attention layers. However, ViTPose (ViT) models closely follow EfficientDet (EfficientNet) in terms of OOD robustness.\n\nBased on the OOD robustness and latency trade-off in Fig. 6, only EfficientDet (EfficientNet), EfficientDet (EfficientNetV2) and ViTPose (ViT) are considered for further analyses.# Table 3\n\nPeak training memory and latency of select models from Table 2 are measured on an onboard GPU of NVIDIA Jetson Nano 4GB with PyTorch C++ API for batch size 1 and FP32 precision. The boldface denotes the best performance within each group.\n\n|Backbone|Norm. Param.Num.|Mem. [MB]|Latency [ms]|Train|Test|lightbox|sunlamp|\n|---|---|---|---|---|---|---|---|\n|EfficientNet-B0|BN|4.1M|212|446|57|7.75(25.41)|14.70(35.11)|\n|EfficientNet-B1|BN|6.6M|274|595|72|6.07(21.58)|9.34(26.40)|\n|EfficientNet-B2|BN|7.7M|292|622|71|5.21(19.44)|8.05(23.99)|\n|EfficientNet-B3|BN|10.6M|369|758|75|4.67(18.19)|7.08(21.24)|\n|EfficientNet-B4|BN|17.2M|499|985|87|5.67(20.27)|7.79(23.76)|\n|EfficientNet-B5|BN|27.8M|707|1317|98|3.79(15.59)|5.26(16.63)|\n|EfficientNet-B6|BN|39.9M|944|1660|110|3.65(15.14)|5.19(18.03)|\n|EfficientNetV2-B0|BN|6.1M|208|473|65|7.25(24.30)|12.55(32.43)|\n|EfficientNetV2-B1|BN|7.1M|236|553|72|5.87(21.20)|9.44(27.60)|\n|EfficientNetV2-B2|BN|8.9M|263|598|74|5.63(20.59)|8.48(25.36)|\n|EfficientNetV2-B3|BN|12.9M|335|746|82|5.11(19.08)|7.41(23.00)|\n|EfficientNetV2-S|BN|20.3M|468|996|96|4.24(16.63)|6.10(18.91)|\n|ViT-T/16|LN|6.2M|129|221|25|7.88(23.48)|14.49(32.25)|\n|ViT-S/16|LN|22.7M|377|349|22|5.12(17.75)|9.29(24.94)|\n|ViT-M/16|LN|39.6M|647|546|23|4.32(15.77)|8.11(22.57)|# 2. Latency on NVIDIA Jetson Nano\n\nThe models chosen from the previous section are implemented on an NVIDIA Jetson Nano 4GB and tested for training and inference latency on its GPU. Specifically, all training and inference are performed with full FP32 precision for single images. The FP32 training and inference are due to the lack of support for FP16 or INT8 quantization on the Nano\u2019s GPU, whereas a single image training is due to the fact that it is undesirable for a satellite to wait and collect a batch of images of the target spacecraft during in-space RPOD, both logistically and computationally.\n\nThe latency measured on the Jetson Nano reported in Table 3 reaffirms the observation made in Table 2 and Fig. 6. For the same number of parameters, EfficientDet models dominate on the OOD robustness front compared to the ViTPose models. However, ViTPose (ViT) models are consistently faster for inference regardless of the size. In fact, the largest ViT backbone considered in this study\u2014ViT-M/16 with nearly 40M parameters\u2014runs nearly 2.5 times faster than the smallest variant of the EfficientNet backbone that is 1/10 in size, and its training latency is on par with that of EfficientNet-B1 which only has 6.6M parameters. It also requires at most about 650 MB of GPU virtual memory for each training session with a single image input. Considering the 4GB of GPU memory available on the Jetson Nano and assuming that most of the GN&C-related operations run on CPUs, 650 MB or less is completely within the range of operation excluding the possibility of running other image-processing algorithms on the GPU.\n\nRecall from the earlier discussion that the training is expected to run only occasionally whenever there has been a substantial change in the scenery and the target\u2019s pose (Section III.A). Therefore, even the EfficientNet-B6 backbone could perform onboard online training asynchronously on a GPU given that there is enough power generated to support such a computationally heavy operation. In terms of the inference speed, all models considered in Table 3 can effectively run in real-time for 0.5 Hz GN&C updates of the CPOD reference mission [82] assuming that the pre- and post-processing of the NN inputs and outputs do not become the computational bottleneck.# 3. Replacing BN Layers\n\nJudging from the comparison on the NVIDIA Jetson Nano reported in Table 3, it appears that EfficientDet models are more OOD robust for the same number of parameters, but the larger ViTPose models are still more computationally efficient than small EfficientDet models, overcoming its comparative weakness in OOD robustness with increased size. There is one more reason that ViTPose is favored over EfficientDet, and that is due to the requirement that the onboard NNs must be batch-agnostic for online training (Section III.C). As evidenced in Table 2, most CNN backbones designed for ImageNet classification adopt BN layers, and even hybrid architectures always pair BN layers to each convolution.# Table 4\n\nEfficientDet architecture with the EfficientNet-B0 backbone evaluated with different NORM layers. The architecture\u2019s and the backbone\u2019s NORM layers during IN-1K pretraining are both noted. Mean(std. dev.) of average performances over 3 training sessions with different random seeds are reported. Latency is measured on an NVIDIA Jetson Nano 4G. The boldface denotes the best performance within each group.\n\n|NORM (Pre-train)|NORM (Current)|Training Epochs|Time [ms]|Train|Test|synthetic|lightbox|sunlamp|prisma25|\n|---|---|---|---|---|---|---|---|---|---|\n|BN|BN|30|446|57|0.81(0.00)|7.34(0.27)|13.90(0.37)|7.54(3.96)| |\n|BN|GN|30|471|68|1.10(0.01)|12.44(0.39)|23.18(2.01)|12.24(1.32)| |\n|BN|GN|60|471|68|0.88(0.02)|9.76(0.34)|18.16(1.28)|11.47(4.33)| |\n|GN|GN|30|471|68|0.84(0.00)|6.67(0.49)|12.20(0.78)|3.99(2.50)| |\n\nOperation. Then, is it possible to replace those BN layers with other batch-agnostic layers (e.g., LN, GN) when the overwhelming majority of publicly available CNN backbones pre-trained on ImageNet come with BN layers? Table 4 reports the orientation error of the EfficientDet (EfficientNet-B0) model (4.1M parameters) with different NORM layers during the IN-1K pre-training and the main training on SPEED+ synthetic images. The baseline is using BN layers for both sessions, which reports \ud835\udc38q = 6.63\u25e6 on lightbox and \ud835\udc38q = 10.77\u25e6 on sunlamp across 3 training sessions with different random seeds. When the BN layers of a pre-trained backbone is replaced with GN layers with group size 8, fully inheriting the trained affine parameters of the BN layers, the mean orientation error degrades by more than a factor of two. Training the network longer for 60 epochs instead of 30 does improve the overall performance, but it still does not reach the baseline level for all image domains. This was the conclusion of SPNv2 which also reported degradation in performance after replacing the BN with GN layers [18].\n\nHowever, when the backbone is pre-trained on IN-1K with GN layers from the very beginning, the OOD performance shows consistent improvement on not only lightbox and sunlamp but also prisma25. The result indicates that it is possible to replace the BN layers in existing CNN architectures with GN or other batch-agnostic layers, but in order to preserve the OOD robustness, the backbone must be pre-trained on IN-1K or other large-scale datasets with those batch-agnostic layers from the very beginning. The problem is that ImageNet training is extremely expensive, both computationally and in terms of training time. For this study, EfficientNet-B0 was trained on 1M images of IN-1K loosely following the training Recipe B of Wightman et al. [129, Appendix F] for 350 epochs. Training this small NN on two NVIDIA RTX 4090 24GB GPUs took well over 100 hours. To train larger models would require even more computing power. If such heavy offline training can be afforded, one can use the EfficientNet-B0 backbone with GN layers that show OOD robustness comparable to the ViT-M/16 backbone with 40M parameters at the expense of increased inference time but with much less power consumption. However, if ImageNet pre-training cannot be afforded, it would be preferred to adopt ViT which is available for various ImageNet pre-trained models without BN layers.# 4. Transfer Learning\n\nThe results so far indicate that ViTPose models are superior in terms of computational efficiency and batch-agnostic by nature, satisfying both Requirements #1 and #3 of Section III. However, they still fall short in OOD robustness when compared to EfficientDet models of comparable sizes. Therefore, it would be favorable to improve the OOD robustness of ViTPose models without substantial sacrifice of computational advantages. One potential remedy is to improve the transfer learning from the ImageNet pre-training. The ViT backbones of the ViTPose architecture so far are all pre-trained on IN-1K according to the DeiT III recipe [124]. This is an improved training method compared to DeiT [130] which is, in turn, an improved training method compared to the vanilla ViT [22]. In this section, the effect of ImageNet pre-training for ViT backbones is analyzed in detail by comparing the two different pre-training methods (DeiT vs. DeiT III) and datasets in Table 5. First, using random initialization without any pre-training on a large-scale dataset does not train well on SPEED+. For ViT-T/16 and ViT-S/16, using DeiT III leads to improved performance over DeiT when using only IN-1K for supervised pre-training. When the backbone is trained instead on a much larger ImageNet-22K (IN-22K) [34] which contains nearly 14M images and 22K class labels then fine-tuned on IN-1K, there is a marginal improvement in OOD robustness for both ViT-S/16 and ViT-M/16 models. The results overall indicate that better generalization of the backbone NN on a large-scale image classification dataset leads to a noticeable improvement in the downstream pose estimation task across the sim2real gap, though it seems the size of the pre-training dataset has.**Table 5 Performance of the ViTPose pose estimation architecture with three backbones (ViT-T/16 & ViT-S/16 & ViT-M/16) with different pretraining datasets (None & ImageNet-1K (IN-1K) & ImageNet-22K (IN-22K)) and methods (DeiT [130 ] & DeiT III [ 124 ]). Mean(std. dev.) of average performances over 3 training sessions with different random seeds are reported. The boldface denotes the best performance within each group.**\n|Backbone (Pre-Train Method)|Pre-Train Dataset|Num. Param.|IN-1K top-1 (%)|synthetic|lightbox|sunlamp|prisma25|\n|---|---|---|---|---|---|---|---|\n|ViT-T/16|-|6.2M|-|16.12(1.14)|59.72(0.82)|83.34(0.71)|66.46(7.75)|\n|ViT-T/16 (DeiT)|IN-1K|6.2M|72.17|1.13(0.02)|8.92(0.38)|18.90(0.53)|17.67(3.21)|\n|ViT-T/16 (DeiT III)\u2020|IN-1K|6.2M|75.12|1.02(0.01)|8.27(0.09)|14.23(0.47)|10.51(4.80)|\n|EfficientNetV2-B0|IN-1K|6.2M|78.37|0.84|7.25|12.55|2.81|\n|ViT-S/16|-|22.7M|-|6.40(1.35)|41.63(3.86)|65.74(2.57)|39.39(7.96)|\n|ViT-S/16 (DeiT)|IN-1K|22.7M|79.86|0.87(0.02)|6.11(0.15)|13.20(0.89)|8.79(0.36)|\n|ViT-S/16 (DeiT III)|IN-1K|22.7M|81.36|0.75(0.00)|4.98(0.14)|8.81(0.29)|4.26(3.08)|\n|ViT-S/16 (DeiT III)|IN-22K|22.7M|83.07|0.74(0.02)|4.84(0.01)|8.29(0.16)|2.24(0.08)|\n|EfficientNetV2-S|IN-1K|20.3M|83.91|0.62|4.24|6.10|4.55|\n|ViT-M/16|-|39.6M|-|36.19(1.85)|79.61(1.13)|95.43(0.98)|82.80(9.73)|\n|ViT-M/16 (DeiT III)|IN-1K|39.6M|83.10|0.71(0.01)|4.46(0.20)|8.36(0.15)|2.19(0.06)|\n|ViT-M/16 (DeiT III)|IN-22K|39.6M|84.56|0.67(0.00)|4.20(0.11)|7.73(0.11)|4.18(2.01)|\n|EfficientNet-B6|IN-1K|39.9M|84.12|0.53|3.65|5.19|1.99|\n\n|Input Res.|\ud835\udf0e [pix]|Mem. [MB]|Latency [ms]| |synthetic|lightbox|sunlamp|prisma25| |\n|---|---|---|---|---|---|---|---|---|---|\n|224 \u00d7 224|1|377|349|22|0.74(0.02)|4.84(0.01)|8.29(0.16)|2.24(0.08)| |\n|288 \u00d7 288|1|401|566|23|0.58(0.01)|3.88(0.12)|6.70(0.22)|5.79(2.82)| |\n|384 \u00d7 384|1|498|1046|27|0.47(0.01)|3.30(0.12)|5.61(0.40)|2.22(0.17)| |\n|384 \u00d7 384|2|498|1046|27|0.51(0.00)|2.99(0.11)|5.11(0.08)|1.86(0.01)| |\n|448 \u00d7 448|2|585|1500|34|0.47(0.01)|3.16(0.13)|4.53(0.16)|1.78(0.11)| |\n|EfficientDet (EfficientNet-B6)|224 \u00d7 224|1|944|1660|110|0.53|3.65|5.19|1.99|# Input Resolution\n\nAnother potential remedy to improve the OOD robustness of ViTPose models is to increase the input image resolution. The effect of input image resolution of ViTPose on the OOD robustness and latency on the Jetson Nano is reported in Table 6. Unsurprisingly, increasing the resolution of the input image cropped around the target leads to improved performance across the board but at the cost of increased peak training memory and latency. Specifically, doubling the input resolution from 224 \u00d7 224 to 448 \u00d7 448 nearly quadruples the training time for each input. This is expected, since doubling the input resolution but maintaining the same patch size (16 \u00d7 16) leads to double the length of patch tokens fed into the transformer blocks, and the memory footprint of the scaled dot-product operation of self-attention modules grows quadratically to the token length. Even though the training latency is tantamount to that of EfficientDet (EfficientNet-B6) operating on 224 \u00d7 224 inputs with nearly 40M parameters, the inference latency is still# Table 7\n\nPerformance of the ViTPose-S/16 different data augmentation configurations. Mean(std. dev.) of average performances over 3 training sessions with different random seeds are reported. The boldface denotes the best performance.\n\n|SA|DA|RC|\ud835\udc38q / \ud835\udc38\u2217q [\u25e6]|synthetic|lightbox|sunlamp|prisma25| |\n|---|---|---|---|---|---|---|---|---|\n| | |-|-|-|0.64(0.01)|6.49(0.22)|13.18(0.62)|36.45(5.28)|\n| | |\u2713|-|-|0.69(0.01)|6.16(0.08)|9.80(0.38)|2.40(0.21)|\n| | |\u2713|\u2713|-|0.71(0.01)|5.66(0.35)|10.30(0.75)|2.20(0.16)|\n| | |-|\u2713|\u2713|0.73(0.01)|4.84(0.18)|9.22(0.08)|3.58(0.88)|\n| | |\u2713|\u2713|\u2713|0.72(0.01)|4.68(0.27)|8.50(0.15)|2.22(0.09)|\n\nAbout a third and the training memory nearly half of EfficientDet (EfficientNet-B6) while performing better on all image domains. The result suggests that it could be beneficial to use ViT-based models with higher input resolutions as long as the training time does not become the bottleneck.# 6. Data Augmentation\n\nThe experiments so far used all three data augmentation techniques\u2014Style Augmentation, DeepAugment and RandConv. In order to verify the individual contribution of these augmentations, Table 7 reports the mean orientation errors for different data augmentation configurations. The default baseline only consists of 5 random augmentations from the Albumentations library implemented in the style of RandAugment [105] (Section IV.C).\n\nNote that adding just the style augmentation to the training significantly improves the performance on sunlamp, and incrementally adding DeepAugment and RandConv leads to consistent performance improvement on lightbox. While it does not bear much statistical significance, an interesting observation is that the performances on 25 flight images of prisma25 dramatically improve and stabilize (as observed from the standard deviation) once the style augmentation and other techniques are implemented during the training.# D. Summary\n\nThe previous section investigates different aspects of designing and training a robust pose estimation NN. Under the same training conditions with identical input image resolutions, EfficientDet (EfficientNet) consistently outperforms all other models in terms of the OOD robustness as measured on the SPEED+ HIL domains. On the other hand, ViTPose (ViT) dominates the training and inference latency for different model sizes but with inferior OOD robustness. However, various ablation studies reveal that the ViTPose models can be made as robust as EfficientDet with improved pre-training of the ViT backbone, extensive data augmentation and increased input image resolution. Specifically, even with twice higher image resolution, the inference of ViTPose (ViT-S/16) is still faster than that of EfficientDet (EfficientNet-B0) that is 1/5 in size. Doubling the input resolution does lead to the increase of training latency by a factor of 4, but as stated in Requirement #1 (Section III.A), the increased training time is less likely to become as mission-critical as the inference time on a satellite avionic with an onboard GPU. Moreover, ViTPose satisfies Requirement #3 by design (Section III.C), completely lacking BN layers that could interfere with accurate online training on single images.\n\nMoving forward, the ViTPose models trained with the data augmentation proposed in Section IV.C are referred to as SPNv3 for convenience. The default SPNv3 configuration is the ViT backbone pre-trained via DeiT III [124] on IN-22K then fine-tuned on IN-1K, patch size \ud835\udc43 = 16, and input resolution 448 \u00d7 448.# 1. Comparison on SPEED+\n\nTable 8 compares the final performances of SPNv3-S, a mid-size variant that takes the ViT-S/16 backbone (22.7M parameters), against the top entries of SPEC2021 [17] and other state-of-the-art models introduced during the post-mortem competition following SPEC2021 and others. It can be seen that, with a higher input resolution at 448 \u00d7 448, SPNv3-S already outperforms the top entries of the lightbox category. Moreover, ensembling the output heatmap predictions from three models trained with different random seeds further improves the overall performance, achieving \ud835\udc38q = 2.69\u25e6 on lightbox and \ud835\udc38q = 3.89\u25e6 on sunlamp. Some of the pose predictions by the ensemble are visualized in Fig. 7 via reprojection of the Tango wireframe model. Note that the winning teams for respective HIL# Table 8 Performance of SPNv3 models and state-of-the-art.\n\nMean(std. dev.) denotes the average performance over 3 training sessions with different random seeds. The boldface denotes the best performance in each group.\n\n|Team/Model|Num. Params.|lightbox \ud835\udc38\u2217 [-]\u00aft|lightbox \ud835\udc38\u2217[\u25e6]|lightbox \ud835\udc38\u2217pose [-]|sunlamp \ud835\udc38\u2217 [-]\u00aft|sunlamp \ud835\udc38\u2217[\u25e6]|sunlamp \ud835\udc38\u2217pose [-]| |\n|---|---|---|---|---|---|---|---|---|\n|SPEC2021 Top-3 [17]|-|0.018|3.187|0.073|0.015|4.299|0.090| |\n|TangoUnchained [17]*|-|0.022|4.577|0.101|0.012|2.827|0.061| |\n|lava1302 [78]*|-|0.048|6.664|0.165|0.011|2.728|0.059| |\n|Post-mortem & misc.|SPNv2 [18]|52.5M|0.025|5.577|0.122|0.026|9.788|0.197|\n|Chen et al. [131]*|-|0.018|2.865|0.068|0.014|2.750|0.062| |\n|FA-VAE [132]|41.6M1|0.027|4.939|0.114|0.028|5.185|0.118| |\n|EagerNet [80]|88.6M2|0.009|1.748|0.039|0.013|2.664|0.059| |\n|Proposed|SPNv3-S|22.7M|0.016(0.001)|3.044(0.234)|0.069(0.004)|0.020(0.000)|4.371(0.314)|0.097(0.006)|\n|SPNv3-S\u2020|22.7M|0.017|2.689|0.064|0.020|3.883|0.088| |\n|SPNv3-M\u2020|39.6M|0.014(0.000)|2.595(0.123)|0.060(0.002)|0.018(0.001)|4.158(0.254)|0.090(0.005)| |\n|SPNv3-B\u2020|86.3M|0.013(0.001)|2.183(0.129)|0.051(0.003)|0.016(0.001)|3.612(0.164)|0.079(0.003)| |\n\n* Included HIL domain images into training\n\n\u2020 From ensemble of heatmap predictions from the models of three training sessions\n\n1 Approx. from DarkNet-53 backbone [133]\n\n2 Approx. from ConvNeXt-B backbone [134]\n\ncategories\u2014TangoUnchained and lava1302\u2014adopted adversarial training, directly utilizing the unlabeled images of the HIL domains. On the other hand, the proposed SPNv3 does not utilize any information about the HIL domain during the training phase. The performance of SPNv3 is comparable to that of EagerNet [80] which also does not involve the HIL domain images during training. Note that EagerNet uses a pre-trained ConvNeXt-Base network with 89M parameters and generates multiple pose hypotheses from dense predictions of model coordinates and predicted errors, which are further refined iteratively using a region-based approach. On the other hand, SPNv3-S is smaller and simpler, predicting keypoint locations from lower-resolution heatmaps after just a single forward propagation.\n\nIn the end, the ensemble performance with 448 \u00d7 448 inputs would rank SPNv3-S at first place in the lightbox category and third place in the sunlamp category of SPEC2021 [17]. Indeed, running an ensemble of multiple models may be computationally expensive especially onboard the satellite avionics. However, considering that the inference of SPNv3-S with 448 \u00d7 448 inputs only takes about 34 ms on a GPU of an NVIDIA Jetson Nano (see Table 6), it may be possible to run an ensemble of a few models in real-time during RPOD. The largest variant of equivalent size, SPNv3-B with 86.3M parameters, marginally improves the accuracy, nearly matching that of EagerNet on lightbox yet still remaining in third place on sunlamp. However, such a difference in performance between SPNv3-S and SPNv3-B would likely not yield significant improvement that justifies a near quadruple increase in size when processed by an onboard navigation filter.# E. Limitations\n\nThe proposed SPNv3 based on ViTPose is not without limitations. One major limitation is its dependence on a pre-trained ViT backbone. More specifically, Table 5 reveals that the OOD robustness of the downstream pose estimation task depends not just on what dataset the ViT backbone is pre-trained on but also on how the pre-training was conducted. This observation implies that those who cannot afford the computational power nor the expertise of ImageNet pre-training are restricted in exploring and fine-tuning different architectural options. In fact, the dependence on ImageNet pre-training prevents this work from exploring other aspects of ViT backbones, e.g., varying the patch sizes (\ud835\udc43). Furthermore, the optimally pre-trained backbone may not exist, in which case the sub-optimally pre-trained alternative may have to be used for training on SPEED+. In response to such cases, the authors\u2019 previous work [19] shows how one can further train the sub-optimal NN onboard the satellite avionics in real-time during RPOD.# VI. Conclusion\n\nThis paper presented SPNv3, a computationally efficient and robust Neural Network (NN) model based on the Vision Transformer (ViT) architecture. SPNv3 is trained exclusively on computer-generated synthetic images to achieve state-of-the-art robustness on unknown spaceborne imagery. Then, it is tested exhaustively on the hardware-in-the-loop images from a robotic facility that can simulate high-fidelity spaceborne illumination conditions. Extensive analyses are performed to identify data augmentation, transfer learning, NN size and input image resolution as key aspects of the design and training that contribute the most to its Out-Of-Distribution (OOD) robustness. In the end, a mid-size SPNv3-S with 22.7M parameters can perform inference on a larger 448 \u00d7 448 input images at nearly 30Hz on a restrictive GPU environment, which is well above the nominal update frequency of modern satellite guidance, navigation and control systems. Therefore, this paper successfully demonstrated that SPNv3 is flight-ready, capable of achieving state-of-the-art.",
        "context_id": 22,
        "question": "What facility at Stanford's Space Rendezvous Laboratory was used to create the HIL images for datasets such as SPEED+ and SHIRT?",
        "answer": [
            "TRON"
        ],
        "context_length": 75895
    },
    {
        "context": "# 1 Introduction\n\nResearch on outlier detection has a long and rich history. As early as 1620, Francis Bacon recognized the existence of substantial deviations from commonly occurring phenomena in nature [6]. In the 19th century, Legendre and Gauss discovered the least squares methodology [70]. Legendre was the first mathematician to realize the impact of outliers (which he referred to as \u201cerrors\u201d) on the method. He suggested rejecting models that produce errors too large to be admissible [32]. Later, Edgeworth and Ysidro proposed dropping a certain portion of abnormal data points (i.e., most likely outliers) to avoid their substantial influence on least squares estimates [21].\n\nToday, outlier detection remains a popular research topic due to its wide range of applications. For instance, it can help financial institutions identify suspicious loan applications [58, 64]. It can be employed to detect faults in mechanical units [64]. It can also be used in network anomaly detection to build a security management system that protects.# Footnotes\n\n\u2217 The Department of Mathematics and Statistics, Auburn University, rzs0112@auburn.edu\n\n\u2020 The Department of Mathematics and Statistics, Auburn University, billone@auburn.edu\n\n\u2021 The Department of Mathematics and Statistics, Auburn University, ezc0066@auburn.edu# Outlier Detection\n\nOutlier detection is crucial in diagnosing diseases such as brain cancer and leukemia [28]. There have been various definitions of outliers since the start of outlier detection research. Ayadi et al. [5] summarized twelve different definitions according to different researchers in chronological order. Among all the definitions, the one from Hawkins is widely accepted by statisticians: \u201cAn outlier is an observation that deviates so much from other observations, and it arouses suspicions that it was generated by a different mechanism.\u201d [34]# Classification of Outliers\n\nAccording to the previous surveys, outliers can be classified as point, collective, and local outliers [80, 64]:\n\n1. Point outliers: An individual point that is outlying.\n2. Collective outliers: Several or a group of close data points showing a nonconforming pattern compared to the entire data set. Identifying an outlying group is generally a more challenging task.\n3. Local outliers: A single (or a group of) points exhibits anomaly in terms of its (their) neighbors.# Importance of Outlier Detection\n\nOutlier detection is essential for data analysis and pre-processing. It is easy to spot outliers visually in one or two-dimensional space. However, virtual inspection becomes challenging in higher dimensions. Thus, developing outlier detection algorithms is necessary, especially for a space with many dimensions.# Challenges in Outlier Detection\n\nAlthough many methods have been proposed, outlier detection remains challenging for the following reasons:\n\n- (i) It is difficult to find precise support for regular data points in real-life data [13];\n- (ii) the definition of outliers varies substantially from one domain to another [75];\n- (iii) distinguishing outliers from noise is not trivial [75].\n\nFurthermore, most outlier detection algorithms require input parameters that are too technical for non-professionals to understand, and the trial-and-error processes can be tedious and time-consuming. For this reason, we propose outlier detection methods that are either input-parameter-free or require only understandable input parameters that can be determined easily beforehand.# Common Problems in Outlier Detection\n\nAdditionally, masking and swamping are common problems in outlier detection. The masking problem occurs when an outlier is hidden by similar outliers that are close. Generally, it occurs among collective outliers. On the other hand, the swamping problem occurs when a regular observation is falsely labeled as outliers given either the effect of nearby true outliers or other close regular points that exhibit different behaviour [9].# Strategies to Avoid Masking and Swamping\n\nSeveral strategies are proposed to avoid masking and swamping in outlier detection:\n\n- Employing robust statistics like median, trimmed means, and Median Absolute Deviation about the median (MAD) [37];\n- Visualizing data with graphics (e.g., box plots) [76];\n- Set the number of outliers to detect as an input parameter [24].\n\nThese approaches help identify true outliers accurately without mislabeling non-outliers.# Proposed Outlier Detection Algorithms\n\nWe propose outlier detection algorithms based on Cluster Catch Digraphs (CCDs), which were first introduced by Devinney [19] and improved by Marchette [48], developed from a similar classification digraph called Class Cover Catch Digraphs (CCCDs). Later, Manukyan and Ceyhan [47] modified and improved this approach further, developing two variants that use a Kolmogorov-Smirnov (KS) based statistic and Ripley\u2019s K function, respectively, calling the associated digraphs KS-CCDs and RK-CCDs. RK-CCDs and KS-CCDs work similarly in clustering, and RK-CCDs are almost parameter-free, making them especially appealing. However, our experimental analysis shows that RK-CCDs may not be suitable for moderate to high dimensionality. Thus, we introduce another CCD-based# Approach\n\nApproach that uses nearest neighbors instead of Ripley\u2019s K function to test underlying point-process patterns.\n\nGiven a data set, RK-CCDs and UN-CCDs construct an open (hyper)sphere for each latent cluster, called covering balls. Experimental results show that the covering balls catch the majority of points of a data set, which are considered regular points [47]. On the other hand, we can find outliers among those points not covered by any covering balls, which are generally far away from any clusters and located in low-density regions. This is appealing and is also the motivation of this paper. We adapt RK-CCDs and UN-CCDs on two CCD-based outlier detection algorithms called the U-MCCD and UN-MCCD algorithms; then, we propose two \u201cflexible\u201d variations called the SU-MCCD and SUN-MCCD algorithms aiming at outlier detection on the data sets with arbitrary-shaped clusters.\n\nBy conducting comprehensive Monte Carlo experiments, we demonstrate that our algorithms exhibit wide adaptability and can deliver promising results across different data sets, even with high dimensionality. The paper is organized as follows:\n\n- Section 2 covers previously proposed algorithms in outlier detection. We focus on the graph-based, density-based, cluster-based methods and previous works on CCCDs and CCDs.\n- In Section 3, we proposed Mutual Catch Graphs (MCGs) based on KS-CCDs and its application on outlier detection given a single cluster. Then, we combine MCGs and CCDs, proposing four CCD-based outlier detection algorithms, called U-MCCDs, UN-MCCDs, SU-MCCDs, and SUN-MCCDs, respectively.\n- We conduct extensive simulations to assess the performance of all the CCD-based outlier detection algorithms starting from Section 5.\n\nTo help readers navigating the specialized terminology used throughout this paper, we enumerate a list of acronyms and their full terms below.\n\n|Abbreviation|Full Term|\n|---|---|\n|CCDs|Cluster Catch Digraphs|\n|RK-CCDs|The CCDs based on the Ripley\u2019s K function|\n|KS-CCDs|The CCDs based on the a KS-based statistic|\n|UN-CCDs|Uniformity- and Neighbor-based CCDs|\n|D-MCGs|Density-based Mutual Catch Graphs|\n|U-MCCDs|Uniformity-Based CCDs with Mutual catch graph|\n|SU-MCCDs|Shape-adaptive Uniformity-based CCDs with Mutual catch graph|\n|UN-MCCDs|Uniformity- and Neighbor-based CCDs with Mutual catch graph|\n|SUN-MCCDs|Shape-adaptive Uniformity- and Neighbor-Based CCD with Mutual catch graph|\n|SR-MCT|Spatial Randomness Monte Carlo Test|\n|HPP|Homogeneous Poisson Process|\n|CSR|Complete Spatial Randomness|\n|NND|Nearest Neighbor Distance|\n|MAD|Median Absolute Deviation about the median|\n|MADN|Normalized Median Absolute Deviation about the median|\n|TPR|True Positive Rate|\n|TNR|True Negative Rate|\n|BA|Balance Accuracy|# 2 Background and Preliminaries\n\nResearchers have proposed various outlier detection methods, and they are mainly categorized into graph-based, density-based, cluster-based, and statistical-based methods based.# 2.1 Graph-Based Methods\n\nGraph-based outlier detection methods employ graph theoretic techniques that capture outliers by constructing interdependence ties among observations [75]. These methods are suitable in scenarios where data is inherently relational, such as social networks, biological networks, and communication networks. We enumerate some well-known algorithms in this category below.\n\nNoble and Cook proposed two graph-based anomaly detection methods with the Subdue system [55], which flag unusual subgraphs or substructures. OddBall [1] discovers substantial outlying patterns by four features. Hautamaki et al. introduced Outlier Detection using In-degree Number (ODIN), which assumes that outliers have a substantially lower in-degree than regular points in a k-Nearest Neighbor (kNN) graph [33]. Liu et al. proposed an unsupervised-learning algorithm called Isolation Forest [44], with the notion that outlier points have distinct characteristics, making them easier to isolate than regular data points in a binary tree. Other well-known methods include OutRank [52], Community Outlier Detection Algorithm (CODA) [26], and Local Information Graph-based Random Walk model (LIGRW) [74].# 2.2 Density-Based Methods\n\nDensity-based methods identify outliers among points in low-density regions. Typically, these approaches measure a point\u2019s outlyingness by comparing its local density with those of its nearest neighbors.\n\nLocal outlier factor (LOF) [10] is one of the prototype methods in this category, which introduces local reachability density to compute the local outlyingness of a point. Tang et al. proposed Connectivity-based Outlier Factor (COF) [71] that performs better than LOF on the outliers that deviate from their neighbor patterns but with similar local density. A similar method called LOcal Correlation Integral (LOCI) [57] was proposed by Papadimitriou et al., coming with a data-orientated threshold for outlyingness score. Kriegel et al. formulated a new outlyingness score called Local Outlier Probabilities (LoOP) [43], which represents the probability of a point being an outlier, greatly enhancing the interpretability. Other density-based outlier detection algorithms include Relative Density Factor (RDF) [61], INFLuenced Outlier-ness (INFLO) [38], Resolution-based Outlier Factor (ROF) [23], Dynamic Window Outlier Factor (DWOF) [51], High Contrast Subspaces (HiCS) [42], Simplified LOF [67], Global-Local Outlier Scores from Hierarchies (GLOSH) [12], and Simple uni-variate Probabilistic Anomaly Detector (SPAD) [4].# 2.3 Cluster-Based Methods\n\nClustering is an unsupervised method that groups points that are close or behave similarly. Small clusters with substantially fewer points or isolated points far apart from other clusters could be labeled as outliers. Outliers often come as by-products of clustering algorithms.# 2.3 Clustering Methods\n\nSo far, cluster-based methods have been classified into several subgroups, known as partitional, hierarchical, and density-based. Many are formulated with robust mechanisms against outliers [75].# Partitional Clustering\n\nPartitional clustering methods create a single-level partition of the data set [75]. These algorithms typically begin with a pre-specified number of clusters, often represented by their centers, which can be obtained through a simple method like random selection. The partitions are then iteratively updated until a specific object function is optimized. The most commonly known algorithms include k-means [27], MacQueen [45], Partitioning Around Medoids (PAM) [41], Clustering LARge Applications (CLARA) [41] and Clustering Large Applications based on RANdomized Search (CLARANS) [54].# Hierarchical Clustering\n\nHierarchical clustering methods construct a hierarchical tree-like structure called dendrogram and partition the whole data set based on the desired granularity. It can be divided into two subgroups called agglomerative and divisive clustering [80]. One of the popular algorithms is the Minimal Spanning Tree (MST) method [79], which constructs a minimal spanning tree that connects all data points and removes \u201cinconsistent\u201d edges to obtain clusters and outliers. Other algorithms include Clustering Using Representatives (CURE) [30], CHAMELEON [40], Robust Clustering using links (ROCK) [31].# Density-Based Clustering\n\nThe core idea of the density-based clustering method involves identifying the regions where data points are dense as clusters. Some well-known examples include Density-Based Spatial Clustering of Applications with Noise (DBSCAN) [22], which captures clusters by first finding some core points and expanding them to clusters. Other well-known algorithms include Ordering Points To Identify the Clustering Structure (OPTICS) [3], Distribution Based Clustering of LArge Spatial Databases (DBCLASD) [78], and DENsity-based CLUstEring (DENCLUE) [35].# 2.4 Evaluation Metrics in Outlier Detection\n\nAlthough many outlier detection algorithms have been introduced over the last two decades, there has yet to be an agreed-upon answer to how to measure the performance of an outlier detection algorithm [75]. Although researchers have always concluded that their approaches are comparable to or outperform existing algorithms, some of their conclusions are subjective due to the choice of the evaluation metric, and a more comprehensive empirical analysis is needed [75]. We choose True Positive Rate (TPR), True Negative Rate (TNR), Balanced Accuracy (BA), and F2-scores as evaluation metrics in Monte Carlo simulations.\n\nTPR (i.e., recall) and TNR measure the ratio of correctly identified outliers and regular points. However, outlier detection is essentially a classification problem over highly imbalanced data sets, the performance of which should not be solely measured by plain accuracies or errors [14, 18]. Therefore, we also consider using BA and F2-scores. BA is the mean of TPR and TNR, and F2-scores is the weighted harmonic mean of precision and recall. Both of them focus on positive and negative observations and are widely used in highly imbalanced data sets; they are suitable for evaluating the performance of outlier detection algorithms [69].\n\nThe outlier detection algorithms we propose are based on CCDs. CCDs are digraphs with all data points as vertices and arcs determined by the spherical balls centered at the vertices. Actually, Class Cover Catch Digraphs (CCCDs), formulated by Priebe et al. [59], are prototypes of CCDs. CCCDs are powerful tools for supervised classification. Following the chronological order, we will first discuss CCCDs briefly.# 2.5 Class Cover Catch Digraphs\n\nGiven a data set X \u2192 Rd that consists of i.i.d points from two classes X0 = {x1, x2, ..., xn} and X1 = {y1, y2, ..., ym}, i.e., X = X0 \u222a X1. Without loss of generality, here we refer to the class of interest X0 as target class and X1 as non-target class.# Class Cover Problem (CCP)\n\nCCP aims to distinguish the target class (X0) from the non-target class (X1) by finding a minimum collection of open balls or hyperspheres Bi = B(a, r) = {x | d(a, x) < r, x \u2208 X} such that \u222a Bi covers all the points of the target class X0 while excluding the non-target class (X1) [48].\n\nCCCDs address the CCP. A CCCD for X0, denoted as D0 = (V0, A0), is a digraph with vertex set V0 = X0 and arc set A0. It starts by constructing a covering ball B(xi, rxi) centered at each xi \u2208 V0. For any two distinct vertices xi, xj \u2208 V0, the arc (xi, xj) \u2208 A0 if and only if xj \u2208 B(xi, rxi). We could also build a CCCD for X1 by swapping the roles of the two classes. Currently, there are two variants of CCCDs: pure-CCCDs (P-CCCDs) and random walk-CCCDs (RW-CCCDs). They differ in the criterion used to determine the radius rxi for each covering ball B(xi, rxi) [46]. We will not discuss their details here.# 2.5.1 The Approximate Minimum Dominating Sets\n\nWith the above construction, a digraph Di = (V, Ai) and a cover \u222a Bi for the target class Xi (i = 0 or 1) either by P-CCCDs or by RW-CCCDs can be obtained. However, to avoid the over-fitting problem, we may want to reduce the complexity of the covers by keeping only a certain number of covering balls and dropping the others [59]. The centers of these retained covering balls are called the prototype set. Obtaining a minimum dominating set (MDS) Si for Di is one way to achieve this goal.\n\nFinding an MDS is generally an NP-Hard optimization problem [39]. Fortunately, the Greedy Algorithm 1 below provides an efficient way to find an approximate MDS in O(|V0|2) time [15, 36]. The algorithm initializes with all vertices as uncovered and an empty dominating set. It iteratively selects the vertex with the maximum outdegree, adds it to the dominating set, and removes its closed neighborhood from the set of uncovered vertices. This process repeats until all vertices are covered. Additionally, there are two more variants of greedy algorithms proposed by Manukyan and Ceyhan [47], differing in the way of choosing vertex at each iteration. The first variant is presented as the Greedy Algorithm 2 below, and this variant is tailored for CCDs. At each iteration, it selects the vertex with the maximum outdegree in the initial digraph, such that the members of the dominating set will be closer to the cluster centers. The second variant is greedy in a score function, i.e., chooses a vertex v that maximizes a score function sc(v) at each iteration. It is presented as the Greedy Algorithm 3.\n\nIn general, RW-CCCD outperforms P-CCCD in classification, especially when the data set is highly imbalanced [46].# 2.6 Cluster Catch Digraphs Using a KS-Based Statistic\n\nThe CCCD approach for classification was adapted to clustering, and CCDs were introduced by DeVinney [19]. Suppose there is an unlabeled data set X = {x1, x2, ..., xn} in Rd drawn from a mixture distribution, where each component of the mixture represents a cluster, the goal is to determine the number of clusters and the optimal partition. Unlike CCCDs, CCDs determine the optimal radius of each covering ball using a Kolmogorov-Smirnov (KS)-based statistic. The KS-based statistic measures the# 1: (A greedy algorithm finding an approximate MDS)\n\nD sub (S) is the induced sub-digraph of vertex set S from a digraph D, N\u00af (v) is the closed neighborhood of a vertex v. V temp represents the uncovered vertices at current iteration.# Input:\n\nA digraph D = (V (D), A(D)). for a given data set X = {x1, x2, ..., xn}# 2 while V temp \u2198= \u2197 do\n\nv temp \u2194 arg max v\u2192V (D) {d out (v)}; (d out (v):the outdegree of v in A(D))\nV temp \u2194 V temp \\\u00af (v temp ); N\nS\u02c6 \u2194 S \u2191 {v temp };\u02c6\nD \u2194 D sub (V temp );\nend# Greedy Algorithm 2: (A greedy algorithm finding an approximate MDS)\n\nThis greedy algorithm is adapted for Cluster Catch Digraphs (CCDs).# Input:\n\nA digraph D = (V (D), A(D)) for a given data set X = {x1, x2, ..., xn}# 1 Algorithm Steps:\n\nIt is similar to Greedy Algorithm 1, except that it iteratively selects the vertex with the maximum outdegree in the initial digraph.\n\n\u201cclustered-ness\u201d around a point xi \u2208 X [48], and it is defined as follows,\n\nT KS (xi, r) = F rw (xi, r) \u2243 F 0 (xi, r),\n\nwhere F rw (xi, r) equals the number of points caught by the covering ball B(xi, r). The second term F 0 (xi, r) represents the expected number of points in B(xi, r) under a null distribution. For example, under the common assumption of Complete Spatial Randomness (CSR), which is also known as Homogeneous Poisson Process (HPP), we can take F 0 (xi, r) = \u03c9rd [48], where d represents the dimensionality and \u03c9 is an input density parameter. Based on the Kolmogorov-Smirnov (KS) type test, the optimal radius rxi is chosen to maximize T KS (xi, r), i.e.,\n\nrxi = arg max{T KS (xi, r)}.r\u21910\n\nBy maximizing T KS (xi, r), the value of the radius is selected with the notion that the most clustered points around xi are covered by B(xi, rx) [48]. Once the radii are determined, a CCD for X, denoted as D = (V (D), A(D)), can be constructed. The weakly connected components of D (i.e., \u2191i=1 Ci = X) can be returned as clusters. However, for each cluster found, its covering balls are not equally important.# Greedy Algorithm 3: (A greedy algorithm finding an approximate MDS)\n\nThis algorithm is similar to Greedy Algorithm 1, except that it is greedy in a score function sc(v) at each iteration.# Input:\n\nA digraph D = (V (D), A(D)) for a given data set X = {x1, x2, ..., xn}# 2.7 Cluster Catch Digraphs using Ripley\u2019s K Function\n\nThus, for the same reason as CCCDs, obtaining a lower complexity cover is desired. Similar to CCCDs, this goal can be achieved by finding an approximate MDS. Marchette proposed two versions of modified greedy algorithms to find an approximate MDS for CCDs [48]. Despite the two modified versions, Manukyan and Ceyhan prefer the Greedy Algorithm.\n\nAlthough an approximate MDS \u02c6 S reduces the cover complexity, not all its covering balls are necessary. To further reduce the complexity of the cluster cover, one can identify the \u201ccore\u201d covering balls by constructing an intersection graph, denoted as GM D = (VM D, EM D), where VM D = \u02c6, and for any points u, v \u2193 S, the edge uv \u2193 S EM D if and only if B(u, ru) and B(v, rv) cover some common points in X. With the intersection graph GM D, one can implement Greedy Algorithm 1 to prune \u02c6 again. The approximate MDS of GM D is denoted as S\u02c6(GM D), and each covering ball of S\u02c6(GM D) represents a latent cluster.\n\nAlthough we have reduced the cover complexity in two sequenced phases and can obtain a partition P = {P1, P2, ..., Pk} for X, the clustering result is not robust to noise and outlier clusters. Therefore, Manukyan and Ceyhan [47] employs the silhouette index [25] to identify and remove redundant clusters. Silhouette index of xi, written as sil(xi), is a metric measuring how well xi is clustered in terms of the partition P. Manukyan and Ceyhan [47] first rank the partitions in P in a decreasing order based on their size. Starting from the first two, they add partitions incrementally as valid clusters until the average silhouette index of the entire data set (denoted as sil(P)) is maximized. Indicating that no more clusters are necessary, and we call the covering balls retained as the dominating covering balls of the intersection graph.\n\nFor the point xi \u2193 X that is not covered by any selected clusters (covering balls), it can be assigned to the nearest cluster (covering ball) with minimal relative similarity measure. The relative similarity measure between xi and the covering ball B(xj, rx j), denoted as \u03b5(xi, B(xj, rx j)), can be computed as follows,\n\n\u03b5(xi, B(xj, rx j)) = d(xi, xj)/rx j.\n\nFor simplicity, Manukyan and Ceyhan [47] refer to the CCDs based on a KS-based statistic as KS-CCDs.\n\nAlthough utilizing silhouette index enhances the robustness of KS-CCDs to outliers or noise clusters, there are still a few shortcomings due to the intrinsic property of the KS-based statistic. It is a density-based statistic falling short of delivering insight into the spatial distribution of data points. As a result, it may falsely return two or more clusters as one [47]. Additionally, the input density parameter \u03c9 is usually unknown beforehand. As a result, an appropriate value of this parameter can only be obtained via a costly trial-and-error process in most cases.\n\nTo tackle the shortcomings above, instead of using the KS-based statistic, Manukyan and Ceyhan [47] applied Ripley\u2019s K function [62], denoted as K(t), and designed a distribution-based test to determine whether the points inside each covering ball follow an HPP. This test will be referred to as the Spatial Randomness Monte Carlo Test (SR-MCT) with Ripley\u2019s K function in this article. For each covering ball, an optimal radius can be specified as the maximum possible value that the points covered satisfy an HPP. The resulting algorithms are called the RK-CCD algorithm. It is worth noting that the only difference between RK-CCDs and KS-CCDs is the way to determine the values of.# 2.8 Our Contribution\n\nIn this paper, we first introduce the RU-MCCD algorithm, which combines RK-CCDs and the Mutual Catch Graph from KS-CCD, and find potent outliers within some low-density regions. To tackle the data sets in high dimensional space, we introduce another CCD-based clustering algorithm called UN-CCDs, which utilizes the Nearest-Neighbor Distance (NND) to test CSR. Then, we adapt UN-CCDs similarly for outlier detection, and the resulting algorithm is called the UN-MCCD algorithm.\n\nThe RU-MCCD and UN-MCCD algorithms find clusters in (approximate) spherical shapes. To construct covers for arbitrary-shaped clusters, we introduce the SU-MCCD and SUN-MCCD algorithms, the \u201cflexible\u201d variants of the first two CCD-based outlier detection algorithms. Extensive experiments show they deliver better performance in general when the shape of the clusters is arbitrary or the dimensionality of a data set is high.\n\nBesides the four CCD-based outlier detection algorithms, we have also introduced two types of scores, Outbound Outlyingness Score (OOS) and Inbound Outlyingness Score (IOS), to quantify the outlyingness of a point. To be used, they must be combined with a CCD-based algorithm. In experimental analysis, we found that IOS performs exceptionally well; it is robust to the masking and swamping problem and achieves promising results even on a data set with a dimensionality of 100.\n\nIn summary, we enumerate our contributions as the follows:\n\n1. The RU-MCCD algorithm: Combines RK-CCDs and Mutual Catch Graphs (MCGs) for outlier detection in low-density regions.\n2. UN-CCDs for clustering: Utilize the Nearest-Neighbor Distance instead of Ripley's K function to test CSR and are more effective in high-dimensional spaces.\n3. The UN-MCCD algorithm: An adaptation of UN-CCDs specifically tailored for outlier detection.\n4. The SU-MCCD and SUN-MCCD algorithms: The shape-adaptive version of the UN-MCCD and SU-MCCD algorithms to handle data sets with clusters of arbitrary shapes.\n5. Outbound Outlyingness Score (OOS) and Inbound Outlyingness Score (IOS): New metrics to quantify how much a data point deviates from regular points, particularly with IOS demonstrating robustness to masking and swamping issues.# 3.1 The Mutual k-Nearest-Neighbor Graphs\n\nBrito et al. [11] proposed an approach that uses the mutual k-nearest neighbor (mkNN) graph to detect latent clusters, and Marchette [48] pointed out that this approach is appropriate for outlier detection. The main idea of Brito\u2019s approach is to identify latent clustering structures or outliers by examining the local connectivity of each point of a# data set X\n\nTo achieve this goal, they formulated a test measuring the connectivity of the mkNN graph (which is denoted as G\u02dc k (X)) for the given data set X. Under the null hypothesis# H0: \u201cno clustering structure or no outliers\u201d (i.e., data forms a single cluster)\n\nthe test assumes that \u02dc k (X) should be connected given a k value less than or equal to a certain threshold kmax. Appropriate value(s) of kmax is(are) determined by Monte Carlo simulation and a model using the Ordinary Least Squares (OLS). Once found, the mkNN graph, G\u02dc kmax (X), is examined to determine whether it can be partitioned into multiple components. In general, these components are returned as separate clusters or outliers. However, labeling these components can be somewhat challenging [11]. Later, Marchette et al. [48] suggested that this method is more suitable for outlier detection because it is susceptible to the presence of contextual (local) outliers.# 3.2 The Mutual Catch Graphs\n\nInspired by Brito\u2019s approach that focuses on the connectivity of the mkNN graph, we have adopted a similar idea to KS-CCDs or RK-CCDs. Rather than constructing an mkNN graph, we introduce mutual catch graphs (MCGs), and it is defined as follows:# Definition 3.1 (Mutual Catch Graphs (MCGs))\n\nGiven a data set X = {x1, ..., xn} of i.i.d points and a CCD denoted as D(X), a Mutual Catch Graph (MCG), denoted as GM(X) := (V(X), EM(X)), is constructed with V(X) = X. EM(X) is comprised with the edges xixj for distinct xi, xj \u2208 X such that d(xi, xj) < min(rxi, rxj). Here, rxi and rxj represent the radii of covering balls for xi and xj regarding D(X), respectively, implying an edge exists if their covering balls satisfy the \u201cmutual catch\u201d property (i.e., catch (or cover) each other mutually).# 3.3 The Density-based Mutual Catch Graph Algorithm\n\nRecall that a covering ball B(xi, ri) in CCDs captures the largest possible latent cluster structure around a point xi. Thus, any points captured by B(xi, ri) seem to belong to the same cluster with xi. With this notion, a pair of points connected in GM(X) are likely to belong to the same cluster.\n\nSimilar to Brito et al.\u2019s approach, we take the same null hypothesis that there is only one cluster with no outliers. Under H0, every point is drawn from a distribution F with compact and connected support S and bounded density f. Therefore, with all observations aggregating within S, the MCG GM(X) obtained from a KS-CCD should be connected even when the density parameter \u03c9 for the KS-based statistic is relatively large. Therefore, \u03c9 is analogous to the k in an mkNN graph. Hence, we want to find a threshold for \u03c9 and to test H0, identifying latent clusters or outliers when possible. Similar to Brito et al.\u2019s approach, the threshold can be determined by Monte Carlo simulations. More specifically, we simulate a data set X\u2193 from the distribution F (estimate it if unknown) with the same size as the given data set X. We record the maximal value of \u03c9 such that the MCG, GM(X\u2193), is connected. We repeat this procedure m times and obtain a sample of m \u03c9 values. Finally, we use a chosen sample quantile as the threshold for \u03c9.\n\nAlthough Brito et al.\u2019s and our approaches are similar, our approach is density-based. In contrast, Brito\u2019s approach only measures the connectivity of the whole data set globally and ignores local density. Our approach is proposed as Algorithm 1 below, and we call# Density-based Mutual Catch Graph (D-MCG) Algorithm for Clustering and Outlier Detection\n\nIn Algorithm 1, the vertex set V(X \u2193) = X \u2193 in step 8. In step 9, for any u, v \u2193 X\u2193, the edge uv \u2193 E(X \u2193) if and only if v and u are \u201cmutually caught\u201d with their covering balls Bu and Bv, respectively. Finally, if more than one component is detected, further investigation is needed to decide whether these components are clusters or outliers.# Algorithm 1: (D-MCG algorithm)\n\nTests for presence of clusters or outliers in X, utilizing a density parameter \u03c9 adjusted through simulation. Parameters: initial density \u03c90, density decrement #, simulation count M, quantile \u03d1.# Output:\n\nConnected components of X (potential clusters or outliers);# Algorithm Steps:\n\n1. Initialize under the assumption that X has no outliers or other clusters, based on a distribution F with connected (estimated) support S;\n2. n \u2194 |X| (i.e., the size of X);\n3. i \u2194 1;\n4. \u03c9 seq \u2194 \u2197;\n5. while i \u21d0 M do\n6. \u03c9 \u2194 \u03c9; 0\n7. Simulate a data set X \u2192 of size n D(X \u2192) = (V(X \u2192), A(X from the distribution F; \u2192 (with density parameter \u03c9);\n8. Construct \u2192 )): the KS-CCD of X\n9. Construct (G M \u2192)(X \u2192) = (V(X \u2192), E(X \u2192)): the MCG of D(X \u2192);\n10. while G M X is not connected do\n11. \u03c9 \u2194 \u03c9 \u2243 #;\n12. Repeat steps 8 and 9 to update D(X \u2192) and G M (X \u2192);\n13. end\n14. \u03c9 seq \u2194 \u03c9 seq \u2191 {\u03c9};\n15. i \u2194 i + 1;\n16. end\n17. Find the \u03d1 quantile of \u03c9 seq, and denote it as \u03c9 \u03c9;\n18. Construct D \u03c9 (X) = (V(X), A(X)): the KS-CCD of X (with density parameter \u03c9 \u03c9);\n19. Construct G \u03c9,M (X) = (V(X), E(X)): the MCG of D \u03c9 (X);\n20. if G \u03c9,M (X) is connected then\n21. Retain H 0, and return X as the single component;\n22. else\n23. Reject H 0 at \u03d1 level and return the connected components of G M (X) either as clusters or outliers;\n24. end\n\nRecall that when we apply RK-CCDs and KS-CCDs for clustering, we use an intersection graph to reduce the cover complexity of the approximate MDS because covering balls for the same cluster are likely to overlap. We want to find only one representative covering ball for each cluster. On the contrary, considering applying CCDs for outlier detection, we see that the covering balls of an outlier and a regular observation often cover some common points but rarely catch the center of each other simultaneously. Therefore, we employ the MCG technique instead of an intersection graph for outlier detection to avoid any edges between outliers and regular observations.\n\nWe illustrate this algorithm under two simple artificial data sets with outliers. Under the first simulation setting, the regular data points are generated uniformly within a unit hypersphere B(0d, 1) (0 d is the origin of a d-dimensional space), i.e., x i are drawn from Uniform[B(0d, 1)]. Outliers are drawn uniformly from another unit hypersphere with acertain distance (3 units) from the first. Figure 1(a) presents a realization under this setting when d = 2, where we have 3 outliers out of 50 (the contamination level is 6%). Under the second simulation setting, the regular data points are also generated uniformly within a unit hypersphere B(0d, 1). Outliers are distributed uniformly within the annulus between two hyperspheres B(0d, R1) and B(0d, R2), where R1 = 1.5 and R2 = 3. Thus, the distance from any outliers to 0d is at least 1.5, making the outliers separable from the regular data points. A realization in R2 is presented in Figure 1(b), where a data set of size 50 is generated with 6% of it being outliers.\n\nFigure 1: (a) A data set with 45 regular points (black) generated uniformly within a unit circle B((0, 0), 1), and 5 outliers (red crosses) are drawn (uniformly) from another unit circle B((3, 0), 1) that is 3 units away from the first one. (b) A data set that consists of 45 regular points (black) which are distributed uniformly within a unit circle B((0, 0), 1), and 5 outliers (red) that are drawn uniformly in the annular region between B((0, 0), 1.5) and B((0, 0), 3). (c) & (d) The connected components returned by the D-MCG algorithm, the circles are the estimated support for regular data points, which are obtained by SVDD with the polynomial kernel of degree 1.\n\nHere we know the support of the regular data points is hypersphere under both simulation settings, but this information is usually unavailable in real-world applications. Thus, when the support is unknown, we try to estimate the support using Support Vector Data Description (SVDD) [72] by assuming the regular data points are uniformly distributed. SVDD is a one-class classification method that constructs a boundary encompassing all regular points while excluding potential outliers. Similar to Support Vector Machine (SVM), there are many kernels to choose from when conducts SVDD. We adopt# 3.4.1 Mutual Catch Graph with Cluster Catch Digraphs\n\nAlthough the D-MCG algorithm gives promising results on data sets with simple simulation settings, several limitations may affect their performance under more complex settings. These limitations include:\n\n1. The difficulty in determining whether the resulting connected components are outliers or clusters. It is a common problem for most outlier detection algorithms. Decisions can be made based on the cardinality, density, and (spatial) layout of connected components, but this approach is often unreliable and subjective, especially for high-dimensional data sets.\n2. An appropriate distribution F with support S must be specified for the given data set before any simulations. Although we assume F and S are known in the D-MCG algorithm, they are usually unavailable beforehand. One solution would be estimating F and S, and we conduct SVDD to estimate the support S in Section 20, but the performance is mediocre when the data size gets larger. Other possible ways include empirical CDF and kernel density estimation, but they are feasible only when d \u2264 5, especially the latter, which requires large samples for reliable results with high dimensions.\n3. The intensity parameter \u03c9 obtained by simulations in the D-MCG algorithm (line 14 of Algorithm 1) is a global parameter. While relying heavily on \u03c9, this approach may not work well for local outliers or clusters differing drastically in densities.\n\nTo address the abovementioned limitations, we can use the RK-CCDs clustering approach to the given data set and then apply the D-MCG algorithm to each resulting cluster. For each cluster, points within the dominating covering ball are considered part of the cluster rather than outliers. With this approach, we only need to focus on points not covered by the covering balls. Under the MCG obtained from a KS-CCD, any point# Not connected to the dominating covering ball of its respective cluster will be considered an outlier.\n\nBy conducting RK-CCDs first on a given data set X, we can obtain a reasonable partition of clusters by their local distribution. Under the MCG for a cluster, any connected component other than the dominating covering balls is more likely to be outliers than a cluster. We could address the limitation (1) above with this approach. RK-CCDs capture clusters by Spatial Randomness Monte Carlo Test (SR-MCT). Thus, following this notion, we could specify F as an HPP under the null assumption H0, specified in the D-MCG algorithm (Algorithm 1). Thus, limitation (2) could also be resolved.\n\nFinally, since we are applying the D-MCG algorithm on each cluster separately rather than on the entire data set globally, we can get the intensity threshold \u03c9 for each cluster in the data set. In this sense, limitation (3) should no longer be a problem.\n\nWe call this approach the Uniformity-based Cluster Catch Digraphs with Mutual catch graphs (U-MCCD) algorithm.\n\nHowever, obtaining the threshold \u03c9 for each cluster via hundreds or thousands of simulations is computationally expensive. Therefore, we propose a faster alternative, called the Rapid Uniformity-based Cluster Catch Digraphs with Mutual catch graphs (RU-MCCD) algorithm (Algorithm 2), which sets the threshold \u03c9 as the largest density parameter \u03c9 such that the points within the dominating covering ball are connected under the D-MCG. Therefore, we can skip the intensive simulation step.\n\nMore specifically, the algorithm first partitions the data set into clusters using RK-CCDs. For each cluster, it determines the dominating covering ball and creates a KS-CCD with a given density parameter \u03c9j. It then constructs the MCG of this cluster. If the MCG is not connected, the intensity parameter is adjusted (i.e. reduced by #) iteratively until connectivity is achieved. The algorithm identifies outliers as points not connected within the final MCG of each cluster.# Theorem 3.2 (Time Complexity of Algorithm 2)\n\nGiven a data set X \u2192 Rd of size n (d < n). The time complexity of Algorithm 2 is O(n3(log n + N) + n2(d + log n)), where N is the number of simulated data sets for the confidence envelopes of \u0302K(t).# Proof\n\nIn Algorithm 2, we first obtain a partition of P = {P1, P2, ..., Pm} for X with RK-CCDs, which takes O(n3(log n + N) + n2d) time [47]; then, we loop through each partition Pj, constructing KS-CCDs and finding connected components for both Pj and Pj,c. According to Theorem 3.1, and given the fact that distance matrix (which costs O(n2)) is already available with RK-CCDs, the above process runs in O(n2 log n) time at most for all partitions in total. Therefore, Algorithm 2 costs O(n3(log n + N) + n2(d + log n)) time in the worst case, which boils down to O(n3 log n) for fixed d and N.\n\nWe present several synthetic data sets in Figure 2. These data sets vary in several factors, including the sizes of data sets, the number of clusters, and the percentage of outliers within the entire data set. Each cluster\u2019s observations follow a uniform distribution within a unit circle. It is important to note that the number of observations within each cluster may not necessarily be identical since we want to evaluate the effectiveness of the RU-MCCD algorithm (Algorithm 2) on local outliers, which may not be easy to capture when considered globally.\n\nFigure 3 presents the connected components and outliers identified by the RU-MCCD algorithm (Algorithm 2). The RU-MCCD algorithm demonstrates effectiveness across all six data sets, accurately identifying nearly all outliers while excluding regular data points.# Algorithm 2: (RU-MCCD Algorithm, a faster version of the U-MCCD algorithm)\n\nOutlier detection by utilizing RK-CCDs for initial cluster partition and KS-CCDs for determining clusters and outliers, based on density adjustments \u03c90 and #.\n\nInput: \u03c90, # and a dataset X = {x1, x2, ..., xn};\n\nOutput: Clusters and outliers in X;# Algorithm Steps:\n\n1. Partition X into clusters P = {P1, P2, ..., Pm} using RK-CCDs;\n2. for Pj \u2193 P do\n3. Bj \u2194 the dominating covering ball of Pj;\n4. Pj,c \u2194 {x : x \u2193 X \u21d3 Bj};\n5. \u03c9j \u2194 \u03c90;\n6. D(Pj,c) = (V(Pj,c), A(Pj,c)): the KS-CCD of Pj,c (with density parameter \u03c9j);\n7. GM(Pj,c) = (V(Pj,c), E(Pj,c)): the MCG of D(Pj,c);\n8. while GM(Pj,c) is not connected do\n9. \u03c9j \u2194 \u03c9j \u2243 #;\n10. Repeat steps 6 and 7 to update D(Pj,c) and GM(Pj,c);\n11. end\n12. D(Pj) = (V(Pj), A(Pj)): the KS-CCD of Pj (with density parameter \u03c9j);\n13. GM(Pj) = (V(Pj), E(Pj)): the MCG of D(Pj);\n14. Label x / Pj,c \u2193 as an outlier if disconnected in GM(Pj);\n15. end\n\nReturn the constructed clusters P and outliers;\n\nThis is true even in scenarios where the clusters vary in size. Furthermore, the RU-MCCD algorithm successfully connects regular data points outside the dominating covering balls to the main clusters, thereby minimizing the number of false positives.\n\nThe experimental analysis in Section 5 demonstrates that the RU-MCCD algorithm performs effectively on simulated data sets when each cluster is uniformly distributed and the dimensionality d \u2264 10, where the F2-scores exceed 0.9 under most simulation settings. The TPRs (for outlier detection) are generally satisfactory under low dimensions (d = 2, 3, 5), with most TPRs exceeding 90% or even 95%. Additionally, due to the effectiveness of RK-CCDs on clustering with less dimensions, the TNRs under almost all simulation settings are substantially above 95%, even when the size of a data set is as low as 50.\n\nHowever, the performance of the RU-MCCD algorithm begins to decline with more dimensions (d \u2248 20), as shown in Section 5. Although the TPRs tend to increase towards 1 in almost all the cases, the TNRs become substantially lower than those within a lower-dimensional space. Increasing the data size to 1000 does not yield substantial improvement. This can be explained by the increased sparsity of regular data points as d increases, complicating clustering with RK-CCDs and leaving more regular observations uncovered by the dominating covering balls. Additionally, higher dimensions bring considerable intensity differences between a cluster\u2019s center and boundary. As a result, regular data points not covered by the dominant covering balls are unlikely to be connected in an MCG. This phenomenon further decreases the TNRs. Some shortcomings of RK-CCDs also contribute to this decreased performance, which will be discussed in subsequent sections.\n\nIn Section 5, we also conduct the simulations with Gaussian clusters. We aim to in-|(a)|(b)|(c)|\n|---|---|---|\n|(d)|(e)|(f)|\n\nFigure 2: Some simulated uniform data sets, black points are regular data points, red crosses are outliers, (a) 2 clusters, 5% outliers, n = 100. (b) 2 clusters with different sizes, 5% outliers, n = 100. (c) 3 clusters, 10% outliers, n = 100. (d) 3 clusters with different sizes, 10% outliers, n = 100. (e) 4 clusters, 10% outliers, n = 200. (f) 4 clusters with different sizes, 5% outliers, n = 200.# 3.4.2 Mutual Catch Graph with Shape-Adaptive Cluster Catch Digraphs\n\nInvestigate the performance of the M-CCD algorithm (Algorithm 2) when points within a cluster are non-uniformly distributed. As expected, the U-MCCD algorithm yields less satisfactory results with substantially lower TNRs due to the SR-MCT of RK-CCDs, which implies approximately uniformity within each covering ball. However, this is not true for a Gaussian cluster due to nonuniform intensity. As a result, the resulting dominating covering balls tend to be much smaller than the span of Gaussian clusters and are generally located around the center of Gaussian clusters, leaving many regular points uncovered. Additionally, due to the substantial intensity difference over a (multivariate) normal distribution, it is unlikely for the mutual catch digraphs to connect relatively sparse points with the points covered by the dominating covering balls, which generally have much higher intensities.\n\nAs shown in Section 5, with the Gaussian clusters, the RU-MCCD algorithm can result in a substantially low TNR with regular points labeled as outliers due to the nonuniform intensity within a cluster. Although the RU-MCCD algorithm can still identify the correct number of clusters in most cases, the dominating covering balls only cover the densest part of Gaussian clusters, leaving many regular points of lower intensity uncovered. Thus, a single dominating covering ball may not be sufficient to cover a Gaussian cluster entirely.# Figure 3\n\nThe connected components and outliers determined by the RU-MCCD algorithm (Algorithm 2) for the settings in Figure 2. The solid black circles are the dominating covering balls of RK-CCDs.\n\nIn order to address this limitation, an intuitive solution is to increase the number of covering balls for each latent cluster. Thus, we propose a flexible approach using multiple covering balls of RK-CCDs for each cluster. Similar to the RU-MCCD algorithm, we first implement the clustering process with RK-CCDs and obtain a dominating covering ball for each cluster. Although a single dominating covering ball may not be large enough to cover a cluster fully, it can be perceived as the core and location of the corresponding cluster. Therefore, one may want to expand the coverage outward from the core.\n\nWe consider the MCGs based on RK-CCDs for the objective. A pair of points are more likely to be drawn from the same local HPP when connected. Generally, this happens when two close points from the same cluster have similar local intensities and spatial distributions. With this notion, each connected component of an MCG can be considered a latent cluster, which was first proposed by Marchette [48]. However, this approach is not robust to noise. In experimental analysis (not presented here), when noise is in the gaps between different clusters, the above approach may falsely identify two or more clusters as one since noise may \u201cconnect\u201d them together. This is due to the \u201cover-fitting\u201d effect when we use all the covering balls of a data set. Due to this reason, we only consider the points connected to the center point of one of the dominating covering balls, and we extend the coverage of a cluster to the union of the corresponding covering balls. All the points belonging to an enlarged coverage are assigned to the same cluster. Theoretically, this new approach could return clusters with more precise boundaries compared to the RU-MCCD algorithm, especially when the shape of clusters is not spherical or the point intensities.\n\n17# Optimal Number of Clusters\n\nTo determine the optimal number of clusters, we apply an approach similar to the RK-CCDs and KS-CCDs algorithms [47], which employs the silhouette index. All connected components are ranked in decreasing order in terms of their cardinalities. Following the rank, we incrementally add components as valid clusters (starting from the first two components) until the maximum average silhouette index for the whole data set is reached. However, when a group of connected outliers is far from true clusters, they could be identified as small but valid clusters. To handle this problem, we introduce another input parameter Smin. A point set can only be considered a valid cluster when its cardinality is at least Smin. The value of Smin is flexible and can be specified by the user.\n\nGiven a data set, we may have some points (mainly outliers) that are far from others. As a result, they are not in the scope of any existing clusters. Either the partition size they belong to is smaller than Smin, or the corresponding partition has yet to be added as a valid cluster. Therefore, we must find a method that assigns these points to appropriate clusters. We have tried the Local Distance-based Outlier Factor (LDOF) [81] and adapted it differently so that this measurement can be used to determine the optimal cluster for each unlabeled point. Unfortunately, the algorithms utilizing LDOF do not work well in simulation. Therefore, we have to give up this measure.\n\nRecall that Manukyan and Ceyhan [47] introduced the convex distance between an uncovered point and a dominating covering ball and assigned every uncovered point accordingly. We utilize this idea in the new algorithm.# SU-MCCD Algorithm\n\nThe new approach is presented in Algorithm 3 below, and we call it the Shape-adaptive Uniformity-based CCDs with Mutual catch graph (SU-MCCD) algorithm.# Figure 4\n\nFigure 4 presents the realization of the SU-MCCD algorithm on a synthetic data set (Figure 4(a)) with two Gaussian clusters (black points) of different intensities and a few outliers (red crosses); Figure 4(b) presents the dominating covering balls for the two clusters, which are not large enough to cover all the regular points; Figure 4(c) shows all the covering balls (dashed lines) of the points that are connected to the center of any dominating covering balls under the MCG of RK-CCDs, the union of these covering balls exhibits an extension of cluster covers. Figure 4(d) presents the connected components and outliers identified by the SU-MCCD algorithm, by using multiple covering balls for each cluster, it manages to connect most regular points from the same cluster and excludes all the outliers.# Theorem 3.3\n\nThe Time Complexity of Algorithm 3 Given a data set X \u2192 Rd of size n (with d &lt; n being fixed). The time complexity of Algorithm 3 is O(n3 (log n + N) + n2 (d + log n)) (the same order as Algorithm 2), where N is the number of simulated data sets for the confidence envelopes of \u0302K(t).# Proof\n\nWhen implementing Algorithm 3, we need to construct an RK-CCD and obtain dominating covering balls for X first, which takes O(n3 (log n + N) + n2 d) time [47]. Constructing the MCG of the RK-CCD and extending the coverage from each dominating covering ball costs O(n2) time at most. Each time we add a new cluster, we need to re-partition the data set. Finding the nearest cluster based on the relative distance needs O(n) at most for each xi \u2193 X, thus a maximum of O(n2) time for the entire data set at each iteration and O(n3) time for all iterations. Updating and maximizing the average silhouette measure take less than O(n3) time [47]. Finally, similar to Theorem 3.2, looping through each partition in P to identify outliers takes no more than O(n2 log n) time. Hence, Algorithm# Figure 4: A illustration of the SU-MCCD algorithm.\n\n3 runs in O(n3 log n) when n \u21d2 \u21d1.3 (log n + N ) + n2 (d + log n)) time. Note that for fixed d and N, Algorithm 3 runs in O(n\n\nIn Section 5, we evaluate the SU-MCCD algorithm\u2019s performance under the same simulation settings as the RU-MCCD algorithm. And we set Smin (the minimal size of a cluster) to be half of the contamination level.\n\nThe results are summarized from Tables 2 to 5. Generally, when points within each cluster are uniformly distributed (Tables 2 and 3), the performance of the SU-MCCD algorithm is comparable to or slightly better than that of the RU-MCCD algorithm under lower dimensions (d \u21d0 5). Additionally, it can achieve substantially higher TNRs when d = 10 and 20. Besides, this flexible approach can identify all the outliers (i.e., TPRs are near 100%) while maintaining relatively high TNRs.\n\nUnder the second simulation setting, where clusters are (multivariate) normally distributed (Tables 4), the SU-MCCD algorithm yields considerably higher TNRs when d \u21d0 10 compared to the RU-MCCD algorithm. This can be attributed to the flexibility of the SU-MCCD algorithm in capturing clusters with arbitrary shapes or uneven intensities and returning precise boundaries.\n\nFurthermore, the SU-MCCD algorithm is robust against the contamination level, as shown in the simulation study in Section 5. A higher contamination level results in high-intensity outliers, and a masking problem typically arises in such cases due to a substantial increase in small outlier groups. Thanks to the mechanism that filters small clusters, the\n\n19# Algorithm 3: (SU-MCCD Algorithm)\n\nOutlier detection using RK-CCDs for cluster formation and KS-CCDs for density-based validation, incorporating Smin for minimum cluster size, with initial density \u03c90 and decrement # as in Algorithm 1. Adapted for arbitrary-shaped clusters.# Input:\n\n\u03c90, #, k, Smin, and a data set X = {x1, x2, ..., xn};# Algorithm Steps:\n\n1. Construct D(X) = (V(X), A(X)): a RK-CCD of X;\n2. Obtain the dominating covering balls from D(X);\n3. Construct GM(X) = (V(X), E(X)): the MCG of D(X);\n4. P = {P1, P2, ..., Ps} \u2194 the partition obtained by extending the coverage from each dominating covering ball, ordered from high to low by size (is by the number of points of a partition);\n5. Incrementally form clusters C from P, assigning isolated points based on smallest relative distance, until maximizing sil(C) or partition sizes drop below Smin;\n6. C = {C1, C2, ..., Cm} \u2194 the clusters obtained in step 4 (which also serves as a partition of X);\n7. for Cj \u2193 C do\n8. &nbsp;&nbsp;&nbsp;&nbsp;Cj,c \u2194 Pj (the cluster Cj is constructed from the partition Pj);\n9. &nbsp;&nbsp;&nbsp;&nbsp;\u03c9j \u2194 \u03c90;\n10. &nbsp;&nbsp;&nbsp;&nbsp;Construct D(Cj,c) = (V(Cj,c), A(Cj,c)): a KS-CCD of Cj,c (with density parameter \u03c9j);\n11. &nbsp;&nbsp;&nbsp;&nbsp;Construct GM(Cj,c) = (V(Cj,c), E(Cj,c)): the MCG of D(Cj,c);\n12. &nbsp;&nbsp;&nbsp;&nbsp;while GM(Cj,c) is not connected do\n13. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u03c9j \u2194 \u03c9j \u2243 #;\n14. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Repeat lines 9 and 10 to update D(Cj,c) and GM(Cj,c);\n15. &nbsp;&nbsp;&nbsp;&nbsp;end\n16. Construct D(Cj) = (V(Cj), A(Cj)): the KS-CCD of Cj (with density parameter \u03c9j);\n17. Construct GM(Cj) = (V(Cj), E(Cj)): the MCG of D(Cj);\n18. Under GM(Cj), for each x / Cj,c, x is labeled as an outlier if it is not connected to any vertices in Cj,c;\n19. end\n20. Return the constructed clusters C and outliers;\n\nSU-MCCD algorithm can correctly label small outlier groups whose sizes are smaller than Smin as outliers. Furthermore, as an input parameter, Smin is relatively easy to specify in various disciplines (e.g., with a pilot study).\n\nHowever, similar to the RU-MCCD algorithm, the SU-MCCD algorithm tends to underperform in higher dimensions (d \u21d4 20). Like the RU-MCCD algorithm, the SU-MCCD algorithm still employs RK-CCDs for clustering, inheriting the same limitations. When the dimensionality is high, the covering balls of RK-CCDs tend to be much smaller, making it challenging for any two points to connect under the MCG, even if they are nearest neighbors. Consequently, the simulation results of the SU-MCCD algorithm are disappointing when d > 20 and close to or even the same as the RU-MCCD algorithm when d \u21d4 50 as almost every point is isolated in the MCG. Additionally, increasing the data size to as large as 1000 only provides a small improvement.# 3.5.1 Complete Spatial Randomness and the Nearest Neighbor Distance\n\nThe Monte Carlo experiments conducted starting from Sections 5 show that the outlier detection algorithms based on RU-MCCDs and SU-MCCDs typically deliver low TNRs when applied to high-dimensional data sets, particularly when d \u21d4 10. A similar limitation encountered by these outlier detection algorithms in higher dimensions is the size of the dominating covering balls returned by RK-CCDs. These balls are not sufficiently large and leave too many regular observations uncovered. And unfortunately, this shortcoming is only partially addressed by the subsequent D-MCG algorithm.\n\nWe have identified several limitations of RK-CCDs that eventually lead to the shortcomings mentioned above (on high-dimensional data sets). Firstly, recall that to find the optimal radius rxi for each covering ball B(xi, rxi), RK-CCDs expand the size of B(xi, rxi) from the center xi incrementally until the points captured within can no longer pass SR-MCT (Spatial Randomness Monte Carlo Test). It is known that Ripley\u2019s K function can be used to describe the second-order moments of a point process [62]. The SR-MCT was developed based on one of Ripley\u2019s K functions (K(t)), which measures the number of pairs of points whose distance is less than t within a window or a region of interest. However, the first point xi to be involved in the test is not random as it is always the center of B(xi, rxi). Thus, any successive points to be covered will be less than 1 unit distance (scaled by radius) apart from xi. It may not be a big issue when d is small because, inside a unit ball, it is expected to see a pair of points whose distance is less than 1 under CSR with sufficiently high probability; adding a few more such non-random pairs would not considerably affect the validity of the test with a high probability. However, close pairs of points become extremely rare when d is large. For example, the probability that two random points are at most 1 unit away is approximately 0.122 (estimated by simulation) when d = 10; this probability decreases to roughly 0.0222 when d = 20. Under high-dimensional settings, adding a few more close non-random pairs can make a huge difference. Therefore, the test conducted on these covering balls is no longer accurate.\n\nExcept for the non-random center xi, the point-wise confidence band for K(t) raises another problem on the test. In RK-CCDs, tmax was specified to be half of the radius of a unit sphere, namely 0.5 [47]. The commonly chosen values for t are 0.1, 0.2, ..., 0.5. The point-wise confidence band (for K(t)) built on these fixed values is only appropriate when d is small because the distances between any points increase substantially as d increases. Consequently, the small and fixed t values are no longer suitable.\n\nSome potential ways to improve RK-CCDs include the following: (1) Remove the center point xi when conducting the SR-MCT on a covering ball B(xi, rxi). (2) Make the values for t dynamically adaptable to d. The first should be easy to implement, while the second may be challenging. Determining appropriate t values for different dimensions is difficult because the distribution of Ripley\u2019s K function is unknown, and so are the theoretical quantiles, whose values change with d.\n\nNevertheless, we have attempted to obtain appropriate values for t through Monte Carlo simulations. First, we simulate M data sets of the same size as the given data set. Then, with each simulated data set, we record the distances between any two points and aggregate these distances from all data sets into a sample. Finally, we take the 10%, 20%, 30%, 40%, and 50% quantiles of the sample and set these quantiles as the values for t. The experimental results (not presented here) exhibit substantial improvement but are still not good enough, and determining the values of t is another hurdle against RK-CCDs in real-life applications. Therefore, the test based on Ripley\u2019s K function seems unsuitable.# for high-dimensional clustering.\n\nTo address this shortcoming, we introduce an alternative way to test CSR using the Nearest Neighbor Distance (NND) and employ the CCDs with NND for outlier detection. First, we review NND and the existing methods for testing CSR.\n\nSuppose we have a set of i.i.d points X = {x1, x2, ..., xn} in a subspace of Rd with a specified intensity \u03b5. Let di be the distance of xi to its nearest neighbor. \u2211n=0 di. To measure how much X departs from randomness, we also want to know the expected mean NND of X (denoted as \u03bcd) under CSR. Fortunately, when d = 2, Clark and Evans [16] have shown the following,\n\n\u03bcd = 2\u2196\u03b5, 1 \u03d6\u0304 = 0.26136,\u2196\u03b5\n\nwhere \u03d6\u0304 is the standard deviation of d\u0304.\n\nThe significance of the difference between \u03bcd and d\u0304 can be measured by the widely used Gaussian Z-score when n is sufficiently large [16],\n\nZ = d\u0304 \u2243 \u03bcd.\n\n\u03d6\u0304d\n\nClark and Evans [17] had also generalized the expressions in Equation (4) to arbitrary dimensionality as\n\n\u03bcd = $(d/2 + 1) 1+1/d, \u03b51/d \u03f11/2 \u03d6\u0304 = $(d/2 + 1) 1/d ((2/d + 1) \u2243 $(d/2 + 1) 2 ) 1/2.\n\nAlthough the normality test conducted by measuring d\u0304 is convenient and easy to interpret for non-statisticians, its accuracy is questionable when the sample size is too small. Actually, the distribution of d\u0304 is skewed to the left, and its skewness cannot be ignored when n is relatively small [16]. In addition, Besag and Diggle [8] argued that Clark and Evans\u2019s derivation of \u03bcd and \u03d6\u0304 ignored the fact that the NNDs d1, d2, ..., dn are not i.i.d. Therefore, they proposed an alternative, more reliable way by employing the Monte Carlo test [8]. They simulate m data sets of size n that are uniformly distributed, the mean NND values d1, d2, ..., dm can be obtained for the m simulated data sets. Then, the significance level of d\u0304 can be measured by its quantile in the m simulated mean NND values. This Monte Carlo test for CSR is easy to conduct and does not require formulas or parameters. It is also well adapted to subspaces with any shape, as correction for edge effects is not needed [8]. With these advantages, we consider using Besag and Diggle\u2019s Monte Carlo approach to test CSR rather than calculating theoretical values of the quantiles.# 3.5.2 Mutual Catch Graph with the Nearest Neighbor Cluster Catch Diagrams\n\nWe propose another outlier detection method based on CCDs, which conducts the SR-MCT with the NND instead of Ripley\u2019s K function. However, Clark\u2019s and Besag\u2019s approaches [8, 16, 17] only consider the mean NNDs when measuring the significance of outlyingness, which is not robust and can be highly affected by a few extreme values, especially with in lower-dimensional space where the distances between points could be very different. For example, a group of observations consists of a cluster and a few outliers can still pass the SR-MCT if those outliers are far from the cluster. Thus, when implementing# The Monte Carlo Test and Clustering Approaches\n\nThe Monte Carlo test, we consider using the median and mean of NND simultaneously when conducting the SR-MCT. Furthermore, we make three additional modifications to the previous Monte Carlo test:\n\n1. The center point *xi of the covering ball B(x, rxi)* will not be used in the test.\n2. When the dimensionality *d is large, larger covering balls are preferred to compensate for the increasing sparseness. Thus, we offer the option to test the candidate values for the radius in descending order and stop decreasing the radius once the H0* (i.e., CSR) is not rejected.\n3. We make the test lower-tailed as we are not interested in the upper tail when the point pattern is significantly \u201cregular\u201d.\n\nThe Monte Carlo test is presented in Algorithm 4.# Algorithm 4: Spatial Randomness Monte Carlo Test (SR-MCT) Using NND\n\nInput: A hypersphere in *Rd with radius r covering i.i.d point set Xsub of size nsub from X*;\n\nOutput: Decision on CSR rejection for *Xsub at level \u03d1*;# Algorithm Steps:\n\n1. Compute mean *d\u0304 and median d of nearest neighbor distance (NND) in Xsub, scaled by r*;\n2. Simulate *m sets within a unit sphere in Rd, each of size nsub*, under CSR;\n3. Calculate mean *{ di\u0304 } and median { d }* NNDs for simulations;\n4. Determine empirical p-values *p1 for d\u0304 and p2 for d, then order p(1) \u21d0 p(2)*;\n5. Reject CSR for *Xsub if p(1) \u21d0 \u03d1/2 or p(2) \u21d0 \u03d1* using Holm\u2019s Step-Down Procedure [77];\n\nWith the above construction, we propose a clustering approach based on the NND as Algorithm 5, and call it Uniformity- and Neighbor-based CCD (UN-CCD) clustering algorithm. The UN-CCD clustering algorithm identifies cluster centers in a data set *X using Cluster Catch Digraphs (CCDs) based on the Nearest Neighbor Distance (NND). For each point in X, the algorithm calculates the distances to all other points and sorts them. A Monte Carlo test is performed on increasing radii until rejection at a specified level \u03d1*. Using the determined radii, a CCD is constructed, and an approximate minimum dominating set is found. An intersection graph is created from this set, and another minimum dominating set is found using a greedy algorithm, aiming to maximize the silhouette score. The final set of cluster centers are returned.# Theorem 3.4: Time Complexity of Algorithm 5\n\nGiven a data set *X \u2192 Rd of size n, the time complexity of Algorithm 5 is O((N + d)n2), where N* represents the number of simulated data sets.# Proof\n\nThe UN-CCDs are similar to KS-CCDs and RK-CCDs. The only difference between them is the way to determine *rxi for each xi \u2193 X*.\n\nFor each simulated data set of size *n, the median and mean of the NNDs can both be obtained in O(n) time (e.g., the median can be found by the Quick-select Algorithm [63], which only costs O(n) time). Repeating for subsets of sizes 2, 3, ..., n (take one subset for each size) takes less than O(n2) time, that is O(N n2) time in total for N* simulated data sets.\n\nConsidering the given data set *X, the distance matrix can be computed in O(dn2) time. For each xi \u2193 X, sorting the distances D(xi) takes O(log n)* time, and we need# Algorithm 5: (UN-CCD Clustering Algorithm) Cluster Catch Digraphs based on the Nearest Neighbor Distance (NND).# Carlo test with NND.\n\nInput: \u03d1, data set X = {x1, x2, ..., xn};\n\nOutput: Cluster centers of X;# Algorithm Steps:\n\n1. foreach xi \u2193 X do\n2. Calculate distances D(xi) = {d(xi, xj) | xj \u2193 X, xi \u2198= xj};\n3. foreach distance r(j) in D(xi) sorted do\n4. Perform Monte Carlo test (Algorithm 4) on B(xi, r(j));\n5. if test rejected at level \u03d1 then\n6. Set rxi = r(j\u21941); break;\n7. end\n8. end\n9. Construct a CCD D = (V, A) using the pre-determined radii;\n10. Find the approximate minimum dominating set \u02c6(V) in D with the Greedy Algorithm 2;\n11. Create intersection graph GM D = (VM D, EM D) with \u02c6(V);\n12. Find an approximate minimum dominating set \u02c6(GM D) in GM D using the Greedy Algorithm 3 with a score function measuring the number of points covered, stops when the average silhouette index sil(P) is maximized;\n13. Output \u02c6(GM D) as cluster centers;\n\n\u03d1 is the level of the Monte\n\nO(n) time at most to obtain the median and mean of the NNDs of the points covered by B(xi, r(j)). Thus, a total of O(n2) time is needed for all r(j) \u2193 D(xi). Therefore, constructing a UN-CCD for the entire data set takes O(n(log n + n2)) time. Finding an approximate minimum dominating set \u02c6(V) by the Greedy Algorithm 2 costs O(n2) time in worst cases. Finally, we can construct GM D and \u02c6(GM D) in O(n3) time [47]. Therefore, Algorithm 5 runs in O((N + d)n2 + n3) time. Note that if N and d are fixed, the time complexity reduces to O(n3).\n\nAdditionally, we propose an outlier detection algorithm based on UN-CCDs as Algorithm 6 and refer to it as the Uniformity- and Neighbor-based CCD with mutual catch graph (UN-MCCD) algorithm. Although its acronym is suggestive, we want to emphasize that it is based on UN-CCDs to distinguish it from one of the previous approaches, the RU-MCCD algorithm (Algorithm 3). Furthermore, it is worth noting that the UN-MCCD algorithm is the same as the U-MCCD algorithm, except that RK-CCDs are replaced by UN-CCDs for clustering.# Theorem 3.5 (Time Complexity of Algorithm 6)\n\nGiven a data set X \u2192 Rd of size n (d < n). The time complexity of Algorithm 6 is O((N + d + log n)n2 + n3), where N represents the number of simulated data sets when constructing UN-CCDs.# Proof\n\nFrom Theorem 3.4, we know UN-CCD partitions X in O((N + d)n2 + n3) time. Similar to time in the worst cases. Therefore, Algorithm 6 runs in O((N + d + log n)n2 + n2 log n) time, building an MCG for each partition and identifying outliers takes O(n3) time, the same as UN-CCD (Algorithm 5).# Algorithm 6: (UN-MCCD Algorithm)\n\nAn outlier detection algorithm with UN-CCDs and KS-CCDs, incorporating \u03c90 and # adjustments as in Algorithm 1.# Input:\n\n\u03c90, # and a data set X = {x1, x2, ..., xn};# Algorithm Steps:\n\n1. The same as Algorithm 2, except that RK-CCDs are replaced by UN-CCDs for clustering (line 1 of Algorithm 2).\n\nIn Section 5, we evaluate the average performance of the UN-MCCD algorithm and compare its results with those of the RU-MCCD algorithm. We perform Monte Carlo simulations under the same two simulation settings with uniform clusters and Gaussian clusters, respectively. The performance of them are summarized in Tables 2 to 5.\n\nThe simulation results from both simulation settings show that the SU-MCCD algorithm outperforms the RU-MCCD algorithm under most simulation settings. In the first simulation setting, where points in each cluster are uniformly distributed following CSR, the UN-MCCD algorithm performs comparable or better than the RU-MCCD algorithm when d \u2264 5. Under most simulation cases, the TPRs and TNRs are much higher than 0.95. Notably, both TPRs and TNRs are relatively insensitive to the number of clusters, the size of each cluster, and even the contamination level (which are shown in Section 5.2), which we will discuss in detail.\n\nWhen compared with the RU-MCCD algorithm with d \u2248 10, the UN-MCCD algorithm reduces the number of false negatives substantially while still maintaining high TPRs (\u2199 1), the TNPs remain acceptable even when d = 20, as most of them are around 0.9.\n\nThe advantages of the UN-MCCD algorithm are even more apparent under the second simulation setting, where it outperforms the RU-MCCD algorithm in nearly all the dimensions we considered. However, we do not expect the UN-MCCD algorithm to achieve as high TNRs as in the first simulation setting because UN-CCDs are conducting SR-MCT while Gaussian clusters are distributed nonuniformly, which deviates from CSR.# 3.5.3 Shape-Adaptive Uniformity- and Neighbor-Based CCD with Mutual Catch Graph\n\nRecall that in the previous section, we adapted the RU-MCCD algorithm to the cases where the cluster\u2019s shapes are arbitrary, or the intensities within clusters are nonuniform. We called the resulting algorithm the SU-MCCD algorithm. Different from the RU-MCCD algorithm that uses only one covering ball for each cluster, the SU-MCCD algorithm extends the coverage of each dominating covering ball by finding points that are connected to the center in the MCG obtained from an RK-CCD, and the union of their covering balls represents the scope of a latent cluster. With the above construction, we find the optimal number of clusters (connected components) by maximizing the silhouette index. Meanwhile, we assign each isolated point to a \u201cnearest\u201d cluster with the smallest relative distance. Furthermore, we have introduced an input parameter, Smin, representing the minimal size of a cluster. The Smin value should be easy to specify in real-life applications.\n\nHowever, as discussed earlier, the SU-MCCD algorithm\u2019s performance shows little or no improvement when d is large (see Tables 2 to 5) due to the limitations of RK-CCDs: the covering balls are too small for any two points to be connected in the MCG even if they are nearest neighbors.Fortunately, we were able to fix these limitations by introducing another version of CCDs that uses the NND to conduct SR-MCT, and the resulting approach is called the UN-MCCD algorithm. Like the SU-MCCD algorithm, we modify the UN-MCCD algorithm in a similar fashion, hence the name SUN-MCCD (Shape-adaptive Uniformity- and Neighbor-based CCD with Mutual catch graph) algorithm, presented as Algorithm 7 below. SUN-MCCDs differ from SU-MCCDs only in the clustering phase, and we expect this new algorithm to outperform the SU-MCCD algorithm.# Algorithm 7: (SUN-MCCD Algorithm)\n\nOutlier detection using RK-CCDs for cluster formation and KS-CCDs for density-based validation, incorporating Smin for minimum cluster size, with initial density \u03c90 and decrement # as in Algorithm 1. Adapted for arbitrary-shaped clusters.\n\nInput: \u03c90, #, k, Smin, and a data set X = {x1, x2, ..., xn};\n\nOutput: Clusters and outliers of X;# Algorithm Steps:\n\n1. The same as Algorithm 3, except that RK-CCDs are replaced by UN-CCDs for clustering (line 1).# Theorem 3.6 (Time Complexity of Algorithm 7)\n\nGiven a data set X \u2192 Rd of size n (d < n), the time complexity of Algorithm 7 is O((N + d + log n)n2 + n3), where N represents the number of simulated data sets when constructing UN-CCDs.# Proof\n\nAs shown in Theorem 3.4, constructing a UN-CCD for X costs O((N + d)n2 + n3) time. According to Theorem 3.3, the remaining steps take O(n3 + n2 log n + n2). So, Algorithm 7 requires O((N + d + log n)n2 + n3) time to capture outliers, and it reduces to O(n3) for fixed N and d.\n\nSimilar to the previous Monte Carlo experiments, we assess the average performance of the SU-MCCD algorithm and compare it with the SU-MCCD algorithm that is based on RK-CCDs. We perform Monte Carlo simulations using the same two settings presented in Section 5. In the first setting, the points of each cluster are uniformly distributed, while in the second simulation setting, they follow Gaussian distributions. The results are summarized from Tables 2 to 5.\n\nAccording to the simulation results, the SUN-MCCD algorithm performs well. Under most simulation settings, the TPRs are close to 1, which is comparable to the previous algorithms. Additionally, when compared to the UN-MCCD algorithm, the SUN-MCCD algorithm delivers higher TPRs when the size of a data set is large enough or larger TNRs when the dimensionality d is relatively large. We will discuss its performance in detail in the next section.# 4 The Space Complexity of CCD-Based Algorithms\n\nIn this section, we analyze the space complexity of all the CCD-based algorithms, which determines the memory consumption. We prove that each algorithm requires O(n2) space in the following.# The KS-CCD, RK-CCD, and UN-CCD algorithms:\n\n1. Data storage: The space requirement for a d-dimensional data set is O(dn).# Distance matrix:\n\nRequire O(n2) space. Computing and storing pairwise distances between all points requires O(N dn + N n2) space, which boils down to O(n2) when N and d are fixed. The space requirements for the n upper envelopes of the Ripley\u2019s K function [47], and the 2n confidence intervals of the mean and median NNDs, are both O(n).# Radii of the covering balls:\n\nThere are n covering balls in total, whose radii require O(n) space to store.# Misc:\n\nThe two approximate MDSs (i.e., \u02c6 and S(G M D)) require O(n) space at most. The silhouette index of the data set takes O(n) in memory.\n\nIn summary, the space complexities of the KS-CCD, RK-CCD, and UN-CCD algorithms are O(n2). This complexity arises primarily from the need to store distance matrices.# The RU-MCCD and UN-MCCD algorithms:\n\n1. Clustering: Constructing RK-CCDs or UN-CCDs for clustering takes O(n2) space.\n2. D-MCGs: The D-MCG algorithm involving constructing KS-CCDs for each cluster, whose space complexity is O(n) at most.\n\nTherefore, the space complexities of the U-MCCD and UN-MCCD algorithms are O(n2).# The SU-MCCD and SUN-MCCD algorithms:\n\nBoth algorithms are similar to their prototype except that they use multiple covering balls for each cluster, which does not take additional memory. Thus, the space complexity remains O(n2).\n\nBesides, we summarize the time and space complexity of all CCD-based algorithms in Table 1, which we have proven.\n\n|Algorithms|Time Complexity|\n|---|---|\n|KS-CCDs|O(n3 (log n + N) + n2 d)|\n|RK-CCDs|O(nO((N + d)n2 + n3))|\n|UN-CCDs|O(n3 (log n + N) + n2 (d + log n))|\n|RU-MCCDs|O(n3 (log n + N) + n2 (d + log n))|\n|SU-MCCDs|O((N + d + log n)n2 + n3)|\n|UN-MCCDs|O((N + d + log n)n2 + n3)|\n|SUN-MCCD| |# 5.1 Monte Carlo Experiments: General Settings\n\nIn this section, we conduct Monte Carlo experiments under various simulation settings to evaluate the performance of the new CCD-based outlier detection algorithms.# Empirical Analysis Under Focus Settings\n\nExperiments are conducted under general settings that involve many factors (e.g., dimensionality, data set sizes, cluster volumes, etc.) whose values vary among different data sets. In the next section, we will conduct an empirical analysis focusing on only one factor each time.\n\nWe will begin with the simulation settings where points within each cluster are uniformly distributed, and we will refer to them as uniform clusters in the rest of the section. The simulated data sets involve two clusters, each exhibiting a standard spherical shape whose radius is a uniform random value ranging from 0.7 to 1.3. We aim to assess whether our proposed algorithms can effectively identify local outliers that may not be prominent when considered globally. Additionally, we will consider data sets with dimensionality (d) as high as 100, which is particularly challenging as the distance between an outlier and a regular point gets closer to that of any two points due to the low spatial intensity with more dimensions. This effect is particularly pronounced when the size of a data set is small.# Simulation Settings\n\nEach simulation setting varies in:\n\n1. The dimensionality (d) of the simulated data sets with values 2, 3, 5, 10, 20, 50, 100;\n2. The size of data sets (n) with values 50, 100, 200, 500, 1000;\n\nOn the other hand, all the simulated data sets have the following common features:\n\n1. The cluster sizes are equal (although the volume of the supports can be different);\n2. The radius of each cluster is randomly chosen between 0.7 and 1.3;\n3. The centers of clusters are:\n- \u03bc1 = (3, ..., 3),\n- \u03bc2 = (6, 3, ..., 3);\n4. The proportion of outliers over the entire data set is fixed to 5%;\n5. The outlier set Coutlier is generated uniformly within a much larger hypersphere with radius 5, centered at the mean of the cluster center, and each outlier is at least 2 units away from any cluster center.\n\nTwo realizations of the simulation settings in 2-dimensional space with data sizes of 100 and 200 are presented in Figure 5.\n\nWe repeat each subsequent simulation setting 1000 times to ensure precise and meaningful evaluation. The average TPR for outliers (i.e., the percentage of outliers captured) and TNR for regular data points (i.e., the percentage of regular points falsely identified as outliers) are recorded. However, outlier detection is essentially a classification problem over highly imbalanced data sets. Therefore, in this study, we also use BA and F\u03b5-score with \u03c2 = 2, indicating recall is two times as important as precision.\n\nConsider the RU-MCCD and SU-MCCD algorithms, which depend on RK-CCDs for clustering. Although RK-CCDs are parameter-free, the levels of the SR-MCT based on Ripley\u2019s K function can be adjusted. Under moderate and high dimensions, notice that the average inter-cluster distance between any two points increases substantially, and the odds that points located near the border of clusters are substantially higher. As a result, covering balls with much higher volumes is generally preferred. Therefore, we choose the optimal levels of \u03d1 for each dimensionality d. We set \u03d1 to 1% when d < 10, and 0.1% when d \u2265 10. This adjustment boosts the performance of both RK-CCD based algorithms under higher-dimensional space. Similarly, we tune the levels of the SR-MCT based on.# Figure 5\n\nTwo realizations of the simulation settings described in Section 5.1 with n = 100 and 200 respectively. Each data set has 2 clusters of the same size but different intensities. Black points are regular data points, and red crosses are outliers.\n\nNN distances to optimize the performance of the UN-MCCD and SUN-MCCD algorithms, and we set \u03d1 to {15%, 10%, 5%, 1%, 0.1%, 0.1%, 0.1%} as the dimensionality d increases from 2 to 100. The simulation results are summarized in Tables 2 and 3, providing comprehensive information. For better visualization, we summarize the simulation results in the following line plots (Figures 6 and 7), illustrating the trend of the performance with varying dimensions and data sizes.\n\nWe first focus on the simulation results under low- and moderate-dimensional space, which are (d \u2264 20) presented in Tables 2 and 3.\n\nThe RU-MCCD algorithm delivers satisfactory performance considering the percentage of outliers captured. Most of the TPRs are well above 0.9 or even 0.95 and equal to 1 when d = 10, 20; additionally, due to the effectiveness of RK-CCDs on clustering in low dimensions (d = 2, 3, 5), the TNRs are well above 0.95, even when the number of observations is as low as 50. Therefore, the RU-MCCD algorithm also delivers high BAs and F2-score under those dimensions, most of which are above 0.9. However, there are some exceptions: (1) when d = 2, 3, the effectiveness of the RU-MCCD algorithm declines substantially with a higher number of observations (e.g., the TPRs of the RU-MCCD algorithm are 0.986, 0.986, 0.931, 0.814, 0.681 when d = 2 as n increases). The declining performance is due to the increasing intensities of outliers, especially in the cases with fewer dimensions (d = 2, 3) where the volume or the area of the support is relatively small. With high intensities, RK-CCDs may falsely construct clusters for a bunch of close outliers and perceive them as regular (i.e., non-outlier) observations, which is called the masking problem in outlier detection. Thus, all four measures reduce as the number of observations increases. The lowest readings are observed when n = 1000 and d = 2, each falling below 0.9. (2) While most TNRs are near 1, they fall substantially and are less than 0.9 when d = 20. Increasing the number of observations provides little help. We have discussed its reason in Section 3.5.2. In short, several drawbacks of SR-MCT based on the Ripley\u2019s K function lead to the limitation, which is negligible when d is small but is greatly exacerbated as d increases when RK-CCDs provide much smaller covering balls and leaves many regular observations uncovered. Therefore, although the BAs are still above 0.9 when d = 20, the F2-scores drop to less than 0.7 since F2-score is much more sensitive to TNR and has less tolerance on false positives.\n\nNext, we consider the performance of the UN-MCCD algorithm. UN-CCDs work# The Size of Data Sets\n\n|Data Sets| | | | |Size of Data Sets| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | |50|100| |200| |500| |1000| | | | | |\n|U-MCCDs|0.986|0.989|0.986|0.993|0.931|0.995|0.814|0.997|0.681|0.999| | | | |\n|SU-MCCDs|0.973|0.993|0.994|0.998|0.997|0.999|1.000|1.000|1.000|1.000| | | | |\n|UN-MCCDs|0.992|0.979|0.988|0.983|0.961|0.988|0.935|0.993|0.930|0.995| | | | |\n|SUN-MCCDs|0.979|0.987|0.995|0.992|1.000|0.994|1.000|0.996|1.000|0.997| | | | |\n|U-MCCDs|0.995|0.980|0.987|0.985|0.967|0.992|0.926|0.997|0.872|0.998| | | | |\n|SU-MCCDs|0.988|0.995|0.997|0.998|1.000|0.999|1.000|1.000|1.000|1.000| | | | |\n|UN-MCCDs|0.997|0.979|0.991|0.986|0.983|0.991|0.963|0.996|0.922|0.998| | | | |\n|SUN-MCCDs|0.991|0.990|0.998|0.997|1.000|0.998|1.000|0.999|1.000|0.999| | | | |\n|U-MCCDs|0.998|0.950|0.999|0.972|1.000|0.988|0.999|0.996|0.996|0.999| | | | |\n|SU-MCCDs|0.998|0.978|1.000|0.989|1.000|0.996|1.000|0.999|1.000|1.000| | | | |\n|UN-MCCDs|0.997|0.975|0.997|0.984|0.996|0.992|0.997|0.997|0.996|0.999| | | | |\n|SUN-MCCDs|0.997|0.994|1.000|0.997|1.000|0.999|1.000|1.000|1.000|1.000| | | | |\n|U-MCCDs|1.000|0.935|1.000|0.957|1.000|0.976|1.000|0.993|1.000|0.999| | | | |\n|SU-MCCDs|1.000|0.961|1.000|0.975|1.000|0.991|1.000|0.996|1.000|1.000| | | | |\n|UN-MCCDs|1.000|0.973|1.000|0.986|1.000|0.994|1.000|0.999|1.000|1.000| | | | |\n|SUN-MCCDs| | | | |1.000|0.998|1.000|0.999|1.000|0.999|1.000|1.000|1.000|1.000|\n|U-MCCDs|1.000|0.846|1.000|0.865|1.000|0.883|1.000|0.861|1.000|0.850| | | | |\n|SU-MCCDs|1.000|0.881|1.000|0.896|1.000|0.924|1.000|0.908|1.000|0.893| | | | |\n|UN-MCCDs|1.000|0.951|1.000|0.971|1.000|0.984|1.000|0.992|1.000|0.994| | | | |\n|SUN-MCCDs|1.000|0.974|1.000|0.983|1.000|0.992|1.000|0.996|1.000|1.000| | | | |\n|U-MCCDs|1.000|0.567|1.000|0.542|1.000|0.534|1.000|0.534|1.000|0.543| | | | |\n|SU-MCCDs|1.000|0.568|1.000|0.542|1.000|0.534|1.000|0.534|1.000|0.542| | | | |\n|UN-MCCDs|1.000|0.659|1.000|0.681|1.000|0.708|1.000|0.723|1.000|0.733| | | | |\n|SUN-MCCDs|1.000|0.682|1.000|0.727|1.000|0.794|1.000|0.824|1.000|0.864| | | | |\n|U-MCCDs|1.000|0.550|1.000|0.540|1.000|0.529|1.000|0.521|1.000|0.514| | | | |\n|SU-MCCDs|1.000|0.550|1.000|0.541|1.000|0.530|1.000|0.522|1.000|0.515| | | | |\n|UN-MCCDs|1.000|0.131|1.000|0.161|1.000|0.228|1.000|0.456|1.000|0.434| | | | |\n|SUN-MCCDs|1.000|0.131|1.000|0.161|1.000|0.228|1.000|0.456|1.000|0.435| | | | |\n\nTable 2: Summary of the TPR and TNR of all the CCD-based outlier detection algorithms, with the simulation settings elaborated in Section 5.1.\n\nsimilarly to RK-CCDs except for the SR-MCT. Instead of using the Ripley\u2019s K function, UN-CCDs conduct SR-MCT based on the average and median NND, which avoids RK-CCDs\u2019 shortcomings. Therefore, the UN-MCCD algorithm performs better than the RU-MCCD algorithm across all the simulation settings. However, since both algorithms share almost identical mechanisms, the UN-MCCD algorithm captures almost all outliers with slight errors when d = 2, 3, and the lowest TPR of 0.930 is observed when d = 2 and n = 1000, where BA and F2 \u2243 score are 0.963 and 0.925 respectively. When d = 20 and n = 50, the TNR decreases slightly to 0.951 due to the low spatial intensity in R20, where BA and F2 \u2243 score are 0.976 and 0.843. Fortunately, all four measures increase with increasing data sizes (n) when d \u2264 5.\n\nThe SU-MCCD and SUN-MCCD algorithms are the flexible adaptations of the RU-MCCD and UN-MCCD algorithms, respectively. Rather than using a single dominating covering ball, they look for a bunch of points connected to the center of a dominating covering ball in the MCG and construct a cluster by taking the union of their covering balls. Theoretically, both could deliver better performance when clusters are arbitrarily shaped (including the cases when the intensities of clusters are uneven). Nevertheless, it is still interesting to compare the performance of the two \u201cflexible\u201d algorithms with their prototypes when the support of each cluster is standard hyperspheres.# TPRs with Uniform Clusters\n\n|U-MCCDs|SU-MCCDs|\n|---|---|\n|Data Size|Data Size|\n|Density|Density|\n|UN-MCCDs|SUN-MCCDs|\n|Data Size|Data Size|# Figure 6\n\nThe line plots of the TPRs and TNRs of all CCD-based outlier detection algorithms, under the simulation settings (with uniform clusters) elaborated in Section 5.1.\n\nThe SU-MCCD and SUN-MCCD algorithms deliver higher TNRs when d \u2264 20, especially the SUN-MCCD algorithms, whose TNRs are close to 1 under all simulation settings. This is expected since using more covering balls leads to better coverage for each cluster. For example, when d = 20, notice that the SU-MCCD algorithm performs better compared to its prototype (the U-MCCD algorithm) due to much higher TNRs, the F2-scores of the SU-MCCD algorithms are 0.689, 0.717, 0.776, 0.741 and 0.711 versus 0.631, 0.661, 0.692, 0.654 and 0.637 of the U-MCCD algorithm. Recall that the effectiveness of both the U-MCCD and UN-MCCD algorithms declines due to the masking problem as the intensity of outliers grows when d = 2, 3. Fortunately, the SU-MCCD and SUN-MCCD algorithms overcome this problem and yield high TPR even when n = 1000, attributed to the new mechanism that filters small clusters.\n\nThe performance of the two \u201cflexible\u201d algorithms is comparable when d \u2264 5, and the SUN-MCCD algorithm delivers slightly greater F2 scores when the data size n is small, and it performs much better when d = 10, 20 due to the disadvantages of the SU-MCCD algorithm under a high-dimensional space. For example, when d = 20, the F2-scores of...# The Size of Data Sets\n\n|Data Set|50|50|100|100|200|200|500|500|1000|1000| | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| |BA|F2-score|BA|F2-score|BA|F2-score|BA|F2-score|BA|F2-score|\n|U-MCCDs|0.980|0.949|0.990|0.963|0.963|0.926|0.906|0.836|0.840|0.724|\n|SU-MCCDs|0.983|0.953|0.996|0.988|0.998|0.994|1.000|1.000|1.000|1.000|\n|UN-MCCDs|0.986|0.920|0.986|0.930|0.975|0.926|0.964|0.922|0.963|0.925|\n|SUN-MCCDs|0.983|0.937|0.994|0.967|0.997|0.978|0.998|0.985|0.999|0.989|\n|U-MCCDs|0.988|0.926|0.986|0.936|0.980|0.945|0.962|0.929|0.935|0.888|\n|SU-MCCDs|0.992|0.972|0.998|0.990|1.000|0.996|1.000|1.000|1.000|1.000|\n|UN-MCCDs|0.988|0.924|0.989|0.943|0.987|0.954|0.980|0.956|0.960|0.929|\n|SUN-MCCDs|0.991|0.956|0.998|0.987|0.999|0.992|1.000|0.996|1.000|0.996|\n|U-MCCDs|0.974|0.839|0.986|0.903|0.994|0.956|0.998|0.984|0.998|0.993|\n|SU-MCCDs|0.988|0.921|0.995|0.960|0.998|0.985|1.000|0.996|1.000|1.000|\n|UN-MCCDs|0.986|0.911|0.991|0.940|0.994|0.967|0.997|0.986|0.998|0.993|\n|SUN-MCCDs|0.996|0.975|0.999|0.989|1.000|0.996|1.000|1.000|1.000|1.000|\n|U-MCCDs|0.968|0.802|0.979|0.860|0.988|0.916|0.997|0.974|1.000|0.996|\n|SU-MCCDs|0.981|0.871|0.988|0.913|0.996|0.967|0.998|0.985|1.000|1.000|\n|UN-MCCDs|0.987|0.907|0.993|0.949|0.997|0.978|1.000|0.996|1.000|1.000|\n|SUN-MCCDs|0.999|0.992|1.000|0.996|1.000|0.996|1.000|1.000|1.000|1.000|\n|U-MCCDs|0.923|0.631|0.933|0.661|0.942|0.692|0.931|0.654|0.925|0.637|\n|SU-MCCDs|0.941|0.689|0.948|0.717|0.962|0.776|0.954|0.741|0.947|0.711|\n|UN-MCCDs|0.976|0.843|0.986|0.901|0.992|0.943|0.996|0.970|0.997|0.978|\n|SUN-MCCDs|0.987|0.910|0.992|0.939|0.996|0.970|0.998|0.985|1.000|1.000|\n|U-MCCDs|0.784|0.378|0.771|0.365|0.767|0.361|0.767|0.361|0.772|0.365|\n|SU-MCCDs|0.784|0.379|0.771|0.365|0.767|0.361|0.767|0.361|0.771|0.365|\n|UN-MCCDs|0.830|0.436|0.841|0.452|0.854|0.474|0.862|0.487|0.867|0.496|\n|SUN-MCCDs|0.841|0.453|0.864|0.491|0.897|0.561|0.912|0.599|0.932|0.659|\n|U-MCCDs|0.775|0.369|0.770|0.364|0.765|0.358|0.761|0.355|0.757|0.351|\n|SU-MCCDs|0.775|0.369|0.771|0.364|0.765|0.359|0.761|0.355|0.758|0.352|\n|UN-MCCDs|0.566|0.232|0.581|0.239|0.614|0.254|0.728|0.326|0.717|0.317|\n|SUN-MCCDs|0.566|0.232|0.581|0.239|0.614|0.254|0.728|0.326|0.718|0.318|\n\nTable 3: Summary of the Balanced Accuracy (BA) and F2-score of all the CCD-based outlier detection algorithms, with the simulation settings elaborated in Section 5.1.\n\nthe SUN-MCCD algorithms are 0.910, 0.939, 0.970, 0.985, and 1.000 versus 0.689, 0.717, 0.776, 0.741, and 0.711 of the SU-MCCD algorithm.\n\nHowever, when d = 50, 100, all the four algorithms perform worse. The TNRs become substantially smaller than those with fewer dimensions, particularly when d = 100, where most BAs are between 0.5 and 0.7, close to random guesses. The F2-scores, sensitive to precision, drop between 0.2 and 0.5. This is because, under high-dimensional space, all the regular points tend to be distributed along the border of the clusters they belong to, even if they are uniformly distributed. Hence, the difficulty in capturing most of them increases substantially as d increases, and few clustering-based outlier detection algorithms could still deliver promising performance without dimensionality reduction techniques.\n\nAdditionally, it is worth noting that the performance of the two flexible algorithms degrades and is close or equal to the results of their prototypes. It can be explained by the reason that almost every point is isolated points under the MCG constructed on extremely high-dimensional space, and there are none or few points that are connected to the center of dominating covering balls, resulting in only one covering ball for most clusters.\n\nWe know that RK-CCDs and UN-CCDs conduct SR-MCT that finds clusters following HPP, which means the points within each constructed cluster are approximately uniformly distributed. Therefore, the CCD-based algorithms prefer the simulation experiments with only uniform clusters, particularly the RU-MCCD and UN-MCCD algorithms. Thus, in addition to the above experiments, we perform similar simulations under Gaussian settings, where regular data points from the same cluster are multivariate-normally distributed (but uncorrelated). We aim to investigate the effectiveness of these CCD-based algorithms.\n\n32# BAs with Uniform Clusters\n\n|U-MCCDs|SU-MCCDs|\n|---|---|\n|Data Size n|Data Size n|\n|UN-MCCDs|SUN-MCCDs|\n|Data Size n|Data Size n|# F2-scores with Uniform Clusters\n\n|U-MCCDs|SU-MCCDs|\n|---|---|\n|Data Size|Data Size|\n|1|0|\n|UN-MCCDs|SUN-MCCDs|\n|Data Size|Data Size|\n|1|8|\n\nFigure 7: The line plots of the BAs and F2-scores of all CCD-based outlier detection algorithms, under the simulation settings (with uniform clusters) elaborated in Section 5.1.\n\nWhen data points within a cluster are nonuniformly distributed, there are two major challenges to finding outliers with Gaussian clusters: capturing the regular data points near the boundary of a cluster where the intensity is much lower than the center while distinguishing outliers with similar intensities.\n\nTo make the simulation experiments with Gaussian clusters comparable to the previous ones with uniform clusters, we choose the scale of the covariance matrix according to the dimensionality d and radius R such that approximately 99% of points of the Gaussian cluster are located within a hypersphere with radius R (recall R is a random number from 0.7 to 1.3), and the approximate 1% of points located beyond the hypersphere are perceived to be noise near the cluster (The noise level here represents the percentage of data points that are randomly generated near the range of the clusters. The outliers are data points that are far away from the cluster centers). Similarly, R is a random variable generated uniformly between 0.7 and 1.3, so clusters with different volumes and intensities can be constructed. Except for the way to simulate Gaussian clusters, which we have elaborated on particularly.\n\n33# Performance Measures of Algorithms\n\nAll the other settings (dimensionality, the sizes of data sets, the centers of clusters, etc.) remain the same. Two realizations with data sizes of 100 and 200 are presented in Figure 8. The performance measures of the four algorithms are summarized in Tables 4 and 5. Similarly, we present the line plots of the results in Figures 9 and 10.# Figure 8\n\nTwo realizations of the simulation settings with Gaussian clusters, where n = 100 and 200 respectively. Each data set has 2 clusters of the same size but different intensities. Black points are regular data points, and red points are outliers. The numbers of observations are indicated below each sub-figure.# Discussion of Results\n\nAs in the previous cases, we discuss the results under low and moderate dimensionality (d \u2264 20) and consider the RU-MCCD algorithm first. The RU-MCCD algorithm generally performs much worse with Gaussian clusters; although it can still capture most outliers and provide high TPRs, the TNPs exhibit a substantial decrease. For example, when d = 3, the RU-MCCD algorithm delivers TNRs of 0.880, 0.849, 0.818, 0.784, and 0.760, which show a major drop from 0.980, 0.985, 0.992, 0.997, and 0.998 under similar simulation settings with uniform clusters.\n\nIt is within our expectation because RK-CCDs find support for each cluster by conducting SR-MCT; the point pattern of each constructed cluster is close to a uniform distribution, deviating from Gaussian clusters with uneven intensities. Furthermore, a Gaussian density has unbounded support, but each covering ball has bounded volume. Consequently, the resulting dominating covering balls tend to be smaller than the scope of Gaussian clusters and generally located around the center, leaving many regular points of less intensity uncovered. It is unlikely for the D-MCG algorithm to connect these relatively sparse uncovered points to the points of dominating covering balls, which generally have much higher intensities.\n\nFurthermore, notice that as the number of observations increases from 50 to 1000, the TNR decreases from 0.880 to 0.760, and as a result, the F2-score decreases from 0.686 to 0.523. The reason can be explained as follows: the larger the size of a Gaussian cluster, the more deviation of its point pattern from a uniform density. Therefore, it becomes more difficult for the RU-MCCD and UN-MCCD algorithms to capture regular observations.\n\nAlso, it is worth noting that the RU-MCCD algorithm performs worse with more dimensions d. For instance, when n is fixed to 200, the RU-MCCD algorithm delivers F2-scores of 0.638, 0.591, 0.541, 0.466, and 0.378 as d increases from 2 to 20; it is due to the same reason that the effectiveness of RK-CCDs degenerates rapidly with increasing number of dimensions.\n\nIn Tables 4 and 5, observe that the UN-MCCD algorithm also exhibits a performance drop in the simulation cases with Gaussian clusters; e.g., when d = 5, the TNRs are# The Size of Data Sets\n\n|Data Sets| | | |Size| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| | |50|100| |200|500| |1000| | |\n|U-MCCDs|0.994|0.918|0.998|0.886|1.000|0.851|1.000|0.818|1.000|0.792|\n|SU-MCCDs|0.994|0.970|0.999|0.962|1.000|0.951|1.000|0.927|1.000|0.903|\n|UN-MCCDs|0.995|0.942|0.978|0.927|0.988|0.914|0.994|0.895|0.999|0.880|\n|SUN-MCCDs|0.996|0.964|0.995|0.964|0.999|0.962|1.000|0.958|1.000|0.950|\n|U-MCCDs|0.999|0.880|1.000|0.849|1.000|0.818|1.000|0.784|1.000|0.760|\n|SU-MCCDs|0.999|0.948|1.000|0.943|1.000|0.937|1.000|0.921|1.000|0.903|\n|UN-MCCDs|0.998|0.922|0.995|0.902|0.997|0.884|0.999|0.862|1.000|0.842|\n|SUN-MCCDs|0.998|0.957|0.999|0.958|1.000|0.959|1.000|0.955|1.000|0.947|\n|U-MCCDs|1.000|0.821|1.000|0.797|1.000|0.777|1.000|0.755|1.000|0.727|\n|SU-MCCDs|1.000|0.887|1.000|0.886|1.000|0.890|1.000|0.891|1.000|0.879|\n|UN-MCCDs|1.000|0.888|0.999|0.865|1.000|0.846|1.000|0.820|1.000|0.794|\n|SUN-MCCDs|1.000|0.939|1.000|0.941|1.000|0.943|1.000|0.945|1.000|0.942|\n|U-MCCDs|1.000|0.748|1.000|0.715|1.000|0.698|1.000|0.693|1.000|0.684|\n|SU-MCCDs|1.000|0.813|1.000|0.797|1.000|0.791|1.000|0.797|1.000|0.794|\n|UN-MCCDs|1.000|0.856|1.000|0.832|1.000|0.816|1.000|0.797|1.000|0.770|\n|SUN-MCCDs|1.000|0.960|1.000|0.949|1.000|0.945|1.000|0.946|1.000|0.945|\n|U-MCCDs|1.000|0.620|1.000|0.592|1.000|0.567|1.000|0.541|1.000|0.531|\n|SU-MCCDs|1.000|0.660|1.000|0.644|1.000|0.625|1.000|0.609|1.000|0.606|\n|UN-MCCDs|1.000|0.736|1.000|0.701|1.000|0.668|1.000|0.627|1.000|0.604|\n|SUN-MCCDs|1.000|0.826|1.000|0.804|1.000|0.796|1.000|0.789|1.000|0.784|\n|U-MCCDs|1.000|0.580|1.000|0.562|1.000|0.552|1.000|0.542|1.000|0.544|\n|SU-MCCDs|1.000|0.581|1.000|0.562|1.000|0.553|1.000|0.542|1.000|0.544|\n|UN-MCCDs|1.000|0.448|1.000|0.420|1.000|0.380|1.000|0.308|1.000|0.275|\n|SUN-MCCDs|1.000|0.457|1.000|0.444|1.000|0.417|1.000|0.366|1.000|0.351|\n|U-MCCDs|1.000|0.574|1.000|0.547|1.000|0.521|1.000|0.513|1.000|0.510|\n|SU-MCCDs|1.000|0.575|1.000|0.547|1.000|0.521|1.000|0.513|1.000|0.511|\n|UN-MCCDs|1.000|0.302|1.000|0.317|1.000|0.318|1.000|0.275|1.000|0.231|\n|SUN-MCCDs|1.000|0.302|1.000|0.317|1.000|0.319|1.000|0.276|1.000|0.232|\n\nTable 4: Summary of the TPR and TNR of all the CCD-based outlier detection algorithms, with the simulation settings elaborated in Section 5.1.\n\n0.888, 0.865, 0.846, 0.820, and 0.794, compared to 0.975, 0.984, 0.992, 0.997, and 0.999 with uniform clusters. The corresponding F2-scores also decrease substantially from 0.911, 0.940, 0.967, 0.986, and 0.993 under uniform setting to 0.701, 0.660, 0.631, 0.594, and 0.561 under Gaussian setting. For the same reason as the RU-MCCD algorithm, the F2-score of the UN-MCCD algorithm decreases when n increases. The performance of the UN-MCCD algorithm also shows a downward trend with increasing dimensionality d (e.g., for n = 100, the F2-scores are 0.768, 0.726, 0.660, 0.610, and 0.468), but much less severely affected than the UN-MCCD algorithm. In summary, although the performance of the UN-MCCD algorithm deteriorates from uniform to Gaussian clusters, it still outperforms compared to the RU-MCCD algorithm thanks to the improved SR-MCT with NND.\n\nNext, we consider the SU-MCCD and SUN-MCCD algorithms, both of which yield promising results compared with the two prototypes because they provide much better coverage for Gaussian clusters with multiple covering balls. For instance, when d = 10, the TNRs of the SUN-MCCD algorithm are 0.960, 0.949, 0.945, 0.946, and 0.945, much higher than those of the UN-MCCD algorithm, therefore, the SUN-MCCD algorithm deliver F2-scores of 0.868, 0.838, 0.827, 0.830, and 0.828, versus 0.646, 0.610, 0.589, 0.565, and 0.534 of the UN-MCCD algorithm. A similar performance gap is observed from the RU-MCCD to the SU-MCCD algorithms. Additionally, unlike the RU-MCCD and UN-MCCD algorithms.# TPRs with Gaussian Clusters\n\n|U-MCCDs|SU-MCCDs|\n|---|---|\n|0.905|0.905|\n|0.999|0.900|# Data Size n\n\n|UN-MCCDs|SUN-MCCDs|\n|---|---|\n|0.044n|0.044d|# TNRs with Gaussian Clusters\n\n|U-MCCDs|SU-MCCDs|\n|---|---|\n|Data Size n|Data Size n|\n|UN-MCCDs|SUN-MCCDs|\n|Data Size|Data Size|\n\nFigure 9: The line plots of the TPRs and TNRs of all CCD-based outlier detection algorithms, under the simulation settings (with Gaussian clusters) elaborated in Section 5.1.\n\nMCCD algorithms, the two \u201cflexible\u201d algorithms perform better when n is larger. The reason is that when multiple covering balls are allowed for a single cluster, increasing the size of a cluster results in performance gain since the point pattern is easier to capture with more observations.\n\nWhen d \u2264 3, the SU-MCCD algorithm slightly outperforms the SUN-MCCD algorithm; e.g., when d = 3, the F2-scores of the SUN-MCCD algorithm are 0.858, 0.862, 0.865, 0.854 and 0.832, higher than the F2-scores of the SU-MCCD algorithm, which are 0.834, 0.822, 0.807, 0.769, and 0.731. Starting from d = 5, the SUN-MCCD algorithm outperforms the SU-MCCD algorithm substantially. The most substantial performance difference is observed when d = 10, where the F2-scores of the SUN-MCCD algorithm are 0.868, 0.835, 0.827, 0.830, and 0.843, substantially higher than those of the SU-MCCD algorithm, which are less than 0.6. This is due to the same reason for the degeneration of the U-MCCD algorithm when d is large.\n\nFor a similar reason explained under the simulation settings with only uniform clusters, all four CCD-based algorithms fail to deliver promising results without dimensionality.# The Size of Data Sets\n\n|Data Set| |50| |100| |200| |500| |1000|\n|---|---|---|---|---|---|---|---|---|---|---|\n| |BA|F2-score|BA|F2-score|BA|F2-score|BA|F2-score|BA|F2-score|\n|U-MCCDs|0.956|0.759|0.942|0.697|0.926|0.638|0.909|0.591|0.896|0.559|\n|SU-MCCDs|0.982|0.893|0.981|0.873|0.976|0.843|0.964|0.783|0.952|0.731|\n|UN-MCCDs|0.969|0.816|0.953|0.768|0.951|0.746|0.945|0.711|0.940|0.686|\n|SUN-MCCDs|0.980|0.877|0.980|0.876|0.981|0.873|0.979|0.862|0.975|0.840|\n|U-MCCDs|0.940|0.686|0.925|0.635|0.909|0.591|0.892|0.549|0.880|0.523|\n|SU-MCCDs|0.974|0.834|0.972|0.822|0.969|0.807|0.961|0.769|0.952|0.731|\n|UN-MCCDs|0.960|0.770|0.949|0.726|0.941|0.692|0.931|0.655|0.921|0.625|\n|SUN-MCCDs|0.978|0.858|0.979|0.862|0.980|0.865|0.978|0.854|0.974|0.832|\n|U-MCCDs|0.911|0.595|0.899|0.565|0.889|0.541|0.878|0.518|0.864|0.491|\n|SU-MCCDs|0.944|0.700|0.943|0.698|0.945|0.705|0.946|0.707|0.940|0.685|\n|UN-MCCDs|0.944|0.701|0.932|0.660|0.923|0.631|0.910|0.594|0.897|0.561|\n|SUN-MCCDs|0.970|0.812|0.971|0.817|0.972|0.822|0.973|0.827|0.971|0.819|\n|U-MCCDs|0.874|0.511|0.858|0.480|0.849|0.466|0.847|0.462|0.842|0.454|\n|SU-MCCDs|0.907|0.585|0.899|0.565|0.896|0.557|0.899|0.565|0.897|0.561|\n|UN-MCCDs|0.928|0.646|0.916|0.610|0.908|0.589|0.899|0.565|0.885|0.534|\n|SUN-MCCDs|0.980|0.868|0.975|0.838|0.973|0.827|0.973|0.830|0.973|0.827|\n|U-MCCDs|0.810|0.409|0.796|0.392|0.784|0.378|0.771|0.364|0.766|0.359|\n|SU-MCCDs|0.830|0.436|0.822|0.425|0.813|0.412|0.805|0.402|0.803|0.400|\n|UN-MCCDs|0.868|0.499|0.851|0.468|0.834|0.442|0.814|0.414|0.802|0.399|\n|SUN-MCCDs|0.913|0.602|0.902|0.573|0.898|0.563|0.895|0.555|0.892|0.549|\n|U-MCCDs|0.790|0.385|0.781|0.375|0.776|0.370|0.771|0.365|0.772|0.366|\n|SU-MCCDs|0.791|0.386|0.781|0.375|0.777|0.371|0.771|0.365|0.772|0.366|\n|UN-MCCDs|0.724|0.323|0.710|0.312|0.690|0.298|0.654|0.276|0.638|0.266|\n|SUN-MCCDs|0.729|0.326|0.722|0.321|0.709|0.311|0.683|0.293|0.676|0.289|\n|U-MCCDs|0.787|0.382|0.774|0.367|0.761|0.355|0.757|0.351|0.755|0.349|\n|SU-MCCDs|0.788|0.382|0.774|0.367|0.761|0.355|0.757|0.351|0.756|0.350|\n|UN-MCCDs|0.651|0.274|0.659|0.278|0.659|0.278|0.638|0.266|0.616|0.255|\n|SUN-MCCDs|0.651|0.274|0.659|0.278|0.660|0.279|0.638|0.267|0.616|0.255|\n\nTable 5: Summary of the Balanced Accuracy (BA) and F2-score of all the CCD-based outlier detection algorithms, with the simulation settings elaborated in Section 5.1.# 5.2 Monte Carlo Experiments: Focus Settings\n\nIn the simulations we conducted in the previous section, we set up two clusters of data points with 5% outliers and 1% noise (the latter is only for Gaussian clusters). We fixed the distances between the cluster centers and the minimal distances between the cluster centers and the outliers to 3 and 2 units, respectively. We compared the balanced accuracies and F2-scores of the CCD-based outlier detection algorithms on this setting. Next, we will investigate how the performance of these algorithms changes with varying factors such as the number of clusters, the noise level, the outlier percentage, and the distances between the clusters and the outliers, which we call focus settings. We conduct such simulation analysis to get a better understanding of the robustness and behaviors of the four CCD-based algorithms under different simulation settings and to identify the sensitivity of each algorithm.# 5.2.1 Varying the Number of Clusters\n\nAfter assessing the effectiveness of CCD-based outlier detection algorithms on data sets with two distinct clusters, the next goal involves examining how their performance changes as the number of clusters increases from 2 to 5, while keeping other factors constant as in Section 5.1. We conduct two series of simulations, one with uniform clusters and another.# BAs with Gaussian Clusters\n\n|U-MCCDs|SU-MCCDs|\n|---|---|\n|Data Size n|Data Size n|\n|UN-MCCDs|SUN-MCCDs|\n|Data Size|Data Size|\n|3|480|# Fz-scores with Gaussian Clusters\n\n|U-MCCDs|SU-MCCDs|\n|---|---|\n|Data Size|Data Size|\n|1|0|\n|amesian|amesian|\n|UN-MCCDs|SUN-MCCDs|\n|Data Size|Data Size|\n| |8|\n\nFigure 10: The line plots of the TPRs and TNRs of all CCD-based outlier detection algorithms, under the simulation settings (with Gaussian clusters) elaborated in Section 5.1.\n\nwith Gaussian clusters. Additionally, we simulate both 3-dimensional and 10-dimensional data sets to understand how d impacts performance on data sets of both small and high dimensions. Specific details are outlined below.\n\n1. The dimensionality (d) of the simulated data sets: 3, 10;\n2. The size of data sets (n): 200;\n3. The size of each cluster is equal (although the volume of the supports is different), and we conduct two series of simulations with uniform clusters and Gaussian clusters, respectively;\n4. Number of clusters: 2, 3, 4, and 5 (the study of focus in this section);\n5. The radius of each cluster is randomly chosen between 0.7 and 1.3;\n\n38# vi. When d = 3, the centers of clusters are:\n\n1. Two clusters:\n- \u03bc1 = (3, 3, 3) and\n- \u03bc2 = (6, 3, 3);\n2. Three clusters:\n- \u03bc1 = (3, 3, 3),\n- \u03bc2 = (6, 3, 3), and\n- \u03bc3 = (3, 6, 3);\n3. Four clusters:\n- \u03bc1 = (3, 3, 3),\n- \u03bc2 = (6, 3, 3),\n- \u03bc3 = (3, 6, 3), and\n- \u03bc4 = (3, 3, 6);\n4. Five clusters:\n- \u03bc1 = (3, 3, 3),\n- \u03bc2 = (6, 3, 3),\n- \u03bc3 = (3, 6, 3),\n- \u03bc4 = (3, 3, 6), and\n- \u03bc5 = (6, 6, 3);# vii. When d = 10, the centers of clusters are:\n\n1. Two clusters:\n- \u03bc1 = (3, ..., 3) and\n- \u03bc2 = (6, 3, ..., 3);\n2. Three clusters:\n- \u03bc1 = (3, ..., 3),\n- \u03bc2 = (6, 3, ..., 3), and\n- \u03bc3 = (3, 6, 3, ..., 3);\n3. Four clusters:\n- \u03bc1 = (3, ..., 3),\n- \u03bc2 = (6, 3, ..., 3),\n- \u03bc3 = (3, 6, 3, ..., 3), and\n- \u03bc4 = (3, 3, 6, 3, ..., 3);\n4. Five clusters:\n- \u03bc1 = (3, ..., 3),\n- \u03bc2 = (6, 3, ..., 3),\n- \u03bc3 = (3, 6, 3, ..., 3),\n- \u03bc4 = (3, 3, 6, 3, ..., 3), and\n- \u03bc5 = (3, 3, 3, 6, 3, ..., 3);# viii. The proportion of outliers is fixed to 5%;# ix. The outlier set Coutlier is generated uniformly within a much larger hypersphere of radius 5, centered at the mean of the cluster center. and each outlier is at least 2 units away from any cluster center;# x. The noise level of each Gaussian cluster is set to 1%.# Table 6: The TPRs and TNRs of the CCD-based algorithms as the number of uniform clusters increases from 2 to 5.\n\n|Number of Clusters|2|2|3|3|4|4|5|5|\n|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs (d = 3)|0.970|0.992|0.984|0.988|0.993|0.985|0.989|0.982|\n|SU-MCCDs|1.000|0.999|0.999|0.999|0.997|0.998|0.994|0.996|\n|UN-MCCDs|0.983|0.991|0.989|0.989|0.997|0.985|0.993|0.984|\n|SUN-MCCDs|1.000|0.998|0.998|0.997|0.995|0.996|0.990|0.995|\n|RU-MCCDs (d = 10)|1.000|0.976|1.000|0.916|1.000|0.900|1.000|0.900|\n|SU-MCCDs|1.000|0.991|1.000|0.933|1.000|0.916|1.000|0.917|\n|UN-MCCDs|1.000|0.995|1.000|0.990|1.000|0.985|1.000|0.984|\n|SUN-MCCDs|1.000|0.999|1.000|0.999|1.000|0.998|1.000|0.998|\n\nConsidering the simulation settings with uniform clusters (Tables 6 and 7), observe that almost all the algorithms perform well with F2-scores exceeding 90% except the RU-MCCD and SU-MCCD algorithms, which tends to have low TNRs when the same reason that has been discussed in Section 3.5.2). Algorithms decrease slightly as the number of clusters increases because when we fix n to 200, more clusters indicate less intensity for each uniform cluster; thus, the difficulty level to identify the correct number of clusters and capture an entire cluster increases.\n\nWith Gaussian clusters, similar to the results we obtained in the previous section, the SU-MCCD and SUN-MCCD algorithms outperform their prototypes by a large margin, especially the SUN-MCCD algorithm, which delivers high F2-scores of 0.827, 0.832, 0.840, and 0.846 when d = 10 as the number of clusters increases.# Table 7: The TPRs and TNRs of the CCD-based algorithms as the number of uniform clusters increases from 2 to 5.\n\n|Number of Clusters|d = 3|d = 10| | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | |BA|F-score|BA|F2-score|BA|F-score|BA|F2-score| | | | | |\n| | | | |2|RU-MCCDs|0.981|0.947|0.986|0.944|RU-MCCDs|0.988|0.916|0.958|0.758|\n| |3|SU-MCCDs|1.000|0.996|0.999|0.995|SU-MCCDs|0.996|0.967|0.967|0.797| | | |\n| | |4|UN-MCCDs|0.987|0.954|0.989|0.951|UN-MCCDs|0.998|0.981|0.995|0.963| | |\n|5|SUN-MCCDs|0.999|0.992|0.998|0.987|SUN-MCCDs|1.000|0.996|1.000|0.996| | | | |\n| | | | |2|RU-MCCDs|0.950|0.725|0.950|0.725|RU-MCCDs|0.950|0.725|0.950|0.725|\n|3|SU-MCCDs| |0.958|0.758|0.959|0.760|SU-MCCDs|0.958|0.758|0.959|0.760| | | |\n| | | | |4|UN-MCCDs|0.993|0.946|0.992|0.943|UN-MCCDs|0.993|0.946|0.992|0.943|\n|5|SUN-MCCDs|0.999|0.992|0.999|0.992|SUN-MCCDs|0.999|0.992|0.999|0.992| | | | |# Figure 11: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the number of uniform clusters increases.\n\n(a) The BAs for d = 3. (b) The F2-scores for d = 3. (c) The BAs for d = 10. (d) The F2-scores for d = 10.\n\nthat the F2-scores of the RU-MCCD and SU-MCCD algorithms increase with the cluster numbers when d = 3, e.g., the F2-score of the RU-MCCD algorithm rises from 0.591 to 0.653 when the cluster number increases; because when the intensities of Gaussian clusters decrease, their point patterns are closer to uniform clusters, which give advantage to the performance of the two algorithms and outweigh the effect of intensity drops.\n\nIn summary, the effectiveness of all four algorithms is relatively robust against the number of clusters. With other factors fixed, although their performance tends to decrease as the number of clusters increases, the decrease is minimal. The SUN-MCCD algorithm offers better overall performance and could deliver promising results even if there are 5 Gaussian clusters.# 5.2.2 Varying the Outliers\u2019 Percentage\n\nThe main goal of this section is to evaluate the performance of the four CCD-based algorithms under different levels of contamination. In Section 5.1, we present the results of the data sets with 5% outliers, which is a moderate level of contamination. In this section, we# Table 8: The TPRs and TNRs of the CCD-based algorithms as the number of Gaussian clusters increases from 2 to 5.\n\n|Number of Clusters|2|3|4|5| | | | | |\n|---|---|---|---|---|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR| |\n| |RU-MCCDs|1.000|0.818|1.000|0.836|1.000|0.847|1.000|0.860|\n| |SU-MCCDs|1.000|0.938|1.000|0.941|1.000|0.945|1.000|0.947|\n| |UN-MCCDs|0.997|0.884|0.995|0.986|0.996|0.902|0.997|0.908|\n| |SUN-MCCDs|1.000|0.959|1.000|0.958|1.000|0.958|1.000|0.958|\n| |RU-MCCDs|1.000|0.698|1.000|0.689|1.000|0.700|1.000|0.708|\n| |SU-MCCDs|1.000|0.791|1.000|0.771|1.000|0.779|1.000|0.782|\n| |UN-MCCDs|1.000|0.817|1.000|0.825|1.000|0.832|1.000|0.836|\n| |SUN-MCCDs|1.000|0.945|1.000|0.947|1.000|0.950|1.000|0.952|# Table 9: The BAs and F2-scores of the CCD-based algorithms as the number of Gaussian clusters increases from 2 to 5.\n\n|Number of Clusters|2|3|4|5| | | | |\n|---|---|---|---|---|---|---|---|---|\n| |BA|F-score|BA|F-score|BA|F-score|BA|F-score|\n|RU-MCCDs|0.909|0.591|0.918|0.616|0.924|0.632|0.930|0.653|\n|SU-MCCDs|0.969|0.809|0.971|0.817|0.973|0.827|0.974|0.832|\n|UN-MCCDs|0.941|0.692|0.946|0.714|0.949|0.726|0.953|0.739|\n|SUN-MCCDs|0.980|0.865|0.979|0.862|0.979|0.862|0.979|0.862|\n|RU-MCCDs|0.849|0.466|0.845|0.458|0.850|0.467|0.854|0.474|\n|SU-MCCDs|0.896|0.557|0.886|0.535|0.890|0.544|0.891|0.547|\n|UN-MCCDs|0.909|0.590|0.913|0.601|0.916|0.610|0.918|0.616|\n|SUN-MCCDs|0.973|0.827|0.974|0.832|0.975|0.840|0.976|0.846|\n\naim to investigate the sensitivity of these algorithms by conducting a series of simulations with the percentage of outliers increasing from 2% to 15%. To increase complexity, we set the number of clusters to 3 rather than 2; all the other factors, such as the number of observations, the distances between cluster centers, noise level, etc., are fixed at the same values as in Section 5.2.1. We expect that the algorithms show different degrees of sensitivity to the presence of outliers.\n\nSimilar to Section 5.2.1, we conduct two sets of simulations with uniform and Gaussian clusters, and we choose to simulate data sets with 3 and 10 dimensions. Details are presented below, it is worth noting that we only list the difference and skip the common parts compared to the simulation setting in Section 5.2.1. Some realizations of data sets with Gaussian clusters in 2-dimensional space (although the simulation experiments are conducted on 3 and 10-dimensional space) are presented in Figure 13. (for illustration purposes)\n\ni. The proportion of outliers: 2%, 5%, 7%, 10%, and 15% (the study of focus in this section).\n\nThe simulation results are summarized from Tables 10 to 13. We also present the results of BAs and F2-scores (Tables 11 and 13) as barplots in Figures 14 and 15, respectively.\n\nIn the current setting, the percentage of outliers is not fixed. As a result, the F2-score is not an appropriate measure to compare the efficiency across the data sets with different outlier contamination levels, because precision is highly dependent on the size of outliers. For instance, suppose we have two data sets, each with 100 observations. The first data set has one outlier, and the second has 20 outliers. If an algorithm captures all the outliers and returns one false positive for the first data set and 20 false positives for the second, then the algorithm performs better on the first data set because it has much fewer false positives and higher overall accuracy (99% versus 80%). However, the algorithm would# Figure 12: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the number of Gaussian clusters increases.\n\n- (a) The BAs for d = 3.\n- (b) The F2-scores for d = 3.\n- (c) The BAs for d = 10.\n- (d) The F2-scores for d = 10.# Table 10: The TPRs and TNRs of the CCD-based algorithms as the percentage of outliers over the entire simulated data set increases from 2% to 15% (for simulations with uniform clusters).\n\n|Percentage of Outliers|2%|2%|5%|5%|7%|7%|10%|10%|15%|15%|\n|---|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs|0.999|0.988|0.985|0.988|0.961|0.989|0.943|0.987|0.878|0.987|\n|SU-MCCDs|0.999|0.998|0.999|0.998|0.997|0.998|0.991|0.998|0.961|0.998|\n|UN-MCCDs|0.999|0.989|0.991|0.990|0.977|0.989|0.965|0.989|0.919|0.987|\n|SUN-MCCDs|0.999|0.997|0.998|0.998|0.996|0.997|0.996|0.997|0.973|0.998|\n|RU-MCCDs|1.000|0.911|1.000|0.920|1.000|0.907|1.000|0.918|1.000|0.914|\n|SU-MCCDs|1.000|0.926|1.000|0.934|1.000|0.924|1.000|0.933|1.000|0.931|\n|UN-MCCDs|1.000|0.990|1.000|0.990|1.000|0.991|1.000|0.988|1.000|0.989|\n|SUN-MCCDs|1.000|0.999|1.000|0.999|1.000|0.999|1.000|0.999|0.999|0.998|\n\nhave the same F2-score of 0.882 for both data sets, which is misleading. Therefore, we consider accuracies only instead of F2-scores in the current setting.\n\nWe first consider the settings with uniform clusters, whose results are summarized in Tables 10 and 11. All the algorithms achieve good performance with BAs close to 1. Similar to the previous simulation results, the RU-MCCD and SU-MCCD algorithms lag behind the other two when d = 10. Furthermore, observe that the TPRs of the RU-MCCD and UN-MCCD algorithm decreases at a faster rate than the other two \u201cflexible\u201d algorithms when the contamination level increases, e.g., when d = 3, the TPRs of the RU-MCCD algorithm are 0.999, 0.985, 0.961, 0.943, and 0.878 as the contamination level rises from 2% to 15%. It is due to the masking problem that we have explained in Section 5.1, which happens more frequently when the intensity of outliers is high. Fortunately, thanks to their mechanism that filters small clusters, the SU-MCCD and SUN-MCCD algorithms exhibit more robustness against a high percentage of outliers, e.g., when d = 3, the SUN-MCCD algorithm can still provide a TPR of 0.973 when the contamination level is as high as 15%.# 5.2.3 Varying the Minimal Distance Between Outliers and Cluster Centers\n\nIn the previous simulation settings, the distances between outliers and cluster centers are at least 2. Given the fact that the support of each cluster is a hypersphere with a radius that varies from 0.7 to 1.3, there is a noticeable distance between an outlier and a regular observation. Under those settings, all four CCD-based algorithms can separate most outliers from regular observations in the majority of cases (except the RU-MCCD and UN-MCCD algorithms, which are affected by the masking problem when the intensity of outliers is relatively high). In this section, instead of fixing the minimal distance to 2, we simulate data sets with outliers and clusters being much closer in proximity. We conduct five simulations with the minimal distance between outliers and any cluster centers increasing from 1.25 to 2.25 and investigate the performance of all 4 CCD-based algorithms. We expect the difficulty of capturing most outliers to increase substantially, especially when the minimal distance is set to 1.25, where the outlier sets may even overlap with some clusters.\n\nFigure 13: Some realizations (with Gaussian clusters) of the simulation setting in Section 5.2.2, the contamination level increases from 2% to 15%. Red crosses are outliers, black points are regular observations. Contamination levels are indicated below each sub-figure.\n\nConsider the simulations with Gaussian clusters (Tables 12 and 13), the SU-MCCD and SUN-MCCD algorithms are slightly better than the other two prototypes and perform similarly when d = 3, and deliver BAs of at least 95%. When d = 10, the SUN-MCCD algorithm offers substantially better results than the others. Furthermore, all the algorithms are insensitive to the changing contamination level under Gaussian simulation settings with the cost of some false positives.# Percentage of Outliers\n\n|Method|Percentage of Outliers| | | | |\n|---|---|---|---|---|---|\n|2%|5%|7%|10%|15%| |\n|RU-MCCDs|0.994|0.987|0.975|0.965|0.933|\n|SU-MCCDs|0.999|0.999|0.998|0.995|0.980|\n|UN-MCCDs|0.994|0.991|0.983|0.977|0.953|\n|SUN-MCCDs|0.998|0.998|0.997|0.997|0.986|\n|RU-MCCDs|0.956|0.960|0.954|0.959|0.957|\n|SU-MCCDs|0.963|0.967|0.962|0.967|0.966|\n|UN-MCCDs|0.995|0.995|0.996|0.994|0.995|\n|SUN-MCCDs|1.000|1.000|1.000|1.000|0.999|\n\nTable 11: The BAs and F2-scores of the CCD-based algorithms as the percentage of outliers over the entire simulated data set increases from 2% to 15% (for simulations with uniform clusters).# (d) The F2-scores when d = 10.\n\nFigure 14: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the percentage of outlier increases (points within each clusters are uniformly distributed). (a) The BAs for d = 3. (b) The F2-scores for d = 3. (c) The BAs for d = 10. (d) The F2-scores for d = 10.\n\nSimilarly, all other factors are set to the same values as in the previous simulations, and details are presented below. Again, we only list the differences and skip the common parts compared to the simulation setting in Section 5.2.1. Some realizations of data sets with uniform clusters in 2-dimensional space (although the simulation experiments are conducted on 3 and 10-dimensional space) are presented in Figure 16 (for illustration purposes).\n\ni. The minimal distance between an outliers and any cluster center varies with values: 1.25, 1.5, 1.75, 2, and 2.25 (the study of focus in this section).\n\nThe simulation results are summarized from Tables 14 to 17. BAs and F2-scores (Tables 15 and 17) are also presented as barplots in Figures 17 and 18, respectively.\n\nIn the simulations with only uniform clusters, observe that when d = 3 and the minimal distance is 1.25, the four algorithms yield TPRs of 0.979, 0.966, 0.980, and 0.962, slightly lower than those in other scenarios. It aligns with our expectations since a few outliers.# Percentage of Outliers\n\n|Algorithm| |Percentage of Outliers| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| |2%|5%|7%|10%|15%| | | | | | |\n| |RU-MCCDs|1.000|0.834|1.000|0.833|1.000|0.838|1.000|0.839|0.998|0.840|\n| |SU-MCCDs|1.000|0.940|1.000|0.941|1.000|0.943|1.000|0.943|0.997|0.945|\n| |UN-MCCDs|1.000|0.893|0.997|0.890|0.992|0.895|0.991|0.898|0.971|0.899|\n| |SUN-MCCDs|1.000|0.957|1.000|0.956|1.000|0.958|0.998|0.959|0.983|0.960|\n| |RU-MCCDs|1.000|0.684|1.000|0.698|1.000|0.690|1.000|0.689|1.000|0.693|\n| |SU-MCCDs|1.000|0.767|1.000|0.777|1.000|0.772|1.000|0.767|1.000|0.773|\n| |UN-MCCDs|1.000|0.827|1.000|0.829|1.000|0.826|1.000|0.827|1.000|0.825|\n| |SUN-MCCDs|1.000|0.947|1.000|0.947|1.000|0.948|1.000|0.948|1.000|0.948|\n\nTable 12: The TPRs and TNRs of the CCD-based algorithms as the percentage of outliers over the entire simulated data set increases from 2% to 15% (for simulations with Gaussian clusters).# Percentage of Outliers\n\n|Algorithm| |Percentage of Outliers| | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| |2%|5%|7%|10%|15%| | | | | |\n|RU-MCCDs|0.917|0.381|0.917|0.612|0.919|0.699|0.920|0.775|0.919|0.845|\n|SU-MCCDs|0.970|0.630|0.971|0.817|0.972|0.868|0.972|0.907|0.971|0.939|\n|UN-MCCDs|0.947|0.488|0.944|0.703|0.944|0.777|0.945|0.839|0.935|0.876|\n|SUN-MCCDs|0.979|0.704|0.978|0.857|0.979|0.900|0.979|0.930|0.972|0.943|\n|RU-MCCDs|0.842|0.244|0.849|0.466|0.845|0.548|0.845|0.641|0.847|0.742|\n|SU-MCCDs|0.884|0.305|0.889|0.541|0.886|0.623|0.884|0.705|0.887|0.795|\n|UN-MCCDs|0.914|0.371|0.915|0.606|0.913|0.684|0.914|0.763|0.913|0.834|\n|SUN-MCCDs|0.974|0.658|0.974|0.832|0.974|0.879|0.974|0.914|0.974|0.944|\n\nTable 13: The BAs and F2-scores of the CCD-based algorithms as the percentage of outliers over the entire simulated data set increases from 2% to 15% (for simulations with Gaussian clusters).# Minimal Distances between Outliers and Cluster Centers\n\n|Algorithm|Minimal Distances| | | | |\n|---|---|---|---|---|---|\n| |1.25|1.5|1.75|2.00|2.25|\n|RU-MCCDs|0.979|0.988|0.982|0.988|0.986|\n|SU-MCCDs|0.966|0.998|0.983|0.999|1.000|\n|UN-MCCDs|0.980|0.989|0.989|0.989|0.985|\n|SUN-MCCDs|0.962|0.997|0.976|0.997|0.992|\n|RU-MCCDs|1.000|0.914|1.000|0.914|1.000|\n|SU-MCCDs|1.000|0.929|1.000|0.929|1.000|\n|UN-MCCDs|1.000|0.990|1.000|0.990|1.000|\n|SUN-MCCDs|1.000|0.999|1.000|0.999|1.000|\n\nTable 14: The TPRs and TNRs of the CCD-based algorithms as the minimal distance from outliers to any cluster centers increases from 1.25 to 2.25 (for simulations with uniform clusters).# Figure 15: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the percentage of outlier increases (points within each clusters are (multi-variate) normally distributed).# Minimal Distances between Outliers and Cluster Centers\n\n|Distance|BA|F-score|2BA|F-score|2BA|F2-score|BA|F-score|2BA|F-score| |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|1.25|RU-MCCDs|0.984|0.940|0.985|0.942|0.987|0.946|0.987|0.945|0.986|0.944|\n| |SU-MCCDs|0.982|0.964|0.991|0.983|0.998|0.990|0.999|0.992|0.999|0.992|\n| |UN-MCCDs|0.985|0.895|0.989|0.951|0.987|0.945|0.991|0.956|0.990|0.952|\n| |SUN-MCCDs|0.980|0.953|0.987|0.970|0.995|0.982|0.997|0.983|0.999|0.989|\n|1.5|RU-MCCDs|0.957|0.754|0.957|0.754|0.957|0.754|0.960|0.767|0.960|0.767|\n| |SU-MCCDs|0.965|0.788|0.965|0.788|0.965|0.788|0.967|0.799|0.967|0.799|\n| |UN-MCCDs|0.995|0.963|0.995|0.963|0.995|0.963|0.995|0.963|0.995|0.963|\n| |SUN-MCCDs|1.000|0.996|1.000|0.996|1.000|0.996|1.000|0.996|1.000|0.996|# Table 15: The BAs and F2-scores of the CCD-based algorithms as the minimal distance from outliers to any cluster centers increases from 1.25 to 2.25 (for simulations with uniform clusters).\n\nOutliers. In other words, with Gaussian clusters, these algorithms identify most or all of the outliers, even if the outliers are close to regular observations at the cost of some false positives along the border of each cluster. Echoing the results of previous simulations, the \u201ccost\u201d is much lower for the SU-MCCD and SUN-MCCD algorithms than their prototypes because these two \u201cflexible\u201d algorithms generally end up with more than one covering ball for each cluster, which has better coverage for the regular observations.# 5.2.4 Varying The Distances Between Cluster Centers\n\nIn this section, we investigate whether the distance between clusters affects the performance of the four CCD-based outlier detection algorithms. Previously, the first cluster center is (3, ..., 3), and others are obtained by shifting three units from the first one in various directions. Therefore, these simulated clusters are always distinct and easy to separate. As a result, the four CCD-based algorithms could identify each cluster without.\n\n46# Figure 16\n\nSome realizations (with uniform clusters) of the simulation setting in Section 5.2.3, the minimal distance between outliers and cluster centers increases from 1.25 to 2.25. Red crosses are outliers, black points are regular observations. The minimal distances are indicated below each sub-figure.\n\n|(a) 1.25|(b) 1.5|(c) 1.75|\n|---|---|---|\n|(d) 2|(e) 2.25| |\n\nIn this setting, we alter the difficulty level of clustering by changing the inter-cluster distances (the distances between pairs of points from different clusters). We keep the first cluster centered at (3, ..., 3), but we vary its distances to other cluster centers from 1.5 to 4. Here is where things get interesting: when the distance is smaller than 2, the chance that two or more clusters overlap is high, making it challenging to figure out the correct number of clusters and their locations. We are curious to see if the increasing difficulty level of clustering will affect the accuracy of outlier detection. Some challenges include (1) capturing the outliers close to two or more overlapping clusters with different intensities and (2) dealing with the swapping problem when clusters with different intensities overlap, since some regular observations from low-intensity clusters could be located near high-intensity clusters or the overlapping area, which could lead to many false positives for some outlier detection algorithms. Similar to the previous simulations, all other irrelevant factors are fixed, and we only list the relevant parts below. Again, we present realizations of synthetic data sets with uniform clusters in 2-dimensional space (although the simulation experiments are conducted on 3 and 10-dimensional space) in Figure 16 (for illustration purposes). It is not hard to see that when the distance is equal to 1.5 (Figure 16 (a)), the three clusters are highly overlapping, and separating them from each other is a challenging task.# (d) The F2-scores when d = 10.\n\nFigure 17: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the minimal distance from outliers to any cluster centers increases (points within each clusters are uniformly distributed). (a) The BAs for d = 3. (b) The F2-scores for d = 3. (c) The BAs for d = 10. (d) The F2-scores for d = 10.# Minimal Distances between Outliers and Cluster Centers\n\n|Minimal Distance|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1.25|RU-MCCDs| | |1.000|0.835|SU-MCCDs|1.000|0.941|UN-MCCDs|0.987|0.893|SUN-MCCDs|0.999|0.957|\n|1.50|RU-MCCDs| | |1.000|0.837|SU-MCCDs|1.000|0.941|UN-MCCDs|0.983|0.895|SUN-MCCDs|1.000|0.957|\n|1.75|RU-MCCDs| | |1.000|0.833|SU-MCCDs|1.000|0.942|UN-MCCDs|0.983|0.893|SUN-MCCDs|1.000|0.959|\n|2.00|RU-MCCDs| | |1.000|0.838|SU-MCCDs|1.000|0.941|UN-MCCDs|0.983|0.896|SUN-MCCDs|1.000|0.956|\n|2.25|RU-MCCDs| | |1.000|0.836|SU-MCCDs|1.000|0.942|UN-MCCDs|0.985|0.894|SUN-MCCDs|1.000|0.959|\n\nTable 16: The TPRs and TNRs of the CCD-based algorithms as the minimal distance from outliers to any cluster centers increases from 1.25 to 2.25 (for simulations with Gaussian clusters).\n\ni. The centers of clusters are: \u03bc1 = (3, ..., 3), \u03bc2 = (3 + s, 3, ..., 3), and \u03bc3 = (3, 3 + s, 3, ..., 3), where s could be 1.5, 2, 2.5, 3.0, 3.5, and 4 (the study of focus in this section);\n\nWe summarize the results we obtained from Tables 18 to 21. The same as before, the BAs and F2-scores (Tables 18 and 20) are also presented as barplots in Figures 20 and 21, respectively.\n\nRecall that when clusters with different intensities overlap (the inter-cluster center distance s \u2264 2), the challenges include identifying the outliers near overlapping clusters with different intensities and addressing the swapping problem.\n\nFirstly, we explore the simulations with uniform clusters. When s \u2264 2, d = 3, all four algorithms...# Minimal Distances between Outliers and Cluster Centers\n\n| |1.25|1.50|1.75|2.00|2.25| | | | | | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Algorithm|BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|\n|RU-MCCDs|0.918|0.615|0.919|0.618|0.917|0.612|0.919|0.619|0.918|0.616| | | | | | | | | | | | |\n|SU-MCCDs|0.971|0.817|0.971|0.817|0.971|0.819|0.971|0.817|0.971|0.819| | | | | | | | | | | | |\n|UN-MCCDs|0.940|0.703|0.939|0.704|0.938|0.701|0.940|0.706|0.940|0.704| | | | | | | | | | | | |\n|SUN-MCCDs|0.978|0.859|0.979|0.860|0.980|0.865|0.978|0.857|0.980|0.865| | | | | | | | | | | | |\n|RU-MCCDs|0.847|0.462|0.847|0.462|0.845|0.458|0.849|0.466|0.846|0.460| | | | | | | | | | | | |\n|SU-MCCDs|0.888|0.539|0.888|0.539|0.885|0.533|0.889|0.541|0.886|0.536| | | | | | | | | | | | |\n|UN-MCCDs|0.913|0.602|0.913|0.602|0.913|0.602|0.914|0.603|0.914|0.605| | | | | | | | | | | | |\n|SUN-MCCDs|0.974|0.835|0.974|0.835|0.974|0.835|0.974|0.832|0.975|0.838| | | | | | | | | | | | |\n\nTable 17: The BAs and F2-scores of the CCD-based algorithms as the minimal distance from outliers to any cluster centers increases from 1.25 to 2.25 (for simulations with Gaussian clusters).# Cluatet Centere\n\n(a) The BAs when d = 3.\n\n(b) The F2-scores when d = 3.\n\n(c) The BAs when d = 10.\n\n(d) The F2-scores when d = 10.# Figure 18\n\nThe barplots summarizing the performances of the CCD-based outlier detection algorithms as the minimal distance from outliers to any cluster centers increases (points within each clusters are (multivariate) normally distributed). (a) The BAs for d = 3. (b) The F2-scores for d = 3. (c) The BAs for d = 10. (d) The F2-scores for d = 10.\n\nThe algorithms address the two challenges effectively. The SU-MCCD and SUN-MCCD algorithms exhibit stable behavior regardless of the cluster distances. However, the TPRs and TNRs of the RU-MCCD and UN-MCCD algorithms are slightly lower when s \u2264 2, compared to the other cases where clusters are distinct. When increasing the number of dimensions to 10, all the algorithms become insensitive to cluster distances, even when clusters overlap. For example, the F2-scores of the RU-MCCD algorithm are stable (0.771, 0.797, 0.785, 0.767, 0.767, and 0.754), although they lag behind other algorithms.\n\nThen, we consider the simulation settings with Gaussian clusters. It is interesting to see that the cluster distance has minimal influence on the performance, no matter how close the simulated clusters are. This could be explained as follows: the two challenges we discussed at the beginning of this section exist for Gaussian clusters even when they do not overlap because outlier and regular points can be close due to the wide span of Gaussian clusters. Similar to the previous simulations, the two \u201cflexible\u201d algorithms perform better than the others when d = 3, and the SUN-MCCD algorithms deliver the best results and outperform other algorithms by a large gap when d = 10.\n\n49# 5.2.5 Varying the Noise Level of Gaussian Clusters\n\nThe second last factor to study is the noise level for Gaussian clusters. Therefore, this simulations are conducted only on data sets with Gaussian clusters. In the previous study, \u201cnoise\u201d is defined as the points close to the clusters, typically exhibiting much lower vicinity intensity than the observations deep in the clusters. In the previous work, we constructed the support with a radius randomly chosen between 0.7 and 1.3 for a Gaussian cluster. We tune the covariance such that approximately 1% of the regular observations fell beyond the desired support and were thus perceived as noise. In other words, each support is a 99th percentile contour of an uncorrelated Gaussian density. In the current setting, without changing the range of the radii, we conduct simulations with the noise level increasing from 1% to 10%. Different noise levels can be achieved by adjusting the scale of the covariance matrix. All the other factors remain consistent with previous simulations.\n\nOnce the radius of the support is known, the desired scale can be obtained via a \u03c6 d distribution. Some realizations in a 2-dimensional space are presented in Figure 22. Observe that the Gaussian clusters have a wider span as the noise level increases, and the noise and outliers get much closer. Therefore, we expect the severity level of the swamping problem to rise incrementally, and we are particularly interested in the behaviour of all four CCD-based algorithms under these conditions.\n\ni. We only conduct the simulations with Gaussian clusters since we study the noise\n\nFigure 19: Some realizations (with uniform clusters) of the simulation setting in Section 5.2.4, the distance between cluster centers increases from 1.5 to 4. Red crosses are outliers, black points are regular observations. The distance between clusters are indicated below the sub-figures.\n\n|(a) 1.5|(b) 2|(c) 2.5|\n|---|---|---|\n|(d) 3|(e) 3.5|(f) 4|# Distances Between Cluster Centers\n\n| |1.5|2|2.5|3|3.5|4| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR| |\n|RU-MCCDs|0.987|0.969|0.961|0.982|0.970|0.987|0.985|0.988|0.985|0.988|0.986|0.988|\n|SU-MCCDs|1.000|0.984|0.999|0.992|0.999|0.997|1.000|0.998|0.999|0.999|0.995|0.998|\n|UN-MCCDs|0.991|0.972|0.984|0.982|0.983|0.987|0.991|0.990|0.993|0.989|0.991|0.990|\n|SUN-MCCDs|0.999|0.985|0.998|0.991|0.999|0.996|0.998|0.998|0.998|0.998|0.997|0.998|\n|RU-MCCDs|1.000|0.922|1.000|0.933|1.000|0.928|1.000|0.920|1.000|0.920|1.000|0.914|\n|SU-MCCDs|1.000|0.941|1.000|0.947|1.000|0.940|1.000|0.939|1.000|0.929|1.000|0.925|\n|UN-MCCDs|1.000|0.965|1.000|0.987|1.000|0.990|1.000|0.990|1.000|0.990|1.000|0.991|\n|SUN-MCCDs|1.000|0.982|1.000|0.994|1.000|0.998|1.000|0.999|1.000|0.999|1.000|0.999|\n\nTable 18: The TPRs and TNRs of the CCD-based algorithms as the distance between cluster centers increases from 1.5 to 4 (for simulations with uniform clusters).# Distances Between Cluster Centers\n\n| |1.5|2|2.5|3|3.5|4| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score|2BA|F-score| |\n|RU-MCCDs|0.978|0.885|0.972|0.906|0.979|0.930|0.987|0.945|0.987|0.945|0.987|0.946|\n|SU-MCCDs|0.992|0.943|0.996|0.970|0.998|0.988|0.999|0.992|0.999|0.995|0.997|0.988|\n|UN-MCCDs|0.982|0.897|0.983|0.924|0.985|0.940|0.991|0.956|0.991|0.954|0.991|0.956|\n|SUN-MCCDs|0.992|0.945|0.995|0.965|0.998|0.984|0.998|0.991|0.998|0.991|0.998|0.990|\n|RU-MCCDs|0.961|0.771|0.967|0.797|0.964|0.785|0.960|0.767|0.960|0.767|0.957|0.754|\n|SU-MCCDs|0.971|0.817|0.974|0.832|0.970|0.814|0.970|0.812|0.965|0.788|0.963|0.778|\n|UN-MCCDs|0.983|0.883|0.994|0.953|0.995|0.963|0.995|0.963|0.995|0.963|0.996|0.967|\n|SUN-MCCDs|0.991|0.936|0.997|0.978|0.999|0.992|1.000|0.996|1.000|0.996|1.000|0.996|\n\nTable 19: The BAs and F2-scores of the CCD-based algorithms as the distance between cluster centers increases from 1.5 to 4 (for simulations with uniform clusters).\n\nii. The noise level of each Gaussian cluster is set to 1%, 3%, 5%, 7%, and 10% (the study of focus in this section).\n\nThe results obtained from this simulation setting are summarized in Tables 22 and 23. The BAs and F2-scores, which can be found in Table 23, are also represented as a barplot in Figure 23.\n\nObserve that all four CCD-based algorithms perform stably, regardless of the noise level. For instance, when d = 3, the F2-scores of the SUN-MCCD algorithm are 0.860, 0.859, 0.858, 0.857, and 0.855, presenting a slight downtrend, it suggests that all the algorithms are highly adaptable to the span of Gaussian clusters and their distances to outliers. This phenomenon can be attributed to a similar reason discussed in Section 5.2.3. Notably, the TPRs of all the algorithms are 1 or close to 1, while the TNRs are substantially lower, particularly when d = 10. Therefore, all the CCD-based algorithms isolate outliers from regular observations at the expense of some false positives, and this mechanism dynamically adapts to the scale of the covariance matrix of a Gaussian cluster. Moreover, the four algorithms achieve different levels of TNRs, with the SUN-MCCD algorithm performing the best and the RU-MCCD algorithm comparatively inferior (the worst).# Distances Between Cluster Centers\n\n| |1.5|2|2.5|3|3.5|4| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR| |\n|RU-MCCDs|1.000|0.849|1.000|0.835|1.000|0.835|1.000|0.833|1.000|0.834|1.000|0.836|\n|SU-MCCDs|1.000|0.948|1.000|0.942|1.000|0.941|1.000|0.944|1.000|0.945|1.000|0.941|\n|UN-MCCDs|0.961|0.901|0.983|0.896|0.988|0.895|0.997|0.890|0.999|0.895|0.999|0.894|\n|SUN-MCCDs|0.999|0.961|1.000|0.959|1.000|0.959|1.000|0.956|1.000|0.959|1.000|0.958|\n|RU-MCCDs|1.000|0.687|1.000|0.691|1.000|0.698|1.000|0.697|1.000|0.690|1.000|0.690|\n|SU-MCCDs|1.000|0.769|1.000|0.771|1.000|0.777|1.000|0.775|1.000|0.780|1.000|0.773|\n|UN-MCCDs|1.000|0.817|1.000|0.827|1.000|0.829|1.000|0.829|1.000|0.829|1.000|0.829|\n|SUN-MCCDs|1.000|0.939|1.000|0.948|1.000|0.947|1.000|0.947|1.000|0.947|1.000|0.947|\n\nTable 20: The TPRs and TNRs of the CCD-based algorithms as the distance between cluster centers increases from 1.5 to 4 (for simulations with Gaussian clusters).# 5.2.6 Collective Outliers in Convex Hull\n\nIn all the previous simulation settings, the outliers are scattered around the ground truth clusters as they are drawn from a large hypersphere of radius 5. That said, most outliers are isolates far from one another, except when the contamination level is exceptionally high (we investigated the cases when the contamination level is as high as 15% in Section 5.2.2). In this section, we study the scenarios when outliers form a small group, called collective outliers. We want to explore the robustness of all the CCD-based algorithms to the mask problem, which usually emerges when collective outliers exist. Therefore, in the artificial data sets of this section, outliers are generated within a hypersphere of radius 1. To add more challenges, the hypersphere covering outliers is located inside the convex hull of regular points, with the distance between the hypersphere and cluster centers varying. We conduct the simulations with only uniform clusters to ensure all the outliers are within the convex hull. Simulation details are as follows. Similarly, only the different factors (compared to the first focus study in Section 5.2.1) are presented.\n\n- Number of clusters: 2;# Figure 20: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the distance between cluster centers increases (points within each clusters are uniformly distributed).\n\n|Distances Between Cluster Centers|BA|F-score|BA|F-score|BA|F-score|BA|F-score|BA|F-score|BA|F-score|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1.5|0.925|0.635|0.918|0.615|0.918|0.615|0.917|0.612|0.917|0.613|0.918|0.616|\n|2|0.974|0.835|0.971|0.819|0.971|0.817|0.972|0.825|0.973|0.827|0.971|0.817|\n|2.5|0.931|0.702|0.940|0.706|0.942|0.707|0.944|0.703|0.947|0.714|0.947|0.712|\n|3|0.980|0.870|0.980|0.865|0.980|0.865|0.978|0.857|0.980|0.865|0.979|0.862|\n|3.5|0.844|0.457|0.846|0.460|0.849|0.466|0.849|0.465|0.845|0.459|0.845|0.459|\n|4|0.885|0.533|0.886|0.535|0.889|0.541|0.888|0.539|0.890|0.545|0.887|0.537|\n| |0.909|0.590|0.914|0.603|0.915|0.606|0.915|0.606|0.915|0.606|0.915|0.606|\n| |0.970|0.812|0.974|0.835|0.974|0.832|0.974|0.832|0.974|0.832|0.974|0.832|# Table 21: The BAs and F2-scores of the CCD-based algorithms as the distance between cluster centers increases from 1.5 to 4 (for simulations with Gaussian clusters).# Figure 21: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the distance between cluster centers increases (points within each clusters are (multivariate) normally distributed).\n\n- (a) The BAs for d = 3.\n- (b) The F2-scores for d = 3.\n- (c) The BAs for d = 10.\n- (d) The F2-scores for d = 10.# Table 22: The TPRs and TNRs of the CCD-based algorithms as the approximate noise level of each Gaussian cluster increases from 1% to 10%.\n\n|Level of Noise|1%|1%|3%|3%|5%|5%|7%|7%|10%|10%|\n|---|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs d = 3|1.000|0.833|1.000|0.833|1.000|0.833|1.000|0.833|1.000|0.833|\n|SU-MCCDs d = 3|1.000|0.941|1.000|0.941|1.000|0.941|1.000|0.941|1.000|0.941|\n|UN-MCCDs d = 3|0.997|0.890|0.998|0.890|0.997|0.890|0.997|0.890|0.998|0.890|\n|SUN-MCCDs d = 3|1.000|0.957|0.999|0.957|0.998|0.957|0.997|0.957|0.994|0.957|\n|RU-MCCDs d = 10|1.000|0.697|1.000|0.697|1.000|0.697|1.000|0.698|1.000|0.698|\n|SU-MCCDs d = 10|1.000|0.777|1.000|0.777|1.000|0.776|1.000|0.777|1.000|0.777|\n|UN-MCCDs d = 10|1.000|0.829|1.000|0.829|1.000|0.829|1.000|0.829|1.000|0.829|\n|SUN-MCCDs d = 10|1.000|0.948|1.000|0.948|1.000|0.948|1.000|0.948|1.000|0.948|# ii. The centers of clusters are:\n\n\u03bc1 = (3, 3, ..., 3) and \u03bc2 = (9, 3, ..., 3) (where d = 3, 10);# iii. The outlier set Coutlier is generated uniformly within a hypersphere of radius 1.\n\nThe center of the hypersphere is \u03bc0 = (3 + s, 3, ..., 3), where s represents the distance of it to the first cluster center, and it is set to 1.5, 2, 2.5, and 3, respectively. When s \u2264 2, the outlier set and the first cluster overlap, and there is no minimal distance between outliers and any cluster centers.\n\nFigure 22 illustrates some realizations of the data set in a 2-dimensional space. Apparently, in the first two sub-figures where s \u2264 2, the support of the outlier set and the left cluster overlap, and separating them is challenging.\n\nThe simulation results are summarized in Tables 24 and 25. Similarly, the BAs and F2-scores are also represented as a barplot in Figure 25.\n\n53# Figure 22\n\nSome realizations of the simulation setting in Section 5.2.5, the noise level of Gaussian cluster centers increases from 1% to 10%. Red crosses are outliers, black points are regular observations. The noise levels are indicated below each sub-figure.\n\nWhen \\( s \\leq 2 \\), the simulation results show that the RU-MCCD and UN-MCCD algorithms perform comparably and are superior to the other two \u201cflexible\u201d algorithms when \\( d = 3 \\) or \\( d = 10 \\). For example, when \\( d = 3 \\) and \\( s = 1.5 \\) or \\( s = 2 \\), the \\( F^2 \\)-scores of the RU-MCCD algorithm are 0.850 and 0.926, substantially higher than 0.686 and 0.892 delivered by the SU-MCCD algorithm. The reason is that the SU-MCCD and SUN-MCCD algorithms use multiple covering balls for each cluster. Thus, the chance of capturing the outliers close to regular points is much higher, yielding more false negatives. When \\( s > 2 \\), the outlier set and regular points are well separated, and all four algorithms deliver similar performance and handle the collective outliers well with high \\( F^2 \\)-scores (at least 0.9). Generally, the two \u201cflexible\u201d algorithms perform slightly better in these cases.# Level of Noise\n\n|Algorithm|Noise Level| | | | | |\n|---|---|---|---|---|---|---|\n|1%|3%|5%|7%|10%| | |\n|RU-MCCDs|0.917|0.612|0.917|0.612|0.917|0.612|\n|SU-MCCDs|0.971|0.817|0.971|0.817|0.971|0.817|\n|UN-MCCDs|0.944|0.703|0.944|0.703|0.944|0.704|\n|SUN-MCCDs|0.979|0.860|0.978|0.859|0.978|0.858|\n|RU-MCCDs|0.849|0.465|0.849|0.465|0.849|0.466|\n|SU-MCCDs|0.889|0.541|0.889|0.541|0.888|0.540|\n|UN-MCCDs|0.915|0.606|0.915|0.606|0.915|0.606|\n|SUN-MCCDs|0.974|0.835|0.974|0.835|0.974|0.835|# Table 23\n\nThe BAs and \\( F^2 \\)-scores of the CCD-based algorithms as the approximate noise level of each Gaussian cluster increases from 1% to 10%.# Figure 23: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the approximate noise level increases (points within each clusters are (multivariate) normally distributed).\n\n- (a) The BAs for d = 3.\n- (b) The F2-scores for d = 3.\n- (c) The BAs for d = 10.\n- (d) The F2-scores for d = 10.# 6 Monte Carlo Experiments Under Random Cluster Process\n\nIn the previous sections, we conducted Monte Carlo experiments to evaluate the performance of each proposed outlier detection algorithm. The UN-MCCD algorithm delivers comparable or better performance compared to the RU-MCCD algorithm when the dimensionality d is small (d \u2264 5) and superior when d = 10 and 20. The conclusion is similar when comparing the SU-MCCD and SUN-MCCD algorithms. Additionally, the two \u201cshape-adaptive\u201d algorithms outperform their \u201cvanilla versions\u201d under the simulation cases with Gaussian clusters (except the simulation settings when d \u2248 50), especially the SUN-MCCD algorithm, which outperforms other CCD-based algorithms when d = 5, 10, and 20.\n\nHowever, the previous simulation settings (including the general simulation settings)\n\n|Distance|1.5|1.5|2|2|2.5|2.5|3|3|\n|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs|0.756|0.993|0.943|0.992|0.998|0.992|1.000|0.992|\n|SU-MCCDs|0.639|0.999|0.873|0.999|0.985|0.999|1.000|0.999|\n|UN-MCCDs|0.740|0.993|0.931|0.993|0.997|0.992|1.000|0.992|\n|SUN-MCCDs|0.604|0.999|0.837|0.999|0.971|0.999|0.999|0.999|\n|RU-MCCDs|0.730|0.977|0.900|0.977|0.996|0.977|1.000|0.977|\n|SU-MCCDs|0.710|0.991|0.904|0.991|0.997|0.991|1.000|0.991|\n|UN-MCCDs|0.695|0.995|0.880|0.994|0.998|0.994|1.000|0.994|\n|SUN-MCCDs|0.608|0.999|0.837|0.999|0.992|0.999|1.000|0.999|# Table 24: The TPRs and TNRs of the CCD-based algorithms as the distance between the collective outlier center and one of the cluster centers increases from 1.5 to 2.# Figure 24: Some realizations of the simulation setting containing collective outliers\n\nwhere s (indicated below each sub-figure) represents the distance between the left cluster center and the outlier center, and it increases from 1.5 to 3. Red crosses are outliers, black points are regular observations. All the outliers are within the convex hull of regular points.\n\n|Distance|1.5|1.5|2|2|2.5|2.5|3|3|\n|---|---|---|---|---|\n| |BA|F-score 2|BA|F2-score|BA|F-score 2|BA|F2-score|\n|RU-MCCDs|0.875|0.773|0.968|0.926|0.995|0.969|0.996|0.970|\n|SU-MCCDs|0.819|0.686|0.936|0.892|0.992|0.984|1.000|0.996|\n|UN-MCCDs|0.867|0.759|0.962|0.919|0.995|0.968|0.996|0.970|\n|SUN-MCCDs|0.802|0.653|0.918|0.862|0.985|0.973|0.999|0.995|\n|RU-MCCDs|0.854|0.706|0.939|0.843|0.997|0.917|0.999|0.920|\n|SU-MCCDs|0.851|0.727|0.948|0.891|0.994|0.965|0.996|0.967|\n|UN-MCCDs|0.845|0.725|0.937|0.881|0.996|0.976|0.997|0.978|\n|SUN-MCCDs|0.804|0.657|0.918|0.862|0.996|0.990|1.000|0.996|# Table 25: The BAs and F2-scores of the CCD-based algorithms as the distance between the collective outlier center and one of the cluster centers increases from 1.5 to 2.\n\nin Section 5.1 and the focus simulation settings in Section 5.2) are relatively simplistic as the cluster centers are fixed. Additionally, the sizes of data sets, the number of clusters, the inter-cluster distances, the contamination levels, etc., are also fixed values under each simulation setting. In order to evaluate the CCD-based algorithms we proposed thoroughly and compare them with existing outlier detection algorithms, we conduct additional Monte Carlo experiments with more flexible settings.\n\nUnlike previous simulation settings with levels of factors predetermined (e.g., n = 50, 100, ..., 500, number of cluster= 2, 3, 4.), converting those factors to random variables is a good solution towards our objective. To approach this goal, we try to simulate the Neyman-Scott cluster process [53], a class of cluster generation mechanisms with great randomness used widely in general practice. The realization of a general Neyman-Scott cluster process consists of two major steps, which are described as follows [7],\n\n1. Firstly, a point set S = {s1, s2, ..., sm} is generated from an HPP with intensity parameter \u21bc > 0, these points are called \u201cparents\u201d. In the second step, each cluster is generated around one of the parents.\n2. A finite set/cluster Ci = {yi1, yi2, ..., yini} is generated around each size of Ci (i.e., ni) follows a Poisson distribution with mean {yi1, yi2, ..., yini} are generated i.i.d from the following probability density function, which depends on the distances (or similarities) to their parent P(x|s) = \u03d6i 12h(||x \u2243 s||), i \u03d6i.# Figure 25: The barplots summarizing the performances of the CCD-based outlier detection algorithms as the approximate distance between the center of collective outliers and the center of one of the clusters increases from 1.5 to 3 (points within each clusters are normally distributed).\n\n|(a) The BAs when d = 3.|(b) The F2-scores when d = 3.|\n|---|---|\n|(c) The BAs when d = 10.|(d) The F2-scores when d = 10.|\n\nwhere ||x\u2243s || represents a distance measure between x and s, \u03d6 is a scale parameter, and h is called the kernel function of the Neyman-Scott cluster process. The points generated for cluster Ci are also called the \u201coffspring\u201d or the \u201cchildren\u201d of si. Finally, the union of all offspring points \u2191 s \u2192S Ci is a realization of a general Neyman-Scott cluster process, and the parent point set S will be dropped from the simulated data sets eventually.\n\nOne of the advantages of using the Neyman-Scott cluster process is the randomness of the intensity, location, and number of clusters. To simulate a general Neyman-Scott cluster process, \u21bc, \u03bc, and the kernel function h need to be specified. We shall consider two standard models, the Mat\u00e9rn cluster process [50] and the Thomas cluster process [73, 20]. They only differ on the kernel function h.# The Mat\u00e9rn and Thomas Cluster Processes\n\n1. The kernel of Mat\u00e9rn cluster process is h(x) = \u03d1 11{||x|| \u2264 1}, i.e., a uniform density on a unit disc. The scale parameter \u03d6 is the radius of the disc.\n2. On the other hand, the Thomas cluster process employs the Gaussian kernel h(x) = 21\u03d1 exp(\u2212||x||2), and \u03d6 is the standard deviation, controlling the intensity of each cluster.\n3. The formulas of the kernels above are for the subsequent Monte Carlo experiments for high-dimensional spaces.# Simulation Settings for Mat\u00e9rn and Thomas Cluster Processes\n\nWith the Mat\u00e9rn and Thomas cluster processes, we consider the following 3 simulation settings within a unit (hyper) square across a different number of dimensions (d = 2, 3, 5, 10, 20). (\u21bc M , \u03bc M , \u03d6 M ) and (\u21bc T , \u03bc T , \u03d6 T ) are the parameter sets of the two cluster processes. It is worth noting that any offspring falling beyond the unit (hyper) square will be dropped. For compensation, the values of \u03bc M and \u03bc T vary for different dimensions, such that the expected sizes of generated data sets are approximately 200. Except for the pure Mat\u00e9rn or Thomas cluster process, we consider the hybrid of them as the third simulation setting and call it the \u201cmixed\u201d point process. The details of each simulation setting are presented below:# I\n\nSimulate a Mat\u00e9rn cluster process with parents intensity \u21bc M = 6, radius \u03d6 M = 0.1. The mean size of each cluster \u03bc M is set to be 33.00, 35.26, 37.45, 40.37, and 44.48 as the number of dimensions d increases from 2 to 20.# II\n\nConduct a Thomas cluster process with \u21bc T = 6, \u03d6 T = 0.07 (the covariance matrix is \u03d6 T 2I d ). The mean size of each cluster \u03bc T is set to 33.70, 36.13, 42.38, 55.16, and 90.54 as the dimensionality d increases from 2 to 20.# III\n\nConduct a Mat\u00e9rn cluster process and a Thomas cluster process synchronously with \u21bc M = \u21bc T = 3, \u03d6 M = 0.1, and \u03d6 T = 0.07. \u03bc M and \u03bc T are set to 33.30, 36.15, 39.72182, 46.78, and 60.31 as d increases from 2 to 20.\n\nUnder the above simulation settings, latent outliers follow an HPP with an intensity of 20. Outliers have certain distances to parents depending on the type of the corresponding cluster process (the minimum distance to any parents in the Mat\u00e9rn and Thomas cluster processes are 2\u03d6 M and 3.33\u03d6 T, respectively). Additionally, to avoid generating data sets where the sizes of regular observations and outliers are close, we set the lower bound of the size of regular observations to 80. We want to ensure that every simulated data set is strictly imbalanced (regular points outnumber outliers by a large margin).\n\nFigures 26, 27, and 28 present some realizations of the three simulation settings on R\u00b2. We compare the performance with some other existing outlier detection algorithms, including Local Outlier Factor (LOF) [10], Density Based Spatial Clustering of Applications with Noise (DBSCAN) [22], the Minimal Spanning Tree (MST) Method [79], Outlier Detection using In-degree Number (ODIN) [33] and isolation Forest for outlier detection [44].\n\nLOF [10] is a density-based outlier detection algorithm. It measures the outlyingness of points by comparing their local reachability density with their nearest neighbors. The number of nearest neighbors is an input parameter, denoted as k. Rather than choosing only one value for k, Breunig et al. provided a heuristic that considers a range of k value instead and computes the corresponding LOF values; then all the points are ranked by their highest LOF values [10]. We conduct our experiment following this heuristic and choose the lower and upper bound of k to be 11 and 30, respectively, consistent with the guidelines provided by Breunig et al. After several experiments, we found the optimal threshold is 1.5, which is as expected, given the fact that the LOFs of most regular points are close to 1 [10].\n\nDBSCAN [22] is a density-based clustering method proposed by Ester et al., tuned for data sets with noise or outliers. Thus, it can also be used for outlier detection. This approach is constructed based on the idea that points deep inside a cluster generally have a minimum number (denoted MinPts) of neighbors within a given radius (denoted Eps); Ester et al. call these points core points or seeds. To find a cluster, DBSCAN starts with an arbitrary seed, denoted as p; then it builds a cluster with p by finding all the points.# Clustering and Outlier Detection\n\nthat are density-reachable from it; after that, the above steps are repeated on the next unassigned seed until no more new seed can be found; finally, the points that are not connected to any seeds are labeled as noise or outliers. To determine the value of the input parameters MinPts and Eps, Ester et al. offered a heuristic which sets MinPts to 4, then sorts the 4-dist (the distance of a point to its 4th nearest neighbor) of the entire data set, and find the value at the first \u201celbow\u201d, setting it to be Eps [22]. Although finding the first \u201celbow\u201d point is easy with the naked eye, it is not feasible in our Monte Carlo experiments with 1000 data sets. Fortunately, Ester et al. provided another heuristic allowing users to enter the estimated percentage of outliers to derive proper value for Eps. To give DBSCAN some advantages, we adopt the second heuristic and set the percentage of outliers 9%.\n\nThe MST method [79] is a graph-based approach used for clustering. It can label any minority clusters or isolates as outliers. First, it constructs a graph with data points as nodes and the distance between any two points as edge weight. The MST is then created by linking all nodes with the minimum sum of weights while avoiding cycles. Then, the edges with substantially larger weights than the average weight of their adjacent edges are considered \u201cinconsistent\u201d and are removed, effectively breaking the MST into subtrees that correspond to clusters. Clustering based on MST helps identify clusters with arbitrary shapes. However, constructing the MST can be computationally expensive for large data sets, and its performance is sensitive to the choice of threshold for identifying inconsistent edges [80]. In the subsequent Monte Carlo experiments, we tested several thresholds ranging from 1.1 to 3 and found the optimal thresholds are 1.7, 1.7, 1.4, 1.2, and 1.1 as d increases from 2 to 20, that they deliver the best overall performance. Additionally, we label any clusters with sizes smaller than 4% of the size of the entire data set as outliers, which is consistent with our CCD-based algorithms. We want to explore the performance of a typical clustering algorithm on outlier detection.\n\nODIN [33] is a graph-based outlier detection algorithm using k-nearest-neighbor graphs. The in-degree of an observation refers to the number of times that point appears within the k nearest-neighbor sets of other points. The main idea of ODIN is based on the assumption that outliers typically have lower in-degrees because they deviate from regular observations. The observations with in-degrees smaller than a pre-specified threshold T are labeled as outliers. ODIN is simple, computationally efficient, and can work without assumptions on data distribution [75]. However, like most other algorithms, it is sensitive to the choice of k and T, and the optimal values depend on the specific data set and domain knowledge. We make k and T in the following Monte Carlo simulations dynamic. ODIN delivers decent overall performance when setting the two input parameters to 0.5 and 0.33 degrees of the size of the corresponding data set.\n\niForest [44] is an unsupervised graph-based outlier detection algorithm. The main idea is based on the fact that outliers are rare and generally distinctive and are more likely to be separated from other regular points in a binary tree. Specifically, this is done by constructing a random decision tree (called iTrees) and partitioning a random sub-sample based on randomly chosen features and split values; the depth of the tree is determined by sample size. iForest (also called iForest) is a collection of iTree, and the outlyingness score of a point is determined by the average path length to the root; regular points will be more easily isolated near the root of these trees, leading to shorter average path lengths and smaller outlyingness score. We construct an iForest with 1000 iTrees with sub-sample size 64 for each to ensure the convergence of the outlyingness scores, which align with the guidance offered by Liu et al. [44]. Additionally, we found that a threshold of 0.57 (for the outlyingness score) delivers decent overall performance and is close to Liu et al.\u2019s choice.\n\n59# Performance of Outlier Detection Algorithms\n\nThe mean performance (out of 1000 repetitions) of each outlier detection algorithm under the three simulation settings are summarized in the subsequent tables (Tables 26 to 30, and 31). The same as the previous simulation settings, we select TPR, TNR, BA, and F2-score to assess their performance.# Figures\n\nFigure 26: Three realizations of a Mat\u00e9rn cluster process (the simulation setting I on Section 6) on a 2-dimensional plane with \u21bc M = 6, \u03d6 M = 0.1, and \u03bc M = 33, where black dots are regular points, green dots are parents, and red dots are outliers.\n\nFigure 27: Three realizations of a Thomas cluster process (the simulation setting II on Section 6) on a 2-dimensional plane with \u21bc T = 6, \u03d6 T = 0.005, and \u03bc T = 33.7, where black dots are regular points, blue dots are parents, and red dots are outliers.# Algorithm Performance\n\nLOF delivers excellent overall performance, outperforming other algorithms under most simulation settings as the TPRs exceed 0.95 and TNRs larger than 0.85 substantially, with F2-scores approximately equal to or larger than 0.8 regardless of the type of point process. The results align with our expectation because the outliers generated have low local density, and LOF has the advantage of identifying those low-density points thanks to its mechanism involving Local Reachability Density (LRD). Furthermore, unlike most clustering-based algorithms, the performance of LOF does not depend on the quality of the clustering result. However, its performance declines gradually when d \u21d4 10, e.g., under the Mat\u00e9rn cluster process, the F2-scores are 0.866, 0.926, 0.844, 0.802, and 0.774 as d goes from 2 to 20. Here is the reasoning: With the data size remaining at the same level, increasing the number of dimensions results in more regular observations with low local densities. Therefore, the chance that LOF misclassifies regular points as outliers increases, leading to higher False Positive Rates (FPRs).\n\nDBSCAN exhibits strong performance when d = 2, e.g., under the Thomas cluster process, its F2-score reaches 0.755, outperforming all other algorithms.# Figure 28:\n\nThree realizations of a mixed cluster process (the simulation setting III on Section 6) on a 2-dimensional plane with M = T = 3, \u03d6 M = 0.1, \u03d6 T = 0.005, and \u03bc M = \u03bc T = 33.40, where black dots are regular points, green and blue dots are parents, and red dots are outliers.# Table 26:\n\nThe TPRs and TNRs of selected outlier detection algorithms under a Mat\u00e9rn cluster process (the simulation setting I in Section 6).\n\n|Algorithms|d = 2|d = 2|d = 3|d = 3|d = 5|d = 5|d = 10|d = 10|d = 20|d = 20|\n|---|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs|0.949|0.926|0.941|0.932|0.973|0.923|0.982|0.828|0.981|0.654|\n|SU-MCCDs|0.969|0.954|0.970|0.940|0.971|0.945|0.982|0.849|0.979|0.678|\n|UN-MCCDs|0.939|0.931|0.940|0.936|0.942|0.957|0.978|0.948|0.978|0.841|\n|SUN-MCCDs|0.952|0.948|0.970|0.932|0.940|0.973|0.977|0.961|0.977|0.853|\n|LOF|0.999|0.962|0.999|0.962|1.000|0.927|0.999|0.866|0.999|0.842|\n|DBSCAN|0.891|0.988|0.789|0.996|0.768|1.000|0.771|1.000|0.750|1.000|\n|MST|0.659|0.661|0.558|0.875|0.623|0.881|0.713|0.855|0.757|0.802|\n|ODIN|0.912|0.937|0.918|0.977|0.905|0.988|0.898|0.991|0.870|0.999|\n|iForest|0.855|0.904|0.756|0.946|0.800|0.967|0.915|0.974|0.982|0.972|\n\nTNRs are almost 1 under all simulation cases thanks to the exceptional clustering quality. However, its TPRs decrease gradually as d increases, particularly under the simulation settings with Gaussian clusters. For example, the TPRs are 0.849, 0.789, 0.749, 0.746, and 0.725 under the mixed cluster process. DBSCAN\u2019s distance-based mechanism can explain this issue. The algorithm labels outliers by identifying points whose 4th-nearest-neighbor distances (4th-dists) are substantially greater than others. However, the 4th-dists of outliers become close to those of regular points located along the edges of Gaussian clusters, making it challenging to differentiate them with DBSCAN, and this issue deteriorates as the number of dimensions increases and distances between points become close. Consequently, even if an outlier has a slight chance of being a \u201cseed\u201d, the likelihood that this outlier being density-reachable to an existing seed grows with higher dimensionality, leading to smaller TPRs. Nonetheless, DBSCAN remains a top-performing algorithm when d \u2264 5.\n\nThe MST algorithm consistently delivers the poorest performance under each simulation setting. For instance, under the Thomas cluster process, its F2-scores are 0.240, 0.384, 0.557, 0.569, and 0.582 \u2013 substantially lower than those of the other algorithms, even after carefully tuning the thresholds for different dimensions. The MST algorithm generally possesses several inherent weaknesses in clustering and outlier detection. Firstly, it lacks robustness against noise or outliers when identifying and removing \u201cinconsistent\u201d.# Table 27: The BAs and F 2 -scores of selected outlier detection algorithms under a Mat\u00b4ern cluster process (the simulation setting I in Section 6).\n\n|Algorithms|d = 2|d = 2|d = 3|d = 3|d = 5|d = 5|d = 10|d = 10|d = 20|d = 20|\n|---|---|---|---|---|---|\n| |BA|F-score 2|BA|F2-score|BA|F-score 2|BA|F2-score|BA|F-score 2|\n|RU-MCCDs|0.938|0.732|0.937|0.824|0.948|0.853|0.905|0.747|0.818|0.595|\n|SU-MCCDs|0.962|0.822|0.955|0.863|0.958|0.886|0.916|0.886|0.829|0.610|\n|UN-MCCDs|0.935|0.730|0.938|0.833|0.950|0.875|0.963|0.892|0.910|0.755|\n|SUN-MCCDs|0.950|0.787|0.951|0.851|0.957|0.902|0.969|0.912|0.915|0.768|\n|LOF|0.981|0.866|0.981|0.926|0.964|0.884|0.933|0.802|0.921|0.774|\n|DBSCAN|0.940|0.827|0.893|0.794|0.884|0.786|0.886|0.789|0.875|0.767|\n|MST|0.660|0.283|0.717|0.450|0.752|0.525|0.784|0.556|0.780|0.536|\n|ODIN|0.925|0.783|0.948|0.882|0.947|0.901|0.945|0.901|0.932|0.879|\n|iForest|0.880|0.615|0.851|0.691|0.884|0.775|0.945|0.877|0.977|0.925|# Table 28: The TPRs and TNRs of selected outlier detection algorithms under a Thomas cluster process (the simulation setting II in Section 6).\n\n|Algorithms|d = 2|d = 2|d = 3|d = 3|d = 5|d = 5|d = 10|d = 10|d = 20|d = 20|\n|---|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs|0.924|0.907|0.966|0.852|0.987|0.772|0.976|0.669|0.974|0.497|\n|SU-MCCDs|0.880|0.959|0.942|0.922|0.983|0.849|0.976|0.734|0.973|0.534|\n|UN-MCCDs|0.875|0.932|0.943|0.897|0.979|0.860|0.990|0.822|0.980|0.660|\n|SUN-MCCDs|0.824|0.963|0.918|0.941|0.970|0.922|0.989|0.889|0.979|0.744|\n|LOF|0.979|0.943|0.960|0.960|0.967|0.961|0.997|0.921|0.996|0.862|\n|DBSCAN|0.824|0.990|0.684|0.998|0.728|0.999|0.726|0.999|0.707|0.999|\n|MST|0.602|0.697|0.485|0.875|0.668|0.868|0.769|0.809|0.869|0.739|\n|ODIN|0.891|0.930|0.903|0.917|0.916|0.907|0.899|0.895|0.859|0.879|\n|iForest|0.857|0.892|0.708|0.938|0.644|0.961|0.716|0.975|0.789|0.972|\n\nFor example, given two distinct clusters of points and a few noise points or outliers between them, the MST algorithm might falsely link them, misinterpreting two clusters as one. Secondly, it lacks the mechanisms to address the masking problem. Since the distances between closely grouped outliers can be similar, the MST algorithm may retain most edges connecting them, resulting in low TPRs.\n\nThe performance of the ODIN algorithm is stable across different dimensions. It delivers the best performance under the Mat\u00b4ern cluster process, where the F 2 -scores are 0.783, 0.882, 0.901, 0.901, and 0.879, close to or even higher than those by LOF. However, its performance degrades when there are Gaussian clusters, which is still comparable to LOF under the mixed point process but substantially worse under the Thomas cluster process. It is expected when considering the characteristics of the kNN graph, where the points along the border of Gaussian clusters tend to have low in-degree numbers.\n\nUnlike other algorithms, iForest behaves uniquely compared to other algorithms: its performance improves incrementally as d increases. For instance, under the Thomas cluster process, the F 2 -scores progress from 0.545 to 0.774 as the dimensions increase from 2 to 20. While delivering mediocre performance when d \u2264 5, it outperforms most other algorithms under most simulation settings when d exceeds 10. For example, under the Mat\u00b4ern cluster process, the F 2 -scores reach 0.925 when d = 20, substantially higher than any other algorithms. This behavior can be explained by its sensitivity to swamping and masking problems, which are prevalent in low-dimensional space where the data points are relatively dense. Although building iTrees on smaller subsets of the data reduces the# Table 29: The BAs and F 2 -scores of selected outlier detection algorithms under Thomas cluster process (the simulation setting II in Section 6).\n\n|Algorithms| |d = 2| |d = 3| |d = 5| |d = 10| |d = 20|\n|---|---|---|---|---|---|---|---|---|---|---|\n| |BA|F-score 2|BA|F2-score|BA|F-score 2|BA|F2-score|BA|F-score 2|\n|RU-MCCDs|0.916|0.611|0.909|0.706|0.880|0.682|0.823|0.603|0.736|0.509|\n|SU-MCCDs|0.920|0.711|0.932|0.794|0.916|0.763|0.855|0.652|0.754|0.526|\n|UN-MCCDs|0.904|0.639|0.920|0.751|0.920|0.756|0.906|0.743|0.820|0.601|\n|SUN-MCCDs|0.894|0.687|0.930|0.806|0.946|0.845|0.939|0.822|0.862|0.664|\n|LOF|0.961|0.741|0.960|0.877|0.964|0.908|0.959|0.876|0.929|0.802|\n|DBSCAN|0.907|0.755|0.841|0.708|0.864|0.751|0.863|0.744|0.853|0.726|\n|MST|0.650|0.240|0.680|0.384|0.768|0.557|0.789|0.569|0.804|0.582|\n|ODIN|0.911|0.634|0.910|0.749|0.912|0.778|0.897|0.759|0.869|0.713|\n|iForest|0.875|0.545|0.823|0.632|0.803|0.633|0.846|0.717|0.881|0.774|# Table 30: The TPRs and TNRs of selected outlier detection algorithms under a mixed cluster process (the simulation setting III in Section 6).\n\n|Algorithms|d = 2|d = 3|d = 5|d = 10| |d = 20| | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs|0.942|0.907|0.953|0.884|0.978|0.856|0.975|0.745|0.980|0.588|\n|SU-MCCDs|0.925|0.952|0.955|0.921|0.975|0.900|0.974|0.775|0.974|0.617|\n|UN-MCCDs|0.910|0.926|0.927|0.912|0.952|0.914|0.984|0.883|0.975|0.757|\n|SUN-MCCDs|0.891|0.952|0.939|0.932|0.946|0.950|0.983|0.915|0.974|0.779|\n|LOF|0.990|0.948|0.984|0.957|0.984|0.942|0.998|0.893|0.993|0.857|\n|DBSCAN|0.849|0.988|0.789|0.996|0.749|0.998|0.746|0.998|0.725|0.997|\n|MST|0.639|0.682|0.525|0.875|0.657|0.880|0.736|0.836|0.809|0.784|\n|ODIN|0.899|0.943|0.906|0.944|0.911|0.952|0.885|0.956|0.827|0.968|\n|iForest|0.851|0.898|0.730|0.941|0.708|0.960|0.837|0.963|0.955|0.943|\n\nIntensity, making it easier to isolate outliers, it is not a perfect solution. If swamping or masking is severe within a data set, even iTrees with sub-samples struggle to differentiate outliers effectively.\n\nNow, we focus on the four CCD-based algorithms. Due to the reasons outlined earlier, the SUN-MCCD and SU-MCCD algorithms consistently perform better than their prototypes (the RU-MCCD and UN-MCCD algorithms) under all the simulation settings, which is consistent with the result of the previous Monte Carlo simulations. For instance, under the Thomas cluster process, the SUN-MCCD algorithm attains F 2 -scores of 0.687, 0.806, 0.845, 0.822, and 0.664, surpassing those of the UN-MCCD algorithm. On the other hand, the SUN-MCCD and SU-MCCD algorithms exhibit similar performance when d \u2264 3 due to the same mechanisms they share. However, once d exceeds 5, the SUN-MCCD algorithm achieves superior performance thanks to its better adaptability in high dimensions. For example, their F 2 -scores are 0.850 and 0.685 under the mixed cluster process when d = 20. Consequently, our primary comparison will focus on the SUN-MCCD algorithms against other established approaches.\n\nUnder the Mat\u00e9rn cluster process, the SUN-MCCD algorithm delivers decent results. When d \u2264 3, its F 2 -scores are 0.787 and 0.851, following ODIN closely and slightly lower than those of LOF and DBSCAN. When d = 5 and 10, it attains the highest F 2 -scores among all the algorithms, with both surpassing 0.9. It performs worse than ODIN and iForest when d increases to 20; this is because SUN-MCCD is distribution-based, and capturing the distribution patterns in a data set with limited size within high-dimensional.# Algorithms\n\n| | |d = 2| |d = 3| |d = 5|d = 10| | |d = 20|\n|---|---|---|---|---|---|---|---|---|---|---|\n| |BA|F-score 2|BA|F2-score|BA|F-score 2|BA|F2-score|BA|F-score 2|\n|RU-MCCDs|0.925|0.660|0.919|0.752|0.917|0.763|0.860|0.658|0.784|0.550|\n|SU-MCCDs|0.939|0.756|0.938|0.813|0.938|0.819|0.875|0.685|0.796|0.582|\n|UN-MCCDs|0.918|0.678|0.920|0.769|0.933|0.815|0.934|0.806|0.866|0.663|\n|SUN-MCCDs|0.922|0.736|0.936|0.816|0.948|0.866|0.949|0.850|0.877|0.682|\n|LOF|0.969|0.794|0.971|0.899|0.963|0.889|0.946|0.835|0.925|0.785|\n|DBSCAN|0.919|0.776|0.893|0.794|0.874|0.764|0.872|0.762|0.861|0.736|\n|MST|0.661|0.266|0.700|0.419|0.769|0.535|0.786|0.566|0.797|0.561|\n|ODIN|0.921|0.699|0.925|0.804|0.932|0.842|0.921|0.832|0.898|0.802|\n|iForest|0.875|0.580|0.836|0.659|0.834|0.685|0.900|0.798|0.949|0.854|\n\nTable 31: The BAs and F2-scores of selected outlier detection algorithms under a mixed cluster process (the simulation setting III in Section 6).\n\nspace poses challenges. Nonetheless, its performance remains comparable to LOF and DBSCAN.\n\nConsidering the Thomas cluster process, nearly all the algorithms degrade due to the non-uniformity of Gaussian clusters. LOF achieves the highest F2 scores across all dimensions: 0.741, 0.877, 0.908, 0.876, and 0.802. In comparison, the SUN-MCCD algorithm achieves the second-best overall performance with F2-scores of 0.687, 0.806, 0.845, 0.822, and 0.664; when d = 3, 5, and 10, it closely follows LOF while substantially outperforming other existing algorithms.\n\nThe situation under the mixed cluster process resembles those of the Mat\u00e9rn cluster process. When d = 2, the SUN-MCCD algorithm performs slightly below LOF and DBSCAN; when d = 3 and 5, it delivers the second best results, closely aligned with LOF\u2019s performance; when d = 10, the SUN-MCCD algorithm achieves a marginal advantage over LOF with the highest F2-score of 0.850.# Table 32\n\n| |Mat\u00e9rn|Mat\u00e9rn|Mat\u00e9rn|Mat\u00e9rn|Mat\u00e9rn|Thomas|Thomas|Thomas|Thomas|Thomas|Mixed|Mixed|Mixed|Mixed|Mixed|\n|---|---|---|---|\n|d|2|3|5|10|20|2|3|5|10|20|2|3|5|10|20|\n|RU-MCCDs|6|6|6|8|8|7|7|7|8|9|7|7|7|8|9|\n|SU-MCCDs|3|3|3|4|7|3|3|4|7|8|3|3|4|7|7|\n|UN-MCCDs|7|5|5|3|6|5|4|5|5|7|6|6|5|4|6|\n|SUN-MCCDs|4|4|1|1|4|4|2|2|2|5|4|2|2|1|5|\n|LOF|1|1|4|6|3|1|1|1|1|1|1|1|1|2|3|\n|DBSCAN|2|7|7|7|5|2|6|6|4|3|2|5|6|6|4|\n|MST|9|9|9|9|9|9|9|9|9|6|9|9|9|9|8|\n|ODIN|5|2|2|2|2|6|5|3|3|4|5|4|3|3|2|\n|iForest|8|8|8|5|1|8|8|8|6|2|8|8|8|5|1|\n\nTable 32: The rankings (by F2-scores) of all the algorithms under each simulation setting of this section, top 3 are highlighted in bold.\n\nIn summary, the SUN-MCCD algorithm consistently ranks among the top-performing algorithms with the \u201cflexible\u201d simulation settings. It performs better than other cluster-based algorithms, such as DBSCAN and MST, while comparable to or better than ODIN and iForest. Although LOF delivers the best overall performance, the SUN-MCCD algorithm remains a compelling choice. Moreover, the SUN-MCCD algorithm simultaneously produces clustering results, a capability absent in LOF. Furthermore, the SUN-MCCD algorithm is almost input parameter-free, strengthening its appeal compared to other algorithms.# 7 Real Data Examples\n\nIn this section, we evaluate the performance of all four CCD-based algorithms in real-life data and compare them with the state-of-the-art methods. Real-life data are much more complicated than the artificial data sets in Sections 5.1, 5.2, and 6. Those data sets are obtained from Outlier Detection Datasets (ODDS) [60] and ELKI Outlier Datasets [66]. Before outlier detection, we need to normalize all the features. A traditional way of normalization is subtracting the sample mean and dividing by the sample standard deviation, which is not robust to outliers exhibiting extreme feature values [49]. Therefore, we employ a robust alternative way with mean and standard deviation replaced by the median (Med) and the Normalized Median Absolute Deviation about the median (MADN). The details of the data sets are summarized below.# Brief descriptions of each real-life data set.\n\n- hepatitis: A data set contains patients suffering from hepatitis that have died (outliers) or survived (inliers).\n- glass: This data set consists of 6 types of glass, and the 6th type is a minority class, thus marked as outliers, while all other points are inliers.\n- vertebral: A data set with six bio-mechanical features, which are used to classify orthopedic patients either as normal (inliers) or abnormal (outliers).\n- ecoli: A data set consists of eight classes, three of which are the minority classes and are used as outliers.\n- stamps: A data set with each observation representing forged (photocopied or scanned+printed) stamps (outliers) or genuine (ink) stamps (inlier). The features are based on the color and printing properties of the stamps.\n- vowels: Four male speakers (classes) uttered two Japanese vowels successively; class (speaker) 1 is used as an outlier. The other speakers (classes) are considered inliers.\n- waveform: This data set represents three classes of waves, where class 1 was defined as an outlier/minority class.\n- wilt: This data set differentiates diseased trees (outliers) from other land covers (inliers).\n\n**Table 33: The size (n), dimensionality (d), and contamination level of each real-life data set.**\n|Data Set|n|d|# of outliers|\n|---|---|---|---|\n|hepatitis|74|19|7 (9.5%)|\n|glass|214|9|10 (4.5%)|\n|vertebral|240|6|30 (12.5%)|\n|ecoli|336|7|9 (2.6%)|\n|stamps|340|9|31 (9.1%)|\n|vowels|1456|12|50 (3.4%)|\n|waveform|3443|21|100 (2.9%)|\n|wilt|4735|5|257 (5.4%)|# Outlier Detection Algorithms Performance\n\nSimilar to the parameter selection in Section 6, for LOF, we choose the lower and upper bound of k to be 11 and 30, finding the highest LOF for each point, and setting the threshold to 1.5. Considering DBSCAN, to get an appropriate cutoff value for the 4-dist, we assume the percentage of outliers is known when conducting DBSCAN. When conducting MST, we set the threshold value for \u201cinconsistent\u201d edges to 1.2, and we label any clusters with sizes smaller than 2% of the size of the entire data set as outliers. As for ODIN, we set the input parameters k and T to 0.5 and 0.33 degrees of the size of the data set; finally, we construct iForests with 1000 iTrees with the sub-sample size of 256 for each, a threshold of 0.55 (for the outlyingness score) is used to capture the outliers. We record TPRs, TNRs, BAs, and F2-scores in Tables 34 and 35.# Table 34: The TPRs and TNRs of selected outlier detection algorithms on real-life data sets.\n\n| |hepatitis|hepatitis|glass|glass|vertebral|vertebral|ecoli|ecoli|stamps|stamps|vowels|vowels|waveform|waveform|wilt|wilt|\n|---|---|---|---|---|---|---|---|---|\n| |TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|TPR|TNR|\n|RU-MCCDs|0.286|0.881|1.000|0.363|0.467|0.643|0.750|0.558|0.065|0.958|1.000|0.327|0.870|0.678|0.763|0.630|\n|SU-MCCDs|0.286|0.925|1.000|0.363|0.200|0.576|0.750|0.680|0.516|0.883|1.000|0.373|0.830|0.774|0.300|0.785|\n|UN-MCCDs|0.714|0.657|0.222|0.765|0.033|0.914|0.750|0.668|0.484|0.812|1.000|0.541|0.860|0.664|0.140|0.897|\n|SUN-MCCDs|0.714|0.657|1.000|0.540|0.100|0.928|0.750|0.741|0.516|0.884|0.978|0.676|0.620|0.898|0.366|0.745|\n|LOF|0.000|0.985|0.778|0.618|0.033|0.938|0.500|0.930|0.161|0.919|0.370|0.985|0.000|1.000|0.031|0.973|\n|DBSCAN|0.000|0.955|0.000|0.980|0.000|0.943|0.000|0.988|0.161|0.955|0.304|0.996|0.090|0.996|0.000|0.959|\n|MST|0.429|0.866|0.778|0.662|0.367|0.695|0.875|0.546|0.774|0.437|0.652|0.553|0.670|0.484|0.553|0.672|\n|ODIN|0.429|0.746|0.111|0.848|0.167|0.848|0.750|0.857|0.290|0.874|0.587|0.925|0.370|0.844|0.062|0.976|\n|iForest|0.143|0.821|0.111|0.936|0.000|0.957|0.750|0.976|0.097|0.961|0.022|0.999|0.000|0.999|0.004|0.953|# Table 35: The BAs and F2-scores of selected outlier detection algorithms on real-life data sets.\n\n| |hepatitis|hepatitis|glass|glass|vertebral|vertebral|ecoli|ecoli|stamps|stamps|vowels|vowels|waveform|waveform|wilt|wilt|\n|---|---|---|---|---|---|---|---|---|\n| |BA|F-score|BA|F-score|BA|F-score|BA|F-score|BA|F-score|BA|F-score|BA|F-score|BA|F-score|\n|RU-MCCDs|0.583|0.263|0.681|0.257|0.555|0.335|0.654|0.164|0.511|0.072|0.664|0.196|0.774|0.278|0.696|0.336|\n|SU-MCCDs|0.606|0.286|0.681|0.257|0.388|0.140|0.715|0.210|0.700|0.455|0.686|0.207|0.802|0.335|0.542|0.185|\n|UN-MCCDs|0.686|0.446|0.493|0.116|0.474|0.036|0.709|0.204|0.648|0.381|0.771|0.263|0.762|0.267|0.519|0.117|\n|SUN-MCCDs|0.686|0.446|0.770|0.324|0.514|0.109|0.745|0.244|0.701|0.457|0.827|0.328|0.759|0.387|0.555|0.206|\n|LOF|0.493|0.000|0.697|0.289|0.488|0.037|0.711|0.328|0.540|0.162|0.677|0.383|0.500|0.000|0.502|0.035|\n|DBSCAN|0.478|0.000|0.490|0.000|0.471|0.000|0.494|0.000|0.557|0.178|0.650|0.343|0.543|0.107|0.673|0.381|\n|MST|0.647|0.375|0.720|0.313|0.531|0.282|0.710|0.186|0.606|0.373|0.603|0.178|0.577|0.153|0.612|0.266|\n|ODIN|0.587|0.313|0.480|0.074|0.507|0.159|0.803|0.353|0.582|0.262|0.756|0.427|0.607|0.193|0.519|0.069|\n|iForest|0.482|0.122|0.524|0.100|0.479|0.000|0.863|0.652|0.529|0.108|0.510|0.027|0.500|0.000|0.479|0.004|\n\nThe UN-MCCD and SUN-MCCD algorithms perform the best with the hepatitis data set. Both achieve TPR and F2-Scores of 0.714 and 0.446, respectively. All the other algorithms deliver much lower TPRs, leading to worse performance.\n\nFor the glass data set, the SUN-MCCD algorithm and MST achieve the highest F2-scores of 0.313 and 0.324. DBSCAN fails to capture any outliers, resulting in 0 F2-score. ODIN and iForest can only capture 11% of outliers. Although the RU-MCCD and SU-MCCD algorithms can identify all the outliers, their TNRs are merely 0.363.\n\nThe RU-MCCD algorithm obtains the highest F2-score of 0.335 under the vertebral data set, while most other algorithms can hardly identify any outliers.\n\nThe performance of the CCD-based algorithms is worse than other algorithms under the ecoli data set, with F2-scores of approximately 0.2. Here is the reason, the intensity varies greatly across each cluster or class of the ecoli data set, making clustering and density-based algorithms unsuitable, as all of them perform badly (including MST and DBSCAN). iForest achieves the highest F2-score of 0.652, with TPR and TNR of 0.750 and 0.976, respectively. LOF performs the second best, with a F2-score of 0.328.\n\nFor the stamps data set, the SU-MCCD and SUN-MCCD algorithms achieve the best F2 Scores of 0.455 and 0.457, respectively. All the other algorithms can barely distinguish.# 8 Summary and Conclusion\n\nIn this paper, we have developed and applied Cluster Catch Digraphs (CCDs) for outlier detection, aiming to identify points that deviate substantially from regular points. One of our algorithms, the U-MCCD algorithm, utilizes RK-CCDs to partition the data into clusters, followed by the D-MCG algorithm to detect outliers within each cluster by identifying the largest connected components. This method effectively captures outliers that lie outside the dominant covering balls, representing the primary clusters.\n\nDespite its effectiveness, the U-MCCD algorithm exhibits limitations when dealing with non-spherical clusters or clusters of varying intensities, often leading to many false positives. To address this, we proposed the SU-MCCD algorithm, which extends cluster coverage by including additional mutually-caught covering balls, thus enhancing its ability to handle clusters of arbitrary shapes or varying intensities. We also introduced a threshold Smin to filter small clusters, improving robustness against the masking problem. Monte Carlo simulations demonstrated that the SU-MCCD algorithm achieves substantially higher TNRs compared to the U-MCCD algorithm, especially with Gaussian clusters.\n\nHowever, both U-MCCD and SU-MCCD algorithms face performance degradation in high-dimensional spaces (when d > 10), due to the intrinsic properties of the Spatial Randomness Monte Carlo Test (SR-MCT) with Ripley\u2019s K function. To overcome this, we formulated the SR-MCT using Nearest Neighbor Distances (NND), resulting in the UN-CCDs for clustering. By integrating UN-CCDs into the U-MCCD and SU-MCCD frameworks, we developed the UN-MCCD and SUN-MCCD algorithms, respectively. Monte Carlo simulations showed that these new algorithms maintain high performance in low-dimensional spaces and substantially improve F2-scores when the number of dimensions exceeds 10.\n\nIn Sections 5 and 6, we compared the performance of the four CCD-based algorithms with existing outlier detection methods through extensive Monte Carlo simulations using artificially generated data. Among the CCD-based algorithms, the SUN-MCCD algorithm consistently delivered the best overall performance, particularly in terms of robustness and adaptability across various simulation settings. While the F2-scores were comparable to or slightly lower than those of the Local Outlier Factor (LOF), the SUN-MCCD algorithm outperformed other cluster-based methods like DBSCAN and MST, and was on par with or better than ODIN and iForest. Additionally, the SUN-MCCD algorithm\u2019s near parameter-free nature makes it a compelling choice. In Section 7, we evaluated the algorithms using eight real-life data sets. Despite some performance degradation due to the increased complexity of real-world data, the CCD-based algorithms still delivered comparable or",
        "context_id": 23,
        "question": "Who first recognized the existence of substantial deviations from commonly occurring phenomena in nature?",
        "answer": [
            "Francis Bacon"
        ],
        "context_length": 186176
    },
    {
        "context": "# 1. Introduction\n\nEndoscopic Submucosal Dissection (ESD) is a groundbreaking minimally invasive procedure that has transformed the management of early gastrointestinal cancers. Compared to traditional surgery, ESD offers patients reduced trauma, faster recovery times, and lower rates of cancer recurrence [1]. However, the intricacy of lesion characteristics and the delicate nature of the gastrointestinal tract pose a risk of unexpected complications during ESD, making proficiency in this technique limited to a few experienced endoscopists. With the increasing detection of gastrointestinal cancers through routine gastric endoscopy screenings, there is a growing demand for skilled endoscopists capable of performing ESD. Computer-assisted surgery (CAS) systems are an advanced medical technology that can significantly enhance surgical efficiency and reduce the risk of complications [2]. Surgical phase recognition (SPR) is a critical and challenging task within CAS systems, aimed at identifying surgical activities from surgical videos [3]. In ESD, the phases carry distinct risks, necessitating accurate phase recognition for real-time monitoring, surgical process optimization, context-aware decision support, and early anomaly detection [4]. Additionally, automated SPR can significantly improve postoperative reporting and video indexing, providing valuable educational resources for novice endoscopists [5]. Therefore, developing efficient and accurate video-based SPR algorithms is essential to meet the demands of modern surgical practice and education.\n\nLimited inter-phase variance and high intra-phase variance are the most critical challenges for automatically recognizing surgical phases from video. Even though a lot of surgical phase recognition methods have been proposed to address this issue, for example, Jin et al. [6] introduced TMRNet, which includes a repository to store remote information to learn the relationship between the current frame and previous frames. Czenpiel et al. [7] proposed TeCNO, an enhanced Multi-Stage Temporal Convolutional Network (MS-TCN) [8]. Gao et al. [9] devised Trans-SVNet, addressing fine-grained# Temporal Features Loss in TCNs with a Compact Transformer Model\n\nHowever, ESD surgical phase recognition is still challenged since their surgical phases are extremely complex and temporally imbalanced. Therefore, applying existing surgical phase recognition methods directly to ESD data leads to performance degradation [10, 11, 12]. A more efficient temporal modeling approach is needed to address the insufficient temporal modeling capabilities of traditional temporal models.\n\nRecently, a state-space model called Mamba has demonstrated substantial potential in modeling temporal contexts. Unlike traditional temporal models, the Mamba model shows higher robustness and accuracy in dealing with complex surgical phase transitions while avoiding the problem of the Transformer\u2019s high computational complexity. Making Mamba well suited for modeling the temporal context in ESD videos. To fully utilize Mamba\u2019s temporal modeling ability, we propose a Mamba-based surgical phase recognition framework called SPRMamba.\n\nSPRMamba leverages a ResNet-50 backbone for spatial feature extraction from ESD videos, followed by the application of four LSTContext modules to effectively combine short-term and long-term temporal contexts for phase recognition. Central to SPRMamba is the Scale Residual TranMamba (SRTM) module, which combines Mamba\u2019s long-term temporal modeling capabilities with the Transformer\u2019s ability to capture fine-grained details, addressing the complex phase relationships inherent in ESD surgeries.\n\nFurthermore, according to [13], allowing the model to operate on the complete input sequence is more beneficial compared to only accessing a subset of the input. Therefore, considering the quadratic complexity of the transformer and the high memory usage of O(L2) (where L represents the input sequence length), it is inappropriate to directly apply the transformer to uncut surgical videos. To address this issue, we designed two sampling strategies, a Long-range Sample and a Window Sample, to reduce computational complexity and support the online phase.# Main Contributions\n\n- We propose a novel surgical phase recognition framework (SPRMamba), leveraging the Scaled Residual TranMamba (SRTM) module to efficiently model short-term and long-term temporal contexts.\n- Introducing Temporal Sample Strategy (TSS) with Window and Long-range sampling to reduce computational burden and support online phase recognition.# Conducting qualitative and quantitative experiments on ESD385 and Cholec80 datasets, demonstrating significant improvements over existing techniques.# 2.1. Video understanding\n\nIn the field of video understanding, researchers are dedicated to developing models that can effectively capture spatiotemporal features to extract semantic information from dynamic scenes. Traditional video understanding methods often rely on 3D Convolutional Neural Networks (3D-CNNs) [14], which extend 2D convolutions to simultaneously process spatial and temporal dimensions. However, 3D-CNNs tend to have high computational complexity, limiting their application in long video analysis.\n\nIn the field of video understanding, effectively capturing spatiotemporal features is crucial for extracting semantic information from dynamic scenes. Early approaches, such as 3D Convolutional Neural Networks (3D-CNNs) [14], extended 2D convolutions to process spatial and temporal dimensions simultaneously, achieving reasonable performance in action segmentation tasks. However, the high computational complexity of 3D-CNNs makes them impractical for analyzing long videos, particularly in medical applications like ESD surgery, where procedures can last several hours. To address computational limitations, Temporal Convolutional Networks (TCNs) [15] emerged, which focus on capturing temporal dependencies through one-dimensional convolutions. While TCNs are more efficient and capable of handling longer video sequences, they struggle to model complex action variations and capture the fine-grained details needed for surgical phase recognition, where subtle movements and transitions between phases play a critical role.\n\nTo address this, Temporal Convolutional Networks (TCNs) [15] were proposed, focusing on capturing temporal information through one-dimensional convolutions, thereby modeling long-term dependencies with lower computational costs. However, TCNs face limitations in handling complex action variations and capturing fine-grained features. Transformer-based models have gained attention in recent years for their ability to model long-range dependencies using self-attention mechanisms [16]. While Transformers achieve state-of-the-art performance in video understanding tasks by capturing spatiotemporal relationships across frames, they are constrained by their quadratic computational complexity, which poses challenges when applying them to# 2.2. Surgical Phase Recognition\n\nResearch in the field of Surgical Phase Recognition mainly focuses on automatically detecting and classifying different phases of surgery by analyzing surgical videos. Early surgical phase recognition methods relied heavily on hand-designed features and statistical learning methods [17, 18, 19, 20, 3, 21, 22, 23]. However, these methods are limited by empirical design features and usually rely on predefined dependencies, making it difficult to accurately capture subtle motion features with strongly nonlinear dynamics, and their performance suffers greatly.\n\nTwinanda et al. [3] proposed the EndoNet model, a CNN-based approach that automatically extracts features and performs phase classification from laparoscopic surgical videos. This approach opened the way for SPR research using deep learning. Since then, researchers have sought to enhance SPR by incorporating more advanced temporal modeling techniques. For instance, long short-term memory (LSTM) networks have been widely adopted in models like PhaseNet [24], EndoLSTM [25], and SVRCNet [26], enabling the capture of both spatial and temporal dependencies.\n\nThese methods, by combining deep residual networks (ResNet) with LSTM modules, have shown promising improvements in recognizing surgical phases by effectively modeling temporal series data. However, these methods are computationally expensive when dealing with long time series and prone to misclassification when dealing with complex phase transitions.\n\nTo address these limitations, Transformer-based architectures have been introduced into SPR due to their capability of capturing long-range temporal dependencies. For example, Czempiel et al. [27] proposed a Transformer-based surgical phase recognition method, which significantly improves the model\u2019s performance in complex surgical scenarios through the self-attention mechanism. More recently, Graph Neural Networks (GNNs) have been employed in SPR, with studies such as Padoy et al. [28] demonstrating their effectiveness in modeling complex spatiotemporal relationships between surgical tools and anatomical structures. Although existing techniques have made significant# 2.3. State Space Models\n\nRecently, State-Space Models (SSMs) have been proven to have transformer-level performance in capturing long sequences in the field of natural language processing [29, 30]. [29] introduced a new model for Structured State-Space Sequences (S4), specifically designed to model long-range dependencies exhibiting the well-established property of scaling linearly with sequence length. Based on this, [31] proposed an advanced layer called S5, which integrates MIMO SSM and efficient parallel scanning into the S4 architecture. This development aims to overcome the limitations of SSMs and improve their efficiency. In addition, [32] contributed a novel SSM layer H3, significantly narrowing the performance gap between SSM and transformer-based attention in language modeling. [33] Expand the S4 model by introducing additional gating units in the gated state space layer to enhance its expressiveness. More recently, [30] developed a universal language model called Mamba by introducing a data-dependent SSM layer and a selection mechanism using parallel scanning. Compared to transformers based on quadratic complexity attention, Mamba excels at handling long sequences with linear complexity.\n\nIn the field of vision, [34] proposed Visual Mamba (Vim), which combines position encoding and bi-directional scanning to efficiently capture the global context of an image. Pioneered the application of Mamba in vision tasks. Li et al. [35] constructed a generic framework called Video Mamba Suite to develop, validate, and analyze Mamba\u2019s performance in video understanding. Besides, the great potential of Mamba has inspired a series of works [36, 37, 38, 39, 40], which demonstrated Mamba\u2019s has better performance and higher GPU efficiency than Transformer on visual downstream tasks such as semantic segmentation and video understanding. In our work, we integrate Mamba into the surgical phase recognition model to efficiently capture the temporal context in surgical video.# 3.1. Preliminaries\n\nState Space Models (SSMs) are typically considered linear time-invariant systems that map a 1-D function or sequence x(t) \u2192 R \u2191 y(t) \u2192 R through a hidden state h(t) \u2192 R N. These systems are commonly represented by linear ordinary differential equations (ODEs) with A \u2192 RN \u2191N as the evolution parameter, and B \u2192 R N \u21911, C \u2192 R 1\u2191N as projection parameters.\n\nh\u2193 (t) = Ah(t) + Bx(t), y(t) = Ch(t). (1)\n\nStructured State Space Sequence Models (S4) and Mamba are discrete versions of continuous systems, which include a time scale parameter \u00af, B. A converts continuous parameters A and B into discrete parameters A! that common conversion method is Zero-Order Hold (ZOH), defined as follows:\n\nA\u00af = exp(!A), B = (!A) \u21941 (exp(!A) \u2193 I) \u00b7 !B. \u00af (2)\n\nAfter the discretization of A\u00af, B\u00af, Eq.(1) can be expressed as discrete parameters:\n\nh t = A\u00afh t\u21941 + B\u00afx t, y t = Ch . t (3)# 3.3. Temporal Sample Strategy\n\nDue to the quadratic complexity of self-attention blocks, it is impractical to apply attention to long sequences of untrimmed videos. This is because the L of the video sequence is very large, so we need to resample it to achieve modeling of long-term and local temporal contexts, as shown in Fig. 1b.# Temporal Window Sample\n\nFor temporal window sampling (WS), we divide the sequence into non-overlapping windows of size W. Fig. 1b illustrates the case of W=4. Given that different tasks may have different time-dependent ranges, W is task-specific. We used W=64 in practice. The impact of W was evaluated in Section 4. Instead of modeling the temporal context over the entire sequence of length T, we model the temporal context T times, where each F b1 \u2192 R W \u2191 4, F b2 \u2192 R W \u2191 4, F b3 \u2192 R W \u2191 2 corresponds to each window.# LSTContext Block\n\nThe top of Fig. 2 illustrates the entire LSTContext block. As in previous work [7], we use a 1D dilated convolution with kernel size 3. This is because the dilated convolution increases the receptive field without the need to increase the parameter number by increasing the kernel size. Where the dilation factor for each layer increases by a factor of 2 and the receptive field exponentially expands as the number of layers increases. Therefore, with a few parameters, we achieved a significantly large receptive field in the temporal sequence, which mitigated model overfitting and effectively promoted the accuracy of surgical phase recognition. The dilated convolution is followed by a Gaussian Error Linear Unit (GELU). In the LSTContext block, we first use the window sample and then the long-range sample, as shown in Fig. 1. Finally, we use a linear layer with residual connectivity to output the features for each frame, F \u2192 R L\u2191D.# 3.4. Overview\n\nThe architecture of the proposed SPRMamba is shown in Fig. 2. For a video feature sequence V \u2192 RL\u2191C\u2191H\u2191W RL\u2191D F \u2192 of length L, we first extract the spatial frame-level from a fixed ResNet-50[44], where D=2048 is the spatial dimension, and then SPHMamba uses a linear layer to reduce the feature dimension to 64. As in previous works, we repeated each LST-Context block N times, where the dilation factor of the dilation convolution was increased in each layer. After the first N layers of the LTContext block, we use an additional linear layer to further reduce the dimensionality D to 32. The dimensionality reduction method reduces the number of parameters from 2.49 million to 1.23 million without degrading the accuracy. We also use an additional Conv1D followed by a softmax layer to generate the frame-level class probabilities P \u2192 R L\u2191C. We proceed with three additional stages, each consisting of N layers of LSTContext blocks. At the beginning of each stage, we reset the dilation factor of the temporal convolution to 1 and compute the frame-level class probabilities P \u2192 RT \u2191C a multi-stage loss. We use cross-entropy loss and the mean squared error smoothing loss introduced by [8] for supervised training.# 4. Experiments\n\nIn this section, we first describe the dataset used in our study, as well as the detailed experimental setup and evaluation metrics employed. Subsequently, we validated the effectiveness of our proposed method in ESD and cholecystectomy by comparing it with the SOTA method and conducting ablation experiments.# 4.1. Dataset\n\n|ESD Surgical phases|Preparation|Estimation|Marking|Injection|Incision|ESD|Vessel-treatment|Clips|Total|\n|---|---|---|---|---|---|---|---|---|---|\n|Train|91739|43198|15585|44464|59695|174201|44317|10817|484016|\n|val|14193|5822|2820|9886|11917|41807|4865|3985|95295|\n|test|14323|16860|6583|15831|22181|75968|14707|4748|171201|# 4.1.1. ESD385\n\nThere is no publicly available dataset on ESD surgery. To validate the effectiveness of the proposed algorithm in this study, we retrospectively selectedpatients who underwent ESD from August 16, 2023, to January 8, 2024, in the endoscopy unit of the Department of Gastroenterology, Renji Hospital, Shanghai Jiao Tong University. A total of 385 videos of ESD procedures were collected. All procedures were performed by experienced endoscopists. The instruments used for ESD procedures included: gastroscope GIF-Q260, GIF-H260, GIF-HQ290, mucosal incision knife KD-612L/U, KD-655L/U, KD-620UR, endoscopic water pump OFP-2, mucosal injection needle NM-400L-0423, NM-400U-0423, thermal biopsy forceps HDBF-2.4-230-S, hemostatic forceps FD-410LR, FD-412LR, soft tissue clips ROCC-D-26-195-C, ROCC-D-26-235-C, hemostatic clips M00521242, methylene blue injection, indigo rouge solution.\n\nAll endoscopic surgical videos were recorded using Image Management Hub, IMH-200, and Olympus and stored in MP4 format. The video resolution was 1920 \u00d7 1080 with a frame rate of 50 fps. After collection, all videos were anonymized to remove identifiable patient information, procedure timestamps, and other identifying details. This work was approved by the Renji Hospital affiliated with the Shanghai Jiao Tong University School of Medicine ethics committee with number RA-2024-457(2024.3.20).\n\nThe ESD video is divided into eight surgical phases, including (1) Estimation; (2) Marking; (3) Injection; (4) Incision; (5) ESD; (6) Vessel-treatment; (7) Clips(portion); and (8) Preparation. Phase (6) Preparation included elements unrelated to the ESD procedure, such as gastric insufflation and device replacement. Four endoscopists independently annotated the video, labeling each frame in the video. After the initial annotation, quality control was performed by two additional experienced endoscopists. Any uncertainties that arose during the quality control process were resolved through collaborative discussions among the six experts. The number of annotated frames in the dataset varied at each phase, with the ESD phase occupying most of the procedure time, which is the most important and skill-demanding phase of ESD. Detailed statistics for each phase are shown in Table 7. Overall, a total of 484016 frames, 95,295 and 171,201 frames were annotated for training, validation, and verification, respectively.# 4.1.2. Cholec80\n\nFurthermore, to verify the robustness of our proposed algorithm, we validated it on the Cholec80 dataset[3] for cholecystectomy procedures. The Cholec80 dataset is a large-scale surgical benchmark dataset containing 80 cholecystectomy videos performed by 13 surgeons. These videos are recorded at 25 frames per second, with each frame having a resolution of either# 4.2. Evaluation Metrics\n\nFor surgical phase recognition, we employed four commonly used evaluation metrics to quantitatively assess model performance, which have been used in previous phase recognition work as well[3, 26, 45, 6]. These metrics include Precision, Recall, Jaccard Index, and Accuracy. Accuracy is defined as the percentage of frames across the entire video correctly predicted to be in their ground truth phase. Given the imbalanced phase presented in the video, Precision, Recall, and Jaccard Index refer to phase-level evaluations, calculated within each phase and then averaged across all phases.# 4.3. Implementation details\n\nOur approach is implemented in Python using Pytorch framework and training is conducted on a workstation equipped with an NVIDIA RTX 4090. In the first stage, we use a ResNet-50 model pre-trained on ImageNet-22K[44]. We then fine-tune the model on our data. To ensure a fair comparison with SOTA methods, we downsample all videos to 1 fps, which is also the approach used in previous works[45, 6]. This operation has additional benefits, including enriching temporal information and saving memory. During training, image frames are further downsampled from the original resolutions of 1920 \u2243 1080 and 854 \u2243 480 to a resolution of 250 \u2243 250 pixels to further reduce memory usage. Data augmentation is performed through 224 \u2243 224 cropping, flipping, and random mirroring to expand the training dataset. The ResNet-50 model is fine-tuned with a batch size of 160 images. In the second stage, LSTContext is trained end-to-end from scratch. The W and G for each task in the LSTContext module are initially chosen based on estimates of task average durations on the training dataset and are then fine-tuned through ablation studies. For both stages, we adopted the same# 4.4.1. Result on the ESD385 Dataset\n\n|Method|Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|FLOPs|Params|\n|---|---|---|---|---|---|---|\n|ResNet-50 [44]|72.31|78.75|66.63|55.39|4.1G|24.56M|\n|SV-RCNet [26]|75.58\u00b113.46|81.27\u00b117.23|70.78\u00b117.46|60.12\u00b117.77|41.1G|28.76M|\n|SAHC [48]|86.64\u00b110.63|86.35\u00b118.07|83.75\u00b117.55|75.14\u00b118.20|9.5G|26.27M|\n|Furube et al. [12]|84.79\u00b111.58|84.85\u00b118.75|82.11\u00b117.52|72.60\u00b118.25|4.5G|24.69M|\n|AI-Endo [11]|83.38\u00b112.09|84.74\u00b117.35|82.17\u00b117.23|72.09\u00b117.23|5.7G|24.72M|\n|SPRMamba(Ours)|87.64\u00b19.83|86.72\u00b116.54|86.76\u00b115.66|77.51\u00b117.83|7.5G|25.42M|\n\nOn the ESD385 dataset, we compared our proposed method with the SOTA method. This includes the method proposed by Furube et al. [12], which fine-tunes ResNet-50 as a feature extractor and then uses MS-TCN for hierarchical prediction refinement for surgical phase recognition. The method proposed by Cao et al. [11]. An Intelligent Surgical Workflow Recognition Suite for ESD is based on Trans-SVNet [9] implementation. In addition, we compared our method with SV-RCNet [26] and SAHC [48], two SOTA methods for cholecystectomy surgical phase recognition. The comparative results are presented in Table 2. We found that applying existing surgical phase recognition methods directly to ESD leads to performance degradation because ESD surgery has more complex workflows compared to traditional surgeries. In the case of image classification using ResNet-50 alone, the average accuracy is 72.31%, the average precision is 78.75%, the average recall is 66.63%, and the average Jaccard is 55.39%. After including the SRTM module and Temporal Sample Strategy, the present method achieves an average accuracy of 87.64%, an average precision of 86.72%, an average recall of 86.76%, and an average Jaccard of 77.51%, increasing the average accuracy from 72.31% to 87.64%. These results demonstrate the effectiveness of the method for automatic phase recognition. Compared to the SOTA method, our method outperforms the second-best method in all four metrics by 1.00%, 0.37%, 3.01%, and 2.37%, respectively. Furthermore, to demonstrate the algorithm efficiency of the proposed method, we also compare the number of parameters and computational cost of our method with the.# 4.4.2. Result on the Cholec80 Dataset\n\n|Method +|Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|\n|---|---|---|---|---|\n|MTRCNet-CL [45]+ Endonet [3]|81.70\u00b14.20|73.70\u00b116.10|79.60\u00b17.90|-|\n| |89.20\u00b17.60|86.90\u00b14.30|88.00\u00b16.90|-|\n|LAST [49]+|93.12\u00b14.71|89.25\u00b15.49|90.10\u00b15.45|81.11\u00b17.62|\n|SV-RCNet [26]|85.30\u00b17.30|80.70\u00b17.00|83.50\u00b17.50|-|\n|TeCNO [7]|89.35\u00b16.70|83.24\u00b17.21|81.29\u00b16.61|70.08\u00b19.08|\n|TMRNet [6]|90.10\u00b17.60|90.30\u00b13.30|89.50\u00b15.00|79.10\u00b15.70|\n|Trans-SVNet [9]|90.27\u00b16.48|85.23\u00b16.97|82.92\u00b16.77|72.42\u00b18.92|\n|SAHC [48]|91.80\u00b18.10|90.30\u00b16.40|90.00\u00b16.40|81.20\u00b15.50|\n|SPRMamba(Ours)|93.12\u00b14.58|89.26\u00b16.69|90.12\u00b15.61|81.43\u00b16.90|\n\nIn the Cholec80 dataset, we conducted a performance comparison of our methods with state-of-the-art (SOTA) methods, including Endonet [3], MTRCNet-CL [45], LAST [49], SV-RCNet [26], TeCNO [7], TMRNet [6], Trans-SVNet [9], and SAHC [48]. Please note that we re-implemented TeCNO and Trans-SVNet using the model weights provided in the original manuscript. The results of the other state-of-the-art methods were extracted verbatim from their respective published works. Our comparison results are presented in Table 3. Our method outperforms the other methods in most of the evaluation metrics, except for the average precision, where it ranks third behind MTRCNet-CL and SAHC. Specifically, for average accuracy, SPRMamba matches the high accuracy of LAST (a multi-task learning method that requires additional information in the form of extra labels) but displays a lower standard deviation of approximately 0.13%. Additionally, for average recall.# Figure 3\n\nQualitative comparisons with some other methods on ESD385 dataset. The following four rows show the ground-truth labels, the predictions by the proposed SPRMamba, the predictions by Furube et al., and the predictions by AI-Endo. P1 to P8 indicate the phase label.\n\nand average Jaccard, SPRMamba achieves the best performance with 90.12% and 81.43%, respectively.\n\nFurthermore, to illustrate the performance of our approach compared to state-of-the-art methods, in Fig. 4 we qualitatively compare two examples from the Cholec80 testing dataset. We compare the proposed method with two SOTA methods, TeCNO [7] and Trans-SVNet [9]. As shown in Fig. 4, our method predicts higher frame-level accuracy, especially on the boundary between two different phases. In addition, we can notice that Trans-SVNet and TeCNO make a lot of mistakes, i.e., they are unable to accurately classify ambiguous frames in different phases. For example, some frames within P3 are misclassified into P4 and some frames within P5 are misclassified into P4 in Video 1. On the contrary, our proposed method performs very accurately compared to ground truth values.# 4.5. Ablation Study\n\nWe conducted a series of ablation experiments on the ESD385 dataset to validate the effectiveness of each component and parameter setting in our proposed method on the model.# 4.5.1. Scale Residual TranMamba\n\nTable 4 shows the importance of the SRTM modules. Transformer and Mamba denote the use of transformer branches alone and Mamba branches alone, respectively. To keep the number of parameters constant, we still use F \u2192 RL\u219164 as input. SRTM improves the accuracy by 1.61% and 0.73%\n\n16# Video_\n\nFigure 4: Qualitative comparisons with some other methods on Cholec80 dataset. The following four rows show the ground-truth labels, the predictions by the proposed SPRMamba, the predictions by TeCNO, and the predictions by Trans-SVNet. P1 to P7 indicate the phase label.# Table 4: ABLATION STUDY ON THE SCALE RESIDUAL TRANMamba\n\nIn the case of Transformer, we only use transformer branches instead of SRTM. In the case of Mamba, we only used the Mamba branch.\n\n| |Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|FLOPs|Params|\n|---|---|---|---|---|---|---|\n|Transformer|86.03\u00b110.89|86.02\u00b117.40|84.89\u00b117.03|75.44\u00b118.41|8.5G|26.15M|\n|Mamba|86.91\u00b112.06|86.59\u00b117.24|86.05\u00b116.74|77.19\u00b118.42|6.5G|25.28M|\n|SRTM|87.64\u00b19.83|86.72\u00b116.54|86.76\u00b115.66|77.51\u00b117.83|7.5G|25.42M|\n\nCompared to Transformer and Mamba, respectively, demonstrating the crucial role of modeling short-term and long-term temporal context for ESD surgical phase recognition.# Table 5: ABLATION STUDY ON THE TEMPORAL SAMPLE STRATEGY\n\nIn the case of STContext SRTM, we use two STContext SRTM blocks instead of a combination of STContext SRTM and LTContext SRTM. In the case of LTContext SRTM, we used two LTContext SRTM blocks.\n\n| |Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|\n|---|---|---|---|---|\n|baseline|86.58\u00b111.58|86.60\u00b117.48|84.16\u00b116.80|75.65\u00b118.70|\n|STContext SRTM|87.03\u00b111.73|86.95\u00b117.57|85.20\u00b117.32|76.86\u00b117.82|\n|LTContext SRTM|87.38\u00b111.15|87.27\u00b117.20|85.42\u00b116.43|77.07\u00b117.89|\n|Ours|87.64\u00b19.83|86.72\u00b116.54|86.76\u00b115.66|77.51\u00b117.83|\n\nTo demonstrate the effectiveness of the Temporal Sample Strategy, we designed four ablation experiments. We also configured the original SRTM framework matching the SPRMamba structure as a benchmark for comparison. As shown in Table 5, baseline achieves an average accuracy of 86.58%, an average precision of 86.60%, an average recall of 84.16%, and an average.# 4.5.3. Effect of Different value of W and G\n\n|W|G|Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|\n|---|---|---|---|---|---|\n|8|64|87.53\u00b111.10|87.73\u00b116.56|85.39\u00b116.21|77.41\u00b117.16|\n|16|64|87.37\u00b19.63|86.56\u00b116.23|86.23\u00b116.30|77.35\u00b117.17|\n|32|64|86.91\u00b111.52|87.62\u00b117.82|85.10\u00b115.87|76.80\u00b118.23|\n|64|64|87.64\u00b19.83|86.72\u00b116.54|86.76\u00b115.66|77.51\u00b117.83|\n|64|32|87.33\u00b110.41|86.50\u00b116.74|85.62\u00b116.32|76.85\u00b117.65|\n|64|16|87.08\u00b110.98|87.09\u00b117.19|85.15\u00b116.42|76.77\u00b117.91|\n|64|8|87.11\u00b110.76|86.35\u00b118.76|85.88\u00b117.40|76.60\u00b118.76|\n\nTo further explore the effect of the Temporal Sample Strategy with different W and G on the model performance, we conducted the experiments shown in Table 6. The parameter W controls the size of the local window and the parameter G controls the range of the long-term temporal context modeling. The experimental results show that the performance of the models (Accuracy and Jaccard) increases as G increases until the optimal performance is reached for G=64. The performance fluctuates as the size of the local window W becomes smaller, but W=64 works best.# 4.5.4. Effect of using convolutions\n\n|Jaccard(%)| | | |Accuracy(%)| |Precision(%)|Recall(%)| | |\n|---|---|---|---|---|---|---|---|---|---|\n| | |Conv without Dilation| | |85.97\u00b111.48|85.75\u00b117.70| |83.96\u00b118.06|74.74\u00b118.92|\n| | |Without Conv| | |86.60\u00b110.95|85.71\u00b117.94| |84.99\u00b117.56|75.46\u00b118.87|\n| | |Conv with Dilation| | |87.64\u00b19.83|86.72\u00b116.54| |86.76\u00b115.66|77.51\u00b117.83|\n\nTo explore the impact of dilated convolution in LSTContext blocks on model performance. We compared 1D dilated convolution with 1D convolution using the same size but without dilation factor and without 1D convolution.# 5. Discussion\n\nThe accurate recognition of surgical phases in ESD within CAS systems is crucial for enhancing surgical efficiency, minimizing patient complications, and providing valuable training material for novice endoscopists. However, considering the duration and complexity of the ESD process, the challenge of handling long sequences of video frames and effectively modeling temporal context with limited computing resources remains significant.\n\nTo address these challenges, we propose SPRMamba, a Mamba-based framework for online ESD surgical phase recognition. Our approach leverages the Scale Residual TranMamba module to effectively model both long-term and short-term temporal contexts, offering superior temporal modeling capabilities compared to traditional methods. Additionally, we introduce a novel Temporal Sample Strategy to mitigate the computational resource constraints, making the framework feasible for real-time surgical phase recognition.\n\nThe advantages of our approach are as follows:\n\n1. Long-Term Modeling Superiority: Mamba enhanced capabilities for long-term temporal modeling with lower computational complexity compared to Transformers. This makes it particularly suitable for handling the extended duration and complexity of ESD surgeries.\n2. Effectiveness of the SRTM Module: The proposed SRTM module achieves the best performance due to its ability to capture complex phase relationships in ESD surgeries. While Mamba ensures overall phase recognition accuracy, the challenge of accurately recognizing short-duration phases remains, which our SRTM module addresses effectively.\n3. Advantages of the Temporal Sample Strategy: The proposed Temporal Sample Strategy outperforms direct long sequence processing and mitigates the secondary complexity typically associated with Transformer models, enhancing the efficiency of temporal context modeling and improving overall recognition performance. In addition, our proposed method allows for flexible modification of the temporal modeling length and specific adjustments for different surgical tasks.\n\nFinally, our proposed method not only surpasses state-of-the-art (SOTA) methods in ESD surgical phase recognition but also demonstrates robustness for other surgical tasks. Specifically, our method achieves an average accuracy of 87.64%, an average precision of 86.72%, an average recall of86.76%, and an average Jaccard of 77.51% on the ESD dataset, outperforming the next best method by significant margins. Additionally, validation on the cholecystectomy Cholec80 dataset further confirms our method\u2019s robustness, achieving superior results in most metrics compared to the SOTA methods[3, 45, 49, 26, 7, 6, 9, 48], except for precision. The qualitative results, as illustrated in Fig. 3 and Fig. 4, further substantiate the effectiveness and reliability of our approach.\n\nAlthough our method shows high surgical phase recognition performance in ESD and cholecystectomy videos, it has some limitations. First, this study primarily focuses on ESD procedures, and while we validated the robustness of our method on the Cholec80 dataset, further experiments are needed to assess its generalizability across a broader range of surgical tasks. In the future, we plan to extend our method to other surgical video analysis tasks, potentially incorporating other innovations such as uncertainty analysis to enhance the robustness and accuracy of SPRMamba across different surgical. Second, the dataset used in this study was limited in size and diversity, which may affect the model\u2019s performance in more diverse clinical settings. Future work will focus on expanding the dataset to include multicenter data and a wider variety of surgical procedures.# 6. Conclusion\n\nIn this paper, we address the critical need for accurate surgical phase recognition in Endoscopic Submucosal Dissection, a key procedure for treating early-stage gastrointestinal cancers. Achieving precise real-time phase recognition in ESD is essential for improving surgical outcomes and efficiency. However, traditional SPR methods face significant challenges, particularly in effectively capturing the temporal context over extended surgery durations and managing the high computational demands of video analysis required for real-time applications. To overcome these challenges, we propose SPRMamba, a novel framework designed to enhance the accuracy and efficiency of ESD surgical phase recognition. Specifically, our method proposes the Scale Residual TranMamba module, which combines the ability of Mamba to extract the long-term temporal context with the ability of the Transformer to extract the short-term temporal context and excels in capturing complex temporal relationships in surgical videos. In addition, considering the real-time requirements of surgical phase recognition, a temporal sampling strategy is designed to optimize computational resources by efficiently modeling.# Current Research on Surgical Video Analysis\n\nBoth short-term and long-term temporal contexts. Extensive experiments demonstrate that SPRMamba not only outperforms current state-of-the-art methods in accuracy and robustness but also significantly reduces the computational burden. In the future, our approach has the potential to advance the field of surgical video analysis, offering a valuable tool for both clinical practice and surgical education.",
        "context_id": 24,
        "question": "What backbone does the SPRMamba framework use for spatial feature extraction from ESD videos?",
        "answer": [
            "ResNet-50"
        ],
        "context_length": 34842
    },
    {
        "context": "# 1 Introduction\n\nThe peer review process plays a key role in advancing research by identifying high-quality, high-impact research. Nevertheless, a widespread decline in the quality of peer review has been observed across various disciplines (Brezis and Birukou, 2020; Cheah and Piasecki, 2022), with notable concerns in machine learning and artificial intelligence centered around the noisiness or arbitrariness of the process (Langford and Guzdial, 2015; Yuan et al., 2022). For instance, a randomized experiment at NeurIPS 2021 indicated that about half of the accepted papers would be rejected upon a second round of reviews (Cortes and Lawrence, 2021; Beygelzimer et al., 2023). One factor contributing to this decline is the stunning increase in the number of submissions to machine learning conferences. For example, NeurIPS 2023\u2014one of three annual top machine learning conferences\u2014received 12,343 submissions; it seems nearly impossible to recruit enough experienced and capable reviewers.# Footnotes\n\n- \u2217 University of Pennsylvania. Email: suw@wharton.upenn.edu.\n- \u2020 Massachusetts Institute of Technology.\n- \u2021 University of North Carolina at Chapel Hill.\n- \u00a7 New York University.\n- \u00b6 Princeton University.# Evaluating the Isotonic Mechanism for Peer Review\n\nTo provide low variance assessments for this volume of submissions on a regular basis (Sculley et al., 2019; Stelmakh et al., 2021). This reality has spurred a continued research effort aimed at improving the peer review process for machine learning conferences, often focusing on reviewer assignments and reviewer biases (Kobren et al., 2019; Wang and Shah, 2019; Jecmen et al., 2020; Leyton-Brown et al., 2022; Stelmakh et al., 2023).\n\nIn contrast to existing research on the peer review process, which has focused primarily on the reviewers, the recently proposed Isotonic Mechanism seeks to enhance peer review by leveraging authors\u2019 opinions of the quality of their own submissions to yield more robust review scores (Su, 2021). This mechanism requires authors with multiple submissions to rank their papers according to their perception of the relative quality of the papers and generates calibrated scores that are modified from the original review scores to align with the author-provided rankings. This ranking-based calibration can be seen as a \u201cde-noising\u201d of the original review scores from the perspective of the authors (Su, 2021, 2022). Accurate review scores are crucial to improving peer review in very large-scale publication venues because they are the most important factor in determining accept/reject decisions.1\n\nThe Isotonic Mechanism is well-suited for contemporary machine learning conferences, where it is commonplace for an author to submit multiple papers at the same time (Sun, 2020; Rastogi et al., 2022). An advantage of this methodology is that it requires minimal effort from authors and imposes no additional burden on reviewers. Rather than exacerbating reviewer workload, this approach uses the authors themselves as a resource for adding information to the peer review process.\n\nIn this paper, we aim to evaluate the empirical effectiveness of the Isotonic Mechanism for peer review. To this end, we conducted a survey experiment at the 2023 International Conference on Machine Learning (ICML), which is one of the top-tier conferences in machine learning and artificial intelligence. In 2023, ICML received 6,538 submissions from 18,535 authors. On January 26, 2023, right after the ICML submission deadline, we sent a survey to all submitting authors who have OpenReview profiles to ask them to provide rankings of their submissions if they submitted at least two papers.# Research Questions\n\nSpecifically, we address the following questions by analyzing the ICML 2023 ranking data together with review scores and accept/reject decisions:2\n\n1. How do the outputs of the Isotonic Mechanism, which we refer to as isotonic scores, compare to original/raw review scores in terms of accurately reflecting submission quality?\n2. What near-term applications might there be for leveraging isotonic scores to enhance the peer review process?\n3. What are the limitations of the study, and which aspects of the mechanism should future experiments investigate?\n\nAddressing the first question requires understanding the relationship between review scores and paper quality, which is challenging due to the absence of ground truth quality of the ICML 2023 submissions. To address this, we leverage the fact that submissions typically receive multiple review scores. We use the average of the remaining scores as a proxy for the ground truth \u201cexpected\u201d quality.\n\nFor example, the NeurIPS 2023 guidelines for area chairs state that any decision \u201cshould be properly explained\u201d if any paper with an average score above (below) the threshold is rejected (accepted).\n\nOur code is publicly available on GitHub.# Review Score Evaluation\n\nIn Section 3.1, we show that although the squared error of the review scores computed with respect to this proxy is an upwards-biased estimator for the unobserved \u201cground truth\u201d squared error with respect to the true expected review score, because the bias terms exactly cancel, it can still be used to obtain an unbiased estimator for the difference in squared error between two estimators, with respect to the ground truth. Figure 1 shows that the Isotonic Mechanism substantially reduces proxy estimation errors\u2014specifically, mean squared error (MSE) and mean absolute error (MAE)\u2014for the 2,592 submissions ranked by authors. Moreover, it suggests that the improvement becomes more substantial as the number of submissions of an author increases. This dependence implies that more author-provided rankings could lead to even more significant error reduction through the Isotonic Mechanism. Because the change in proxy estimation error is an unbiased estimator for the change in ground truth estimation error, this allows us to produce confidence intervals for the decrease in ground truth squared error, and we find substantial decreases at the 99% confidence level. See more details in Section 3.# Applications of Isotonic Scores\n\nIn response to the second question, Section 4 proposes three cautious applications of isotonic scores and author-provided rankings during peer review processes. First, we suggest that senior area chairs (SACs) can use isotonic scores to help direct additional scrutiny of accept/reject recommendations made by area chairs (ACs). Here the scores are used to focus additional human scrutiny, not directly used for accept or reject decisions. Second, we suggest using author-provided rankings to help direct attention during the selection of paper awards. This application occurs after the accept/reject decisions are made, ensuring its impact is circumscribed, and again used to direct human attention rather than to make decisions directly. Third, we propose using the discrepancy between isotonic scores and raw review scores as an indicator of review quality. A significant discrepancy, particularly in comparison to other submissions, could signal the need for an emergency reviewer to provide additional evaluation. We provide empirical evidence from the ICML 2023 data supporting the effectiveness of the latter two applications. These three applications share the crucial feature that the sensitive information of rankings and isotonic scores is visible only to certain high-level roles, such as SACs and program chairs (PCs), and their use does not directly impact the accept/reject decisions.# Conclusion and Future Research\n\nIn Section 5, we conclude our paper by discussing the third question, the limitations of our experimental results, and suggesting avenues for future research. While this method has shown empirical effectiveness at increasing the accuracy of noisy reviews, and is supported by theoretical underpinnings\u2014including \u201ctruthfulness\u201d guarantees\u2014in a stylized setting (Su, 2022), using the isotonic mechanism to make important decisions may give rise to unforeseen consequences because of the possibility of strategic manipulation by authors in ways that are not captured by the stylized analysis. Thus, caution is warranted which is why our policy recommendations are limited, and we recommend a more comprehensive investigation of the Isotonic Mechanism in future experiments.# 1.1 Related Work\n\nContinued efforts have been made to evaluate and quantify the randomness and quality of peer review from the standpoint of authors. It has been demonstrated that there is a consensus among researchers about the declining quality of peer review (Lipton and Steinhardt, 2019; Wang et al., 2019).\n\nIf all ICML 2023 authors were to provide their rankings, this would increase the average length of rankings. See Figure 11 in the Supplementary Material.# Number of Submissions\n\n|Number of Submissions|Isotonic Score|Isotonic Score|Number of Submissions|Raw Score|Raw Score| | |\n|---|---|---|---|---|---|\n|Raw Score| | |Isotonic Score|Raw Score|Isotonic Score|\n|17|1.0|2.16|17|0.81|1.15|\n|16|1.06|2.31|16|0.78|1.21|\n|15| | |15| | |\n|14|1.85|2.1|14|1.01|1.09|\n|13|1.54|2.06|13|1.04|1.27|\n|12|1.14|1.86|12|0.86|1.16|\n|11|0.58|1.69|11|0.59|0.85|\n|10|1.92| |10|1.22| |\n|9|1.66| |9|1.09|1.23|\n|8|2.58|3.72|8|1.31|1.68|\n|7|1.79|2.39|7|0.97|1.18|\n|6|1.7| |6|1.01|1.17|\n|5|1.91| |5|1.07| |\n|4|1.99|2.65|4|1.07|1.26|\n|3|1.88|2.5|3|1.07|1.23|\n|2|2.17|2.55|2|1.15|0.25|# Mean Absolute Error\n\nFigure 1: (Proxy) MSE and MAE averaged over ICML 2023 authors who submitted rankings of the same length in our survey experiment, ranging from 2 to 17 submissions. The original/raw review scores are in red, while the isotonic scores are in blue. Overall, the relative improvement using the isotonic scores grows with the number of submissions by an author. For MSE, the average percentage improvement for numbers of submissions between 2 and 10 is 25% and it is 41% for submissions more than 11. See more details in Figure 10. The experimental setup is described in Section 3.2.# 2 Experimental Design and Summary Statistics\n\nWe first provide an overview of some basic statistics from ICML 2023.\n\n- (a) Number of submissions: 6,538.\n- (b) Number of authors: 18,535.\n- (c) Number of submissions with at least one author having more than one submission: 5,035 (77.0%).\n- (d) Number of authors with two or more submissions: 4,505 (24.3%).\n- (e) Number of authors with at least 5 submissions: 508.\n- (f) Number of authors with at least 10 submissions: 74.\n- (g) Number of authors with at least 15 submissions: 26.\n- (h) Number of authors with at least 20 submissions: 7.\n\nAn ICML 2023 submission was typically reviewed by three or four reviewers. Reviewers rated submissions on a scale from 1 (very strong reject) to 10 (award quality). For the 6,538 submissions, the decisions were as follows:\n\n- (a) Number of \u201cWithdrawn or Desk Rejected\u201d submissions: 991 (15.2%).\n- (b) Number of \u201cRejected\u201d submissions: 3,719 (56.9%).\n\nAmong the 18,535 authors, 20 did not have OpenReview profiles. Our survey was sent via the OpenReview API to those 18,515 authors who had OpenReview profiles. This category includes three papers withdrawn after having been accepted.# Summary Statistics from the Experiment\n\nWe first provide statistics about the rankings obtained from the survey:\n\n|Statistic|Value|\n|---|---|\n|(a) Number of authors who completed the survey|5,634 (30.4%)|\n|(b) Number of authors who had multiple submissions and provided rankings of their submissions|1,342|\n|(c) Number of submissions that were ranked by at least one author|2,592 (39.6%)|# Accepted Submissions\n\n(c) Number of \u201cAccepted as Poster\u201d submissions: 1,674 (25.6%).\n\n(d) Number of \u201cAccepted as Oral\u201d submissions: 154 (2.36%).\n\n(e) Number of submissions awarded the \u201cOutstanding Paper Award\u201d: 6 (0.09%).# Mean Review Scores\n\nThe mean of the average review scores following the rebuttal period for the categories \u201cRejected\u201d, \u201cAccepted as Poster\u201d, \u201cAccepted as Oral\u201d, and \u201cOutstanding Paper Award\u201d is 4.32, 5.93, 6.82, and 7.72, respectively.# Survey-Based Experiment\n\nThe survey-based experiment was conducted in OpenReview, which hosted the peer review process for ICML 2023, in conjunction with OpenRank.cc, which we developed to implement the experiment. On January 26, 2023, immediately following the submission deadline, an official email was sent through OpenReview to all ICML authors requesting information about their submissions. Importantly, participants were informed that the survey data would not be used in the decision-making process for ICML 2023. Figure 8 in the Supplementary Material shows the survey interface.# Solicited Information\n\nSpecifically, we solicited the following information:\n\n- Ranking: Authors with multiple submissions were asked to rank their papers based on their perceived quality, with allowances for ties in the rankings. Authors could order their papers by dragging them up or down at the OpenRank.cc interface.\n- Additional questions: All authors, including those with only one submission, were asked to respond to some questions, such as their confidence in the provided rankings and their perceived probability of inconsistencies between their expectations and the review outcomes. All these questions are shown in Figure 8 in the Supplementary Material.# Data Collection and Privacy\n\nAdditionally, review scores and final decisions were retrieved from OpenReview. This study was conducted with the approval of the Institutional Review Boards (IRB) at the University of Pennsylvania. The experiment and subsequent analyses adhered to strict privacy and confidentiality standards. Specifically, the data were anonymized by excluding all personal identifying information, and analysis began only after the accept/reject decisions were announced. For further information regarding our privacy policy, please refer to https://openrank.cc/legal/privacy. Furthermore, it is our policy that all data collected from this experiment be completely deleted by December 31, 2024.# (d) Number of reviews received by these 2,592 submissions:\n\n7,974 (3.08 reviews on average per submission).8# (e) The longest ranking list provided by an author:\n\n17 submissions. The dependence between the number of submissions by an author and their likelihood of completing the survey is shown in Figure 11 in the Supplementary Material. It appears that authors with more submissions were less likely to provide rankings.\n\nRegarding the additional questions in the survey, 59.8% of the authors reported high confidence in their rankings, and 59.4% would likely provide the same rankings if they were to be used in the decision-making process. More details are given in Figure 12 in the Supplementary Material.\n\nIn response to the question, \u201cWhat is your estimated probability that your lowest-ranked paper will have a higher or equal average rating than your highest-ranked paper?\u201d, over half of the authors estimated this probability to be at least 40%. The average of these estimated probabilities is 36.6%. The distribution of these probabilities is illustrated in Figure 13 in the Supplementary Material. In contrast, the actual proportion of authors whose lowest-ranked papers received higher or equal scores prior to the rebuttal period, compared to their highest-ranked papers, is 42.2%.# Preliminary analysis of rankings.\n\nExamining the relevance of author-provided rankings in predicting the quality of submissions is a key focus of our study. If these rankings were not predictive of review outcomes at all, incorporating them into the Isotonic Mechanism would be unlikely to enhance the efficacy of the review process. Yet, our preliminary analysis suggests that these rankings are indeed predictive, indicating their potential value to improve peer review.\n\nTo investigate this aspect, we grouped the highest-ranked paper by an author into one category and the lowest-ranked paper into another. The mean of average review scores received by the highest-ranked papers is 4.80 before the rebuttal period, while it is 4.50 for the lowest-ranked papers. This difference is statistically significant, with a p-value of 6.17 \u2192 10\u221216 using a one-sided t-test. A more detailed comparison between the two groups depending on the categories of decision is given in Table 1, which shows that highest-ranked submissions were more likely to obtain better review outcomes.\n\nMoreover, this positive correlation extends beyond the rebuttal period, where highest-ranked papers were more likely to receive an increase in score, with an average increase of 0.23, compared to 0.20 for the lowest-ranked. However, our analysis found no statistically significant correlation between the rankings and the length of the rebuttals, as measured by word count. This is illustrated in Table 6 in the Supplementary Material.# 3 Statistical Analysis of Isotonic Scores\n\nIn this section, we analyze the ICML 2023 ranking data, which comprises 1,342 rankings associated with 2,592 submissions. Our main finding is that the Isotonic Mechanism can reduce the MSE of review scores, and this improvement in estimation accuracy is not only statistically significant but also becomes more pronounced as the number of an author\u2019s submissions increases. This empirical finding aligns with the theoretical analysis of the method (Su, 2021, 2022).\n\n8 This is the number of reviews received before the rebuttal, as of March 12, 2023. The average number of reviews per submission increased to 3.29 after the rebuttal period, as of April 22, 2023.# Table 1: Comparison of outcomes (\u201cWithdrawn/Desk Rejected\u201d, \u201cRejected\u201d, \u201cAccepted as Poster\u201d, and \u201cAccepted as Oral\u201d) for highest-ranked versus lowest-ranked submissions.\n\n| |\u201cWithdrawn or Desk Rejected\u201d|\u201cRejected\u201d|\u201cAccepted as Poster\u201d|\u201cAccepted as Oral\u201d|\n|---|---|---|---|---|\n|Highest-ranked|9.02%|53.20%|33.31%|4.47%|\n|Lowest-ranked|16.24%|57.68% \u21923|23.85% 8|2.24% \u21923|\n|p-value|2.43 \u2192 10\u2192|2.20 \u2192 10|7.31 \u2192 10\u2192|1.87 \u2192 10|\n\nHighest-ranked submissions received significantly better review outcomes. We use a one-sided t-test to obtain p-values.# 3.1 Method and Evaluation\n\nThe Isotonic Mechanism operates as follows (Su, 2021). Consider an author who submits n papers to a conference. The mechanism requires the author to rank these submissions in descending order of perceived quality. The ranking is denoted by \u03c9, which allows for ties. Given the (average) raw review scores y := (y1, y2, . . . , yn) for the n submissions, the Isotonic Mechanism outputs the ranking-calibrated scores as the solution to the following optimization program:\n\nmin \u2191y \u2193 r\u21912 s.t. r\u03c9(1) \u2194 \u00b7 \u00b7 \u00b7 \u2194 r\u03c9(n), r:=(r1,...,rn)\n\nwhere \u2191 \u00b7 \u2191 denotes \u03b52 norm or, equivalently, Euclidean distance. Formally, this convex optimization program is equivalent to isotonic regression (Barlow and Brunk, 1972). For example, letting y = (8, 7, 4, 3) and (\u03c9(1), \u03c9(2), \u03c9(3), \u03c9(4)) = (1, 3, 2, 4), the isotonic scores are (8, 5.5, 5.5, 3).\n\nAn essential aspect of this method lies in the assumption that the author is knowledgeable about the quality of their submissions. The Isotonic Mechanism is perhaps the simplest blend of authors\u2019 and reviewers\u2019 perspectives. Furthermore, under certain conditions, authors are incentivized to truthfully report their rankings when these modified scores are used for decision-making (Su, 2021). With truthful author-provided rankings, the isotonic scores are more accurate than the raw scores in estimating the ground truth quality of submissions. Extensions of this mechanism for broader practical settings are discussed in Yan et al. (2023) and Wu et al. (2023).\n\nTo adapt the Isotonic Mechanism to the practical setting where most papers each have multiple authors, we consider three strategies.# Simple-averaging strategy.\n\nThe first strategy runs the mechanism for each author who provides a ranking. For a given submission, different authors often yield different modified scores. The isotonic score for the submission is calculated as the simple average of these different modified scores.# Greedy strategy.\n\nThis strategy starts by running the Isotonic Mechanism for the author who provides the longest ranking, and then removes this author and all submissions of the author from further consideration. This process is repeated for the remaining authors and their submissions until each submission has exactly one isotonic score.\n\nFrom this ranking, the author has the opinion that the last paper has the lowest quality and the first paper has the highest quality. If the length of the ranking is 1, the isotonic score will be identical to the raw review score.# Figure 2: Illustration of the greedy and multi-owner strategies for the Isotonic Mechanism in the setting of multiple authors.\n\nAuthor-submission pairs highlighted in gray (Author 2\u2019s submissions in the greedy strategy) are excluded from consideration in the mechanism. In the multi-owner strategy, any paper in the red block has its score averaged over the two isotonic scores from Author 1 and Author 2.# Multi-owner strategy.\n\nThe first step of this strategy is to partition all submissions into disjoint blocks such that in each block every submission shares a common set of authors. Run the Isotonic Mechanism taking as input a ranking within the block from each author to yield modified scores. The last step is to average the modified scores separately for each block. See more details about this approach in Wu et al. (2023).\n\nIn both the greedy and multi-owner strategies, each run of the Isotonic Mechanism operates on a sub-ranking that involves a subset of the submissions from an author. However, it is important to note that these sub-rankings can be derived from complete rankings. Provide complete rankings, regardless of the strategy being implemented. Notably, under certain conditions, the greedy and multi-owner strategies ensure the truthful reporting of rankings by authors (Wu et al., 2023). In contrast, the simple-averaging approach does not generally guarantee this.# Evaluation metrics.\n\nEvaluating the performance of isotonic scores compared to raw scores presents a challenge due to the unknown ground truth quality of submissions. To address this challenge, we leverage the fact that a submission typically receives multiple reviews, resulting in multiple review scores. For simplicity, consider y and y\u2191 as two independent scores of the same submission, both assumed to be unbiased estimators of the ground truth.11 Let \u0302y denote any estimator of the ground truth using only the data y. The performance of \u0302y is measured using either (\u0302y \u2193 y\u2191)\u00b2 or |\u0302y \u2193 y\u2191|, which we refer to as the \u201cproxy\u201d MSE and MAE, respectively. Note that the conventional MSE and MAE of \u0302y are defined as E(\u0302y \u2193 ground truth)\u00b2 and E|\u0302y \u2193 ground truth|, respectively, which are not observable. In contrast, (\u0302y \u2193 y\u2191)\u00b2 and |\u0302y \u2193 y\u2191| can be precisely calculated from the raw scores.\n\nBoth proxy MSE and MAE are upward-biased estimators of their conventional counterparts.11 Scores may exhibit biases conditional on certain factors (Wang et al., 2020). In our context, \u201cunbiasedness\u201d is understood in a marginal sense, achieved through random selection of scores without conditioning on variables such as confidence level or reviewer seniority. Moreover, it is more appropriate to interpret \u201cground truth\u201d as the \u201cground truth score\u201d rather than the intrinsic merit of a paper. Practically, the ground truth score could be considered as the average score given by a very large number (say, 1,000) of reviewers.This can be seen, as the expectation of the proxy MSE is expressed as\n\nE(\u0302 y \u2193 y\u2191  )2 = E(\u0302y \u2193 Ey \u2191 )2 + Var(y) = MSE(\u0302 y) + Var(y).\n\nIn essence, the bias of the proxy MSE is equal to the variance of the \u201cnoisy target\u201d y\u2191. For the proxy MAE, note that it satisfies E|\u0302 y \u2193 y\u2191  | \u2194 E|\u0302 y \u2193 Ey | = MAE(\u0302 y). Here, the inequality follows from Jensen\u2019s inequality when applied to the convex function |c \u2193 x| for any constant y = c.\u0302\n\nDespite this bias, the proxy MSE retains the ability to compare two estimators in expectation: for any two estimators y and \u02dc, their proxy MSE\u2019s difference satisfies\n\nE(\u0302 y \u2193 y\u2191  )2 - E(\u02dc \u2193 y)2 = MSE(\u0302y) + Var(y) - MSE(\u02dc) - Var(y) = MSE(\u0302y) - MSE(\u02dc).\n\n(3.1)\n\nTherefore, if y outperforms y in terms of MSE, then \u0302y will also have a smaller proxy MSE than \u02dc in expectation, and vice versa.\n\nWhen analyzing the ICML 2023 ranking data, we randomly selected one review score per submission as the data y for estimating the submission\u2019s quality using either the Isotonic Mechanism or the raw-score-estimator. The average of the remaining review scores serves as the noisy target y\u2191. For this purpose, a submission must have at least two review scores. This is applicable to 2,530 out of the 2,592 ranked submissions.# 3.2 Results\n\nTo compare the isotonic and raw scores, Figure 3 presents scatter plots for each of the proxy MSE and MAE across the 2,530 submissions. A least-squares fit without an intercept between the proxy errors using isotonic scores and raw scores indicates that, on average, the proxy MSE of isotonic scores is smaller than that of raw scores, a difference that is statistically significant at the 10-8 level. Similarly, the proxy MAE of isotonic scores is found to be statistically smaller than that of raw scores, with the gap being narrower yet still significant. This is consistent across all three strategies of the Isotonic Mechanism, suggesting an overall improvement in estimating submission quality. Furthermore, Figures 4 and 5 corroborate the findings in Figure 3.\n\nTable 2 demonstrates that the Isotonic Mechanism using any of the three strategies reduces both the overall proxy MSE and MAE compared to raw scores. Specifically, the greedy strategy achieves a 21.3% reduction in the proxy MSE and 11.7% in the proxy MAE. Furthermore, evidence suggests that the reduction in conventional MSE is likely to exceed 21.3% when employing isotonic scores, as elaborated in Section C in the Supplementary Material.\n\n|Proxy MSE|Error|Improvement|p-value|Proxy MAE|Error|Improvement|p-value|\n|---|---|---|---|---|---|---|---|\n|Raw Score|2.57|NA|NA|1.26|NA|NA| |\n|Simple-averaging Strategy|1.97|23.48%|9.99 \u2192 10-9|1.10|12.75%|6.69 \u2192 10-8| |\n|Greedy Strategy|2.02|21.30%|5.53 \u2192 10-7|1.11|11.71%|1.66 \u2192 10-7| |\n|Multi-owner Strategy|2.07|19.38%|1.22 \u2192 10|1.12|10.62%|3.10 \u2192 10| |\n\nA two-sample t-test shows that the reduction in proxy errors is statistically highly significant.Denote by \u0302y Iso \u2193 y ) \u0302y Ave Iso and \u2191 is an unbiased estimator of MSE(\u0302 y Ave ) \u2193 MSE(\u0302 y IsoAs shown in (3.1), the isotonic score and raw score, respectively. (\u0302 y Ave \u2193 y ) 2 \u2191 \u2193 (\u0302 y 2 ). This observation allows us to construct confidence intervals for the average reduction in ground truth MSE\u2014that is, MSE(\u0302 y Ave ) \u2193 MSE(\u0302 y Iso ) averaged over all ranked submissions\u2014and we present the results in Table 3. At the 95% confidence level, MSE(\u0302 y i Iso) on average is smaller than MSE(\u0302 y i Ave) by 0.4 or more.\n\n|Confidence Level|Simple-averaging Strategy|Greedy Strategy|Multi-owner Strategy|\n|---|---|---|---|\n|95%|[0.52, 0.69]|[0.46, 0.63]|[0.42, 0.58]|\n|99%|[0.49, 0.71]|[0.44, 0.66]|[0.39, 0.60]|\n\nTable 3: 95% and 99% confidence intervals for the average reduction of MSE by using isotonic scores compared to raw scores. This makes use of the fact that the reduction in proxy MSE is an unbiased estimator of ground truth MSE reduction, as shown in (3.1).\n\nIn investigating the impact of an author\u2019s number of submissions on the Isotonic Mechanism\u2019s performance, Figure 6 averages the proxy MSE and MAE for isotonic and raw scores across authors with the same number of submissions. The results indicate a tangible and statistically significant improvement in estimation accuracy using the Isotonic Mechanism, irrespective of the number of submissions. Overall, this improvement becomes more pronounced with an increase in submission quantity, as shown in Figure 10 in the Supplementary Material.\n\nThese findings align with the theoretical analysis in Su (2022), which proves that the Isotonic Mechanism achieves greater performance with larger numbers of submissions. However, shorter rankings are typically more common in our dataset, as authors with more submissions in ICML 2023 tended to have lower response rates. Taken together, our results imply that the advantages of the Isotonic Mechanism would be more pronounced if all authors in ICML 2023 provided their complete rankings.# 4 Applications\n\nSection 3 provides strong evidence that the isotonic scores are more accurate estimates of ground truth compared to the raw review scores, as measured by both MSE and MAE. Despite these results, however, we do not advocate for using them to make a major change in how paper acceptance decisions are made at major machine learning conferences at the moment. Instead, we suggest a more modest and cautious approach for using isotonic scores in the review process, while at the same time conducting additional empirical evaluations of the Isotonic Mechanism to attempt to better understand the consequences of using it in the review process.\n\nTowards this end we have identified several specific applications of the Isotonic Mechanism and author-provided rankings that appear to be beneficial without significant negative consequences. These applications primarily target scenarios where authors are aware of important nuances regarding the scientific value of their papers, which might be overlooked by reviewers or ACs. These applications share the common feature that the isotonic scores or author-provided rankings are accessible only to certain high-level roles within the peer review hierarchy, such as SACs and above, and thus give a separation between the isotonic scores and the majority of accept/reject decisions.# Error of Isotonic Score\n\n|Simple-averaging, Squared Error|Simple-averaging, Squared Error|Simple-averaging, Absolute Error|Simple-averaging, Absolute Error|\n|---|---|\n|25|y = x|5|y = x|\n|20| |4| |\n|15| |3| |\n|10| |2| |\n|5| |1| |\n|0| |0| |\n\n0 5 10 15 20 25\n\nError of Raw Score\n\n|Greedy, Squared Error|Greedy, Squared Error|Greedy, Absolute Error|Greedy, Absolute Error|\n|---|---|\n|25|y = x|5|y = x|\n|20| |4| |\n|15| |3| |\n|10| |2| |\n|5| |1| |\n|0| |0| |\n\n0 5 10 15 20 25\n\nError of Raw Score\n\n|Multi-owner, Squared Error|Multi-owner, Squared Error|Multi-owner, Absolute Error|Multi-owner, Absolute Error|\n|---|---|\n|25|y = x|5|y = x|\n|20| |4| |\n|15| |3| |\n|10| |2| |\n|5| |1| |\n|0| |0| |\n\n0 5 10 15 20 25\n\nError of Raw Score\n\nFigure 3: Scatter plots comparing isotonic and raw scores in terms of proxy MSE (left) and proxy MAE (right). Each plot represents a submission, with the x and y axes indicating the proxy errors of the raw score and isotonic score of the submission, respectively. The least-squares fit line is consistently below the 45\u00b0 line passing through the origin, demonstrating that the proxy error of isotonic scores is, on average, smaller than that of the raw scores. The analyses in this section are based on review scores collected prior to the rebuttal phase, as of March 12, 2023. For analyses based on post-rebuttal review scores, see Section B in the Supplementary Material.\n\n12# Simple-averaging, Squared Error\n\n|Number of Submissions|Isotonic Score|Raw Score|\n|---|---|---|\n|1000|600|480|\n|800|360| |\n|600|240| |\n|400|120| |# Greedy, Squared Error\n\n|Number of Submissions|Isotonic Score|Raw Score|\n|---|---|---|\n|1000|600|480|\n|800|360| |\n|600|240| |\n|400|120| |# Multi-owner, Squared Error\n\n|Number of Submissions|Isotonic Score|Raw Score|\n|---|---|---|\n|1000|600|480|\n|800|360| |\n|600|240| |\n|400|120| |# Figure 4:\n\nHistograms comparing the distributions of isotonic and raw scores in terms of proxy MSE (left) and proxy MAE (right). The distribution of isotonic scores is more heavily weighted towards smaller errors.# Figure 5: Difference between cumulative distributions of proxy errors for isotonic and raw scores.\n\n|Simple-averaging|Simple-averaging|Greedy|Greedy|Multi-owner|Multi-owner|\n|---|---|---|\n|Squared Error|Absolute Error|Squared Error|Absolute Error|Squared Error|Absolute Error|\n|250|250|250|250|250|250|\n|200|200|200|200|200|200|\n|150|150|150|150|150|150|\n|100|100|100|100|100|100|\n|50|50|50|50|50|50|\n|0|0|0|0|0|0|\n\nLeft panel: At x-axis value x, the y-axis value represents |{i : (\u0302 y i Iso\u2193 y i \u2191) 2 x}|, where |{i : (\u0302 y i Iso\u2193 y i \u2191) 2 \u2197 x}| denotes the number of submissions with isotonic scores having proxy MSE less than or equal to x.\n\nRight panel: At x-axis value |{i : |\u0302 y i Iso\u2193 y | \u2197 x}| \u2193 |{i : |\u0302 y i Ave\u2193 y | \u2197 x}|. The consistently positive difference demonstrates that isotonic scores generally yield smaller proxy errors than raw scores in distribution.# Mean Squared Error\n\n|Rank|Simple-averaging| |Greedy| |Multi-owner| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|Isotonic Score|Raw Score|Isotonic Score|Raw Score|Isotonic Score|Raw Score| | | | | | |\n|17|0.81|1.15|0.81|1.15|0.78|1.15| | | | | |\n|16|0.72|1.21|0.78|1.21|0.83|1.21| | | | | |\n|15| | | | | | | | | | | |\n|14|0.96|1.00|1.01|0.09|0.77|1.00| | | | | |\n|13|0.95|1.27|0.86|1.04|0.96|1.27| | | | | |\n|12|0.86|1.16| |1.16|0.93|1.16| | | | | |\n|11|0.66|0.85|0.59|0.85|0.85|0.86| | | | | |\n|10|1.11|1.27|1.21|1.27|1.00|1.27| | | | | |\n|9|1.07|1.21|1.09|1.31|1.08|3.35| | | | | |\n| | | | | |8| |1.68| |1.68| |1.68|\n|7|0.90|1.18|0.97|1.18|0.98|1.18| | | | | |\n|6|1.01|1.71|1.01|1.71|1.06|1.17| | | | | |\n|5|1.06|1.33|1.07|1.33| |1.00| | | | | |\n|4|1.07|1.26|1.07|1.26|1.10|1.26| | | | | |\n|3|1.05|1.23|1.07|1.23|1.07|1.23| | | | | |\n|2|1.14|1.25|1.15|2.51|1.16|2.51| | | | | |# Mean Absolute Error\n\n|Rank|Simple-averaging|Simple-averaging|Greedy|Greedy|Multi-owner|Multi-owner| | | |\n|---|---|---|---|---|---|---|\n| |Isotonic Score|Raw Score|Isotonic Score|Raw Score|Isotonic Score|Raw Score|\n|17|1.04|2.16|1.00|2.16|1.02|2.16|\n|16|0.89|2.31|1.06|2.31|1.01|2.31|\n|15| | | | | | |\n|14|1.47|2.10|1.85|2.10|1.35|2.10|\n|13|1.25|2.06|1.14|1.54|1.18|2.06|\n|12|1.06|1.86| |1.86|1.21|1.86|\n|11|0.66|2.44|0.58|2.44|0.92|1.69|\n|10| | | | |1.68|2.44|\n|9|1.66|2.34|1.66|2.34|1.69|2.34|\n|8|2.19|3.72|2.58|3.72|2.79|3.72|\n|7|1.53|2.39|1.79|2.39|1.92|2.39|\n|6|1.71|2.21|1.72|2.21|1.87|2.24|\n|5|1.88|2.88|1.91|2.88|2.00|2.88|\n|4|1.95|2.65|1.99|2.65|2.04|2.65|\n|3|1.80|2.50|1.88|2.50|1.91|2.50|\n|2|2.09|2.55|2.17|2.55|2.17|2.55|# Figure 6\n\nComparison between isotonic and raw scores in terms of proxy MSE and MAE averaged over ICML 2023 authors who submitted the same-length rankings. Figure 1 corresponds to the middle column.# 4.1 Oversight of ACs\u2019 Recommendations\n\nThe isotonic scores can be used to flag submissions in need of more scrutiny by SACs. In this application, the isotonic scores are made visible to SACs and those in higher roles, who can then use these scores to more effectively oversee the recommendations made by ACs. For instance, significant discrepancies between isotonic scores and ACs\u2019 recommendations could serve as red flags, prompting SACs to scrutinize and discuss these cases with the ACs.\n\nThe use of isotonic scores in this application presents low risk because ACs, who make the initial accept/reject recommendations (the majority of which are also the final decisions), do not have access to these scores. To further mitigate risk, when an SAC identifies a red flag for a submission\u2019s review, the SAC could request that the AC conduct a further review of the submission without specifying that the request is due to a large discrepancy between the accept/reject recommendation and the authors\u2019 own opinions.# 4.2 Selection of Paper Awards\n\nMachine Learning conferences select certain papers to receive awards. The process typically begins with the formation of a shortlist, including papers with high average scores or those nominated by ACs. A committee then carefully reviews these shortlisted papers to identify the award recipients. The committee is intended to carefully weigh each paper on its merits, and not simply choose based on reviewer scores. However, the process does not always go smoothly; for insight into the difficulties and controversies that some recent award decisions have generated, we refer the reader to Carlini et al. (2022) and Orabona (2023), which critically examined the ICML Outstanding Paper Awards in 2022 and 2023.\n\nAuthor-provided rankings could be given as an additional useful piece of information for the committee involved in the selection of paper awards. As evidence that this information might be useful, three out of the six papers awarded as Outstanding Papers at ICML 2023 were ranked by one of their authors and, notably, were all ranked first by their authors. Furthermore, of the 84 submissions that received oral presentations (a distinction that is given to the top few percent of papers) and had rankings from their authors, 69.1% were ranked first by at least one of their authors. These statistics highlight a strong correlation between the authors\u2019 rankings and the recognition the papers received.\n\nIn the selection of papers for awards, the rankings could be made visible to some PCs who are not on the selection committee.12 The committee relies on their expertise in the selection of the paper awards without knowledge of the author-provided rankings. Once the recommendation is made by the selection committee, the PCs could then scrutinize and raise flags if a recommended paper receives low rankings from its authors, in which case the committee may need to gather additional evidence before considering it for an award.\n\nThe award selection takes place following the accept/reject decisions. This phase does not impact most authors, thereby minimizing the potential for unforeseen outcomes when using author-provided rankings.\n\n12 Here, the rankings instead of the isotonic scores are used for a reason that will be elaborated in Section 4.3.# 4.3 Recruitment of Emergency Reviewers\n\nIn machine learning conferences, it is common practice to recruit emergency reviewers in response to indicators of low review quality, often triggered by low-confidence reviews or significant disagreement among reviewers for a submission. For instance, NeurIPS 2023 recommended recruiting an additional emergency reviewer for each low-confidence review in addition to the four regular reviewers. An effective mechanism for assigning emergency reviewers is an economical way of utilizing the limited pool of qualified reviewers (Peng, 2018; Stelmakh et al., 2021) by adaptively assigning them to papers based on the quality of the initial round of reviews.\n\nDetermining review quality is an inherently noisy process. Incorporating authors\u2019 elicited rankings into this determination when assigning emergency reviewers could both improve its accuracy and enhance the community\u2019s trust in the credibility of the peer review processes. It is crucial to convey to authors that their concerns are taken seriously, especially when they disagree with the reviews. This can be achieved by leveraging isotonic scores, based on the premise that discrepancies between raw review scores and isotonic scores, which we refer to as isotonic residuals,13 might signal concerns about review quality from the authors\u2019 viewpoint.\n\n|Isotonic Residual|0|1|2|3|4|5|6|7|\n|---|---|---|---|---|---|---|---|---|\n|\u21922|\u21921|\u21924|\u21922|0|1|2|3|4|\n\nFigure 7: Scatter plots showing isotonic residuals using the simple-averaging strategy, plotted against review confidence levels and against raw score variance, for submissions with a confidence level of 3 or above.\n\nTo provide empirical support, we examine the relationship between isotonic residuals and both the variance of review scores and review confidence levels, using data from ICML 2023. High score variance or low confidence levels are often used as indicators for recruiting emergency reviewers (Shah et al., 2018). Figure 7 illustrates that isotonic residuals in absolute value have a strong negative correlation with confidence levels and a positive correlation with score variance. Furthermore, little dependence is found between score variance and confidence levels, with a correlation of only 2.05 \u2192 10\u21922. This suggests that isotonic residuals might offer a more comprehensive measure of review quality.\n\nWe also evaluate the effectiveness of isotonic residuals using data from a second survey, which asked authors to identify submissions where the review outcomes differed most from their expectations. Table 4 shows that isotonic residuals are the most predictive of submissions with the most \u201cunexpected\u201d review outcomes. This finding is expected as isotonic scores, by definition, reflect.\n\nIn contrast to the setup in Section 3.2, here we run the Isotonic Mechanism on the average of all review scores for each submission.\n\n17# Prediction Accuracy\n\n|Isotonic Residual|Score Variance|Score Confidence|\n|---|---|---|\n|254/322 = 78.9%|162/322 = 50.3%|136/322 = 42.2%|# Table 4:\n\nPrediction accuracy of the most \u201cunexpected\u201d review outcomes, determined using the largest mean isotonic residual in absolute value, the greatest variance of review scores, and the lowest average confidence levels.\n\nGiven this empirical evidence, we propose using large isotonic residuals as an indicator of the need for emergency reviewers to provide additional expert opinions. In implementing this mechanism, it is crucial to ensure that submissions receive roughly the same number of reviewers on average, regardless of whether a submission is included in this mechanism or not.14 One approach to achieving this balance is to assign three initial reviewers to papers participating in the mechanism, while four reviewers to those not included. For papers in the participating group, we assign two emergency reviewers for isotonic residual magnitudes in the top 30%, and one for magnitudes between the 30th and 70th percentiles.15 Consequently, on average, a paper has four reviewers, regardless of its group. To ensure a cautious approach, the quantile of the isotonic residual in absolute value will be made available to ACs and above, but not the raw isotonic score itself. Without knowing whether the isotonic residual is positive or negative, ACs cannot ascertain whether authors hold a high or low opinion of their submissions, thereby minimizing the likelihood of bias influencing the ACs\u2019 decisions.\n\nIt is important to note that the isotonic score differs from the raw review score only when at least one author has multiple submissions to the conference. Consequently, the Isotonic Mechanism cannot uniformly increase review accuracy across all submissions, which is also the rationale for suggesting the use of rankings rather than isotonic scores in the selection of paper awards, as discussed in Section 4.2. As demonstrated, isotonic scores are better estimates of the ground truth \u201cexpected review scores\u201d than the raw review scores, and the accuracy improves with the number of papers an author has submitted and ranked (Su, 2021). In particular, authors with fewer submissions are more likely to have higher variance in their isotonic scores. Thus, caution is warranted when comparing isotonic residuals of submissions across authors. Nevertheless, despite the fact that the Isotonic Mechanism does not improve accuracy uniformly for all submissions, as it is accuracy improving, it seems prudent to (cautiously) use the information to improve the review process\u2014using the isotonic residuals to recruit emergency reviewers is one such example of cautious use.# 5 Limitations and Future Work\n\nThis paper has presented analyses of the ICML 2023 ranking data collected from 1,342 submitting authors to empirically evaluate the Isotonic Mechanism. Our findings indicate that this mechanism14 A submission might not be included because no author has multiple submissions. However, this applies to a minority of submissions in large machine learning conferences. For example, 77.0% of the ICML 2023 submissions have at least one author with more than one submission.15 The numbers 30% and 70% are arbitrary as long as they ensure that the expected number of emergency reviewers for a paper is one.# Current Research on Peer Review Processes\n\nCan effectively mitigate noise in review scores. Additionally, we introduce three cautious applications of the Isotonic Mechanism and author-provided rankings to improve peer review processes.\n\nIn interpreting these results, it is crucial to note that the rankings were provided under the condition that they would not influence decision-making processes. Authors might behave strategically if their rankings were used in decision-making. We note here that 59.4% of authors in our survey stated that they would submit the same rankings even if they were to be used for decision making.\n\nAlthough the Isotonic Mechanism has been shown to be dominant strategy truthful in stylized scenarios (Su, 2022), we cannot rule out the possibility that authors could benefit from strategic behaviour in a real world deployment. Examples of strategic behaviors not covered in the game theoretic analysis might include authors showing a preference for papers where they are the first author over those where they are secondary, or a professor ranking a student\u2019s paper higher than warranted to boost the student\u2019s job market prospects, or an author would like to get a weak paper published first and defer strong ones for the next conference series. The possibility of strategic behavior is perhaps the most concerning obstacle for deployment of the Isotonic Mechanism for consequential decision making, and should be the focus of future investigation.\n\nAnother concern is that the variance reduction effect of the Isotonic Mechanism increases with the number of papers an author submits (and does nothing at all for authors who submit only a single paper). This complicates comparisons of scores across authors with different numbers of submissions, and is another aspect that is in need of further study.\n\nThe analysis of the results could be potentially improved by properly addressing the potential for non-response bias, which exists due to the 30.4% response rate in our experiment. For example, the mean of average review scores of all submissions to ICML 2023 is 4.53 before the rebuttal period, while it is 4.66 for the 2,592 ranked submissions. The response rate is largely influenced by the number of submissions per author, with more prolific authors being less inclined to provide rankings. However, it is these authors for whom the Isotonic Mechanism could be most effective. Consequently, this bias might lead to a conservative estimate of the mechanism\u2019s effectiveness. In future experiments, incentivizing authors to participate could increase participation. One possible incentive mechanism would be to provide an earlier notification of decisions to authors who provided rankings.# Table 5: Fractions of Overlapping Top-Scored Submissions\n\n|Percentile|5%|10%|15%|20%|25%|30%|\n|---|---|---|---|---|---|---|\n|Overlapping Fraction|53.17%|64.03%|67.81%|66.40%|73.42%|81.55%|\n\nAnother potentially useful approach to improving the quality of reviews is to aggregate rankings provided by reviewers. Imagine a conference where every reviewer is asked to rank the submissions they have reviewed. Consider each submission as a node and draw an edge between two nodes if they share a common reviewer. This creates a submission-reviewer network, which divides the submissions into several connected graphs. For each connected graph, the Plackett\u2013Luce model can be employed to estimate the preference score of each paper using the spectral method described in Fan et al. (2024b). This approach yields ranking-based scores, which refine raw review scores through the aggregation of preferences from all reviewers. These scores can also be integrated with those generated by the Isotonic Mechanism. Notably, preference scores can be consistently estimated even when reviewers only identify their top choices or provide partial rankings (Fan et al., 19).# To conclude\n\nwe discuss several avenues for future research. From a methodological viewpoint, the Isotonic Mechanism in its current form does not account for reviewers\u2019 confidence levels; developing weighted review scores that feed into the mechanism could potentially enhance its performance. Additionally, investigating how the mechanism could leverage rankings provided by reviewers (Fan et al., 2024b) presents another valuable research direction. The practical challenge of coauthors holding differing opinions on their submissions, as evidenced by the 28.7% of coauthor pairs providing inconsistent comparisons (dropping to 25.9% for submissions with substantial review score differences), calls for developing a variant of the mechanism that adapts to these ranking inconsistencies (Wu et al., 2023). From a practical perspective, whether a paper would be accepted depends largely on whether its score is above the top, say, 20% of all submissions. Table 5 shows how much the top-scored papers would change from using raw scores to using isotonic scores. Notably, the overlapping fraction increases as the cutoff percentage increases. An interesting question for future study is to compare these two different lists of top-scored papers in terms of the citations they have accumulated over the years.",
        "context_id": 25,
        "question": "How many submissions did NeurIPS 2023 receive?",
        "answer": [
            "12,343"
        ],
        "context_length": 49306
    },
    {
        "context": "# Figure 1: Reinforcement Learning Circuit [20]\n\nAdvances in deep learning techniques, particularly the development of deep neural networks, have significantly improved the ability of RL algorithms to handle high-dimensional and continuous state spaces. Due to their ability to approximate complex value functions and policies, DRL algorithms, such as Deep Q-Networks (DQN) and Proximal Policy optimization (PPO), have demonstrated remarkable performance in challenging tasks. The availability of large-scale computing resources, coupled with distributed computing frameworks such as TensorFlow or PyTorch,# 2 State of the Art\n\nAs early as 1995, Zhang und Dietterich [28] explored the application of RL techniques to the JSSP by further developing the results of the scheduling algorithms by Deale et al. [3], who used a simulated annealing approach for job shop scheduling. The system learns to make informed decisions that lead to more efficient schedules and thereby shows the potential of RL to solve JSSPs.\n\nIn 2000, Aydin and \u00d6ztemel [1] presented an approach that is composed of a simulated job shop environment and a RL agent that selects the most appropriate priority rule from a set of available rules to assign jobs to machines. The aim of their work is to provide a flexible and adaptive approach to job shop scheduling, capable of handling a dynamic manufacturing environment, coming one step closer to a fully automated, intelligent manufacturing system.\n\nGabel and Riedmiller [4] introduced a novel approach to solve JSSPs in 2012, by using distributed policy search RL. This multi-agent approach treats the JSSP as a series of sequential decision-making tasks, where each RL-agent operates autonomously and uses a probabilistic dispatching policy for decision making. These dispatching policies are represented by a small set of real-valued parameters that are continuously adapted and refined, to enhance the overall performance of the scheduling process. Although the computation time was reduced, the achieved solutions were not better than those of conventional solvers.\n\nIn 2008, Pezzella et al. [14] presented a genetic algorithm that solves the Flexible Job Shop Scheduling Problem (FJSP). Unlike in a JSSP, the operations of a FJSP can be assigned to one of several machines, instead of only one specific machine. Due to the additional decision layer of selecting machines for each operation, the complexity of a FJSP is even higher than in a JSSP. The presented algorithm outperformed existing models and traditional dispatching rules. Their approach utilizes DRL to tackle the problem more effectively by including innovative approaches for representation learning and policy learning. They demonstrated that genetic algorithms are effective in solving FJSPs.\n\nFoundational research in the field of DRL for continuous control tasks was presented by Lillicrap et al. [8] in 2015. This research expands the capabilities of deep learning beyond the discrete domain to tasks where actions can take any value within a continuous range. They employ an actor-critic architecture where the actor generates actions, and the critic evaluates them based on a learned value function. Also, they use a replay buffer to store and reuse past experiences to mimic successful techniques.\n\nShahrabi et al. [18] address the complex problem of dynamic job shop scheduling, where job arrivals are unpredictable and machine breakdowns occur randomly. The paper presents a Q-factor algorithm that optimizes scheduling decisions dynamically. The authors employ a variable neighbourhood search to explore the solution space and identify the most effective scheduling method. By using RL, the authors determine the optimal parameters for rescheduling processes in response to changes in the environment. This approach is designed to address real-world challenges in manufacturing. The results demonstrate the significance of their method for a dynamic job shop environment, as the optimal strategies are also updated dynamically.\n\nBy employing Google DeepMind\u2019s DQN agent algorithm present a successful application of RL to production scheduling. Their framework consists of multiple DQN agents that cooperate and learn to achieve the defined objectives, resulting in self-organized, decentralized manufacturing systems that are capable of adapting to a dynamic manufacturing environment. After a short training phase the system presents scheduling solutions that are on par with solutions based on expert knowledge. Even though the approach cannot beat heuristics, this research represents a significant step towards the application of AI to real-world industrial processes and intelligent production systems.\n\nFurther developing both Gabel and Riedmiller's [4] and Lillicrap et al.\u2019s approach [8], in 2020 Liu et al. [9] utilized an actor-critic DRL-architecture to approach the JSSP as a sequential decision-making problem. The model consists of an actor network and a critic network, including convolution layers and fully connected layers. The actor network learns how to act in a dynamic environment, while the critic network evaluates these actions. This approach is effective in managing unexpected events such as machine breakdowns, additional orders or material shortages that may interrupt the# STATE OF THE ART\n\nproduction process. It offers more robust and efficient scheduling solutions while still producing comparative solutions for smaller benchmark problems, outperforming traditional dispatching rules and executing almost as fast as simple dispatching rules. With increasing sizes of the instances, the performance eventually declined.\n\nHan and Yang [6] deal with increased complexities and uncertainties of JSSPs by proposing a duelling double DQN with priority replay. This DRL-framework combines the advantages of real-time response and flexibility of a deep convolutional neural network and RL to dynamically respond to complex and changing production environments. Scheduling is seen as a sequential decision-making problem, where the scheduling states at each time step are expressed as multi-channel images. The action space is a combination of easy to execute heuristic rules. The duelling double DQN is used to optimize learning through continuous interactions with the environment, resulting in a more accurate and stable learning performance in environments with high-dimensional action spaces and complex state evaluations. Experimental results show that this method achieves optimal solutions for small-scale problems and outperforms traditional heuristics for larger problems comparable to genetic algorithms but wasn\u2019t tested with a new dataset.\n\nZhang et al. [27] developed a Graph Neural Network (GNN) capable of solving problems regardless of their size. Like Han and Yang [6], they utilized a disjunctive graph to represent the state space of the JSSP. The GNN interprets the disjunctive graph representation, allowing the system to generalize solutions across different scales of problems. The conducted experiments reveal that this approach enables the agent to learn high-quality Priority Dispatching Rule (PDR)s from basic features, outperforming existing PDRs and performing well on much larger instances that were unseen in the training phase, although the generalized results fell short of being optimal.\n\nAlso, Park et al. [13] use the combination of RL and a GNN to solve the JSSP by formulating the scheduling process as a sequential decision-making problem. They employ a GNN for representation learning to encode the spatial structure of the JSSP into node features that are used to determine the optimal scheduling actions. The training of these components is conducted using PPO. Experiments demonstrate that the GNN approach can outperform traditional dispatching rules and other RL-based approaches on various benchmark JSSPs. Furthermore, the framework can learn transferable scheduling policies that apply to new JSSP scenarios (in terms of size and parameters) without additional training, highlighting its adaptability and efficiency.\n\nAn overview of how to model the JSSP as a sequential decision process and how a DRL architecture can be applied to the JSSP is given by Wang et al. [25]. Just like Park et al. [13] they employ a PPO algorithm to reduce the complexity. The performance of the proposed model is compared with heuristic rules and meta-heuristic algorithms. It is shown that this approach not only produces comparative results but is also able to realize adaptive scheduling and shows a certain generalization capability when faced with unseen situations.\n\nThe approach of Oren et al. [11] doesn\u2019t specifically tackle the JSSP itself, but more generally NP-hard COPs [5] that are found in job shop scheduling. The method is designed to work both online and offline, which is crucial for adapting to a dynamic production environment. A DQN is used offline to learn optimal policies through a simulation environment. The graph representation of the states enables the system to handle varying problem sizes and configurations. These policies are applied online to optimize the decisions in real time. The combined approach utilizes available time for deliberations, effectively reducing the gap to dedicated COP solvers.\n\nTassel et al. [22] propose a new DRL algorithm that is specifically tailored for job shop scheduling tasks, using recent advances of DRL to handle the growing complexity of JSSPs. The authors developed a compact state space representation, along with a simple dense reward function that is closely related to the sparse make-span minimization objective of COP methods. Benchmark instances provided by Taillard [21] show that their method finds solutions 11% better make-span than the best dispatching rule on Taillard\u2019s instances, 10% better than Han and Yang [6] and around 18% better than Zhang et al. [27].\n\nZhao and Zhang [29] investigate the application of DRL in dynamic job-shop production control. A dynamic job shop is a type of job shop where the scheduling environment is not static but changes over time. This dynamic nature can stem from several factors, including the arrival of new jobs, machine breakdowns, variable processing times, or changes in job priorities. As intelligent manufacturing becomes more and more important in industrial production systems, they address the lack of an evaluation mechanism that can accurately measure the control efficiencies of different scheduling plans. The authors create a multi-objective optimization model for a production control system, and subsequently introduce DRL to it. This is followed by a proposal of a dynamic job shop production control method that is also based on DRL and the explanation of the collaboration strategy for multiple subsystems. Experiments proved that their approach is effective.\n\nA comprehensive literature review on the applications of DRL in production systems is presented by Panzer and Bender [12] in 2022. They discuss the challenges of modern production environments, such as the high level of complexity and the demand for high throughput, all while maintaining adaptability and robustness in case of variations in the process or unforeseen events. They highlight the increasing use of DRL to optimize production systems and the significant# 3 Methodology\n\ncontributions and developments in the field that are improving the efficiency and flexibility of production processes. 89% of the benchmarked implementations increase the scheduling performance, reached lower total tardiness, higher profits, or other problem-specific objectives. In the field of production scheduling, 67% of the reviewed papers applied value-based algorithms.\n\nWith the growing interest in using RL methods for production scheduling, it becomes increasingly challenging or even impossible to reproduce existing studies with the same degree of accuracy. To make the research more widely applicable and to exploit its strengths for industrial applications, Rinciog and Meyer [16] propose to standardize the approaches. They propose a framework for applying RL in this context by modelling production scheduling as a MDP. The standardization is done in three steps: The standardization of the description of production setups used in RL studies is based on an established nomenclature. This is followed by the classification of RL design decisions from existing publications. Finally, recommendations for a validation scheme that focuses on reproducibility and sufficient benchmarking are proposed.\n\nA novel algorithm for improving the generalization capabilities and solution effectiveness of a DRL agent that solves JSSPs is proposed by Vivekanandan et al. [24]. The authors introduce a new method called Order Swapping Mechanism to achieve better generalized learning. By using a set of known benchmark instances [21] they compare their results with the work of other groups [6][27][22] that used the same benchmark instances and demonstrate that this approach outperforms previous methods. The results demonstrate that the agent does not outperform the approach of Tassel et al. [22], yet it does provide a size-dependent generalization. It outperforms the PDR based DRL approach of Zhang et al. [27] and performs similarly to other state-of-the-art DRL algorithms.\n\nSerrano-Ruiz et al. [17] present a method for scheduling in a quasi-realistic job shop environment. They create a digital twin of the job shop model as a MDP and use DRL for optimization. Their approach uses a deterministic framework for formulation and implementation and is validated by comparison with known heuristic priority rules. Experiments show that the model not only captures the benefits of heuristic rules but also leads to a more balanced performance across various indicators, outperforming traditional heuristic methods.\n\nThe analyzed papers demonstrate that the JSSP can be effectively solved by using various DRL approaches. However, most of the papers are based on simplified models with little relevance to the reality of production. In this reality, a large number of factors play a role, the quantification of which is sometimes difficult and can have a significant influence on the effectiveness or success of the modelling. The decisive factors and their influence on production planning are described in more detail in Section 3.# 3.1 Necessity for an Extended Approach\n\nDespite the existence of various RL approaches proposed by different researchers with the intention of solving generic JSSPs in industry-relevant problem sizes, it remains a challenge to translate the complexity of a real-world production environment into generic JSSPs. The following complexities are identified throughout this work:\n\n1. Generic JSSPs typically involve machines that process various jobs without any specific machine setup. It is therefore necessary to reduce the time required for machine setup to a minimum, as this can otherwise result in a significant loss of production time.\n2. Jobs are typically defined by a machine sequence and a fixed processing time. In the case of varying batch sizes, the processing times may be approximately linearly dependent on the batch size. Previous approaches did not consider varying batch sizes or processing times. With an enhanced generalization capability, contemporary DRL agents should be capable of accommodating varying processing times.\n3. The field of intralogistics is not included in the scope of a generic JSSP. The transportation times between different machines or production facilities can be considerable, often taking several minutes, and therefore play an important role in the scheduling process.\n4. In a real-world production environment, a variety of storage spaces can be found to buffer inconsistencies in production processes or to increase production flexibility. The dimensions of these buffering zones were not considered in previous approaches. An overfilled buffer zone may impede the entire production process and thus necessitates consideration in the scheduling process.\n5. Deadlines are frequently absent from models, even though they are an essential component of ensuring the timely delivery of products.# 3 METHODOLOGY\n\nThese simplifications make the JSSP easier to model mathematically, but real-world manufacturing systems often involve complex scheduling challenges that are not captured by these simplifications. In order to create a training environment that closely resembles the complex processes of a real production environment, it is necessary to develop an extended approach with a higher level of information detail.# 3.2 Research Hypothesis and Objective\n\nBased on a comprehensive analysis of an existing furniture factory, a concept for the implementation of RL in production planning is proposed. In order to formulate a general concept that is valid for a broad industry, the following constraints are given:\n\n- The production is set up as a Job Shop\n- The machines produce products in batches\n- The batches are manufactured on a recurring basis, but the production is frequently switched to enable the production of a wider range of products\n- The overall range of products doesn\u2019t change drastically after training, as this would require a re-training of the agent. Minor changes like \"color changes\" would not disrupt the production process.# 3.3 The Model\n\nGeneric JSSP-models are typically described by n jobs J = {J1, J2, ..., Jn}, where each job has m operations O (Ji = {Oi1, Oi2, ..., Oim}) to be processed on m machines M = {M1, M2, ..., Mm}. Each operation has its designated processing machine and time dij. This model is extended by the introduction of the following elements:# Job Volumes\n\nThe volumes of the jobs before and after each operation are quantified and mapped. With knowledge of the volumes of each job at any given point in the production process, the required storage spaces can be estimated with greater precision. This information is used to forecast and analyze the utilization of the buffers.# Buffers\n\nIn a production system, storage areas are distributed across the shop floor and are used to hold materials, unfinished, or finished goods between different stages of production or between production and shipping. The primary purpose of these areas is to absorb variability in the production process, including fluctuations in demand, supply disruptions, machine breakdowns, or other unforeseen circumstances that may impact the production schedule. These storage areas are defined as buffers and are used to store the jobs before being processed at a machine. Each buffer is characterised by its capacity, which may be expressed in various units, including storage volume, pallet storage capacity, or any other meaningful unit. Buffers serve to absorb variability in the production process, enabling a more flexible production, preventing bottlenecks and thereby smoothing out the production flow.# Quantity Factor \u03c9\n\nThe processing time of each operation is subject to significant variation due to fluctuations in batch sizes, which are quantified using the quantity factor \u03c9. The quantity factor is employed to determine the duration for which a machine is occupied in processing a component, particularly when batch sizes exhibit considerable variability. The total processing time of an operation is calculated as follows:\n\ntotal processing time of an operation = \u03c9 \u00b7 dij with dij as the processing time of one single element of an operation (1)\n\nThis methodology enables the calculation of the processing time of a job on a machine based on the actual batch size of a job.# Transportation Times t\n\nIn order to describe the intralogistic processes, transportation times are introduced. These figures represent the time required to transport a job from one machine to the next. In the majority of cases, the transportation times between two points are symmetrical, as the required transportation time is independent of the direction of travel. Table 1 illustrates an example of transportation times.# Machine Setup Times s\n\nA machine that is involved in the production of various jobs may require different setups in order to perform the specific operations. These setup times are considered, when the production requires a switch from one setup to another. Table 2 illustrates an example of symmetric setup times, where the individual setup times are# 3.3.1 Environment Outline\n\nThe job shop environment can be implemented using toolkits such as OpenAI Gym and libraries such as Stable-Baselines3, which are specifically designed for developing and comparing reinforcement learning algorithms. In the training environment, an agent learns to solve the problem and optimize its policy parameters by interacting with the environment through actions.# 3 METHODOLOGY\n\n| |Machine 3|to neutral Setup|to Setup 1|to Setup 2|to Setup 3|\n|---|---|---|---|---|---|\n|from neutral Setup|0|4|7|5| |\n|from Setup 1|2|0|0|7| |\n|from Setup 2|2|0|0|6| |\n|from Setup 3|2|6|8|0| |\n\n**Table 5: List of operations of the exemplary JSSP (mock-up data)**\n|Operation name|Machine|Machine Setup|Machining time|Quantity|Volume|Deadline|\n|---|---|---|---|---|---|---|\n|O 11|M1|s 1|0.04|400|30|-|\n|O 12|M2|s 1|0.08|400|20|-|\n|O 13|M3|s 1|0.06|400|15|120|\n|O 21|M1|s 2|0.12|100|10|-|\n|O 22|M3|s 2|0.14|100|8|-|\n|O 23|M2|s 2|0.13|100|5|110|\n|O 31|M1|s 3|0.06|400|20|-|\n|O 32|M2|s 3|0.04|400|15|-|\n|O 33|M3|s 3|0.06|400|10|100|\n\nThe following example JSSP is considered:\n\nThree jobs J = {J 1 , J 2 , J 3 } are to be produced on three machines M = {M 1 , M2 , M3 } with a machine sequence of\n\nJ 1 = {M 1, M 2, M 3}\n\nJ 2 = {M 1, M 3, M 2}\n\nJ 3 = {M 1, M 2, M 3}\n\nThe batch sizes of the jobs J are\n\nJ 1 = 400 pcs\n\nJ 2 = 100 pcs\n\nJ 3 = 400 pcs\n\nThe required machine setups for the operations are\n\nO 11 = s 1 O 12 = s 1 O 13 = s 1\n\nO 21 = s 2 O 22 = s 2 O 23 = s 2\n\nO 31 = s 3 O 32 = s 3 O 33 = s 3\n\nand the machining times for a single one element of each job are\n\nd 11 = 0.04 min d 12 = 0.08 min d 13 = 0.0625 min\n\nd 21 = 0.12 min d 22 = 0.14 min d 23 = 0.13 min\n\nd 31 = 0.0625 min d 32 = 0.04 min d 33 = 0.0625 min\n\nWith this information and the assumption that Job 3 is already located at Machine 1, the overall processing time of operation O 31 can be calculated as follows:\n\nT total O31 = \u03c9 \u00b7 d + t + s\n\n= 400 \u00b7 0.0625 min + 0 min + 4 min (3)\n\n= 29 min\n\nThe volumes of the jobs change with every operation\n\nV 11 = 30 m3 V 12 = 20 m3 V 13 = 15 m3\n\nV 21 = 10 m3 V 22 = 8 m3 V 23 = 5 m3\n\nV 31 = 20 m3 V 32 = 15 m3 V 33 = 10 m3# 3.3.2 Action Space\n\nThe environment is controlled by a single discrete action space, through which the agent selects the appropriate jobs for processing on a given machine at each time step. The agent\u2019s choices are limited to the subset of available jobs and machines, ensuring that the agent\u2019s decisions are both relevant and feasible in the context of the current operational parameters and machine availability. The actions taken by the agent modify the environment and change its state.# 3.3.3 Observations\n\nThe decision to assign a machine to a job is based on a set of observations that is provided to the agent. Each observation represents the current state of the environment. It is updated at each time step and holds the following information:# (1) machine info\n\nA matrix of dimensions 3 \u2191 m, which contains information about the currently processed jobs, the progress of the operations, and the current machine setup. Given that m represents the number of machines in the environment, each column of the matrix represents a machine. The first line contains information about the currently processed jobs, the second line contains information about the remaining operation time and the third line contains information about the current machine setup.\n\n| |Machine 1|Machine 2|Machine 3|\n|---|---|---|---|\n|(1)t0 =|0|0|0|\n| |0|0|0|# (2) job info\n\nA matrix of dimensions 2 \u2191 n, containing information about the current job volumes and the remaining time until the deadline is reached. With n representing the number of jobs in the environment, each column represents a job. The first line contains information about the current job volume, while the second line contains information about the remaining time until the deadline is reached.\n\n| |Job 1|Job 2|Job 3|\n|---|---|---|---|\n|(2)t0 =|30|10|20|\n| |120|110|100|# (3) buffer info\n\nA b-dimensional vector represents the capacity status of the buffers in the environment with b representing the number of buffers in the environment. This vector can reflect the utilization of the buffers in units or in percent, depending on what is most appropriate for the model.\n\n| |Buffer 1|Buffer 2|Buffer 3|\n|---|---|---|---|\n|(3)t0 =|60|0|0|\n\nThese three observations provide an overview of the status of the entire environment. Additional information, such as machine availability, can be extracted from these observations. If a machine has no job assigned to it, it is considered to be idling and available.# 3.3.4 Transition Probability Function\n\nTransition probabilities represent the likelihood of transitioning from one state to another after taking a given action. The transition probabilities are estimated based on the experiences of past actions and resulting observations in the training environment. They are then continuously adapted and honed in the training phase. This function is capable of capturing the dynamics of the system under different observations.# 3.3.5 Reward\n\nThe reward function is employed to quantify the system performance and guide the decision-making process. It must be tailored closely to the goals of the specific industrial application, as it is highly sensitive. The reward corresponds to the scheduling goal and defines the specialization of the agent. In order to maintain a high level of learning efficiency, it is necessary for the agent to be rewarded in a densely manner. This implies that the agent is rewarded for actions that are specific to the JSSP, such as achieving a shorter overall processing time. Additionally, the agent may be rewarded for actions that are specific to its industrial application. One method of enhancing the learning process is to provide the agent with a negative reward for unwanted actions, such as overfilling a buffer or failing to meet a job\u2019s deadline. Identifying the optimal reward function is an iterative process that must be conducted on an individual basis.\n\nDiscount Factor \u03b5: As with the reward function, the discount factor must be set according to the specific application in question. The discount factor between 0 and 1 determines the relative importance of future rewards in the decision-making process. A discount factor of 0 indicates that the agent is short-sighted and only considers immediate rewards, while a discount factor close to 1 implies that the agent values future rewards to a similar extent as immediate rewards. In order to identify the optimal policy, the application of PPO balances the trade-off between policy improvement and stability.# 4 Integration in the Furniture Industry\n\nIn a general JSSP, each job is associated with a unique set of tasks that must be completed in a specific order. This concept can also be applied to the production of furniture, where articles are composed of several components that must be processed in a specific sequence on various machines. Consequently, each article corresponds to a group of jobs, with each component considered being a separate job. In order to determine this set of jobs, it is necessary to break down each article into its various components and determine their respective machining sequences. The identification of production information, such as the individual machining sequence, volume alterations, and processing duration for each component at every production step, is typically obtained from the so-called Bill of Materials (BOM), which is a commonly utilized document in the industry. The level of detail provided by the BOM determines the comprehensiveness of the production information. Obtaining such information is indispensable for the development of optimized production planning. Given the intricate nature of furniture production systems, the implementation of DRL-supported production planning necessitates a comprehensive analysis of the factory in question. Consequently, the presented solutions thereby may not be readily transferable to all furniture production facilities, as each facility will require a solution that is precisely tailored to the factory and its individual scheduling goals. This chapter presents an example of how a RL-agent can be employed for production scheduling in the furniture industry, where components are manufactured in batches in a job shop environment. For that purpose, the elements of the extended model that is presented in Chapter 3 may be represented as follows:# Jobs\n\nA piece of furniture is typically constructed from multiple components, which are then assembled or packaged at a designated station in order to create the final product. In the context of the job shop analogy, each component represents a distinct job with its own machine sequence and processing time for each operation. Therefore, the furniture articles are broken down into individual components. For each component, the processing sequence on the respective machines, the processing time, and the machine setups are mapped. If no further processing occurs within the factory, purchased parts may be excluded from this analysis. Each job is then defined by its machine sequence, machine setup, total processing time (cf. 3.3), deadline, and its volume, which undergoes change throughout the production process.# Job volumes\n\nIt is common in furniture production, particularly for solid wood furniture, to undergo significant volume changes throughout the production process. The volume of the raw material at the beginning of the process may be up to five times greater than that of the end product. In order to calculate the space requirements of each job at any point during production, it is necessary to record the volume change in each process step.# Buffers\n\nEach buffer on the shop floor is assigned to a machine and located in front of it in the process flow. Buffers are treated similarly to machines, but they can store several jobs simultaneously and have no processing time. The capacity of these buffers is specified in terms of the number of storage spaces for pallets or by volume.# Machines\n\nAnother aspect of the environment is the machinery used for production. Each machine is mapped in detail, including its features, processing times, possible setups, and setup times required for each operation.# Quantity Factor \u03c9\n\nThe quantity factor \u03c9 is employed to modify batch sizes, which have a significant impact on processing time and, consequently, the make span of a job. Training the agent with varying batch sizes enhances its generalization capability, enabling it to handle varying job sizes. It has been observed that the production of furniture does not always occur in consistent batch sizes. The quantity factor helps determining the duration of a machine\u2019s utilization in the processing of a component.# Transportation Times\n\nThe transportation times between machines and buffers in the production are mapped. This includes a full analysis and documentation of the transportation times between the machines on the shop floor. With this information, the overall completion time of an order can be determined.# Deadlines\n\nEach job is described with a deadline that specifies the latest point in time by which its processing must be completed.# Environment\n\nThe production environment is mirrored as closely as possible by the training environment, which comprises jobs, machines, and buffers. The agent is trained to set up machines, assign jobs to machines, and prevent buffers from overflowing, all while maintaining the correct machine order for each job and meeting the production deadlines.# Action Space\n\nThe agent is controlling the environment in a single discrete action space. At each time step suitable machines and buffers are determined for the available jobs.# Observations\n\nThe observations provided to the agent are (1) machine info, (2) job info, and (3) buffer info (cf. 3.3.3).# Reward\n\nThe reward function guides the decision-making process and thus must be tailored to the specific optimization goal. It needs to include rewards for correctly setting up a machine for an according job, assigning machines to jobs in the correct machine sequence of a job and storing jobs in buffers before assigning them to a machine. Additionally, the reward function contains rewards for the specific optimization goals, such as low buffer levels, reducing make span times, or achieving other, industry-specific objectives.# 4.1 Integration Strategy\n\nThe integration of an RL agent must be compatible with existing production and planning systems and be able to be seamlessly integrated without disrupting ongoing production. Once the training environment has been established and the agent has been trained in accordance with the desired optimization goals, it can then be utilized to support production planning. Two principal concepts may be distinguished with regard to the integration: episodic and continuous production planning. The decision to utilize either episodic or continuous planning depends on various factors, including the nature of the production process, the degree of variability in orders, and the scheduling system\u2019s required flexibility.# 4.1.1 Episodic Planning\n\nIn companies with low levels of automation and networking, episodic planning represents an appropriate approach. It may be employed when orders are processed in batches or when the production line is configured to manufacture specific types of furniture for a defined period before transitioning to a different type. The integration of an episodic planning system is less complex than in a continuous planning system (cf. 4.1.2), as it does not require interfaces with existing production systems. The planning process is divided into distinct episodes, each with a clear beginning and end. These episodes may be defined as the production of a week or the production of an individual order from a specific client. Each episode consists of a limited number of steps and actions, making it easier to predict and evaluate outcomes. The orders are entered into a dashboard, which includes information on quantities and deadlines. Alternatively, a specific time period may be selected for scheduling. Figure 3 illustrates an exemplary dashboard, which displays orders, quantities, and deadlines. The system may offer the users a range of optimization goals to choose from, or alternatively, suggests different schedules based on various models. This enables production planners to respond effectively to the dynamic changes of the production environment.# Figure 3: Exemplary layout of a production planning dashboard\n\nOnce the orders have been entered into the dashboard, the agent starts with the scheduling process. With the JSSP example provided in chapter 3.3.1, the initial observations at t0 are as follows:\n\n[0 0 0](1)t0 = 0 0 00 0 0(2)t0 = [30 10 20]120 110 100\n\n12# Figure 5: Scheduling with a trained agent at t29\n\nThese time step transitions are repeated until no operations are left for processing and the production is completed. The finished schedule is shown in Figure 6. After completion, the system resets, allowing for evaluation and optimization of each episode independently.\n\n13# Limitations of Episodic Planning\n\nIn a real-world production environment, a number of factors may influence the production planning process, including unforeseen events such as machine breakdowns, delivery delays, or changes in customer requirements. As the episodic planning process is conducted in advance, it is not possible to incorporate unforeseen events into the scheduling process. The introduction of a new product to the portfolio necessitates the re-training of the agent. Even an agent with high generalization capabilities is unable to predict the correct order of operations for the components of a new furniture article. A trained agent is constrained to the specific set of jobs for which it was trained.# 4.1.2 Continuous Planning\n\nIn a continuous planning approach, the agent is fully integrated into the production system and plays an active role in the scheduling process. This approach is suitable for highly networked and automated plants where production is subject to significant variations in orders, requiring production schedules to be adjusted on the fly to accommodate new orders, changes in design, or material availability. This integration comes at the cost of a significantly higher level of complexity: The agent is situated between the Enterprise Resource Planning (ERP)-system and the Manufacturing Execution System (MES) (see Figure 7). It processes real-time production data from both systems and adjusts the production accordingly.# Figure 7: Automation Pyramid\n\nThe ERP system provides information regarding orders, material stocks, and delivery changes, while MES provides machine data such as processing times, operation progression, machine breakdowns, or transport times of transport systems. Significant alterations to this data will trigger an event, for which the agent will recalculate the scheduling, analogous to a time step in episodic planning. The scheduling results are written back into the systems, where production planners can supervise and adjust the proposed scheduling, if necessary. Unlike the episodic approach, which only provides various scheduling suggestions for the planned episodes, the production planning is actively influenced by the agent.# Limitations of Continuous Planning\n\nOptimized production planning frequently requires the simultaneous consideration of multiple objectives, including a minimized lead time, maximized machine utilization and minimized inventory.# 5 Conclusion and Outlook\n\nThis paper presents a concept for DRL-supported production planning that can be adapted for optimized job shop scheduling. In Chapter 3, the elements of the DRL model are presented, including factors that aim to bridge the gap between real-world production environments and production models. The consideration of fluctuating job volumes# 5 CONCLUSION AND OUTLOOK\n\nThroughout the production process ensures more precise estimations of the required space conditions within the buffer zones across the shop floor. This prevents overfilled buffer areas, which clog the production flow as well as the accumulation of unnecessary materials and capital lockup. The introduction of the quantity factor \u03c9 determines the processing times of each operation, depending on its batch size, while transport times are added to represent the production duration more accurately. Jobs are specified with a deadline to simulate a delivery date, while machines are described with machine setups, that represent their current state in a more realistic manner. Furthermore, changes in the setup are also taken into account for the processing duration. The introduction of these framework conditions enables the generic JSSP model to be extended in a manner that more accurately reflects the high complexity of a real-world production environment. This extended approach allows the construction of a training environment, in which a RL agent can learn to optimally set up machines, assign jobs to machines, and move jobs between machines and buffers in accordance with specific optimization goals.\n\nThis approach is applied to the industrial furniture production in Chapter 4, tackling industry-specific challenges including the translation from a real-world furniture article to a job in the model. Two implementation concepts are presented for the integration of an RL agent into production planning systems: episodic and continuous planning systems. The production planning process for an episodic planning approach is illustrated using an example JSSP, highlighting the functionality of a trained agent at different time steps in the planning process. While episodic planning can be integrated as a low-tech, standalone solution, a continuous planning agent is fully integrated into the existing production systems. This enables real-time scheduling and allows for the prompt reaction to machine breakdowns as soon as the error message appears in the MES. In the event of predicted material shortages, as indicated by the data from the ERP system, the production of specific articles may be postponed in accordance with the anticipated shortage. However, the interface communication between the agent and the production system, as well as the implementation of the agent into the system, require a complex, customized solution.\n\nA challenge remains in precisely defining a reward function that considers the proposed elements and the industry specific optimization goals. It remains unclear how to best prevent buffer overfill, particularly given that buffer levels are only checked at the time steps in episodic planning, rather than in between time steps. It remains to be investigated whether an overfilled buffer causes a deadlock of the system when a job cannot be moved to the next buffer because it is full. The consideration of the batch size for the processing duration with the introduced quantity factor \u03c9 can be extended to the calculation of the required transportation times. Larger batch sizes may include more pallets to be moved from one place to another, which would result in longer transportation times. A further challenge arises when the production system is not organised as a generic job shop but as a dynamic job shop: When the same furniture components can be produced by different machines on several routes and thus alternative routes through production are possible. This significantly increases the level of complexity, as the number of possible combinations grows at an even faster rate than in a generic job shop. The following step of this process is the realisation of the concept described above in order to examine, how an agent would deal with the increased level of complexity and increasing problem sizes. It is also important to note that the scheduling agents described above have been designed to support human production planners, rather than creating artificial copies of them.",
        "context_id": 26,
        "question": "What year did Liu et al. introduce their actor-critic DRL-architecture to approach the JSSP?",
        "answer": [
            "2020"
        ],
        "context_length": 42510
    },
    {
        "context": "# 1 Introduction\n\nAccurate nanoparticle characterization in terms of size, shape, and composition in complex biological environments is crucial to understanding the relation between nanoparticle structure and function as well as to achieving the full potential of nanoparticle-assisted applications within several fields, including drug delivery[1, 2] and medical diagnostics[3, 4]. Traditionally, such characterization has been performed on the individual particle level using high spatial resolution methods, such as cryogenic.# Characterization of Nanoparticles\n\nTransmission electron microscopy (cryo-TEM) [5], whereas light scattering techniques have been employed to perform quick nanoparticle characterization on an ensemble level. Two examples of such light scattering techniques routinely used to characterize nanoparticle suspensions are dynamic light scattering (DLS) [6], connecting particle diffusivity to size, and multi-angle light scattering (MALS) to characterize both size and structure [7, 8].\n\nHowever, none of these approaches are satisfactory to characterize heterogeneous nanoparticle samples: electron microscopy is an ex-situ approach suffering from low throughput, while ensemble approaches measure an averaged signal over many individual particles, masking their underlying heterogeneity [9]. This is particularly problematic for biological nanoparticles, which often display a pronounced heterogeneity in terms of size and composition, which furthermore may be a deciding factor for their biological function [10].\n\nIn this context, single particle characterization using label-free light scattering microscopy has emerged as an alternative route, achieving widespread use in the last two decades. In fact, the first use of optical microscopy for the characterization of nanoparticles was done over 100 years ago, which relied on orthogonal illumination and detection pathways to achieve darkfield microscopy [11]. Although nanoparticles are smaller than the optical resolution limit, it is still possible to detect individual nanoparticles as long as the signal-to-noise is high enough, which in turn enables detailed measurements on the single particle level. Modern implementations image the particles onto sensitive cameras and quantify a combination of the scattering signal and particle motion to achieve high-throughput characterization of particle samples [12\u201316]. From the nanoparticle motion, the size is estimated via the Stokes-Einstein relation, which relates diffusivity to size for spherical particles in a viscous medium [17]. The use of nanoparticle motion to estimate size has achieved widespread application under the name Nanoparticle Tracking Analysis (NTA) and exemplifies how scattering microscopy extends traditional ensemble-based characterization approaches to characterize nanoparticles with single nanoparticle resolution [14, 15, 18].\n\nGoing beyond diffusivity-based particle sizing, over the past decade numerous optical microscopy methods have been developed, aiming at multiparametric nanoparticle characterization with single-particle resolution [13\u201316, 19\u201323]. All these techniques rely on the following fundamental observation: the amount of light scattered and absorbed by an object is to a first approximation proportional to its volume and refractive index contrast relative to the surrounding medium. The refractive index is a complex-valued material-specific property dictating the efficiency of a material to scatter and absorb light [24]. The real part of the refractive index governs light scattering, while the imaginary part of the refractive index governs light absorption. Thus, optical nanoparticle characterization can distinguish between different types of particles based on their ability to scatter and absorb light [12, 25]. In the specific case of biological nanoparticles, the total amount of light scattered is to a first approximation proportional to particle mass [26\u201328]. In addition to the scattering amplitude, the relative amount of scattering to different scattering angles can also be used to characterize nanoparticle samples [23, 29].\n\nTo give some specific examples, some types of nanoparticles that can be characterized using scattering microscopy are highlighted in Figure 1, showcasing typical values of the respective sizes and refractive indices. For objects much smaller than the wavelength of the illuminating light, such as individual biomolecules, the relative amount of light scattering at different angles has a weak particle size dependence [7, 8]. For such particles, it is sufficient to determine the scattering amplitude in a limited range of angles to characterize particle mass. This has been utilized to determine the mass of individual proteins [12, 27, 30]. For larger biomolecular complexes, such as viruses, liposomes, and protein aggregates, the scattered light amplitude integrated over all scattering angles is still related to their mass. However, interference effects between the scattered light from individual molecular elements within the complexes generate a directionality of the scattered light, such that the light amplitude measured in a scattering microscope will depend on the measurement geometry [24]. For instance, a liposome, consisting of a lipid bilayer shell with a water-filled core, will scatter light differently from a drug-containing nanoparticle of the same size and mass, simply due to a different spatial distribution of biomolecules. This fact has been utilized to distinguish empty liposomes from exosomes filled with biological material through simultaneous characterization of scattering amplitude and size [15].\n\nMetallic nanoparticles present another example of widely used nanoparticles that can be characterized using scattering microscopy [12]. Such nanoparticles interact much more strongly with the illuminating light compared to biological nanoparticles of the same particle size due to the refractive index difference between gold and water is larger than that of biomolecules and water (Figure 1).# Refractive index of water (1.33)\n\nTypical diameter\n5 nm\n100 nm\n200 nm\nFig. 1: Typical nanoparticles studied in scattering microscopy, organized by size and refractive index. The size range of nanoparticles typically studied by scattering microscopy techniques ranges from individual biomolecules to large biomolecular complexes.\n\nMoreover, in contrast to biological nanoparticles, they typically display considerable light absorption in addition to light scattering, as a result of plasmonic resonance. This plasmonic resonance, in turn, depends on particle size and shape [31]. Thus, to fully characterize such nanoparticles it is important to use an experimental design capable of quantifying both the scattered as well as the absorbed light.\n\nFrom the above considerations, it becomes clear that while individual biomolecules can be characterized based on the measured scattered light alone, the characterization of more complex structures requires an experimental design and data analysis pipeline that is optimized for the specifics of the sample. Therefore, although method development and refinement are likely to continue and further expand the information that can be extracted from microscopy images, it will always be imperative to choose an experimental design that maximizes the amount of useful information about the investigated sample in the recorded scattering pattern, and an image analysis approach that optimally utilizes that information.\n\nThe purpose of this tutorial is to guide the reader in this process by providing:\n\n1. a theoretical understanding of how the image formed in a light scattering microscope,\n2. an understanding of how to quantify physical properties of the measured nanoparticles from their corresponding microscopy image,\n3. clear guidelines on how to choose the right measurement modality for specific purposes, and\n4. a toolbox for performing optical nanoparticle characterization using scattering microscopy, in the form of Python notebooks containing ready-to-use code for particle detection and characterization.\n\nThe importance of considering these aspects is highlighted by a few examples from the literature.# 2 Image formation in a scattering microscope\n\nA light scattering microscope is a device where the scattered light from micro- and nanosized objects is collected and recorded by, for example, a camera or photomultiplier tube (Figure 2A). Most commonly, a microscope objective is used to collect the scattered light from the objects (although lens-free solutions exist as well [32]). On the opposite side of the objective, another lens called the tube lens, collects the light from the objective to form an image onto the camera. The objective, tube lens, and camera together define the optical axis.# The properties of the objective and the sample illumination largely determine the scattering information that propagates to the camera in a scattering microscope.\n\nFirst, consider an objective illuminated by a plane wave, which corresponds to a collimated sample illumination with a constant intensity profile propagating along the optical axis. At the back focal plane of the objective, this wave is focused to a spot centered on the optical axis (Figure 2B). A plane wave that is tilted by an angle \u03c9 relative to the optical axis will produce a focused spot at the back focal plane that is offset by a distance f sin \u03c9 relative to the optical axis, where f is the back focal length of the objective.# Box 1: Focusing of a plane wave by an objective\n\nFrom a mathematical standpoint, the microscope objective can be represented by a pupil function P(\u03c9), defining the transmittance of plane waves with incident angle \u03c9 on the objective (Figure 2C). For an ideal objective, the pupil function is\n\n|Condition|P(\u03c9)|\n|---|---|\n|\u03c9 < \u03c9max|1|\n|otherwise|0|\n\nwhere \u03c9max is the largest incident angle accepted by the objective. This is related to the numerical aperture (NA) of the objective as NA = nNA sin \u03c9max, where nNA is the refractive index of the media between the objective front lens and the sample.\n\nNow, consider an objective illuminated by a plane wave propagating at an angle \u03c9 relative to the optical axis and at an angle \u03b5 relative to the x-axis. The optical field a at the back focal plane is then given by\n\nEbfp(x, y) = A P\u02c6(k(x \u2192 f sin \u03c9 cos \u03b5), k(y \u2192 f sin \u03c9 sin \u03b5)),\n\nwhere the function P\u02c6 is the transfer function of the objective, z = f is the back-focal length of the objective lens, and k = 2\u03d1/\u03d6 where \u03d6 is the wavelength of light (Figure 2B). For an objective without optical aberrations, the transfer function under the paraxial approximation is given by\n\nP\u02c6(p, q) = 2 J1(NA\u221a(p2 + q2)),\n\nwhere J1 is the first-order Bessel function. Thus, all incoming plane waves are transformed into tightly focused spots in the back-focal plane, and the position of each spot in the back-focal plane depends on its incident angle.\n\na In optical microscopy, the optical (or light) field is often used instead of the electric field. The difference between the two fields is a normalization constant that affects how the fields relate to the measured light intensity, where the measured light intensity at a camera is equal to the squared modulus of the optical field.\n\nb The transfer function is the Fourier transform of the pupil function.\n\nAs it turns out, a general incident optical field can be described as a linear combination of plane waves with different incident angles \u03c9 relative to the optical axis (Box 2), and \u03b5 relative to an at this point arbitrarily defined x-axis perpendicular to the optical axis. Connecting this to the specific topic of this tutorial, consider a nanoparticle located at the front focal plane of the objective (Figure 2C), illuminated by a plane wave propagating along the optical axis. This produces a scattered optical field E, which can be decomposed as a sum of plane waves E\u02c6(\u03c9, \u03b5), each propagating with specific angles \u03c9 and \u03b5. The field at the back focal plane can then be obtained by summing the contributions from all such plane-wave components, each of which produces a focused spot as in Figure 2B. Although the general expression is fairly complicated, an approximate form of the field at the back-focal plane can be obtained by utilizing that each plane wave component contributes to the summation only close to the center of the corresponding focused spot. Under this approximation, the field at the back focal plane is given by\n\nEbfp(\u03f1 = f sin \u03c9) \u2191 E\u02c6(\u03c9, \u03b5)  \u03c9 < \u03c9max.\n\nThus, the field at the back focal plane mirrors the angular distribution of the scattered light, for scattering angles \u03c9 < \u03c9max (Figure 2C).# Fig. 2: Propagation of the scattered light through a scattering microscope.\n\n|A)|B)|z-axis|L-dxis|\n|---|---|---|---|\n|Illumination|Lamp lens|Sample plane|Sample plane|\n| |Condenser|Front focal plane| |\n| |Sample|Objective| |\n| |Back focal plane|Back focal plane| |\n| |Tube lens|Camera| |\n| |Image plane|Image plane| |\n\nA) A general scattering microscopy setup. The sample is illuminated by a plane wave. Both the illuminating light and the scattered light from particles in the sample propagate through the optical system and are recorded by a camera.\n\nB) The image obtained by a scattering microscope is largely determined by the objective lens. Plane waves entering into the objective are transformed into a focused spot at the back focal plane. When the objective is illuminated by a plane wave impinging on the objective at an angle \u03c9 with respect to the optical axis and at an angle \u03b5 relative to the horizontal axis, this tightly focused spot is offset a distance \u03f1 = f sin \u03c9 away from the optical axis.\n\nC) The scattered field from a nanoparticle can be considered a linear combination of many plane waves, incident on the objective with different angles \u03c9 and \u03b5. Each such plane wave produces a similar spot as described in B). Summing up, all these plane wave contributions produce a field at the back focal plane as shown in the inset, which, in the case of particles much smaller than the wavelength of light to a first approximation, reassembles that of a plane wave. Notice the sharp cutoff, highlighted as a circle in the inset. This is due to the limited angular range admitted by the objective and is set by the objective NA.\n\n5# Box 2: Decomposition of the optical field in plane wave components\n\nSimilar to how a time-dependent signal can be decomposed to a sum of contributions with different amplitudes and frequencies, an optical field can be decomposed to a sum of plane waves with different wave vectors (or spatial frequencies). Each such plane wave component can mathematically be written as:# k (x) = eik\u00b7x (5)\n\nConsider a monochromatic optical field E (meaning that the field consists of light with a single frequency), evaluated on a plane (the xy-plane) perpendicular to the optical axis. Its plane wave decomposition reads:\n\nE(x) = \u222b eik\u00b7x \u02c6(k)dk, E (6)\n\nwhere k is the projection of the wave vector of the plane wave component # k on the xy-plane, and \u02c6(k) is a coefficient describing both amplitude and phase of the plane wave component #.\n\nThese components are found through the inverse transform of Eq. (6):\n\nE\u02c6(k) = (2\u03d1) \u21922\u222b e\u2212ik\u00b7x E(x)dx. (7)\n\nSince k is a projection of the wave vector onto the xy-plane orthogonal to the optical axis, it follows that |k| = k sin \u03c9, where k = (2\u03d1nm/\u03d6) is the wave number of the light and \u03c9 is the angle between the propagation direction of the plane wave component and the optical axis. The mathematically versed reader may recognize that these equations resemble Fourier transforms. In fact, the plane wave decomposition is identical to the Fourier transform, except that the integration over k is restricted for the reason outlined above. Specifically, denoting the Fourier transform operator by F, the above equations can be written as:\n\nE(x) = F\u22121[E(k)C(k)] (8)\n\nE\u02c6(k) = F[E(k)], (9)\n\nwhere C(k) = 1 for |k| < k, and 0 otherwise. In the case of a circularly symmetric scattered field, as is often the case for the scattered field from nanoparticles, the decompositions above are more conveniently expressed in polar coordinates. In this case, one obtains:\n\nE(\u03f1) = \u222b sin \u03c9 cos \u03c9 E\u02c6(\u03c9)J0(k\u03f1 sin \u03c9)d\u03c9, (10)\n\nwhere the integration now runs over the propagation angle \u03c9 with respect to the optical axis instead of the wave vector projection k. The coefficients E\u02c6(\u03c9) are similarly found by:\n\nE\u02c6(\u03c9) = k2\u222b \u03f1E(\u03f1)J0(k\u03f1 sin \u03c9)d\u03f1. (11)\n\nTo form an image of the sample on the camera, a lens (tube lens) is placed between the objective and camera, such that an incoming plane wave to the objective enters the camera as a plane wave, and light scattered from a point source at the focal plane of the objective forms a focused image on the camera. As a result of this, the plane wave components of the field at the camera plane reproduce exactly the plane wave components impinging on the objective, for all \u03c9 < \u03c9max. Mathematically, this is expressed as:\n\nE\u02c6cam(\u03c9, \u03b5) = E\u02c6(\u03c9, \u03b5)P(\u03c9). (12)\n\nNow, we have a mathematical framework for relating the field at the focal plane to the fields at the back-focal plane and the camera. This framework involves decomposing the field into its plane wave components, applying the optical transfer function, and summing up the contributions from the individual components.\n\nIn the context of scattering microscopes, the optical field at the focal plane is typically a superposition of the illuminating field Eill, and the field scattered by objects in the sample Esca. The field atthe camera is then given by\n\nE\u02c6camera(\u03c9, \u03b5) = E\u02c6ill(\u03c9)P(\u03c9) + E\u02c6sca(\u03c9, \u03b5)P(\u03c9) (13)\n\nHowever, the microscope camera does not record the angular components of the incident field; instead, it records the spatial distribution of the incident field intensity. This is given by the modulus squared of the spatial distribution of the optical field. The intensity recorded by the camera can be written as\n\nIcam(x, y) = |Eill + Esca(x, y)|2 = |Eill|2P(\u03c9ill) + |Esca(x, y)|2 + 2|Eill|\u2193(Esca(x, y) \u2191 ei\u03c9)P(\u03c9ill), (14)\n\nwhere the spatial distributions of the illuminated and scattered fields are related to their plane wave components through Eq (10) (Box 2), and \u03c2 represents the phase difference of the illuminating field relative to the scattered field. For completeness, in Box 3 the contributions from the individual plane wave components to the measured intensity is described.# Box 3: Decomposition of the recorded intensity into plane wave components\n\nThe formalism introduced in Box 1 allows us now to investigate how the plane wave components of the scattered and illuminating light contribute to the recorded intensity. At the camera plane, the scattered field is given by the Fourier transform of the field at the back focal plane of the objective. Since the objective only admits plane wave components with angles relative to the optical axis \u03c9 \u2194 \u03c9max, it follows that it only admits plane waves with projected wave vectors |k| \u2194 k sin \u03c9max. The scattered field at the camera plane is therefore given by\n\nEsca(x) = \u222b eik\u00b7x \u02c6sca(k) PE(k) dk, (15)\n\nwhere PE is the pupil function expressed in terms of the projected wave vectors instead of the propagation angle, given by PE(k) = 1 for |k| \u2194 k sin \u03c9max and 0 otherwise. In the specific case of a circularly symmetric scattered field, one has\n\nEsca(\u03f1) = \u222b0\u03b5max cos \u03c9 sin \u03c9 E\u02c6sca(\u03c9)J0(k\u03f1 sin \u03c9) d\u03c9. (16)\n\nIn this case, one finds for the second and last terms in Eq. (14)\n\n|Esca(x, y)|2 = |\u222b0\u03b5max cos \u03c9 sin \u03c9 E\u02c6sca(\u03c9)J0(k\u03f1 sin \u03c9) d\u03c9|2 (17)\n\n|Eill|\u2193(Esca(x, y) \u2191 ei\u03c9)P(\u03c9ill) = P(\u03c9ill)|Eill|P(\u03c9ill) \u222b0\u03b5max cos \u03c9 sin \u03c9 J0(k\u03f1 sin \u03c9)\u2193(E\u02c6sca(\u03c9) \u2191 ei\u03c9) d\u03c9. (18)\n\nIn this expression, it is assumed that the field reaching the camera is not magnified by the objective. The effect of magnification is essentially to reduce the propagation angle \u03c9 of an optical field with respect to the optical axis. The effect of magnification can be taken into account by replacing the argument of J0 with M k\u03f1 sin \u03c9, where M is the magnification. Similarly, the function PE = 1 for |k| \u2194 k sin \u03c9max/M and 0 otherwise in the case of a non-unitary magnification.\n\nThe first two terms of Eq. (14) describe the intensities of the illuminating field and the scattered field independently. The third term describes the interference of the scattered field with the illuminating field and contains information about the relative phases of the two fields. Notice that the first and last of these terms are relevant only for illumination angles for which P(\u03c9ill) > 0.\n\nThe amount of light scattered from a nanoparticle is generally much smaller than the amplitude of the light that is incident on it. Thus, in a transmission scattering measurement, as depicted in Figure 2A, the first term of Eq. (14) will dominate over the last two terms, resulting in a small signal-to-background ratio. Three strategies are traditionally used to overcome this signal-to-background ratio.\n\n7# Limitation for Quantitative Characterization of Subwavelength-Sized Particles\n\ni) Darkfield microscopy enhances the contrast by only allowing the scattered light to reach the camera, ii) interferometric scattering techniques instead focus on quantifying the interferometric term, while iii) quantitative field microscopy uses the interference of the field at the camera plane with another optical field to quantify the complex-valued field itself instead of only the real part of the interference between the optical fields of the particle and the background. All mentioned strategies have in common that they are designed to manipulate one or more of the terms in Eq. (14) to enhance their performance, and the details of how this is achieved using the three techniques mentioned above are discussed in the following subsections.# 2.1 Darkfield Microscopy\n\nDarkfield microscopy is a widely used technique to enhance the contrast of small particles, being a relatively simple but powerful configuration. It has been used for over 100 years [11] and is still one of the standard techniques to characterize the hydrodynamical radius of nanoparticles using NTA [13,35]. Darkfield techniques aim to only allow the scattered light to reach the camera, thereby suppressing all terms except the second term in Eq. (14). In this way, the scattered light from the particles appears as bright dots against a dark background (Figure 3A) (hence the name \u201cdarkfield\u201d). Since there is no background signal, the darkfield signal is given by\n\nIDF = |Esca|2.\n\nSuch background suppression can be achieved by a vast range of microscope configurations, where the choice of configuration affects the relation between the properties of the particles and the measured signal. The most common ways of achieving such background rejection rely on using an illumination angle outside the range captured by the objective (Figure 3B), spatial blocking of the excitation beam (Figure 3C), or an evanescent illumination (Figure 3D) [12].# Fig. 3: Darkfield Microscopy Setups\n\n- A) In darkfield microscopy, the particles are visible on the camera as bright spots against a dark background.\n- B) Darkfield through oblique illumination: the sample is illuminated by light at an angle larger than the maximum angle admitted by the objective so that the illuminating light is prevented from propagating through the optical system.\n- C) Darkfield using spatial blocking: the illumination light is blocked at the back focal plane using a physical filter.\n- D) Evanescent field imaging: one utilizes total internal reflection at the interface between glass and sample to produce an evanescent field. This evanescent field amplitude decays exponentially away from the surface and thus will not reach the camera while scattering from particles close to the interface can be recorded on a camera.\n\nUsing an illumination angle that lies outside the range captured by the objective achieves full background suppression while still allowing for the particle scattering to be measured for a wide range of different suspended particles (Figure 3B) [11,36,37]. This approach is commonly used in NTA setups, in which suspended nanoparticles are tracked and characterized based on their Brownian motion [36]. However, the largest angle admitted by the objective must be smaller than the illumination angle for the incoming light not to be collected, which puts a limit on the plane wave components reaching the camera and, therefore, the information content of the microscope images.\n\nDarkfield microscopy can also be achieved through spatial blocking of the excitation beam using an illumination angle within the numerical aperture of the objective (Figure 3C). It relies on the observation that at the back focal plane of the objective, the illuminating plane wave is tightly focused.# 2.2 Interferometric scattering techniques\n\nA drawback of the darkfield techniques discussed in the previous section is that the illuminating light is not recorded. Since the scattered field from a nanoparticle is proportional to the illuminating field (see Section 4 for details), quantitative nanoparticle characterization using darkfield techniques requires either detailed calibration or separate quantification of the light intensity at the sample plane.\n\nInterferometric is an approach that circumvents this problem by focusing on quantifying the last term of Eq. (14), explicitly containing the illuminating field Eill. The signal is the interference between the particle signal and a background signal, and therefore, the background signal is non-zero, representing the illuminating field amplitude (Figure 4A). This ensures an internal reference to which the scattered field can be compared.\n\nHowever, given the limited dynamic range of the detector recording the image, the signal-to-background ratio will limit the ability to detect weakly scattering particles in this measurement strategy since the particle contrast cannot be enhanced by increasing the illuminating light intensity. One way to overcome this limitation is by introducing a partially transmissive filter centered at the back focal plane of the objective (Figure 4B) [43\u201345]. The presence of the filter can be represented by a function T(\u03c9),\n\nT(\u03c9) = { \u03c6 ei\u03c9filt if \u03c9 < \u03c9filt,\n\n1 otherwise\n\nwith \u03c6 < 1, such that the filter attenuates and phase shifts plane waves with incident angles \u03c9 < \u03c9filt [23,43,46]. If \u03c9filt \u2197 \u03c9max and positioned in the back focal plane such that it attenuates the illuminating light before it can reach the camera, the filter will attenuate the illuminating light while leaving the scattered light unaffected. The recorded intensity is then, to the lowest order in the scattered field\n\nIcam = \u03c62 |Eill|2 + 2\u03c6|Eill|\u2193(ei(\u03c9filt + \u03c9) Esca).\n\nNotice that the first term is quadratic in the attenuation \u03c6, while the second term is linear in \u03c6. Thus, the relative importance of the two terms can be adjusted by adjusting the transmittance of the filter. However, note that depending on the value of \u03c6 the contribution of |Esca|2 may no longer be negligible. Thus, the value of \u03c6 affects which approximation that can be used when relating the scattering signal to particle properties.\n\nA different approach to tune the signal-to-background ratio is to use a reflection rather than transmission geometry, as depicted in Figure 4C [47]. This measurement geometry is in the literature typically denoted iSCAT (interferometric scattering) [48]. When using a reflection geometry with transparent coverslips, a small portion of the illuminating light will be reflected back to the camera at the interface between the coverslip and the sample. This optical field constitutes the term Eill in this geometry. Most of the light will be transmitted through the interface. This transmitted light produces the scattered field Esca from particles in the sample. Denoting the reflectivity of the interface \u03c6, the recorded intensity is again identical to Eq. (21). In some works, these two approaches for background attenuation have been employed in parallel to achieve maximal sensitivity [27, 44]. Moreover, by using two reflections from the top and the bottom of microfluidic channels, the interference between the# 2.3 Quantitative field microscopy\n\nAlthough interferometric scattering techniques have emerged as powerful techniques for particle characterization with low detection limits, only the real part of the scattered light is detected and quantified. Quantitative field microscopy, by contrast, achieves quantification of the full optical field, thereby providing a more complete characterization of the scattered light. Even though quantitative field microscopy historically has mostly been used to investigate samples such as live cells [26], it has recently been shown that it is possible to detect particles down to single proteins [49].\n\nIn quantitative field microscopy, the light incident on the camera interferes with another reference field Eref to achieve quantification of the real and imaginary parts of the scattered field itself (Figure 5A). This can be achieved using several different microscopy configurations, both using a single image and multiple images to obtain the optical field [50, 51]. The trick to achieving single-image quantitative field microscopy is to let the reference field be incident on the camera at an angle \u03c9ref such that sin \u03c9ref > 3 sin \u03c9max /M, where M is the magnification of the optical system. From the interference with the reference field, it is then possible to quantify the optical field and its angular components (Box 4) [52].# 4: Quantifying the optical field\n\nIn quantitative field microscopy, a reference field is introduced at the camera plane, propagating at an angle \u03c9ref with respect to the optical axis. Taking the reference field to propagate parallel to the xz-plane, the field at the camera plane can be represented as a plane wave as Eref(x, y) = |Eref|eikref\u00b7x, where kref = k sin \u03c9ref \u02c6x. As a further simplifying assumption, we consider an illumination propagating along the optical axis. In this case, the intensity recorded by the camera reads:\n\nIcam(x, y) = |Eref|2 + |Eill|2 + 2|Eill||Eref| cos(\u03c9ref x) + 2|Eill| ei\u03c9Esca(x, y) + 2|Eref| eik sin \u03c9ref x Esca(x, y) + |Esca|2.\n\nNow, let us investigate the plane wave decomposition (the Fourier transform) of this recorded intensity. Since typically Esca \u226a Eill and Esca \u226a Eref, we keep only terms up to linear order in Esca. Further, we utilize that at the camera plane, Esca(x) = F-1[Esca(k) P\u02dc(k)] to arrive at Esca(x)eikref\u00b7x = F-1[Esca(k \u2192 kref) P\u02dc(k \u2192 kref)].\n\nTo arrive at:\n\nIqfcam(k) = (|Eref|2 + |Eill|2) P\u21bc(k) + |Eill||Eref|(P\u21bc(k \u2192 kref) + P\u21bc(k + kref)) + |Eill| cos \u03c2[P\u02dc(k) Esca(k) + P\u02dc(\u2192k) Esca(\u2192k)] + |Eref|[P\u02dc(k \u2192 kref) EPsca(k \u2192 kref) + P\u02dc(kref + k) Esca(kref + k)].\n\nAt this point, recall that the function P\u02dc has support only for arguments |k| < k sin \u03c9max/M. It is therefore possible to choose kref such that P\u02dc(k) P(k \u2192 kref) = 0 for all values of k. Specifically, this holds if sin \u03c9ref > 2 sin \u03c9max/M. In this case, one finds that:\n\nIqfcam(k) P\u02dc(k \u2192 kref) = |Eref| P\u02dc(k \u2192 kref) EPsca(k \u2192 kref),\n\nfrom which the optical field can be reconstructed through an inverse Fourier transform. In this analysis, the term |Esca|2 was neglected. To ensure that P(k \u2192 kref) does not overlap with this term either, one can show that it is sufficient that sin \u03c9ref > 3 sin \u03c9max/M.\n\nThe reference field can be constructed via an external reference (referred to as off-axis holography, Figure 5B) or by interference by the illuminating field itself, for instance, by introducing a grating at the camera plane (Figure 5C) [50,51]. The different implementations have some different strengths and weaknesses regarding complexity, noise, and stability. A detailed discussion about different microscopy configurations can be found in Ref. [51].\n\nSimilar to interferometric scattering techniques, optical field measurements can also be combined with optical filters [20, 23, 46] and different illumination strategies [49, 53] to improve the detection limit. In that case, the attenuation factor \u03c6 will affect the relation between the measured particle signal and particle properties.# 3 Scattering of light from nano- and microparticles\n\nIn the preceding section, the optical field was taken to be a scalar quantity to simplify the equations. In reality, the optical field is a vector with components given by the polarization state of the light. The different polarization directions scatter light slightly differently, therefore complicating the description of the scattered light. Nonetheless, as we will show below, under certain circumstances, the scalar description of light still provides an accurate description of light scattering.\n\nThe scattering of light from nanoparticles can generally be described by solving Maxwell\u2019s equation.# Fig. 5: Quantitative field microscopy setups\n\nA) In quantitative field microscopy, both the real (upper) and imaginary parts (lower) of the scattered field are quantified, where the sign and amplitude of the real and imaginary part contain particle material information [14, 54].\n\nB) Off-axis holography: the illumination light is split into two separate paths prior to the sample. The sample arm are recombined close to the camera at an angle.\n\nC) Quadriwave shearing microscopy (QWLS): the light is split using a diffraction grating after the sample. The reference light in this setup is a sheared version of the field impinging on the diffraction grating.# Illuminalion\n\nThe reference arm and for electromagnetism with the appropriate boundary conditions. In the special case of spherical particles, the exact solution to this problem was derived by Gustav Mie in 1908 [24]. The field scattered by a homogeneous sphere is written as an infinite sum of special functions, which is numerically much faster to evaluate than explicitly solving Maxwell\u2019s equations.\n\nTo gain insight into the behaviour of the scattered light and into how the light scattering is affected by particle properties, it is useful to consider limiting cases for which the Mie solution can be evaluated analytically. One such limit, which is of particular relevance for nanoparticles, is the Rayleigh limit, valid for kR \u2197 1, where k = 2\u03c0/\u03bb and R is a characteristic size of the particle. Let the illuminating light be a linearly polarized plane wave with amplitude |Eill|, propagating along the z-axis and defining the x-axis to lie along the polarization direction. Now, the scattered wave is described both by the angle \u03c9 relative to the propagation axis of the light and by the angle \u03b5 relative to the polarization axis. The scattered field will maintain the polarization direction of the illuminating field, and its plane wave decomposition is represented as:\n\nEsca(\u03c9, \u03b5) = |Eill|ik\u03b8 43 \u03c6(\u03c9, \u03b5),\n\nwhere \u03c6 = (cos2 \u03b5 + sin2 \u03b5 cos2 \u03c9) for linearly polarized light, and \u03b1 is the polarizability:\n\n\u03b1 = 3V (np - nm)2 / (np + 2nm)2\n\nIn this expression, V is the particle volume, np is the refractive index of the particle, and nm is the refractive index of the surrounding medium.\n\nAnother useful limit is the weakly scattering limit, that is, for kR|np/nm \u2192 1| \u2197 1 and |np/nm \u2192 1| \u2197 1 [24]. This is a slightly weaker condition than the Rayleigh condition, and the corresponding approximation is valid also for arbitrarily large scatterers as long as the refractive index difference compared to the surrounding medium is sufficiently small so that the inequality kR|np/nm \u2192 1| \u2197 1 still holds. This limit is particularly useful for biological nanoparticles, which typically obey these inequalities. In this case, the polarizability is typically approximated as:\n\n\u03b1 \u2248 2V (\u0394n/nm),\n\nwhere \u0394n = np - nm. In the specific case of biological nanoparticles, this expression has a particular physical interpretation since it enables the treatment of the nanoparticle as a volume made up of biomolecules at a certain concentration. The refractive index of a solution of biomolecules increases approximately linearly with the mass concentration C of molecules, as np = nm + (dn/dc) \u00b7 C, where# (dn/dc) is called the specific refractive index increment, and is material specific. However, since most biomolecules contain similar elements at similar ratios (mostly carbon, hydrogen, oxygen, and nitrogen), the specific refractive index increments of different types of biomolecules are very similar [26]. Typical values range from \u2198 0.16 ml/g for carbohydrates to \u2198 0.2 ml/g for nucleic acids and proteins [26]. The polarizability times n m thus evaluates to n m \u00b7\u21bd \u2191 2(dn/dc)C \u00b7V = 2(dn/dc)m, where m is the total mass of the biomolecules in the nanoparticle. Thus, in the case of weakly scattering particles, the polarizability is proportional to particle mass.# Further, the total scattered field can be calculated as the superposition of the field scattered by infinitesimal volume elements within the particle. The scattering from each such infinitesimal volume element is given by the Rayleigh scattering limit above. For weakly scattering particles, the optical field impinging on each such volume element can be approximated as equal to the incident optical field external to the particle. The scattered field from an isotropic particle, evaluated outside of the particle, is then given by [55]# E sca (\u03c9, \u03b5) = ik 3 |E ill |$(\u03c9, \u03b5)\u222b drr 2 %n(r) sin qr,# where q = 2k sin(\u03c9/2). This approximation is called the Rayleigh-Debye-Gans (RDG) approximation.# The integral above describes the interference of the light scattered from different volume elements in the particle. This factor is denoted the form factor and physically encodes the distribution of refractive index within the particle. The RDG field is often written in the following form,# where f (\u03c9; R) is the form factor, which for an isotropic particle depends on its size R and internal refractive index distribution.# Under uniform illumination, analytical solutions to the form factor within the RDG approximation can be attained for some specific geometries, such as spheres and core-shell spheres. For a spherical particle, the form factor is [7],# and for an infinitesimal spherical shell, which can be used to approximate the signal from a lipid vesicle, the form factor is [56]# Note that the form factor at 0 degree scattering angle is identically equal to unity (f (0) = 1) and that the form factor for all scattering angles \u03c9 > 0 is smaller than unity (f (\u03c9 > 0) < 1). For this reason, the relation between the measured optical signal and particle size is different for different measurement geometries. This is highlighted in Figure 6, in which the form factor of spheres of different sizes is shown as a function of the scattering angle. In accordance with the discussion above, the form factor is close to one for all particle sizes for transmission geometries, for which the scattering angle is close to \u03c9 = 0. For side-scattering (exemplified in Figure 3C) and backward scattering (exemplified in Figure 4B), the form factor greatly influences the relationship between the measured optical signal and particle size. Specifically, the contribution from the form factor to the scattered light becomes appreciable for particles with qR > 1. Recalling that q = 2k sin(\u03c9/2), one has that the form factor contribution is appreciable if R > (2k sin(\u03c9 ill /2)) \u21921. For an illumination wavelength of 532 nm when the particle is in water (n m \u2191 1.33), this amounts to R > 45 nm for side-scattering with \u03c9 ill = \u03d1/2, and R > 30 nm for backscattering with \u03c9 ill = \u03d1.# 4 Relation between the signal measured in a scattering microscope and physical particle parameters# Now, let us use the mathematical framework developed in Sections 2 and 3 to investigate the relation between the scattered light from a nanoparticle and the optical field at the camera plane. The crucial insight is that the angular distribution of the scattered field from a nanoparticle described in Section# 1.0\n\n| | | |R = 50 nm|R = 150 nm|300|\n|---|---|---|---|---|---|\n|0.5|Forward Scattering|Side Scattering|Backward Scattering| | |\n| |n/4|n/2|3n/4| | |\n\nFig. 6: Form factors of homogeneous spheres. The form factor depends strongly on the particle size. Small particles (blue line) scatter almost uniformly, while large particles (green line) scatter predominantly in the forward direction (small angles). In forward scattering, the contribution from the form factor is close to unity in most cases. In side scattering (\u03c9 ill \u2191 \u03d1/2), the contribution is more pronounced, and in backward scattering (\u03c9 ill \u2191 \u03d1), the form factor contribution is maximal. The form factors in this plot are calculated for homogeneous spheres in water illuminated with a wavelength of \u03d6 = 532 nm. Shortening the illumination wavelength will compress all scattering curves to smaller scattering angles. Conversely, for longer illumination wavelengths the scattering curves will be extended to the right in the figure above.\n\n3 describes precisely the individual plane wave components used in Eq. (4). Working within the RDG approximation and assuming that the illuminating light propagates along the optical axis, we write the scattered field as:\n\nEsca(\u03c9, \u03b5) = i|Eill|k3 \u21bdf(\u03c9)$(\u03c9, \u03b5), (34)\n\nwhere \u21bd is the polarizability of the particle, f(\u03c9) is the form factor, |Eill| is the amplitude of the illuminating light. The polarization function $(\u03c9, \u03b5) introduces an angular dependence of the scattered light that does not depend on the properties of the particles in the sample in the case of isotropic particles. In the process of signal calibration utilizing reference samples through darkfield and interferometric microscopy, this contribution will be effectively normalized away. Thus, to simplify the expressions, we will drop the polarization function $(\u03c9, \u03b5) in the following sections of this tutorial.\n\nFor a particle located in the focal plane of the objective (see Box 5 for a description of the general case of a particle located away from the focal plane), the scattered field at the back focal plane is then given by:\n\nEbfp(\u03f1) = ik3|Eill|\u21bd\u222b0\u03b5maxcos \u03c9 sin \u03c9f(\u03c9) P\u02c6(k(\u03f1 \u2192 f sin \u03c9))d\u03c9 (35)\n\nand the scattered field at the camera plane is given by:\n\nEsca(\u03f1) = i|Eill|k3\u21bd\u222b0\u03b5maxcos \u03c9 sin \u03c9f(\u03c9)J0(k\u03f1 sin \u03c9)d\u03c9. (36)# Box 5: Effect of having particle located away from the focal plane\n\nEquation (36) is valid for a scatterer located at the focal plane of the objective. In an experiment, this condition is not necessarily fulfilled. If the particle is located a distance z from the focal plane, the individual plane wave components of the scattered field must additionally be propagated to the focal plane. Since each of the components describes a plane wave propagating at an angle \u03c9 with respect to the optical axis, the angular components propagated a distance z along the optical axis are given by\n\nE\u02c6(\u03c9, z) = E(\u03c9, 0)eikz cos \u03b5 (37).\n\nThus, the scattered field from a nanoparticle located a distance z from the focal plane will, at the camera plane, reads\n\nEsca(\u03f1) = i|Eill|k3\u21bd\u222b0\u03b5maxcos \u03c9 sin \u03c9f (\u03c9)J0(k\u03f1 sin \u03c9)eikz cos \u03b5 d\u03c9 (38).\n\nWe will now use this expression, in combination with the definition of the RDG form factor Eq. (30), to investigate how the signal measured in a scattering microscope is related to the physical parameters of the scattering objects.# 4.1 Darkfield microscopy\n\nIn the case of darkfield microscopy, the intensity measured by the camera from a particle located at the front focal plane of the objective and illuminated by a plane wave propagating along the optical axis is given by\n\nI camera (\u03f1) = |E ill|2 k6 |\u21bd|2\u2223\u222b0\u03b5 maxcos \u03c9 sin \u03c9f (\u03c9)J0(k\u03f1 sin \u03c9)eikz cos \u03b5 d\u03c9\u22232 (39).\n\nSome examples of scattering patterns obtained from Eq. (39) at different defocus values z are shown in Figure 7A.\n\nTo perform particle characterization, one needs to reduce the measured scattering to a set of values representing some physical properties of the particle. The most common way of achieving this is by characterizing the integrated intensity of a scattering pattern, which in the case of darkfield microscopy is proportional to the square of the polarizability. In the case of a particle illuminated by a plane wave with \u03c9 ill = 0, one obtains for an isotropic scatterer (see Box 6 for a derivation of this result)\n\n\u222b |2 k4|\u21bd|2\u222b0\u03b5 maxcos \u03c9 sin \u03c9f (\u03c9)2 d\u03c9.\n\nI camera dA = 2\u03d1|E ill (40)\n\nSimilar expressions for cases in which the illuminating field is not parallel to the optical axis can be derived by adjusting the limits of integration in the above expression. Furthermore, for small enough scatterers such that f (\u03c9) \u2191 1, the integrated darkfield intensity is proportional to the square of the polarizability. For larger particles (kR > 1), the contribution to the measured signal from the form factor of the scatterer will depend on their size and morphology [57].# Box 6: Integrated darkfield intensity\n\nIn order to calculate the integrated scattering intensity in darkfield microscopy, it is useful to start with the expression\n\nIcam(x) = |Esca(x)|2. (41)\n\nIntegrating this over the entire detector surface, one has\n\n\u222b Icam(x)dx = \u222b |Esca(x)|2 dx. (42)\n\nNow, we invoke Parseval\u2019s theorem, stating that\n\n\u222b |f(x)|2 dx = \u222b |f\u0302(k)|2 dk, (43)\n\nwhere f(x) and f\u0302(k) are Fourier transform pairs. One therefore has\n\n\u222b Icam(x)dx = \u222b |E\u0302sca(k)|2 dk, (44)\n\nfrom which Eq. (40) follows after transformation into polar coordinates and switching integration variable from the wave vector |k| to angle \u03c9.\n\n2-+6 um\n\n1 10\n\n2-0 L 10105\n\n] 103101 Sacekwcttertagtering\n\nForward scattering\n\n2=-6 um\n\n50 100 150 200 250\n\nRadius (nm)# Fig. 7: Scattered intensity in darkfield imaging\n\nA) Calculated scattering patterns of nanoparticles (radius 20 nm) suspended in water and measured in darkfield microscopy with \u03c9ill = 0 and wavelength 532 nm at different values of the defocus z. The field of view of each particle image is 10 \u2243 10 microns.\n\nB) The integrated intensity of scattering patterns measured in darkfield microscopy as a function of particle size for three illumination angles. For \u03c9ill = 0, the integrated intensity scales with the square of the volume throughout the sizes included in this calculation. For \u03c9ill \u21d0= 0, distinct minima appear corresponding to the minima of the form factor.\n\nThe approaches to achieve darkfield illumination discussed in Section 2 give rise to slightly different contributions from the form factor. For darkfield imaging through spatial blocking, the illumination angle \u03c9ill = 0 in Eq. (40). In the case of darkfield illumination by oblique illumination, one instead has \u03c9ill \u21d0= 0. The final case of evanescent wave scattering is slightly more complicated.# 4.2 Interferometric scattering techniques\n\nThe optical field is propagating along the interface in which total internal reflection occurs. Therefore, the illumination angle is\n\u03c9ill = \u03d1/2. However, the form factor is slightly different since the optical field decays exponentially in the medium of the scatterers. The appropriate correction was derived in the supporting information of Ref. [57]. In Figure 7B, the integrated scattered intensity is shown as a function of particle size for a fixed refractive index for the specific cases\n\u03c9ill = 0, \u03c9ill = \u03d1/2 and\n\u03c9ill = \u03d1. As discussed in Section 3, the larger the\n\u03c9ill, the smaller the size region for which there is a unique relation between scattering signal and size for a known particle refractive index.\n\nFig. 8: Scattered intensity in interferometric imaging\n\nA) Calculated scattering patterns of nanoparticles (radius 20 nm) suspended in water and measured in interferometric microscopy in reflection geometry\n(\u03c9ill = \u03d1) with illumination wavelength 532 nm at different values of the defocus\nz, indicated in the plot to the right of the scattering patterns. The integrated intensity of the scattering patterns varies sinusoidally with the defocus\nz due to the phase shift\n\u03c2. The field of view of each particle image is 10 \u2243 10 microns.\nB) The integrated intensity of scattering patterns measured in interferometric microscopy with\n\u03c9ill = \u03d1 as a function of particle size.\n\nIn the case of interferometric scattering techniques, the term of interest in Eq. (14) is the final interferometric term. Taking the illuminating field to be a plane wave propagating along the optical axis, the integrated recorded intensity is given by (see Box 7 for a derivation)\n\n\u222b (Icamera) I0 \u2192 1 dx = (2/\u03c6)k\u2193(\u21bdei!\u03c9)f(\u03c9ill), (45)\n\nwhere %\u03c2 = \u03c2 \u2192 \u03c2(z) is the relative phase difference between the background field and the scatterer, \u03c6 is the attenuation factor, and \u03c2(z) describes the depth-dependent relative phase shift of the particle relevant for reflection geometries [58]. Further, we have defined\nI0 = \u03c62|Eill|2, which is experimentally estimated by evaluating the intensity recorded by the camera in locations without particles present. Note that the right-hand side of this final expression does not depend on the intensity of the illuminating light. In other words, the illuminating light does not need to be separately quantified to perform particle characterization in interferometric scattering approaches. However, since \u03c6 may have a spatial dependence or vary between different surfaces [23, 59], background corrections are still sometimes needed. Also, in contrast to darkfield approaches, the recorded intensity is now directly proportional to the real part of the polarizability when %\u03c2 = 0.# Box 7: Integrated interferometric intensity\n\nTo evaluate the integrated signal from a particle in interferometric microscopy, we write the illumination field at the camera plane as\n\nE\u02dcill = \u03c6|Eill|ei(\u03c9\u00b1kz),\n\nwhere \u03c6 is the attenuation coefficient of the illuminating light, \u03c2 is the phase shift of the illuminating field compared to the scattered field, and z is the distance from the scatterer to the focal plane. The \u00b1 reflects the fact that the light propagates in opposite directions compared to the original propagation direction for reflection and transmission geometries. The + sign is relevant for reflection geometries, and the \u2192 sign for transmission geometries. To the lowest order in the scattered field, the recorded intensity, in the RDG approximation and assuming an isotropic scatterer, is\n\nIcam(\u03f1) = \u03c62|Eill|2 + 2\u03c6|Eill|2\u2193[\u21bdk3ei(\u03c9\u00b1kz)\u222b0\u03b5maxcos \u03c9 sin \u03c9f (|\u03c9 \u2192 \u03c9ill|)J0(k\u03f1 sin \u03c9)eikz cos \u03b5 d\u03c9],\n\nwhere \u03c9ill = 0 for interferometric imaging with transmission geometry, and \u03c9ill = \u03d1 for interferometric imaging with reflection geometry. The first term in this equation represents the intensity recorded by the camera in the absence of scatterers. Subtracting and dividing by this value, one obtains for the integrated signal of a particle\n\n\u222b \u03c62|Eill|2dx = (2/\u03c6)k3\u2193[\u21bdei(\u03c9\u00b1kz)\u222b\u222b0\u03b5maxcos \u03c9 sin \u03c9f (|\u03c9 \u2192 \u03c9ill|)\u03f1J0(k\u03f1 sin \u03c9)eikz cos \u03b5 d\u03f1 d\u03c9],\n\n\u21bcI\n\nwhere \u21bcI = Icam \u2192 \u03c62|Eill|2. To evaluate this, recall that the integral over \u03c9 is a Fourier transform in disguise. Utilizing that\n\n\u222b f (x)dx = f\u02c6(0),\n\nwhere f and f\u02c6 are Fourier transform pairs, one immediately finds\n\n\u222b \u21bcI \u03c62|Eill|2dx = (2/\u03c6)k\u2193[\u21bdei(\u03c9+[1\u00b11]kz)]f (\u03c9ill).\n\nTo exemplify the position dependence of the interferometric scattering signal, in Figure 8A scattering patterns calculated according to equation (47) in the case for interferometric backscattering. Notice how the central lobe changes sign as the defocus changes by only a fraction of a wavelength. The integrated signal of the scattering patterns shows a sinusoidal dependence on the defocus z due to the relative phase difference \u03c2(z) between the scattered light from the particle and the light reflected at the coverslip. For this reason, the iSCAT signal is either quantified in the same plane for all particles, as on a coverslip or a specific depth plane [15,27], or the images are transformed using a neural network to remove the depth dependence [23]. When that is done accurately, the size dependence of the integrated signal follows Figure 8B, which shows Eq. (45) when %\u03c2 = 0.# 4.3 Quantitative field imaging\n\nIn quantitative field imaging, the recorded quantity is the scattered field itself. In this case, one has that [54]\n\n\u222b Esca(x)/|Eill|dx = ik\u21bdf (\u03c9ill) = ik\u21bd,\n\nwhere the last equality is valid for \u03c9ill = 0, which is the most common choice for quantitative field methods. This is similar to Eq. (45) with \u03c2 = 0, except for the fact that the polarizability is now allowed to be complex-valued. Note that it is the integrated imaginary part of the optical field that is proportional to the real part of the particle polarizability. In Figure 9A, the real and imaginary parts of the scattering patterns measured in quantitative field imaging are shown for different defocus values.Importantly, following Eq. (37) the optical field signal can be re-propagated after recording the image. This, in turn, enables quantification of the signal of focused scattering patterns even though they are measured out of focus, which reduces the sensitivity of the particle characterization to noise and out-of-focus effects [54]. The integrated imaginary part of the signal is proportional to particle polarizability and hence particle volume (Figure 9B) as predicted from Eq. (51).\n\n|Real|Imaginary|\n|---|---|\n|2=-2 um|105_|\n|1 104_|z-0|\n|103|7 102_101|\n|2=-2 um|100_|\n\n50 100 150 200 250\n\nRadius (nm)# Fig. 9: Scattered intensity in quantitative field imaging\n\nA) Calculated scattering patterns (real and imaginary parts) of nanoparticles (radius 20 nm) suspended in water and measured in quantitative field imaging with illumination wavelength 532 nm at different values of the defocus z. The field of view of each particle image is 10 \u2243 10 microns. B) When the particle is in focus, the integrated imaginary part of scattering patterns is measured in quantitative field microscopy scales with particle volume. Note that for quantitative field imaging, the image can be re-propagated after recording. This makes it possible to refocus the detections individually.\n\nFurthermore, utilizing Eq. (11) one can rewrite the scattered field at the camera plane in terms of the angular components of the form factor directly (Box 8). This shows how quantitative field imaging contains information about particle polarizability and the particle form factor itself, which, if quantified, can be related to particle size [8].\n\n19# Box 8: Quantifying the optical form factor in quantitative field imaging\n\nThe scattered field at the camera plane is related to the optical form factor as\n\nEsca(\u03f1) = ik3 |Eill| &int; cos \u03c9 sin \u03c9 f(\u03c9) J0(k\u03f1 sin \u03c9) eikz cos \u03b5 d\u03c9. (52)\n\nApplying the transform Eq. (11), one finds that\n\n&int; ik |Eill| f(\u03c9) eikz cos \u03b5 = \u03f1 Esca(\u03f1) J0(k\u03f1 sin \u03c9) d\u03f1. (53)\n\nTaking the absolute value of both sides and utilizing that |eix| = 1, one finds\n\n|&int; k |Eill| f(\u03c9)| = |&int; \u03f1 Esca(\u03f1) J0(k\u03f1 sin \u03c9) d\u03f1| (54)\n\nFinally, since k |Eill| = |&int; Esca dx| from Eq. (51), we have that\n\n|&int; \u03f1 Esca(\u03f1) J0(k\u03f1 sin \u03c9) d\u03f1| = |f(\u03c9)| = |Esca dx| (55)# 5 A toolbox for analyzing scattering microscopy data\n\nQuantitative analysis of scattering microscopy data consists of two fundamental steps, namely particle detection and signal characterization [60]. The aim of this section is to provide an easy-to-use toolbox to perform these tasks for the three types of scattering microscopy geometries that are discussed in this tutorial. If the particles are moving during the experiment, a third step, detection linking, is required to form particle traces [61], allowing the particle motion to be related to particle properties such as the hydrodynamic radius as previously described in several review articles [17, 62]. Since the ability to track the motion of particles is generic for optical microscopy methods, the focus of this tutorial is the information contained in the optical signal, where the hydrodynamic radius will complement the optical signal in the case of freely suspended particles. Appended to this tutorial are Jupyter notebooks containing code for performing particle detection and characterization in the three scattering modalities considered here (darkfield imaging, interferometric imaging, and quantitative field imaging). In the following two sections, the content of the notebooks will be briefly explained.# 5.1 Particle detection\n\nThe particle detection task is essentially recognizing scattering patterns in the presence of noise. Traditionally, particle detection has been performed by algorithmic approaches, in which a predefined set of image filters are applied to the microscopy images, followed by a thresholding operation to identify particles [63]. In the past decade, deep learning approaches to particle detection have become increasingly popular, showcasing more accurate detection in particular under low signal-to-noise conditions [60, 64].\n\nThere are, in general, three sources of noise contributing to the noise level in scattering microscopy images: (1) shot noise, arising from the finite number of photons detected in each camera pixel, (2) read noise, which is intrinsic to the camera, and (3) speckle noise, due to coherent reflections and scattering along the beam path of the illuminating light [65].\n\nThe first two noise sources are common to all types of microscopy and have the property of being spatiotemporally independent: the noise at pixel i at time t0 is independent of the noise at pixel j at time t1. This particular property means that the noise from these sources can be reduced by averaging the signal over time and/or across multiple pixels. Speckle noise is special for optical microscopy and originates from the interference between the different optical plane waves of the illumination. Speckle noise is characterized by the fact that noise at neighboring pixels is correlated, where the amplitude and temporal stability of the speckle depends on the light source and experimental setup [66]. Important for image analysis, this noise has a spatial correlation that is similar to the spatial correlation of the nanoparticle scattering signal, and in the absence of mechanical vibrations in the system, it can# 5.2 Signal characterization\n\nAfter having detected a particle, the next step in nanoparticle characterization is to utilize the image of the scattering pattern to extract information about the particle itself.\n\nCommon to all scattering microscopy approaches is that the integral of the particle signal is related to particle polarizability (which, for biological nanoparticles, is proportional to their mass). However, depending on the measurement geometry, when relating the scattering to mass, the effect of the optical form factor needs to be compensated (Figure 6). As a rule of thumb, for particle sizes R &lt; (2k sin \u03c9 ill )\u21921, the form factor can be approximated as f (\u03c9) \u2191 1 within the angles collected by the objective, in which case the integrated intensity is directly related to particle polarizability. For particles larger than this, the integrated intensity must be complemented with independent measurements of size and/or polarizability to perform quantitative characterization, as exemplified in [23, 68].\n\nIn practice, the task in signal characterization is to estimate the integrated particle signal in the presence of noise. Directly summing up all camera pixels is not a good approach in practice since, most commonly, tens or hundreds of particles are present in the field of view of the camera at the same time. The most common approach to signal characterization is, therefore, to crop out a small region around each detected particle (Figure 10B), and fit some kind of function to this limited view of a particle. As a specific example in the case of iSCAT, in [68] the particles were first localized using the radial variance transform, where the particle detection with the maximum positive contrast estimated by Gaussian fitting was used to estimate the particle signal.\n\nAnother approach that has gained increasing attention is to utilize deep learning enhanced analysis techniques to not only detect the particle but also estimate the signal. In the appended notebooks, we provide code to train and apply a convolutional neural network to estimate the integrated signal strength of scattering patterns. In Figure 10C, this step is shown for interferometric imaging in reflection geometry, where the signal estimation follows what is expected from theory. In the appended notebooks, code is also provided to perform this step for darkfield imaging as well as for quantitative field imaging.\n\nGoing beyond particle polarizability, it is, in some instances, possible to quantify also the particle form factor directly from the optical signal using deep learning image analysis by utilizing the fact that the form factor is encoded in the angular components of the scattered field as measured in quantitative field imaging (Eq. (55)). This approach was utilized in Ref. [19] to estimate particle size and refractive index directly from scattering patterns measured in off-axis holographic images. Specifically, particle size (or, more accurately, the radius of gyration of a particle) is related to its scattering form factor as when Rq \u2197 1.\n\nR g2\u2191 3q \u21922 (1 \u2192 f (\u03c9)2).\n\n21# Radius (nm)\n\nFig. 10: Particle detection and characterization in interferometric microscopy. A) An example iSCAT microscopy image containing multiple scatterers. This image was simulated using the notebooks appended to this tutorial. On the right side of the image, particle detections made by the LodeSTAR algorithm, trained using the notebooks appended to this tutorial, are overlaid on the image. The particle detections are intentionally left out on the left half of the image to not obscure the appearance of the scattering data. B) Crops of the scattering patterns of individual particles detected in the frame in A). C) Characterization of the integrated scattering intensity of individual particle crops like the ones shown in B). The characterization is performed by a convolutional neural network, trained using the notebooks appended to the tutorial.\n\nThus, the task of particle sizing directly from quantitative field images amounts to estimating a curvature in the optical form factor from noisy images.# 6 Considerations when designing measurement geometry\n\nFrom the treatment above, it is clear that the different approaches to scattering microscopy have different quantitative power when it comes to particle characterization.\n\nThe first consideration that one should make when designing a scattering-based characterization experiment is the level of detail required in the characterization to answer the scientific questions at hand. In some cases, it may be sufficient to detect and track the motion of particles rather than accurately quantify the particle signal. In this case, darkfield techniques have the advantage that the data is relatively easily analyzed since the particles appear bright against a dark background. For this reason, darkfield imaging is one of the standard techniques for tracking suspended nanoparticles [13,35].\n\nHowever, particle characterization based on the optical signal using darkfield techniques is comparatively challenging since relating particle signal to polarizability requires accurate calibration. In particular, since the particle contrast in darkfield techniques is proportional to the local light intensity at the particle position, a proper calibration procedure would require mapping out the illumination intensity throughout the entire field of view, which is technically challenging. In [13], particle characterization was demonstrated using darkfield imaging with oblique illumination (Figure 11A) of a sample freely diffusing in a macroscopic volume. The challenge of quantifying the scattered signal in such conditions, in particular under a non-uniform illumination, was overcome by utilizing the maximum value of the measured scattering signal of each particle trace as a proxy for the particle scattering, in combination with careful calibration (Figure 11B). This enabled the quantification of hydrodynamic size as well as the scattering cross-section of suspended polystyrene and silica beads (Figure 11C), which was also converted into the estimate for the refractive index of these particles (Figure 11D).\n\nAnother measurement consideration is whether particle dynamics information is critical or not. To follow the same particle over time it needs to be confined, which can be achieved by, for example, tethering the particle to a surface [42]. In particular, by using evanescent illumination (Figure 12A).# Particle Characterization Using Darkfield Imaging# Figure 11\n\n|A)|B)|\n|---|---|\n| |2500|\n| |3200|\n| |141500I000|\n| |500|\n|C)|D)|\n|Silica|Time (s)|\n|Polystyrene| |\n|1|RIE 1 65|\n|RI =| |\n|RI =| |\n|RI = 45| |\n|[2|RI = 1.35|\n|200|5000|\n|400|2500|\n|600| |\n\nIn darkfield imaging with oblique illumination was used for quantitative characterization of suspended nanoparticles. The particle signal was estimated by tracking the motion of nanoparticles and estimating the integrated signal at each time point in a particle trace. The maximum value of the integrated signal was used as a proxy for the signal strength. Using both signal quantification and particle tracking over time enabled quantification of both hydrodynamic size and the scattering cross-section, here for a sample of silica beads and a sample of polystyrene beads. By combining the particle size and scattering cross section the particle refractive index was also estimated for the silica and polystyrene beads. Figure reprinted with permission from American Chemical Society (Copyright 2014).\n\nOnly particles that are adsorbed or very close to the surface will be illuminated and scatter light. In [42], evanescent illumination was used to study protein adsorption to lipid vesicles adsorbed to a surface. Both fluorescence and scattering signal were measured simultaneously (Figure 12 B), enabling time-resolved monitoring of the adsorbed protein mass to individual vesicles and correlating the scattering signal to fluorescence signal (Figure 12C).\n\nA third measurement consideration is whether the particle signal must be accurately related to particle properties such as mass. Interferometric scattering approaches have the advantage compared to darkfield techniques in that the particle contrast is measured relative to the local illumination intensity so that the particle estimate is insensitive to changes in the illuminating light intensity. This enables accurate quantification of the scattering signal that can be related to particle properties in a precise manner [15, 16, 27]. Nonetheless, measuring the scattering from well-characterized calibration particles is still necessary to calibrate the attenuation factor \u03c6 and the relative phase difference \u03c2.\n\nFor suspended particles, which diffuse in three dimensions, the particles will quickly explore a volume sufficiently large to cover all possible values of \u03c2, rendering calibration of this phase unnecessary in this case.\n\nInterferometric methods enable accurate particle characterization both on a surface [27] and when in solution [16,68]. In Ref. [68], signal quantification in combination with particle tracking using iSCAT (Figure 13A) was used to determine the size and refractive index of suspended nanoparticles (Figure 13 B-C). Moreover, by analyzing the particle-size scaling, they could obtain structural information about suspended liposomes. To investigate even smaller suspended particles, in Ref. [16], they used\n\n23# Fig. 12: Characterization of surface-bound particles using evanescent illumination\n\n|A)|Camera|515 nm|488 nm|\n|---|---|---|---|\n| |Lens|Fluorescence|Scattering|\n| |Objective|Sample| |\n| |Objective|Illumination|C)|\n| |3|3| |\n| |t[s]| | |\n\nRef. [42] evanescent illumination was used to study protein binding to lipid vesicles adsorbed to a surface. B) Using a dichromatic mirror, the scattering and fluorescence signals can be simultaneously recorded. C) The protein adsorption event could be resolved by monitoring the integrated fluorescence and scattering intensities as a function of time on the single particle level.\n\nFigure reprinted with permission under the CC-BY license.# Fig. 13: Particle characterization using iSCAT and nanochannel scattering microscopy (NSM)\n\n|A)|In| | |\n|---|---|---|---|\n| |the relative scattering between a nanochannel and the particle to characterize the suspended size and mass of individual biomolecules possible (Figure 13D).| | |\n|Sample|Lens|Quarter-Wave plate|1|\n|Beam Splitter|Illumination|Diameter nm|1|\n|Lens|Camera| | |\n\nA-C: In Ref. [68], particle characterization of suspended nanoparticles using iSCAT (A) was demonstrated through quantitative analysis of the scattered light in combination with particle tracking (B). The iSCAT contrast was shown to be proportional to particle volume for small particles (C, upper row), as anticipated from Section 4, and the deviation from this scaling was used to estimate the internal refractive index of extracellular vesicles (B, lower row). D) By utilizing the interference between a nanochannel and particles residing within the nanochannel, in [16], characterization of polarizability (proportional to mass) as well as the hydrodynamic radius of individual biomolecules was demonstrated. Figure reprinted with permission under the CC-BY 4.0 license.\n\nA fourth measurement consideration is whether more detailed material information is needed in the case of heterogeneous samples. Quantitative field imaging provides the most rich optical signal that can, in turn, be used for the detailed characterization of nanoparticles. For instance, the complex-valued scattered field contains information about both the real and imaginary parts of the particle polarizability and refractive index (Figure 14A-B) [21, 54]. In Ref. [54], this was used to distinguish between gold nanoparticles and polystyrene particles directly from the optical signal. Similarly, in Ref. [14], the sign of the phase signal was used to differentiate between nanobubbles and dielectric particles in the same sample. Furthermore, in Ref. [19], the complex-valued signal was used to determine the size and refractive index of nanobeads and fractal aggregates directly from the scattered.# 7 Conclusions and future opportunities\n\nIn this tutorial, we have given an overview of the particle information from darkfield, interferometric scattering, and quantitative field microscopy measurements. Over the past decades, the single particle detection limit has significantly improved for all different optical imaging methods. Looking ahead, there are still opportunities and challenges for scattering-based microscopy characterization beyond# Figure 14: Particle characterization using quantitative field imaging.\n\nA-B) Using quadriwave shear interferometric imaging, in Ref. [54], it was demonstrated that gold and dielectric nanoparticles can be distinguished based on their complex-valued polarizabilities. C-D) In Ref. [19], it was demonstrated that deep learning enhanced analysis of scattering patterns recorded in off-axis holography is capable of quantifying the size and refractive index of suspended subwavelength particles. Figure reprinted with permission under the CC-BY 4.0 license.\n\n|Gold|Gold (Exp)|PS (Exp)|Vinot|\n|---|---|---|---|\n|8|DDA|Sumpll|1|\n|Obecnc|1|0DuM| |\n|Dblecuyl|GuinJ|8|Polystyrene (PSI661)|\n|Acam|Splillet|L|8.07|\n| |0I1|Himor|Camen|\n|Rela) (nm\" x10f)|Radius (um)| | |\n\nOne major drawback of quantitative field microscopy techniques compared to other optical techniques has been its detection limit. However, it was recently shown that by combining evanescent field imaging with an external reference, it is possible to measure the optical field from single proteins with mass below 100 kDa when binding to a surface [49].\n\nThis leads to another critical consideration when deciding on a measurement technique, namely particle size. The lowest reported detection limit is 9 kDa biomolecules measured using iSCAT (corresponding to a diameter of approximately 3 nm) [70]. As a comparison, the reported detection limit for darkfield microscopy is around 5 nm diameter for non-metallic particles [12], iSCAT for suspended nanoparticles has a reported detection limit of around 40 nm [15], and quantitative field measurements for suspended nanoparticles has a reported detection limit of around 70 nm [21]. In addition to the detection limit, as described in Section 5.2, transmission methods and non-transmission methods have different relationships between particle signal and polarizability. For particle mass determination, it is beneficial to use a measurement geometry for which the form factor contribution to the scattered light can be neglected. In practice, this implies that the illumination angle ideally should be related to the typical size R of scatterers in the sample as sin(\u03c9 ill /2) < (2kR) \u21921.\n\nGenerally speaking, the more parameters that can be quantified at the single particle level, the more likely it is that the particles of interest can be distinguished from other particles in the sample. For instance, if the particles of interest are the strongest scattering particles, then any technique that permits quantifying polarizability is sufficient. If the particles scatter light to a similar extent but have a different material composition, then particle refractive index may be the relevant parameter to characterize. Finally, if the particles of interest differ from other particles in how the mass is distributed within the particle, then some technique capable of resolving the form factor is preferable. If these physical parameters are insufficient for distinguishing the particles of interest from other particles in the sample, it is also possible to augment scattering microscopy techniques with fluorescent imaging to achieve better specificity.# 7.1 Optical fingerprinting\n\nA fundamental limitation in scattering-based particle characterization is that the detection events are nonspecific, as mentioned in the previous section. A challenge in the field of scattering-based nanoparticle characterization is to identify optical fingerprints, which enable distinguishing and characterizing subpopulations in heterogeneous samples without introducing labels.\n\nOne such fingerprinting feature, the integrated scattering amplitude, has been discussed at length in this tutorial. This feature is proportional to particle polarizability and can be used to distinguish subpopulations. Another experimentally measurable feature is the hydrodynamic radius, estimated through particle tracking [13, 15, 16].\n\nBy combining the information-rich scattering patterns of nanoparticles with deep learning-enhanced analysis techniques that go beyond quantifying the integrated scattering signal, we anticipate that more examples of fingerprinting features will be added to this list, enabling more precise population discrimination and characterization. To give a few examples:\n\n- In principle, quantitative field imaging techniques can quantify the scattering form factor across all scattering angles collected by the objective. Utilizing this, it is possible to discriminate particle subpopulations based on their morphology.\n- In biological systems, many processes are driven by weak interactions. In a scattering microscope, such interactions will manifest themselves as temporal fluctuations in particle properties [19]. Such temporal fluctuations can also be used as a fingerprinting feature.\n- Interferometric scattering patterns are, just as the field measured in quantitative field microscopy, related to the form factor evaluated over the scattering angles captured by the objective. This information can likely be decoded using deep learning enhanced analysis techniques, enabling precise sizing of very small objects.\n- Shape information of anisotropic particles is also encoded in the scattering patterns. For suspended anisotropic nanoparticles, the scattered light reaching the camera will temporally fluctuate as the nanoparticle undergoes rotational diffusion. If the exposure time is much shorter than the characteristic time of rotational diffusion, these fluctuations can be resolved in measurement and be related to particle anisotropy [71]. Anisotropy is also encoded in the dependence of light scattering on polarization. This has been used to measure anisotropy of surface-bound particles [72].\n- Finally, the information about the scattering form factor within an image captured in a scattering microscope is limited by the scattering angles collected by the objective. Thus, different scattering approaches carry complementary information about the scattering form factor. By combining measurement modalities, it is possible to obtain a more complete mapping of the scattering form factor of individual nanoparticles. This was recently demonstrated by combining quantitative field imaging and iSCAT to quantify particle size in unknown sample media [23], and we anticipate that the same idea will be applied using other combinations of scattering techniques as well.\n\nThe primary obstacles to achieving such fingerprinting lie in the noise level of interferometric systems, obscuring parts of the scattering signal, and an imperfect characterization of the pupil function P (\u03c9). In the treatment presented in this tutorial, this has been assumed to be perfectly characterized. In practice, this function is affected by aberrations in the optical system and is difficult to characterize perfectly. Furthermore, in this tutorial, all particles have been assumed to be located directly above the central line of the objective. In a real experiment, the position of the particle with respect to the center of the objective will affect the scattering angles that reach the objective. Thus, the image of a particle will be slightly affected by its position [73]. These effects were characterized and accounted for in [19] using quantitative field microscopy, where calibration particles were used to obtain information about the occurring point spread function, but performing such characterization for other scattering microscopy geometries has not been demonstrated. Nonetheless, considering the fast improvement of the detection limits of scattering-based microscopy [27, 49] we anticipate that deep learning enhanced.# 7.2 Characterizing particles in complex environments\n\nMost single particle characterization methods operate in known environments at a controlled particle concentration. Operating in unknown and crowded environments adds several challenges, including the unknown viscosity hindering estimation of the hydrodynamic radius and the overlapping scattering pattern hindering single particle tracking. For example, the cellular interior is both crowded and inhomogeneous, which makes the analysis of scattering patterns more complicated.\n\nAll-optical fingerprinting of nanoparticles would alleviate the need for diffusion measurements in particle characterization, opening up the possibility of performing quantitative particle characterization in environments that were previously limited to qualitative particle analysis. For highly scattering particles, deep learning approaches can be used to identify and characterize the particles inside cells [64]. For less strongly scattering particles, another approach to resolve the particles is to perform confocal scattering microscopy [74, 75], in which only a small volume of the sample is illuminated at a time, thereby minimizing the effect of such unwanted light scattering. The depth selectivity of confocal microscopy significantly reduces the background scattering from other particles, allowing tracking of individual viruses on a cell [74]. Thus, the combination of confocal scattering microscopy and deep learning image analysis will likely significantly extend the quantitative possibilities of label-free optical particle characterization.",
        "context_id": 27,
        "question": "What property relates diffusivity to size for spherical particles in a viscous medium according to the Stokes-Einstein relation?",
        "answer": [
            "size"
        ],
        "context_length": 79854
    },
    {
        "context": "# 1 Introduction\n\nThe Large Hadron Collider (LHC) [1] at CERN represents the pinnacle of scientific engineering, dedicated to unraveling the fundamental constituents of nature. This proton-proton collider, designed to probe the tiniest structures within a controlled laboratory setting, delivers collisions at unparalleled center-of-mass energies: 7 and 8 TeV for Run I, 13 TeV for Run II, and 14 TeV from Run III onwards. Although protons are not elementary particles themselves, they are composed of quarks and gluons. Quantum Chromodynamics (QCD), the theory governing strong interactions within the Standard Model (SM) of particle physics, describes the interaction of quarks and gluons. Despite the challenge of directly detecting free quarks and gluons due to color confinement, they remain pivotal in discussions regarding high-energy hadron collider phenomenology. When quarks or gluons are produced at high energies, they promptly fragment and hadronize, resulting in a collimated spray of energetic particles known as a jet. By measuring the energy and direction of these particles (the jet) one can glean insights into the properties of the original parton. Defining a jet involves a prescription (jet algorithms) [2\u20139] to group hadrons into jets and assign momentum to the resulting jet. In addition to the collimated beam of hadrons resulting from the hadronization of light quarks and gluons (light jets), the hadronic decay of boosted heavy SM particles such as the W/Z-boson, Higgs boson (h), or top quark also leads to a collimated spray of hadrons that can resemble a single jet.# Review of Fat Jets from Boosted Top Quarks\n\nSince its inception, the LHC has pursued evidence of physics beyond the SM (BSM). Despite the remarkable discovery of the Higgs boson [10, 11], which validated aspects of the SM, the absence of concrete evidence supporting BSM physics has prompted researchers to venture into higher energy regimes. These enhanced energies enable the production of boosted heavy SM particles like the top quark, W/Z-boson, and Higgs boson. The hadronic decays of these boosted SM particles result in a collimated cluster of quarks, forming large-radius (large-R) jets known as \u201cfat jets\u201d with distinctive sub-structure features. There are several advantages to designing search strategies for heavy BSM resonances that decay into highly Lorentz-boosted massive SM particles when hadronic decays of these boosted particles are considered. For one, the hadronic decays of these particles result in a higher signal rate compared to their leptonic decays. Further, the visibility of the hadronic decay products of these particles at the LHC detectors enables the kinematic reconstruction of the decay cascade. More importantly, only a small fraction of the SM background events would give rise to fat jets in the final state. Consequently, fat jet final state signatures are beset with considerably less SM background than those with resolved jets. As a result, the analysis of jet substructure at the LHC resulting from the hadronic decay of boosted top quarks, W/Z-bosons, or the Higgs boson has been instrumental in searching for heavy BSM resonances across various new physics scenarios, including supersymmetry [12\u201316], extra-dimensional models [17, 18], leptoquark models [19, 20], and other extensions of the SM [18, 21\u201329]. Efficiently identifying the particle origin of fat jets is crucial to enhancing the sensitivity of the LHC and future colliders. This necessitates a significant shift in analysis strategy and the development of new innovative methodologies for tagging the particle origin of the fat-jets.\n\nIn this review, our focus is on classifying fat jets resulting from the hadronic decay of boosted top quarks and distinguishing them from light quarks and gluon jets (hereafter referred to as QCD jets). Top quarks at the LHC are particularly intriguing due to the substantial tt\u00af production cross-section, essentially making the LHC a \u201dtop factory\u201d. The millions of top quarks produced at the LHC are expected to provide insights into the SM and its potential extensions. While most top quarks are produced near threshold and can be identified using traditional top reconstruction algorithms similar to those used at the Tevatron, some top quarks produced at the LHC are highly boosted. Theoretical interest in top quarks is heightened by their large Yukawa coupling. The large top Yukawa coupling not only plays a critical role in computing electroweak precision observables [30] and determining the vacuum stability [31] of the Standard Model (SM), but it also has a notable impact on the masses and interactions of various BSM resonances. Many of these resonances exhibit enhanced couplings with the top quark, contributing to a final state rich in top quarks at the LHC. Over the past decade, considerable efforts have been made in the literature to develop effective methods for efficiently distinguishing boosted top quark jets from QCD jets. While early literature introduced cut-based strategies for boosted top tagging [32, 33], leveraging substructure information from fat jets resulting from the hadronic decay of boosted top quarks, recent years have seen a surge in the adoption of machine learning-based approaches for top-jet classification. In this article, we provide a comprehensive review of various cut-based and machine learning-based approaches proposed in the literature over the past couple of decades for top-tagging. This review aims to synthesize the advancements in top-jet classification methodologies, highlighting their evolution and effectiveness in distinguishing top quark jets from QCD jets at high-energy collider experiments like the LHC.# Organization of the Review\n\nThe review is organized as follows: In the next section, we review high-level feature (HLF) based top classifiers. Sections 3 and 4 are dedicated to Image-based Classifiers and Graph Neural Network (GNN) classifiers, respectively. In section 5, we provide a list of BSM scenarios that result in boosted top quark final states at the LHC. Finally, we summarize our findings in section 6.# 2 High-Level Feature (HLF) based classifiers\n\nTagging boosted objects is a long-pursued quest dating back to the eras of the Tevatron. These boosted objects (or, in the context of our discussion, boosted fat jets) can have different origins ranging from decays of SM bosons (W/Z/H), the top quark, light quarks/gluons, or some BSM particles. As discussed in the introduction, we will focus our attention on the tagging of boosted top jets, i.e., identifying fat jets originating from hadronic decay of top quarks from those originating from QCD-initiated light quarks and gluon jets. Initial works in this direction rely on identifying b-jets inside the top jet and the reconstruction of the invariant mass of the W-boson inside the top and top quark mass as a whole. The problem with this approach was the isolation of the b-jet and light jets, which are highly collimated due to the boosted mother particle. This makes the procedure inefficient in scenarios where the production cross-section of these fat jets is small. This led to an in-depth investigation of the jet sub-structure and resulted in the development of several jet substructure variables/high-level features (HLFs).\n\nThe literature on jet substructure variables is vast. Some interesting examples include the jet energy moments [34], the energy correlation functions (ECFs) [35], the generalized energy correlation functions (ECFGs) [36], N-subjettiness variables [37\u201339], and Energy Flow Polynomials [40]. For a comprehensive discussion, we direct the interested readers to reference [41\u201347]. We have hand-picked some of these and some physics-inspired algorithmic approaches for the subsequent discussion.\n\nThe Johns Hopkins Top Tagger (JHTT), originally introduced in reference [33], looks into the subjet structure of the fat jet, applying additional kinematic criteria to identify fat jets originating from top quarks. To begin with, C/A fat jets with a given R parameter are considered. These fat jets undergo a sequential declustering procedure. First, the fat jet J is declustered into two subjets, j1 and j2. If the softer subjet is discarded if it has pT,j < \u03c9p \u2192 pT,J, for some predefined \u03c9p while the harder subjet undergoes further declustering until certain conditions are met (if both subjets do not fulfill above criteria they both are considered for further analysis). These conditions include the subjet being comprised of a single calorimeter cell, the daughter jets from declustering being too close (|\u03b5| + |\u03d1| < \u03c9r), or both daughter jets satisfying the initial criteria of pT,j < \u03c9p \u2192 pT,J. Fat jets with 3 or 4 subjets are considered for further analysis. For their final analysis, reference [33] uses SM top pair production and QCD di-jet production for generating signal and background fat jets in Pythia [48]. To incorporate detector effects, final state visible particles are combined in grids of size 0.1 \u2192 0.1 and passed onto the clustering algorithm. Only fat jets with pT > 500 GeV and |\u03b5| < 2.5 are retained. Different R-parameter values are adopted depending on the event\u2019s scalar ET. For ET > 1000, 1600, 2600 GeV they select R=0.8, 0.6, 0.4, \u03c9p = 0.1, 0.05, 0.05, and \u03c9r = 0.19, 0.19, 0.19, respectively. Moreover, the fat jets must also satisfy pT,J > 0.7 \u2192 ET/2. Once the final subjets are identified, they pass through some kinematic requirements. For jets with pT < 1000 GeV, these amounts to the requirement that the invariant mass of the final subjets must be within 145-205 GeV, and there must be two subjets with invariant mass in the window of 65 to 95 GeV. For fat jets with pT > 1000 GeV, the upper window is shifted to pT/20 + 155 GeV and pT/40 + 70 GeV respectively for top and W mass reconstruction. Additionally, the reconstructed W helicity angle must adhere to cos\u03d6h < 0.7 in both scenarios. For a detailed discussion, we encourage the interested reader to refer to the original paper [33]. For completeness, we present their final results in Figure 1.\n\nN-subjettiness is an inclusive jet shape originally introduced in reference [37]. It is designed to identify the energy deposition pattern inside a fat jet and quantify the major sources of these energies. In simpler terms, it quantifies the number of prongs of a fat jet. To define N-subjettiness,# 3.1 Image representation of jets and Preprocessing\n\nJet image, like the images we see in our day-to-day lives, is a grid of numbers. Each grid cell is called a pixel, and the number associated with each pixel is called the pixel intensity. Pixels in jet images can have various sources. The simplest and most common source is the calorimeter tower, defined as the sum of energy deposits in a given rapidity-azimuthal angle bin of the Electromagnetic and hadronic calorimeter. Another source can be the topoclusters [58], i.e., the clusters of calorimeter cells. The topoclusters help as most jet images are sparse with many empty pixels, and combining these empty pixels helps build a smaller image that requires a simpler neural network architecture. Finally, tracks can also be used as a source of pixels. Now, tracks have a much finer granularity than the towers and hence can provide much better recognition of physical features present in a jet. At the same time, analyzing a large image is computationally expensive. Therefore, a bunch of tracks in a given \u03b5 \u2197 \u03d1 direction are usually grouped to form simpler jet images. The CMS collaboration also uses ParticleFlow [59] algorithms to identify calorimeter energy deposits originating from charged particles (i.e., with associated charged tracks). These particle flow candidates can also be used to construct jet images.# 3.2 Convolutional Neural Network\n\nThough image representation of jets can be used as inputs to several NN-based architectures (See [60, 61] for the use of locally connected layers on image data), the most common and efficient ones are CNNs. This popularity is due to the translational invariance of convolution operations and the weight-sharing technique employed by CNNs (see the discussion below).\n\nCNNs use convolutional filters/grids containing weights that scan the input image. This scanning involves performing the inner product between different patches of the image (of the same dimension as the filter) and the filter. The output of this inner product serves as a new pixel for the output image. Once an inner product is performed, the filter is moved by certain units called stride, producing another pixel. This process is repeated till the full image is covered. The output pixels are combined to produce the response map. For uni-layered images, these filters are two-dimensional. On the other hand, for colored images, three-dimensional filters with depth equal to the number of image layers are used. The output of each convolution operation is one single layer of the response map; the response map\u2019s depth is determined by the number of filters used. The use of the same filter for different parts of the image is the weight-sharing mechanism mentioned earlier, and it is highly efficient in reducing the number of trainable parameters. After the convolution operation, the response map is passed through a non-linear activation to produce the activation map.\n\nThe convolution operation produces output images with sizes smaller than the input. This can cause trouble in constructing very deep networks. To overcome this, padding is performed where some empty pixel layers are added to the edges of the input image. This is usually achieved with the help of some pooling operation, usually max pooling or mean pooling. The pooling operation resembles the convolution, where a squared grid is moved across the image to produce the final output. However, the difference is the pooling filter does not contain any weights. Its only job is to combine the pixels through the pooling operation. After many such convolution and pooling operations, the output image is linearised and is passed through a series of fully connected layers called the decoding network to produce the final output (either a classification score or a regression value).\n\nAnother problem commonly encountered in deep CNNs is the so-called vanishing gradient problem (VGP). The updation of network parameters (weights and biases) in NNs happens through backpropagation, which implements the chain rule to determine the gradients of the loss function with respect to the network parameters. The network uses these gradients with the learning rate (which determines the step size) to update the parameters that minimize the loss function. In very deep NNs, the updating of parameters in initial nodes involves a product of a large number.# 3.3 CNN Architectures\n\nThis section discusses some of the state-of-the-art CNN architecture for top tagging. We plan to present our study in chronological order, mainly focusing on five image-based analyses: DeepTop [72], Upgraded Deeptop [73], ResNext-50 [74], CapsNet [75], and Bayesian networks [76]. In all the studies discussed below, the SM top pair production is used to generate the signal images, and QCD di-jet production provides the background sample unless mentioned otherwise.\n\nThe DeepTop CNN was originally introduced in reference [72]. It uses single-layered top images based on the calorimeter energy deposit as input. Signal and background fat jets in the transverse momentum range of 350-450 GeV and |\u03b5| < 1 are considered for the analysis. After detector simulation, the anti-kT fat jets with R=1.5 are reconstructed, and the calorimeter towers are used for image construction with the transverse energy serving as pixel intensity. The images are passed through a series of pre-processing steps: pixelization, translation, rotation, reflection, and scaling. We discussed translation, reflection, and rotation in the previous sections. In pixelization, the image is divided into 40 \u00d7 40 pixels, while scaling ensures all pixel intensities are in the range of 0.# 4.1 Graph representation of jets\n\nIn the previous section, we discussed the remarkable success of CNNs in separating boosted jets originating from the hadronic decay of tops from those originating from light quarks and gluons. The reason for this success is twofold: firstly, CNN architecture respects translational symmetry and can identify patterns in di\"erent parts of the image. Second, CNN\u2019s weight-sharing mechanism helps drastically reduce the number of trainable parameters. This suggests that NN architectures# Figure 8\n\nThe ROC curves for CapsNet and Rutgers DeepTop CNN for di-top tagging (left) and single top tagging (right) [75]. The blue-shaded region represents the uncertainty stemming from the use of di\"erent estimators to build the ROC curve.# ParticleNet\n\nLike PFN [84], ParticleNet [85] also employs the point cloud representation of jet constituents (particle cloud). However, unlike PFN, which follows the Deep Sets approach, ParticleNet implements edge convolution [92] that helps the architecture utilize the local structure of the constituents. The edge convolution operation is inspired by the weight-sharing and hierarchical learning features of CNNs.\n\nHowever, unlike images, the point clouds can have uneven shapes and do not have a grid-like representation. This makes it difficult to construct local patches for the convolution kernel to operate. Edge convolution solves this problem by using the k nearest neighbors to construct local patches in a point cloud. Analytically, for a given point x, the operation of edge convolution results in:\n\nHere, the sum runs over the k-nearest neighbors of the point x, i.e. {xi1 .. xik}. \"s are some learnable parameters parametrizing the function h, and \u21ad denotes the aggregation operator, designed to respect permutation equivariance (ParticleNet uses a mean aggregator). ParticleNet implements h using neural networks with parameters shared across the edges. The main blocks of ParticleNet are the EdgeConv blocks. These blocks first determine the k nearest neighbors of a particle using their position in the rapidity-azimuth plane. The edge features are then passed through the EdgeConv operation. ParticleNet also uses residual connections to pass the input features to the output of the EdgeConv operation. The combined output is passed through the subsequent EdgeConv block. Note that the second EdgeConv block utilizes the latent representation of the point cloud while determining the k nearest neighbors. In other words, the point clouds are dynamically updated, making ParticleNet a Dynamic Graph Convolutional Neural Network (DGCNN). ParticleNet implements three such EdgeConv blocks, each with k=16. After the EdgeConv blocks, the output undergoes a global average pooling followed by a decoding layer to produce the predictions. For complete details on the architecture, we urge the interested reader to consult the original paper [85]. The model performance is evaluated on the public top tagging dataset [80], and we present their results in Figure 10 and Table 1.# Table 1: Performance and architectural complexity of different GNN top taggers [89]\n\n|Architecture|Accuracy|AUC|1/\u21bc B|# Params|\n|---|---|---|---|---|\n|LGN|0.929(1)|0.964(14)|424 \u00b1 82|4.5k|\n|PFN|0.932|0.982|891 \u00b1 18|82k|\n|ResNeXt|0.936|0.984|1122 \u00b1 47|1.46M|\n|ParticleNet|0.938|0.985|1298 \u00b1 46|498k|\n|LorentzNet|0.942|0.9868|2195 \u00b1 173|220k|\n|PELICAN|0.9425(1)|0.9869(1)|2289 \u00b1 204|45k|\n\nThe results are calculated from the average of several training runs (the number of runs varies across networks, see Ref. [89]) with random network initialization. The numbers in the parenthesis represent the uncertainties over these runs.\n\nLorentz invariance is the fundamental symmetry of space-time governing elementary particle interactions. NN architecture respecting this symmetry can provide a relatively simple and physically interpretable design. Lorentz Group Network (LGN) [86] is based on the theory of finite dimensional representation of the Lorentz group and demonstrated for the first time the usefulness of including Lorentz equivariance in the construction of efficient GNN architectures with significantly fewer trainable parameters. LGN is based on the G-equivariant universal approximation theorem.that suggests that any equivariant map between two completely reducible representations of a lie group G can be realized through NNs using vector activations belonging to finite-dimensional representations of the group G. The permutation equivariant and Lorentz equivariant architecture is built by stacking several Clebsch-Gordon layers [86] on top of one another that perform CG decompositions on the activations of the previous layers. It first performs tensor products representing self-interactions and interactions among different particles. The CG operator acts upon these tensor products to decompose them into irreducible representations of the Lorentz group. Finally, an equivariant learnable operator mixes these decompositions to form the channels of the next layer. Apart from the CG layer, the architecture also includes an input layer, several MLP layers, and an Output layer. For the details of the model implementation, see [86]. The performance of the model is evaluated on the public dataset [80], and we present the model performance in Figure 10 and table 1.# Figure 10: ROC curves for the GNN classifiers [89].\n\nLorentzNet [87] is another permutation equivariant and Lorentz equivariant GNN architecture. Like the CG layers in LGN [86], the main building blocks of LorentzNet are called Lorentz Group Equivariant Blocks (LGEB). Their role is to define the edges of the graph and update the node coordinates and the node embeddings while respecting Lorentz equivariance. If we denote the node embeddings of the lth layer as hli (i=1,2...N) and the corresponding node coordinates in the Minkowski space as xli (I = 1,2...N), where N denotes the number of nodes then the action of the LGEBs can be summarised in three simple steps [87]:# Neighbourhood aggregation:\n\nmlij = \u03d1 e(h i l, hj l, \u21bd(||x i l\u2197 xj l || 2 , \u21bd(\u21d0x , x j l\u21d2))) li (4.5)\n\nwhere \u03d1 e is a NN, \u21bd(a) = sign(a)log(|a| + 1) is introduced to normalize large entries, ||a|| 2 is the Minkowski norm, and \u21d0a\u21d2 is the Minkowski inner product. LorentzNet does not assume any prior knowledge of interaction among the nodes; in other words, the graphs in LorentzNet are fully connected.# Updating the node coordinates:\n\nl+1 = xl i + c \u2211 \u03d1 x (ml ij ).x j\n\nx i (4.6)\n\nWhere the constant c is introduced to control the scale of x i l+1, \u03d1 x is another NN, and the sum is over the neighborhood of the node i.# Updating the node embeddings:\n\nl+1 = hi l + \u03d1 h(hi l , \u2211 w ij mlij)\n\nx i (4.7)\n\nHere, w ij = \u03d1 m (m ij ) is introduced to code the significance of the edge between node i and j, \u03d1 m and \u03d1 h are Neural Networks.\n\nApart from the LGEBs, LorentzNet also contains an input/encoding layer that transforms the input node embedding scalars to the latent space and a final decoding layer that uses the output of LGEBs to generate Network predictions. The model\u2019s performance is tested on the publicly available dataset [80], and we present the model\u2019s performance in Figure 10 and Table 1.\n\nSo far, PELICAN [88, 89] is the best-performing NN architecture for top-tagging. It is also a permutation equivariant and Lorentz equivariant GNN. Notably, PELICAN adeptly constructs a comprehensive set of Lorentz invariant functions, as per ref [94], from the four-momentum of jet constituents. These invariants can be constructed from the pairwise dot products of the four vectors, and for PELICAN, these dot products serve as the primary inputs. To ensure permutation equivariance, PELICAN followed reference [95, 96] to construct the complete list of equivariant aggregators (15 to be exact) that can transform the input dot products. Following aggregation, PELICAN further refines the aggregated values through scaling, employing a factor of (N/ N\u02dc ) \u03b5, where \u21c0 represents a learnable parameter, N signifies the count of particles in the event, and N\u02dc denotes a constant indicative of the typical number of particles expected in such events. The main building block of PELICAN is the equivariant block that contains the layers for message formation, followed by the aggregation block that applies the 15 aggregation functions discussed earlier. After five such equivariant blocks, the output passes through a decoding block that generates the model predictions. For a comprehensive understanding of the model architecture, interested readers are directed to reference [89]. Reference [89] also uses the public dataset [80] for the performance assessment of PELICAN. We present their results in Figure 10 and Table 1.# 5 Top Quarks and Physics Beyond the Standard Model\n\nAttempts to alleviate various shortcomings of the SM\u2014theoretical problems like the hierarchy problem, the flavour problem, the strong CP problem, the vacuum instability problem, as well# as experimental inconsistencies like the nonzero neutrino masses and mixing, the baryon asymme-# try of the universe, the presence of cold dark matter (DM) in the universe, and various flavour# anomalies\u2014have led to a plethora of theories or models going beyond the SM. Examples of such# theories or models are Grand Unification Theories (GUTs) [97\u2013103], supersymmetry [104\u2013109],# wrapped extra-dimensions [110\u2013113], technicolour models [114\u2013117], little Higgs theories [118\u2013122]# and theories featuring dynamical or elaborated spontaneous symmetry breaking [109, 123\u2013133].# As briefly discussed below, such models introduces various new particles: extra gauge bosons, ex-# tra scalars, leptoquarks, vector-like quarks, etc. In many scenarios, these new particles can have# preferential couplings to the third-generation fermions, in particular, the top quark. As such, at# the LHC, their production and decay could result in various top-enriched final states: exclusive# single top, top pair and multi-top, or those in association with the SM gauge bosons, leptons and# quarks. Motivated by such scenarios, numerous searches have been performed by various exper-# imental collaborations, particularly the CMS and ATLAS. At the LHC, with the exception of a# few mild excesses (see [134] for a review), the observations are found to be consistent with the SM# expectation. This has led to stringent limits on the new particles\u2019 masses and couplings. In fact, in# most of the cases, the limits have been pushed to the TeV scale, although not in full generality. For# TeV scale states, their decay products\u2014SM leptons, quarks and bosons\u2014could be highly Lorentz-# boosted that the jets emanating from them would be collimated. Consequently, the hadronically# decaying candidates (primarily, the top quarks in our case) are more likely to manifest as a single fat# jet rather than multiple resolved jets. In this section, without pretending to provide an exhaustive# and self-sufficient description of the new physics-induced top searches, we briefly discuss various# new physics scenarios contributing to the top-enriched final states at the LHC and summarise the# relevant LHC searches, including those targeting boosted top quarks in the final state.# A wealth of BSM models, such as Grand Unification Theories (GUTs) [97\u2013103], supersymmetry# [104\u2013109], wrapped extra-dimensions [110\u2013113], technicolour models [115\u2013117], little Higgs theories# [118\u2013122] and theories featuring dynamical or elaborated spontaneous symmetry breaking [109, 123\u2013133],# envisage the presence of extra gauge bosons (W \u2191 , Z ), with properties similar or different to# those of the SM gauge bosons (W, Z). Specifically, Z \u2191 bosons require the SM to be superseded with# at least an extra U (1) symmetry, while W\u2191 bosons require at least an extra SU (2) gauge group.# Some theories assume (or motivate) preferential couplings of W\u2191 , Z \u2191 bosons to the top-quark (or# the third-generation fermions). Examples of such theories include the top-colour [135\u2013138] and# top-flavour [116, 139, 140] models, and gauged-flavour symmetry models [141]. Some simplified# models for dark matter (DM) also predict Z\u2191 bosons mediating the interactions between DM and# normal matter, see Ref. [142] and references therein. Moreover, as possible explanation for the# recent flavour anomalies [143, 144], such models have been pursued with great interest, see for# Motivated by such theories, several dedicated searches for W\u2191 with right-handed or/and left-# handed charged current interactions and Z\u2191 bosons decaying to the third-generation quarks (a# top-quark and a bottom-quark or a pair of top-quarks, see Figure 11) have been performed by both# the CMS and ATLAS Collaborations; see for example [21, 147\u2013151] for W\u2191 searches, and [17, 18,# 23, 29, 146, 152, 153] for Z \u2191 searches. Considering leptonic final states, CMS has excluded left- and# right-handed W \u2191 bosons with mass below 3.9 and 4.3 TeV, respectively, at the 95% confidence level# (CL)[151]. On the contrary, with the all-hadronic final state considered by CMS, the resulting# limits are 3.4 TeV for both left- and right-handed W \u2191 bosons [150]. Considering both leptonic and# All the limits quoted here are valid under the assumption that the new particle has a narrow decay width, and# 5.3 Extra Scalars\n\nThough the properties of the SM Higgs reported so far have been largely consistent with the SM prediction, the minimality of the SM Higgs sector\u2014the presence of a single SU (2) L doublet scalar that simultaneously gives mass to the electroweak gauge bosons and all SM fermions\u2014is not guaranteed by any guiding principle or symmetry. As such, a plethora of models with the extended scalar sector have been proposed in the literature, including the addition of SU (2) L singlets [160\u2013162], doublets [163\u2013167] and triplets [168\u2013173]. Many of these models predict new pseudoscalar (A) and scalar (H) states and sometimes also a charged scalar (H \u00b1 ) coupling strongly to the third-generation quarks (see Fig. 13). The widely studied example of such a model is the two-Higgs-doublet models (2HDMs) [174]. In particular, the type-II variant of 2HDMs, akin to the Higgs sector of the minimal supersymmetric standard model (MSSM) [174\u2013181], predict such states predominantly decaying into t\u00af for t mA,H \u21ab 500 GeV and small tan \u03c2 (where tan \u03c2 is the ratio of the vacuum expectation values of the two Higgs fields).\n\nFigure 13: Feynman diagrams for the production of a pseudoscalar or scalar Higgs, denoted by a common symbol # (left) [182], and in association with a pair of top quarks (middle) [183], and charged Higgs in association with a top quark and a bottom quark (right) [184] with the Higgses decaying into third-generation quarks.\n\nFor the mass range 0.4\u20133 TeV, GKK width varies from 3% to 6% when the characteristic dimensionless coupling is set to 1 (\u03c9 is the curvature of the warped extra dimension and M Pl = MPl/\u21928\u03b5 is the reduced constant \u03c9/M Pl Planck mass). The branching ratio of GKK to tt\u00af increases rapidly from 18% to 50% for masses between 400 and 600 GeV, plateauing at 68% for masses larger than 1 TeV.# 5.4 Leptoquarks\n\nSimilarities between quarks and leptons in the SM, such as their transformations under the SM gauge groups, the number of generations and the hierarchy across generations, motivate a fundamental symmetry connecting them. Such symmetries are embedded in many BSM models, including GUTs [97\u201399], technicolour models [191], or theories of quark and lepton compositeness [192]. Such models predict the existence of \u201cleptoquarks\u201d (LQs) with spin-0 (scalar LQs) or spin-1 (vector LQs) that couple to both leptons and quarks simultaneously. LQs transform as triplets under the SU (3) C and carry fractional electric charges. For vector LQs, in addition, the coupling strength depends on the anomalous magnetic moment (\u03ba). The \u03ba = 1 limit refers to the Yang-Mills-type coupling scenario, and the \u03ba = 0 limit refers to the minimal vector coupling scenario, where the Yang-Mills-type couplings are turned off [193]. While in the minimal Buchmuller-Ruckl-Wyler (BRW) model [194], LQs are assumed to couple only to leptons and quarks from the same generation, cross-generational couplings with varying strengths are also possible. As such, LQs can generate lepton flavour universality-violating (LFUV) interactions and, therefore, have been pursued with great interest as a viable solution to the b-anomalies [195\u2013198] and the muon\u2019s anomalous magnetic dipole moment anomaly [199, 200].\n\nFigure 14: Leading order Feynman diagrams for the production of LQ pairs at the LHC [201].\n\nAt the LHC, LQs are produced singly or in pairs. Pair production proceeds via gluon\u2013gluon fusion and quark-antiquark annihilation mediated by the strong interaction (see Fig. 14). Several searches have been performed to find LQs decaying to a top quark and a lepton [15, 19, 20, 201\u2013].# 5.5 Vector-like Quarks\n\nAttempts to alleviate the fine-tuning or naturalness or hierarchy problem of the SM have led to the idea that the Higgs boson is a composite particle generated by a new strongly interacting sector at the compositeness scale much larger than the electroweak (EW) scale, and the gap between these two scales is explained by interpreting the Higgs boson as a pseudo-Nambu\u2013Goldstone boson (pNGB) associated with spontaneous symmetry breaking at the compositeness scale [114, 117, 128, 211\u2013215].\n\nOn this idea, based are the Composite Higgs models [114, 211, 212] and Little Higgs models [118\u2013121]. Such models usually follow a sequential symmetry-breaking pattern: a large global symmetry group above the compositeness scale is spontaneously broken to a smaller group, which is explicitly broken to the SM EW group. Such models, therefore, naturally accommodate several new particles, in particular, new fermionic resonances called vector-like quarks (VLQs): colour-triplet spin-1/2 fermions with both left- and right-handed chiral components transforming identically under SU (2). Renormalisability and gauge completeness restrict the SU (2) representation of the VLQs to 1, 2 and# 5.6 Supersymmetry\n\nSupersymmetry (SUSY) theories have been accepted as one of the most motivated theoretical constructs going beyond the SM. For one, SUSY, an extension of the Poincar\u00e9 space-time symmetry, relates bosons and fermions, thereby, as a consequence, solving the hierarchy problem of the SM. SUSY theories are promising candidates for a unified theory, as, within such theories, the measured gauge couplings extrapolated from the EW scale through the SUSY renormalization group equations unify at a GUT scale. Also, the lightest SUSY particle can serve as a DM candidate in the presence of an additional conserved quantum number, such as R-parity that distinguishes SM states from their SUSY partners. Moreover, being the only possible extension of Poincar\u00e9 space-time symmetry group, SUSY is a very likely description of Nature if one considers the sheer elegance of SUSY as a theory.\n\nThe SUSY phenomenology, to a large extent, is driven by the presence of R-parity. In most of the SUSY models, as we do here, the conservation of R-parity is assumed. The R-parity-violating scenarios are discussed in a separate section (see Sec. 5.7). This has two significant implications for phenomenology. For one, SUSY particles are produced only in pairs. Second, it ensures the stability of the lightest SUSY particle (LSP), thus implying that each SUSY particle produced will entail the LSP at the end of its decay chain. The LSPs, much like the SM neutrinos, leave the detector undetected. Therefore, their presence is usually sought for as missing transverse momentum pTmiss. Note that, depending on the underlying mechanism of SUSY breaking, different models predict different SUSY particles as the LSP, thereby leading to different phenomenology. For example, the minimal super-gravity models predict the lightest neutralino as the LSP, while the gauge-mediated SUSY breaking models predict the nearly massless gravitino as the LSP. A detailed discussion of various models is beyond the scope of this paper. The phenomenological mentions hereinafter is based on the simplified models proposed by the LHC New Physics Working Group and the phenomenological minimal supersymmetric model (pMSSM). In particular, the scenarios leading to top-enriched final states are mentioned. And, unless otherwise stated, the lightest neutralino is assumed to be the LSP.\n\nAmong the SUSY productions, the pair production of gluinos dominates for a given mass scale, followed by squarks, sleptons, and sneutrinos. Both CMS and ATLAS have performed numerous dedicated searches looking for the SUSY particles. Of these, the ATLAS search targeting gluinos decaying into a pair of top-quarks and the lightest neutralino in final states with missing transverse momentum has been significant.\n\nR-parity is defined as R = (\u21911)3(B\u2192L)+2S, where S is the spin of the particle, and B and L are the baryon and lepton numbers. The SM particles have R-parity of +1, while their SUSY partners have R-parity of -1.# Figure 18: Examples of signal diagrams for the simplified RPV models considered in the ATLAS analysis [287].\n\nFor simplicity, particles and anti-particles are shown using the same symbols, omitting the anti-particle notation.\n\n( \u02dc 1 \u25c1\u00b1) states that are mass-degenerate and carry dominantly higgsino components. Here, the strong production of stop pairs can give rise to a final state with a high jet multiplicity (See Figure 19). The absence of any significant deviations from the Standard Model predictions prompts the establishment of a 95% confidence level upper limit on the stop mass, capped at 950 GeV within the region where m \u02dc t\u2197 m \u02dc 1,2 , \u02dc 1\u03d10 \u03d1\u00b1 \u2194 mtop, an exclusive sensitivity zone for this analysis. Meanwhile, the CMS collaboration also investigates stop pair production [289], where each stop decays into four quarks via an intermediate Higgsino-like LSP. This analysis has effectively ruled out stop masses ranging from 100 to 720 GeV, with the Higgsino mass set to 75% of the stop mass.\n\nThe ATLAS analysis [239] explored a final state characterized by same-sign leptons, multiple jets, and significant missing transverse momentum (depicted in Figure 20a). This search excludes gluino masses below 1.6 TeV for t\u02dc masses up to 1.2 TeV. Subsequently, another ATLAS study [290] revisited a similar final state, further refining the analysis. Here, the previously established bound on gluino mass is extended to 1.65 TeV, with a Stop mass requirement below 1.45 TeV. The ATLAS analysis [291] has also looked into the same sign lepton final state but for the case of a higgsino-like \u25c10/ \u02dc 2 \u02dc 1 \u25c10 (See Figure 20b). The search excludes \u25c1 1 \u02dc0/ \u02dc 2 \u25c10 masses up to 200 GeV. Furthermore, the CMS analysis [260] also investigated the same-sign lepton final state within two simplified RPV scenarios. Both scenarios entail strong production of gluino pairs. In the first scenario, each gluino is assumed to decay into four quarks and a lepton. The non-observation of any excess over the SM background allowed the analysis to exclude gluino mass up to 2.1 TeV. Conversely, the second scenario involves gluino decay in the \u02dc \u2198 tbs channel. This scenario was able to rule out gluino mass up to 1.7 TeV.\n\n\u2013 27 \u2013Figure 19: Diagrams of the signal processes involving pair production of top squarks t\u02dc [288].\n\nFigure 20: Examples of signal diagrams for the simplified RPV models considered in the ATLAS analysis [287]. For simplicity, particles and anti-particles are shown using the same symbols, omitting the anti-particle notation.# 6 Summary and Outlook\n\nThe application of advanced machine learning techniques in the field of top tagging has seen significant progress in recent years. In this review, we have tried to summarise some of the recent developments in top tagging algorithms and their possible applications in the field of high-energy physics.\n\nThe paper begins with a discussion of various top taggers, focusing on high-level feature-based classifiers, Convolutional Neural Networks (CNNs), and Graph Neural Networks (GNNs). We have hand-picked some algorithms in each category and demonstrated their performance for top tagging. It\u2019s important to note that all findings discussed herein are borrowed from their respective sources. Due to variations in datasets used by different algorithms, a direct comparison of their performance isn\u2019t feasible. Nevertheless, our exploration offers valuable insights into their efficacy for the task at hand.\n\nThe second part of the paper discusses various BSM scenarios that can lead to a final state# topology with boosted top quarks at the LHC\n\nWe have also discussed the bounds on these scenarios from previous collider searches. In all these cases, an efficient Identification of the final state top quark can help drastically reduce the SM background. It can lead to a possible discovery or even stronger constraints on the model parameter space. We must mention that numerous studies have already implemented different top tagging techniques for the study of BSM physics. However, a review of those studies is beyond the scope of our present discussion.",
        "context_id": 28,
        "question": "What collider is described as a \"top factory\" due to its substantial tt\u00af production cross-section?",
        "answer": [
            "LHC"
        ],
        "context_length": 41308
    },
    {
        "context": "# 1 Introduction\n\nAgriculture is the largest consumer of freshwater globally, accounting for 70% of the total supply [1]. The growing freshwater scarcity crisis, intensified by climate change and rapid population growth, has placed significant strain on global freshwater resources. While agriculture is significantly affected by this scarcity, it also contributes to the problem through its extensive water use. To address this challenge, implementing efficient water management strategies in agricultural irrigation is essential for mitigating the water scarcity problem and ensuring the sustainability of the agricultural sector. Closed-loop irrigation scheduling, which utilizes feedback to provide precise water amounts to crops at optimal times, has emerged as a promising solution for achieving efficient water management in irrigation.\n\nScheduling, in general, involves the optimal allocation of finite resources over a specified time horizon to achieve a particular objective. Irrigation scheduling, typically conducted on an hourly or daily basis, is important for determining the optimal amount of water to supply to crops and the appropriate timing for irrigation. In the context of daily irrigation, which is the primary focus of this work, the objective is to identify the specific days within a given scheduling horizon on which irrigation should be performed and to determine the precise amount of water to be applied during each irrigation event. Recently, irrigation scheduling approaches have evolved to incorporate management zones (MZs) in their designs. MZs are defined as distinct areas within a large-scale field characterized by uniform soil and crop properties. By integrating MZs, irrigation scheduling schemes can better account for the significant spatial variability present in agricultural fields. When daily irrigation scheduling is required for a field with multiple MZs, the task expands to determining the specific days within the scheduling horizon for irrigation events and identifying the optimal irrigation amounts for each MZ.\n\nBy definition, addressing the daily irrigation scheduling problem involves discrete decision-making, particularly in determining which days within the scheduling horizon should be designated for irrigation. This discrete decision-making process can be represented by assigning a binary decision variable to each day within the scheduling horizon. A value of 1 indicates that irrigation should be performed on that specific day, while a value of 0 signifies that no irrigation should occur. Alongside these discrete decisions regarding the timing of irrigation, continuous decisions must also be made concerning the precise amounts of water to be applied to the various MZs on the days when irrigation is scheduled. To effectively manage both discrete and continuous decisions, and to# Mixed-Integer MPC for Irrigation Scheduling\n\nLeverage the success of model predictive control (MPC) in addressing scheduling problems across various domains [2, 3, 4, 5, 6], mixed-integer MPC has emerged as an attractive optimal control framework for solving the daily irrigation problem.\n\nMixed-integer MPC for irrigation scheduling, initially proposed in [7], leverages agro-hydrological models such as the Richards\u2019s equation, water-balance models, which describe soil moisture dynamics in agricultural fields, to determine irrigation schedules. These schedules involve \u2018yes/no\u2019 decisions for irrigation timing and continuous decisions for irrigation amounts, aiming to maintain soil water content within an optimal range to promote crop growth while minimizing both fixed and variable costs associated with the irrigation process. This optimal control framework has been evaluated under various weather conditions and irrigation management practices, demonstrating its ability to conserve irrigation water while enhancing crop yield compared to the widely used triggered irrigation scheduling approach. However, despite its promise, several near-term improvements, particularly in enhancing the computational efficiency of the mixed-integer MPC-based scheduler, are necessary to facilitate its adoption in practical irrigation settings.\n\nMixed-integer problems are classified as NP-hard. Consequently, despite advancements in optimization techniques and computing power, directly applying mixed-integer solvers to the mixed-integer problems arising in mixed-integer MPC-based irrigation scheduling is only feasible for small-scale scenarios. The requirement to generate irrigation schedules within a reasonable time frame, coupled with the need to extend mixed-integer MPC formulations to fields comprising several MZs, necessitates the development of efficient solution methods.\n\nSeveral approaches have been proposed to address the computational challenges associated with mixed-integer MPC-based schedulers. Notably, the initial work [7] that introduced mixed-integer MPC for daily irrigation scheduling employed two key strategies to enhance computational efficiency. First, instead of directly using a mechanistic agro-hydrological model, such as the Richards equation\u2014which was found to render the mixed-integer MPC framework intractable for moderate prediction horizon values\u2014a long short-term memory (LSTM) network representation of the agricultural field was utilized within the mixed-integer MPC framework. The use of this surrogate model significantly improved the computational efficiency of the framework. To further enhance efficiency, the study proposed an approach that transformed the mixed-integer nonlinear problem (MINLP) resulting from the MPC formulation into a nonlinear program (NLP). Specifically, the logistic sigmoid function was employed to represent the binary decision variables. This transformation from MINLP to NLP enabled the problem to be solved within a reasonable time frame while.# Current Research on Mixed-Integer MPC-Based Irrigation Scheduling\n\nAlso accommodating multiple MZs in the framework. However, despite these benefits, the use of the logistic sigmoid function introduced challenges, including approximation errors and difficulties in result interpretability. Determining the appropriate slope for the sigmoid function proved to be nontrivial, and large slopes often led to ill-conditioned optimization problems.\n\nTwo recent studies [8, 9] have leveraged the success of reinforcement learning (RL), particularly multi-agent RL (MARL), to enhance the computational efficiency of mixed-integer MPC-based irrigation schedulers. In these approaches, multiple RL agents were trained for the various MZs within the field. These agents were tasked with determining the irrigation timing across the scheduling horizon, allowing decentralized MPCs with continuous controls to be used for calculating the irrigation amounts for each MZ. These RL-based approaches effectively addressed the interpretability issues associated with the logistic sigmoid function and facilitated the use of parallel computing to solve the decentralized MPCs, thereby improving overall computational efficiency.\n\nWhile these RL-based approaches have shown promise in enhancing computational efficiency and addressing interpretability issues, they come with some limitations. One notable challenge is the significant computational resources required for training multiple RL agents, particularly in fields with a large number of MZs. The complexity of the training process can lead to prolonged training times and increased computational costs. Moreover, if there is a need to redesign the reward function, which is common for irrigation scheduling since different management practices which affect the reward of the agents have to be carried out during a particular growing season, the agents must be retrained, which can be a time-consuming process. Given the downsides associated with using RL agents to address the computational challenges of mixed-integer MPC-based schedulers, exploring efficient solution approaches that do not rely on RL remains of vital importance.\n\nAs previously mentioned, the objective of the mixed-integer MPC-based irrigation scheduler\u2014to maintain root zone soil moisture within an optimal range while minimizing fixed and variable irrigation costs\u2014is typically modeled with a quadratic cost function. However, the nonlinear nature of the models used to describe soil moisture dynamics\u2014such as the Richards equation or an LSTM network representation\u2014results in a MINLP. By exploring surrogate models that can be represented with linear constraints, it is possible to reformulate the problem as a Mixed-Integer Quadratic Program (MIQP), which is a simpler and more tractable class of MINLPs.\n\nA ReLU (Rectified Linear Unit) network is a neural network where the ReLU activation function is applied to the neurons, except for those in the output layer. These networks have been used to model complex relationships and have been found to outperform other activation functions, such as...# MPC with ReLU surrogate model in practical irrigation scheduling settings# 2 System Description\n\n|Soil Type B|Crop Type B|Crop Type A|\n|---|---|---|\n|Soil Type A|Soil Type A|Soil Type A|\n|MZ 1|MZ 2|MZ|\n|Delineation|Delineation| |\n|MZ 3|MZ 4| |\n\nFigure 1: A schematic diagram of a spatially variable field with variability in crop and soil. The field is divided into 4 distinct MZs, each with uniform soil and crop properties.\n\nThe system studied in this work is a large-scale agro-hydrological field characterized by significant spatial variability in attributes such as soil type, crop type, and elevation. To account for this spatial variability, the field is divided into distinct MZs. Figure 1 illustrates a field with variability with respect to soil and crop. Given the presence of 2 soil types and 2 crop types, and considering that each MZ should have similar soil and crop properties, the field is delineated into 4 distinct MZs based on these attributes.\n\nThe proposed soil moisture modeling framework is particularly well-suited for a delineation approach that incorporates key attributes such as elevation and soil hydraulic parameters. Elevation influences the movement and distribution of water across agricultural fields, thereby affecting irrigation efficiency. Similarly, soil hydraulic parameters directly impact soil moisture dynamics and plant available water. A delineation approach that incorporates soil hydraulic parameters allows for the use of mechanistic models, which rely on these parameters as key inputs, to accurately model soil moisture dynamics within the various MZs. Additionally, incorporating elevation ensures that the resulting MZs have relatively flat profiles, enabling the use of mechanistic models that focus on vertical soil moisture dynamics while ignoring lateral movements.\n\nThe MZs in large-scale fields are generally known to exhibit weak interactions with each other, meaning that the impact of adjacent zones on each other\u2019s moisture dynamics is minimal.|Y k =|<br/>y k,2|y k,1|\n|---|---|---|\n|H(x k,1 , \u03c9 1 ) + v k,1|H(x k,2 , \u03c9 2 ) + v k,2|\n|...|...|\n|y k,M|H(x k,M , \u03c9 M ) + v k,M|\n\nwhere \u03c9 i represents the set of hydraulic parameters that are representative of the soil type in MZ i. Additionally, x k,i, u k,i and y k,i represent the state, input and output vectors of MZ i at time instant k, while \u03c2 k,i and v k,i represent the uncertainties in the state and output equations, respectively.# 3 Problem Formulation\n\nThis work addresses the daily irrigation scheduling problem for a large-scale agricultural field delineated into distinct irrigation MZs. The objective is to develop a scheduling framework that determines both the irrigation timing (which is encoded with a binary variable) and the specific irrigation application amounts for each zone. The problem is formulated as follows:# Given:\n\n- Scheduling Horizon: The known number of days for which the irrigation needs to be scheduled.\n- Management Zones (M): The field is divided into M distinct MZs.\n- Weather Predictions: Daily predictions of reference evapotranspiration and precipitation, essential for determining the water needs of crops.\n- Crop Information: This includes the crop coefficient, derived from empirical relations calibrated specifically for the crop and field under study.\n- Soil Moisture Content: The initial distribution of soil moisture content within the rooting depth across each MZ at the start of the scheduling horizon.# Determine:\n\n- Irrigation Timing: The days within the scheduling horizon on which irrigation should be performed. In line with standard irrigation practices, the timing should be uniform across all MZs of the field.# Irrigation Amounts:\n\nThe daily irrigation amount for each MZ on each day within the scheduling horizon. The scheduler should ensure that the irrigation amounts align with the timing\u2014prescribing zero irrigation amounts on days when irrigation should not be performed and non-zero amounts on days when irrigation is scheduled.\u03c6 (\u00b7) and \u00af (\u00b7) \u03c6 are slack variables that are introduced to relax the target zone (\u21bc (\u00b7), \u21bc\u00af (\u00b7)). For a MZ j, the maximum and minimum irrigation amounts that can be supplied during a particular irrigation event are denoted with \u00af (\u00b7uirr) and u (\u00b7) irr, respectively. The binary variable c k is used in P N,M (d) to encode the irrigation timing, indicating whether the irrigation event should be performed on day k within N. Equation (9g) ensures that the irrigation timing is applied uniformly across the M MZs that make up the field, in accordance with the daily irrigation scheduling problem. The terms Q (\u00b7) and \u00af (\u00b7) Q are the per-unit costs associated with the violation of the lower and upper bounds of the target zone, respectively. The term R c is the fixed cost associated with the operation of the irrigation system, and R u is the per-unit cost of the irrigation amount uirr. The term \u02c6 k u denotes the uncontrollable inputs (namely the reference evapotranspiration, rain, crop coefficient and rooting depth) for day k, while M(\u00b7) is a linear function used to calculate the root zone soil moisture content (\u03d6 RZ) in each MZ. This calculation considers the spatial distribution of soil moisture (y) in the soil column of each MZ. The weighting approach applied assigns 40% weight to the average volumetric moisture content in the upper quarter of the rooting depth (z r), 30% to the second quarter, 20% to the third quarter, and 10% to the last quarter. Note that this weighting approach reflects the relative importance of moisture at different depths within the rooting depth. According to Equation (9e), the initial states, in each MZ, required for the evaluation of P N,M (d), are estimated. Specific details concerning the estimation of the initial soil moisture content are provided in Section 7.1.# Equations\n\nThe terms \u00af j and \u21bc j (also known as the threshold volumetric moisture content \u03d6 j th) are calculated as follows:\n\n|\u21bc\u00af j|=|\u03d6 jFC|\n|---|---|---|\n|\u21bc j|=|\u03d6 j TH= \u03d6 j FC\u2192 [MAD \u2197 \uf8eb\u03d6 j FC\u2192 \u03d6 jWP\uf8f6|\n\nwhere \u03d6 j FC is the volumetric moisture content at field capacity for MZ j, \u03d6 j WP represents the volumetric water content at the wilting point for MZ j, and MAD refers to the management allowable depletion, which indicates the fraction of the total available water that is permitted to be depleted.\n\n11# Enhancing the Computational Efficiency of P N,M (d) Using a ReLU Neural Network\n\nWhile P N,M (d) provides an optimal control framework for addressing the daily irrigation scheduling problem comprehensively, it presents significant computational challenges. These challenges primarily stem from the inherent complexity of mixed-integer problems. Moreover, using the Richards equation to represent soil moisture dynamics in P N,M (d) adds further complexity. The Richards equation is a highly nonlinear partial differential equation, and its nonlinearity, combined with its dependence on complex soil hydraulic functions, makes it computationally expensive to solve, particularly when integrated into optimization frameworks. Additionally, the numerical solution of the Richards equation requires discretization in both time and space, transforming it into a large system of algebraic equations. This discretization increases the problem\u2019s dimensionality. These complexities are further compounded when P N,M (d) is applied to large-scale fields with multiple MZs, where numerous variables must be considered simultaneously.\n\nPast studies seeking to enhance the computational efficiency of P N,M (d) have explored replacing the Richards equation with machine learning surrogate models like LSTM networks.\n\nTo further enhance the computational efficiency of P N,M (d), this work proposes using a feed-forward ReLU neural network to model soil moisture dynamics in each MZ of the field, instead of machine learning surrogate models like the LSTM network, for two main reasons. First, compared to the Richards equation, which is highly nonlinear and requires complex functions for its numerical solution, the ReLU network is a simpler model. Therefore, replacing the Richards equation with a ReLU neural network in P N,M (d) can potentially reduce the solution time.\n\nSecond, a feedforward ReLU neural network can be exactly represented using MIL constraints. Incorporating the MIL constraint representation of the ReLU neural network within P N,M (d) further enhances computational efficiency. Notably, the cost function of P N,M (d) is quadratic, and, except for the constraints describing soil moisture dynamics across the MZs (i.e. Equations (9b) and (9c)), all other constraints in P N,M (d) are linear. Therefore, adopting a MIL representation of the ReLU neural network transforms the problem into a MIQP problem, which is the simplest class of MINLP problems. The resulting MIQP problem can then be solved using solvers such as Gurobi.\n\nBased on the foregoing, this section describes how feedforward ReLU neural networks, which model the soil moisture dynamics in the various MZs of the field, can be trained and integrated into P N,M (d) to enhance its computational efficiency. The section begins with an overview of ReLU# 5.2 ReLU Network Development\n\nThis section details the development of a ReLU neural network for modeling soil moisture dynamics across each MZ of the field. It begins with an overview of the ReLU neural network architecture designed for each MZ, followed by a discussion on the generation of the training data used for neural network training. The section concludes with a description of the methodology employed to evaluate the predictive performance of the trained neural network.# 5.2.1 ReLU Neural Network Design\n\nFor each MZ, a ReLU network is trained to make daily predictions of the root zone volumetric moisture content. In this work, the universal approximation property of neural networks is leveraged to directly model the root zone soil moisture (a scalar value), which is calculated as a weighted sum (i.e. the linear function M(\u00b7) in P N,M (d)) of the soil moisture contents at various depths in the soil column. This approach differs from existing soil moisture surrogate modeling methods, such as the one presented in [29], where multiple neural networks are trained to predict soil moisture dynamics at different depths, and the root zone soil moisture is subsequently computed as a weighted sum of these individual predictions. The proposed ReLU neural network approach offers a more compact model that can further enhance the computational efficiency of P N,M (d).\n\nIn particular, the ReLU neural network is trained to make one-day-ahead root zone soil moisture content predictions. To make these predictions, it employs both current (k) and past inputs (k \u2192 1, k \u2192 2, ..., k \u2192 l), where l represents the time lag (in days). The specific inputs to the network include the root zone volumetric water content \u03d6RZ, precipitation I (the sum of rain R and irrigation amount), crop coefficient Kc, reference evapotranspiration ET0, and the rooting depth zr.\n\n\u03d6RZk+1 = N ({\u21c0} kk\u2192l, \u21c1) (14)\n\nwhere {\u21c0} kk\u2192l := [\u21c0 k\u2192l , \u21c0 k\u2192l\u21921 , .., \u21c0 k ], \u21c0 \u2191 [\u03d6RZ, Kc, ET0, I, zr] and I = uirr + R. In Equation (14), \u21c1 is a compact representation of the weights and biases of the ReLU neural network N.\n\nBased on Equation (14), the input vector to the ReLU network contains 5(l + 1) elements, and its output contains 1 element. Thus, the terms n0 and nL+1 defined in Section 5.1 are equal to 5(l + 1) and 1, respectively.\n\nSince the ReLU neural network directly models the root zone soil moisture content in each zone, Equation (14) approximates and replaces Equations (9b), (9c), and (9d) in P N,M (d). Furthermore,# 5.2.2 Training Data Generation\n\nIn this study, open-loop simulated data from the calibrated 1D Richards equation for each MZ are used to train a ReLU neural network specific to that MZ. The calibration process involves using soil moisture observations from the field to estimate the soil hydraulic parameters, which are then employed to calibrate the Richards equation. This calibration ensures that the resulting Richards equation accurately describes the soil moisture dynamics of the field.\n\nFor a carefully chosen initial state\u2014specifically, the converged soil moisture estimates from an offline data assimilation process in this work\u2014the training dataset is generated by solving Equations (5) and (6) for randomly generated input uk trajectories. Noise is included in the open-loop simulations to account for model uncertainty and to enhance the robustness of the neural network. The use of randomly generated inputs is employed to ensure that the resulting time series dataset encompasses the dynamics of soil moisture under a wide range of conditions.\n\nNote that since the ReLU neural network is trained to directly predict the root zone soil moisture content, the function M outlined in formulation PN,M(d), together with the spatial volumetric moisture content at each time instant, is used to calculate the corresponding root zone soil moisture content for the training dataset before the ReLU neural network is trained.# 5.2.3 ReLU Neural Network Evaluation\n\nTo evaluate the predictive performance of the trained ReLU neural network, a comparison with the calibrated Richards equation is conducted. In this study, a specific initial state is chosen, and predetermined trajectories of the inputs uk are defined for a given period.\n\nBoth the identified ReLU neural network and the calibrated 1D Richards equation are then simulated using the selected initial state and predetermined input trajectories. The predictions from the ReLU neural network are compared to the root zone soil moisture content computed by the calibrated Richards equation, using the root mean squared error (RMSE) metric.# 6 Study Area\n\nThe mixed-integer MPC scheduler with ReLU neural network was applied to a specific quadrant of a large-scale circular field, highlighted by the red rectangle in Figure 2(a). This study area# 6.1 Surrogate Modeling of the Study Area\n\nPrior to generating the open-loop data for training the 3 ReLU neural networks for P N,M (d), the soil moisture dynamics in each of the three MZs of the study area were modeled using the 1D Richards equation. For each MZ, the 1D Richards equation was calibrated with the centroidal soil hydraulic parameters specific to that zone. Subsequently, extensive open-loop simulations of these calibrated Richards equations were conducted, incorporating randomly generated inputs.\n\nThe neural network training stage incorporated two different rooting depths: 0.5 m and 1.00 m.# Performance Evaluation\n\nTwo main studies were conducted to assess the performance of the mixed-integer MPC with the ReLU neural network scheduler. In the first study, referred to as Case Study 1, the computational benefits of employing the ReLU neural network were investigated. In this study, the mixed-integer MPC with the ReLU neural network was used to provide irrigation schedules for the 2015 and 2022 growing seasons, which were specifically chosen to assess the schedulers under relatively dry and wet conditions, respectively. The reference evapotranspiration and the average temperature trajectories for the 2015 and 2022 growing seasons are depicted in Figure 6 of Appendix D.1.\n\nSimilarly, the mixed-integer MPC incorporating the LSTM network, originally proposed by [7], was also employed to generate schedules for the same growing seasons. This approach was chosen for two key reasons: first, the LSTM-based framework was found to significantly enhance computational efficiency compared to the use of the Richards equation PN,M(d); second, the mixed-integer MPC with LSTM demonstrated better performance than the widely used triggered irrigation scheduling approach in terms of water savings and enhancing optimal crop yield. The two approaches were compared in terms of the average solution time of PN,M(d). Additionally, the approaches were compared based on total prescribed irrigation and Irrigation Water Use Efficiency (IWUE), defined.# 7.1 Simulation Settings of Case Study 1\n\nTable 8 of Appendix (C) provides the relevant parameters for the formulation P N,M (d) used in this case study. It is important to note that the per-unit costs were treated as tuning parameters in this study.\n\nIn the mixed-integer MPC with LSTM models, the LSTM models were trained according to the approach proposed in [9]. Specific details for training the LSTM networks are omitted due to space constraints; interested readers may refer to [9] for the specific training details. It is important to note that the same training data were used to train both the LSTM and ReLU neural network models for the three MZs of the study area. Additionally, the ReLU neural network design outlined in the Section 5.2.1 was adopted for the training of the LSTM networks for each MZ. However, the tuning of hyperparameters and the determination of the time lag l for the training of the LSTM and ReLU networks were conducted independently, as each network has different characteristics for achieving optimal performance.\n\nIn both scheduling approaches considered in this study, the simulation period spanned from May 5th to September 4th, for the 2015 and 2022 seasons. Throughout the evaluation process, a rooting depth of 0.5 m was considered from May 5th to July 15th, after which a rooting depth of 1.0 m was used until the end of the growing season. This trajectory of the rooting depth aligns with standard irrigation management for the soft spring wheat crop. A prediction horizon (and control horizon) of 7 days was employed during the evaluation of the two scheduling approaches.# Simulation Experiments\n\nSeveral simulation experiments demonstrated that a 7-day prediction horizon effectively captures the essential dynamics of the study area. This period also allows the scheduler to utilize the most accurate weather forecasts available.# Soil Moisture Content\n\nThe initial root zone soil moisture content in the three MZs of the field, required for evaluating the two scheduling approaches, was not assumed to be known. Instead, the study simulated the presence of a remote sensor in each MZ, reflecting the actual field conditions where microwave remote sensors are mounted. These sensors provided daily soil moisture observations corresponding to the average moisture content in the top 25 cm of the soil column. The soil moisture content was estimated from these daily observations using the EKF, which was chosen due to its demonstrated effectiveness in accurately estimating soil moisture content in agro-hydrological systems [31, 32]. The specific EKF design for each MZ is detailed in Appendix E.# Evaluation of Scheduling Schemes\n\nOn the first day of the evaluation period, the two scheduling schemes were evaluated using the initial guess of the soil moisture distributions for the various MZs, along with the 7-day weather and crop information predictions. Although the actual weather information for the growing seasons was known during this simulation, uncertainty was incorporated into the weather forecasts used in evaluating the agents and in the MPC. This uncertainty was modeled as a normal distribution with a mean of 0 and a specified standard deviation. As the prediction horizon extended further into the future, the standard deviation values were gradually increased to reflect the growing uncertainty associated with longer-term weather predictions.# MPC Solutions\n\nThe mixed-integer MPC with ReLU neural networks was solved using the Gurobi solver, while the mixed-integer MPC with LSTM networks was solved using the BONMIN solver. Additionally, the OMLT package [33] in Python was used to translate the trained ReLU neural networks into the MIL constraints, and the resulting MIQP was solved in Pyomo [34].# Irrigation Implementation\n\nThe irrigation amounts obtained from solving the two MPCs were implemented in a receding horizon fashion, where the first irrigation amount was applied to the actual field, and the rest were discarded. The actual field conditions were represented using three well-calibrated 1D Richards equations. The initial soil moisture contents in the MZs of the actual field were initialized with the converged soil moisture estimates obtained from the offline state and parameter estimation outlined in Section 6.# Measurements and Noise\n\nOn the second day, measurements corresponding to the average soil water content in the top 25 cm were obtained for each MZ from the actual field. To account for sensor noise, noise from a Gaussian distribution with a mean of 0 and a standard deviation of 0.0008 was added to these measurements, reflecting the measurement noise observed in the microwave radiometers mounted on.# 8 Results and Discussion\n\nIn this section, the results of the case studies are presented and analyzed in detail. The section begins with a discussion of the outcomes from the validation experiment conducted on the trained ReLU neural networks. This is followed by a presentation and analysis of the results comparing the mixed-integer MPC with ReLU neural networks to the mixed-integer MPC with LSTM networks. Finally, the section concludes with a detailed discussion of the results comparing the mixed-integer MPC with ReLU neural networks to the triggered irrigation scheduling approach.# 8.1 Predictive Performance of the Identified ReLU Neural Networks\n\nThe results of the evaluation of the predictive performance of the ReLU networks, conducted over a 25-day period, are depicted in Figure 3. From this figure, the soil moisture predictions can be considered accurate, as there is a strong agreement between the predicted root zone soil moisture content and the actual values derived from the calibrated 1D Richards equation. This accuracy is further confirmed by the low RMSE values shown in Table 1, which highlight the good predictive capacity of the ReLU neural networks for root zone soil moisture content across all three MZs.\n\nThe results demonstrate that the proposed ReLU neural network modeling approach for root zone soil moisture in the MZs of the study area is effective. These findings are consistent with other studies that have utilized ReLU neural networks to describe soil moisture dynamics in agro-hydrological systems [14, 15]. Furthermore, these results suggest that the ReLU neural networks\u2019 capacity for universal approximation can be leveraged to directly model root zone soil moisture content, potentially enhancing computational efficiency when employed in schedulers that require the solution of a numerical optimization problem. This approach contrasts with earlier methods [29], where separate neural network models were trained to predict soil moisture content at different depths within the soil profile, and a weighted approach was subsequently used to estimate root zone soil moisture content from these predictions.\n\nThe results obtained from the validation experiment further highlight the accuracy of the identified...# 8.2 Comparison of PN,M(d) with ReLU and PN,M(d) LSTM Approaches\n\nIn this experiment, the ReLU networks, originally trained to perform one-step-ahead predictions, were employed recursively over a 25-day period. This recursive strategy involved using the output from one time step as an input for the next. This property is particularly desirable in MPC frameworks because MPC relies on the ability to make accurate predictions over the prediction horizon to determine optimal control actions.# Figure 3: Predictive performance of the identified ReLU neural networks.\n\n|Year|Total Irrigation|IWUE|Average Solution Time|\n|---|---|---|---|\n|2015| | | |\n|2022| | | |# Table 1: Predictive performance of the trained ReLU Neural Networks over a period of 25 days.\n\n|Model|RMSE|\n|---|---|\n|MZ 1|0.0028|\n|MZ 2|0.0025|\n|MZ 3|0.0022|# Table 2: Comparison between the P N,M (d) with LSTM and P N,M (d) with ReLU neural network for the 2015 season.\n\n| |P N,M (d) with LSTM|P N,M (d) with ReLU|\n|---|---|---|\n|Total irrigation (m)|0.984|0.983|\n|IWUE (kg/m3)|8.84|8.63|\n|Average solution time of P N,M (d) (minutes)|107.5|0.44|# Table 3: Comparison between the P N,M (d) with LSTM and P N,M (d) with ReLU neural network for the 2022 season.\n\n| |P N,M (d) with LSTM|P N,M (d) with ReLU|\n|---|---|---|\n|Total irrigation (m)|0.790|0.761|\n|IWUE (kg/m3)|10.71|10.65|\n|Average solution time of P N,M (d) (minutes)|82|0.52|\n\nfrom a computational standpoint compared to training an LSTM network [36]. However, future studies will be needed to compare the performance of the MIL representations of LSTM and ReLU neural networks in the irrigation scheduling setting to better understand their relative strengths and limitations.# 8.3 Comparison of P N,M (d) with ReLU and Triggered Approaches\n\nTables 4 and 5 provide a comparison between the ReLU-based scheduling approach and the traditional triggered irrigation scheduling approach in terms of total irrigation and Irrigation Water Use Efficiency (IWUE) for the 2015 and 2022 growing seasons.\n\nFor the 2015 season (Table 4), which represents a relatively dry period with a total rainfall of 14.7 mm, the total irrigation required by the ReLU-based approach is 0.983 m, representing a reduction of approximately 9.8% compared to the 1.09 m required by the triggered approach. The ReLU approach also demonstrates an enhanced IWUE of 8.63 kg/m3, which is an improvement of approximately 14.6% over the triggered approach\u2019s IWUE of 7.53 kg/m3.\n\nFor the 2022 season (Table 5), which represents a relatively wet period with a total rainfall of 230.9 mm, the ReLU-based approach requires 0.761 m of total irrigation, a reduction of about 21.6% compared to the 0.97 m required by the triggered approach. The IWUE for the ReLU approach is 10.65 kg/m3, reflecting an enhancement of approximately 36.9% over the IWUE of 7.78 kg/m3 for the triggered approach.\n\nAdditionally, as illustrated in Figures 4 and 5, the ReLU-based approach more effectively maintains soil moisture levels within the optimal range across both seasons, with fewer and more con-# Table 4: Comparison between the triggered and P N,M (d) with ReLU neural network for the 2015 season.\n\n| |P N,M (d) with ReLU|Triggered|\n|---|---|---|\n|Total irrigation (m)|0.983|1.09|\n|IWUE (kg/m3)|8.63|7.53|\n\nThese results, combined with the ReLU approach\u2019s significantly enhanced solution time, highlight its potential as an attractive option for real-time irrigation scheduling.# Figure 4: Prescribed irrigation schedules and the trajectories of root zone soil moisture content for the 2015 season.# 9 Conclusion\n\nIn conclusion, this study demonstrates the use of ReLU surrogate models to enhance the computational efficiency of mixed-integer Model Predictive Control (MPC)-based irrigation schedulers. By leveraging the mixed-integer linear representation of the ReLU operator, the proposed approach transforms the mixed-integer MPC-based scheduler, which features a quadratic cost function, into a Mixed-Integer Quadratic Program (MIQP). The MIQP represents the simplest class of mixed-# Figure 5\n\nPrescribed irrigation schedules and the trajectories of root zone soil moisture content for the 2022.# Table 5\n\nComparison between the triggered and P N,M (d) with ReLU neural network for the 2022 season.\n\n| |P N,M (d) with ReLU|Triggered|\n|---|---|---|\n|Total irrigation (m)|0.761|0.97|\n|IWUE (kg/m3)|10.65|7.78|\n\nInteger nonlinear programming problems, for which global optimization solvers exist. A case study involving other machine learning surrogate models, specifically the Long Short-Term Memory (LSTM) network, showed that the proposed ReLU-based approach significantly reduces the solution time without compromising the water-saving benefits or the Irrigation Water Use Efficiency (IWUE) enhancement previously established by the mixed-integer MPC-based irrigation scheduler. Additionally, a comparative study with the widely used triggered irrigation scheduling approach demonstrated that the ReLU-based method offers enhanced performance in terms of both total irrigation and IWUE.\n\nThese findings confirm that the ReLU-based approach enhances the computational efficiency while maintaining the effectiveness of mixed-integer MPC-based irrigation schedulers, making it a practical and efficient solution for real-time irrigation management in large-scale agricultural.# settings.\n\nDespite the promise demonstrated by the proposed approach, several modifications could be made to further enhance its effectiveness. For instance, using estimated hydraulic parameters, derived from an offline parameter estimation approach, as a basis for identifying the ReLU neural network models, plays an important role in reducing parametric uncertainty and effectively addressing plant-model mismatch within scheduler. While this offline strategy provides a good foundation for the real-time implementation of the scheduler, it has limitations in adapting to real-time variations in parameters of the field. To address this limitation, an offset-free MPC approach could be adopted by incorporating a disturbance model. The design of an offset-free MPC, as discussed in [37], could serve as a helpful reference in this regard.",
        "context_id": 29,
        "question": "Which specific model was used within the mixed-integer MPC framework to improve computational efficiency according to the initial work [7]?",
        "answer": [
            "LSTM network"
        ],
        "context_length": 38213
    },
    {
        "context": "# Chiral patterning of rough surfaces with vortex laser beams: from structured polarization to twisted forces# I. INTRODUCTION\n\nThe interaction of intense laser radiation with surfaces of solids leads to the emergence of laser-induced periodic surface structures (LIPSS), which significantly modify both the topographical and functional properties of the irradiated materials [1]. These periodic undulations, first observed in the mid-1960s [2], have seen a surge in research interest following the advent of ultra-short laser pulses [3]. LIPSS can be viewed as surface ripples with varying depth and periodicity, the formation of which is influenced by the spatiotemporal coherence and polarization of the laser pulse [4, 5]. The process of LIPSS formation and their properties are affected by a number of parameters, including pulse duration [6], laser fluence [7], polarization direction [5], wavelength [8, 9], and the number of applied pulses [10]. LIPSS can be generated on the surface of a wide variety of materials, including metals [11\u201313], semiconductors [3, 8, 14, 15], glasses [16, 17] and polymers [18\u201320], demonstrating the versatility and broad applicability of LIPSS in various areas of materials science. Coherent irradiation of multiple points on a rough surface initiates a complex interaction between various physical mechanisms, ranging from interference between scattered waves [21] to near-field enhancement effects [22], optical resonances [22, 23] such as surface plasmons [24, 25], and activation of hydrodynamic instabilities [26]. As a result of these processes, local temperature gradients arise, which, through a thermo-mechanical response, lead to the formation of a modulated surface relief with certain axes of symmetry. Traditionally, LIPSS are divided into two classes: low-spatial-frequency LIPSS (LSFL), which often occur near the ablation threshold, and high-spatial-frequency LIPSS (HSFL), which are driven by thermo-convective effects [27]. Typically, these surface structures have only one axis of symmetry, which is determined by the polarization of the laser pulse. However, recent experiments have demonstrated that using multiple time-delayed laser pulses, it is possible to produce surface structures with two or even three axes of symmetry, creating complex patterns such as cross-hatching or hexagonal lattices [28]. In addition to the pursuit of maximum miniaturization, one of the central challenges of laser surface processing has become the creation of new surface structures with unusual geometry [25, 29]. The compelling question now is: can we go beyond conventional symmetry and create surface structures with fully asymmetric patterns, independent of laser polarization direction? Typically, chiral patterns lacking mirror symmetry naturally possess these geometric properties and thus compare favorably with currently created LIPSS. It is conceivable that such chiral patterns could be produced using laser pulses that have their own intrinsic chirality. In turn, the intrinsic chirality of laser pulses is closely related to the ability of light to have angular momentum. Light can carry angular momentum in two forms: spin angular momentum (SAM) and orbital angular momentum (OAM) [30, 31]. SAM is associated with circular polarization of light and manifests itself in the form of two discrete states: left-hand and right-hand circular polarization, where the polarization handedness determines the sign of the angular momentum. In contrast, OAM has a continuous range of values determined by the so-called topological charge, which can be any positive or negative integer. The laser pulses carrying the OAM have a doughnut-like shape and a spiral wavefront [30]. Although SAM has received much more attention in LIPSS research, studying the effects of OAM may reveal new patterns and lead to new functional surfaces. For further study of chiral interactions of light and matter, it is ex-# Footnotes\n\n\u2217 v.y.fedorov@gmail.com\n\n\u2020 jean.philippe.colombier@univ-st-etienne.fr# II. THE NUMERICAL MODEL\n\nTo study the interaction of structured light pulses with rough surfaces, we numerically solve the following system of Maxwell equations using the FDTD method [49]:\n\n\u2207 \u00d7 E = \u2212 \u2202t B, \u2207 \u00d7 H = \u2202t D, (1)\n\nwhere E(r, t) and H(r, t) are the electric and magnetic field vectors, r = {x, y, z} is the coordinate vector and B = \u03bc0 H, with \u03bc0 being the vacuum permeability. The medium response can be expressed through the displacement field D(r, t) written in the frequency domain as Dr, \u03c9 = \u03b50\u03b5(\u03c9) Er, \u03c9, where \u03b5 denotes the temporal spectrum, \u03b50 is the vacuum permittivity and \u03b5(\u03c9) is the frequency-dependent permittivity of the medium.\n\nAs the source of radiation, we consider a laser pulse launched from an xy plane in the -z direction (like in an experiment, from top to bottom). We can express the electric field vector E of such laser pulse through its Ex(r, t) and Ey(r, t) components as:\n\nEx = Ex0 cos \u03b8 \u2212 Ey0 sin \u03b8, (2a)\n\nEy = Ex0 sin \u03b8 + Ey0 cos \u03b8. (2b)# 3\n\nwhere at \u03bb0 = 1.03 \u03bcm has the real and imaginary parts equal to n\u2032 = 3.02 and n\u2032\u2032 = 3.51 with the corresponding skin depth l = 1/(2n\u2032\u2032\u03c90/c0) = 23.34 nm.\n\nSince LIPSS originate from the interference of incoming light and light asymmetrically scattered at surface inhomogeneities [21], as well as from contributions of nonradiative field enhancement on roughness [50], it is essential to account for the rough surface of the stainless steel sample. To enhance realism, we assume that the rough surface has a continuous distribution of heights that can be statistically described. In particular, the surface roughness is represented by the function R(x, y), which defines the random deviations of the surface height relative to a reference plane [51].\n\nTo express the statistical properties of the surface roughness we use the correlation function C(X, Y) = \u27e8R(x, y)R(x + X, y + Y)\u27e9/\u03c32 where \u27e8. . . \u27e9 denotes the spatial averaging and \u03c3 = p\u27e8R2\u27e9 is the root-mean-square (rms) surface height. The correlation function C(X, Y) describes the spatial coherence between surface heights at different points separated by the distance d = \u221a(X2 + Y2).\n\nIn our simulations we assume the Gaussian correlation function C(X, Y) = \u03c32 exp(\u2212(X2 + Y2)/\u03be2), where \u03be is the correlation length. For details on converting this correlation function into an actual roughness function R(x, y), refer to [52\u201354]. In our simulations we use the rms height \u03c3 = 50 nm and the correlation length \u03be = 100 nm, which are approximately ten times smaller than the laser wavelength. These values correspond to a well-polished surface with subwavelength inhomogeneities required for HSFL observation.\n\nIn our simulations we assume that the laser pulse can be represented by a plane wave with the temporal envelope defined as a one period of sin2 function:\n\nA(x, y, t) = A0 sin2(\u03c0\u03c40,2 t)\n\nwhere A0 is the peak amplitude of the electric field and \u03c40 is the full width at half maximum pulse duration. Since the beginning and the end of the sin2 pulse in time are well-defined, we can save a significant amount of computation time by avoiding modeling slowly rising pulse front and tail, like, for example, in the case of the Gaussian envelope. To take into account that any phase \u03d5(x, y) other than flat one changes the amplitude distribution in time, we also apply the temporal transformation where in Eq. (4) we replace the time t by t + \u03d5(x, y)/\u03c90 (see Appendix A).\n\nIn our simulations we do not consider any intensity-dependent effects of laser-matter interaction and, therefore, without loss of generality, we take A0 = 1 V/m. Additionally, we assume the pulse duration \u03c40 = 100 fs and the central wavelength \u03bb0 = 1.03 \u03bcm.\n\nBelow the radiation plane, we place a semi-infinite stainless steel medium. To model the dispersive response of stainless steel we apply the auxiliary differential equation method [49] assuming the Drude permittivity \u03b5(\u03c9) = 1 \u2212 \u03c9p2/\u03c92 and the damping rate \u03b3/(\u03c92 + i\u03c9\u03b3) with the plasma frequency \u03c9p = 19.2 \u00d7 1015 1/s and \u03b3 = 9.15 \u00d7 1015 1/s [50]. With these parameters the complex refractive index n = n\u2032 + in\u2032\u2032 of stainless steel can be calculated as the integral of W(x, y, z) over all z layers of the surface: Q(x, y) = \u222b\u2212\u221e\u221e W(x, y, z)dz. Finally, the total laser energy Qtot absorbed by the surface can be obtained as Qtot = \u222b\u2212\u221e\u221e Q(x, y)dxdy.# 4\n\nthat, compared to the type-s features, oriented along they (\u03bcm) kx direction, the type-r features, oriented along the ky direction, consist of spectral components with higher frequencies. Considering that smaller shapes in space correspond to higher spectral frequencies, we can conclude that the spots of absorbed energy are indeed, on average, compressed in the y direction and stretched in the x direction. The spatial spectra of Q(x, y) also provide information on the typical size of the absorbed energy spots. We can use this information to estimate the period of the resulting LIPSS and to distinguish LSFL from HSFL. The spectral components of Q(x, y) located at spatial frequencies close to or less than k0 are responsible for the formation of LSFL since they correspond to large spots with characteristic sizes less than or equal to the laser wavelength \u03bb0. In turn, the spectral components of Q(x, y) at spatial frequencies much larger than k0 correspond to the small-scale sub-wavelength spots responsible for the formation of HSFL. In Fig. 2(d) we see that the highest frequency components of Q(x, y) spectrum lie in the region of 5k0 which means that the minimum size of the absorbed energy spots is approximately five times smaller than the laser wavelength \u03bb0. Thus, we can predict that the minimum period of the resulting LIPSS will be \u03bb0/5.\n\nNext, let us consider the same linearly polarized laser pulse but with the polarization rotated by 45 degrees relative to the x axis (\u03b8 = 45\u25e6 in Eq. (2)). Figures 2(b) and (e) show the corresponding distribution of absorbed energy Q(x, y) and its spatial spectrum. From the comparison of Figs. 2(d) and (e) we see that the spectrum of Q(x, y) in Fig. 2(e) is rotated by 45 degrees, which means that the orientation of the corresponding absorbed energy spots is also changed. A closer look at the absorbed energy spots in Fig. 2(b) shows that they are indeed elongated along the polarization direction. Thus, our simulations confirm the known fact that the orientation of LIPSS follows the direction of laser polarization.# A. Linear and circular polarizations\n\nAs a starting point, let us consider a laser pulse that has linear polarization oriented along the x direction (\u03f5 = 0 and \u03b8 = 0\u25e6 in Eqs. (2) and (3)). Figure 2(a) shows the distribution of the laser energy Q(x, y) absorbed by the stainless steel sample irradiated by such laser pulse. The surface roughness causes the distribution of Q(x, y) to resemble a chaotic pattern of absorbed energy spots. However, upon closer inspection we find that the regions of high absorption are elongated in the direction of laser polarization. Taking into account that the regions of high losses act as a seed for LIPSS growth, we can expect that the resulting LIPSS will be also oriented in the x direction \u2014 parallel to the laser polarization.\n\nTo obtain more information about the orientation and size distribution of the absorbed energy spots, we calculate the spatial spectrum of Q(x, y). Figure 2(d) shows the spectrum of Q(x, y) in the spatial-frequency coordinates kx and ky normalized by the wave number k0 = \u03c90/c0. We see that the spatial spectrum of Q(x, y) has a well-recognizable shape with spectral features known as \"type-s\" and \"type-r\" [26]. We also see that for all three polarization cases the total amount of absorbed laser energy Qtot is the same (the difference is less than numerical errors). We also checked that for a circularly polarized laser pulse with the opposite direction of polarization rotation (\u03f5 = \u22121) the distribution of absorbed laser energy Q(x, y) does not have any preferred orientation and the corresponding spatial spectrum is symmetrical about the origin.# Linear (\u03b8 = 45\u25e6)\n\n|y (\u03bcm)|Circulark/ky|\n|---|---|\n|6|a|\n|4|b|\n|2|c|\n|0| |\n|\u22122| |\n|\u22124| |\n|\u22126| |# FIG. 2.\n\nThe distributions of absorbed energy Q(x, y) (a,b,c) and their spectra (d,e,f) for laser pulses with linear polarization rotated by \u03b8 = 0\u25e6 (a,d) and \u03b8 = 45\u25e6 (b,e), and for a circularly polarized laser pulse (c,f). The arrows in (a\u2013c) show the direction of the laser polarization. The arrows in (d) mark the characteristic spectral patterns known as \u201dtype-r\u201d and \u201dtype-s\u201d features.\n\nThe distribution of absorbed energy Q(x, y) changes insignificantly, while the total losses Qtot remain unchanged, indicating the absence of circular dichroism. Thus, we can conclude that the morphology of resulting LIPSS does not depend on the polarization handedness and, consequently, on the sign of the SAM.# B. Inhomogeneous polarization distribution\n\nAs we have just seen, for linearly polarized laser pulses the distribution of absorbed laser energy Q(x, y) looks like a set of elongated spots aligned along the polarization direction, which allows us to assert that the laser polarization determines the orientation of the resulting LIPSS. We can exploit this dependence on the polarization direction to obtain LIPSS with complex morphology. For this purpose we can use laser pulses with inhomogeneous polarization distribution, where the local polarization direction will determine the orientation of LIPSS at a given point. As an example, let us consider three laser pulses with the radial, spiral, and azimuthal polarization patterns. In terms of Eqs. (2) and (3) such laser pulses are defined by the linear polarization with \u03f5 = 0 and the spatially-dependent polarization angle \u03b8(x, y) = arctan(y/x) + \u03c8, where \u03c8 \u2208 [\u2212\u03c0/2, \u03c0/2] determines the angle between the polarization direction and the radius vector of a given point with the coordinates x and y. In particular, \u03c8 = 0 and \u03c8 = \u00b1\u03c0/2 correspond to the radial and azimuthal polarization patterns, respectively, while the intermediate values of \u03c8 define the spiral polarization patterns of different vorticity (the sign of \u03c8 allows us to switch between the left-handed and right-handed rotation of the spiral). Experimentally, such laser pulses can be generated, for example, using so-called q-plates [56]. Figure 3 shows the distributions of absorbed laser energy Q(x, y) and their spectra obtained for laser pulses having the radial (\u03c8 = 0), spiral (\u03c8 = \u03c0/4), and azimuthal (\u03c8 = \u03c0/2) polarization patterns as a result of their interaction with the rough stainless steel surface. The arrows in Figs. 3(a)\u2013(c) allow us to visualize the distribution of the polarization for each of the patterns. Figures 3(a)\u2013(c) show that, as expected, locally the spots of absorbed laser energy are oriented along a given polarization direction, forming a distribution that repeats the polarization pattern; the central region with zero absorption appears due to the zero on-axis intensity caused by the polarization singularity at this point. Since the regions# Azimuthal (\u03c8 = \u03c0/2)\n\n|y (\u03bcm)|k/ky|0|\n|---|---|---|\n|6|a| |\n|4| | |\n|2| | |\n|0| | |\n|\u22122| | |\n|\u22124| | |\n|\u22126| | |\n\n|x (\u03bcm)|0.0|\n|---|---|\n|\u22126| |\n|\u22124| |\n|\u22122| |\n|0| |\n|2| |\n|4| |\n|6| |# FIG. 3.\n\nThe distributions of absorbed energy Q(x, y) (a,b,c) and their spectra (d,e,f) for laser pulses with inhomogeneous polarization distribution defined by the polarization rotation angle \u03b8(x, y) = arctan(y/x) + \u03c8 with \u03c8 = 0 for the radial (a,d), \u03c8 = \u03c0/4 for the spiral (b,e), and \u03c8 = \u03c0/2 for the azimuthal (c,f) polarization patterns. The arrows in (a\u2013c) indicate the direction of the laser polarization in a given point.\n\nof high losses act as seeds for the formation of LIPSS, the resulting LIPSS will be organized in accordance with the polarization pattern. Such intricate LIPSS formations, obtained using complex polarization states, have already been observed in several experiments [38\u201340]. Thus, the laser pulses with spiral polarization distributions allow us to create LIPSS patterns with controllable vorticity.\n\nInterestingly, Fig. 3(d)\u2013(f) show that the spectra of Q(x, y) for the radial, spiral, and azimuthal polarization patterns are practically identical and resemble the spectrum obtained for the circularly polarized laser pulse (see Fig. 2(f)). We can explain this observation by the fact that for each of the three polarization patterns there are regions in the beam cross section containing all possible orientations of linear polarization. As a result, the final spectrum can be seen as a set of Q(x, y) spectra for a linearly polarized laser pulse (see Figs. 2(d) and (e)), averaged over all possible angles of polarization.\n\nDuring the simulation, we did not find any significant difference in the total absorbed energy Qtot between the laser pulses with the radial, spiral, and azimuthal polarization \u2014 the maximum difference did not exceed 0.5%. We also verified that the value of Qtot does not change by any noticeable amount for laser pulses having the spiral polarization with an opposite handiness given by \u03b8(x, y) = arctan(y/x) \u2212 \u03c0/4. Thus, we can conclude that the total amount of absorbed laser energy is insensitive to the polarization vorticity.# C. Orbital angular momentum\n\nIn the previous section we saw that it is possible to create chiral LIPSS formations using inhomogeneous polarization states. Let us now explore the possibility of creating chiral LIPSS patterns using laser pulses carrying OAM. According to the Sipe\u2019s theory [21] LSFL arise as a result of interference between incident laser radiation and surface electromagnetic waves generated by scattering on a rough surface. Therefore, we can expect that for laser pulses with OAM, their interference with the light scattered on the surface will form a pattern of interference maxima which will inherit the helical structure of the wavefront and which, being imprinted on the surface in the form of regions of high absorption, will lead to emergence of chiral LIPSS. In order to explore this possibility we consider a linearly polarized laser pulse (\u03f5 = 0 and \u03b8 = 0\u25e6 in Eqs. (2) and (3)) with a helical phase.# FIG. 4. The distribution of absorbed energy Q(x, y) (a) and its spectrum (b) for the linearly polarized laser pulse carrying OAM with the topological charge \u2113 = +1.\n\nThe arrows in (a) show the direction of the laser polarization.\n\n| |0|2|4|6|8|10|\n|---|---|---|---|---|---|---|\n|\u22126|0.0|0.2|0.4|0.6|0.8|1.0|\n|\u22124|0.0|0.2|0.4|0.6|0.8|1.0|\n|\u22122|0.0|0.2|0.4|0.6|0.8|1.0|\n|0|0.0|0.2|0.4|0.6|0.8|1.0|\n|2|0.0|0.2|0.4|0.6|0.8|1.0|\n|4|0.0|0.2|0.4|0.6|0.8|1.0|\n|6|0.0|0.2|0.4|0.6|0.8|1.0|# FIG. 5. Geometry of the fractal structure on the surface of stainless steel used to study the response of a surface with its own chirality.\n\nThe line plots show the cross-sections of the fractal structure at y = 0 (top) and x = 0 (right).\n\n\u03d5(x, y) = \u2113 arctan(y/x), where \u2113 is an integer number known as the topological charge. The magnitude of \u2113 dictates the number of rotations the wavefront undergoes in one period of the laser pulse, while the sign of \u2113 indicates the direction of this rotation.\n\nFigure 4 shows the distribution of absorbed laser energy Q(x, y) and its spectrum for the laser pulse having the OAM with \u2113 = +1. In Fig. 4(a) we see that the Q(x, y) distribution does not show any traces of vorticity and is very similar to the distribution obtained for a linearly polarized laser pulse with a flat phase (see Fig. 2(a)): the only difference is the region of zero losses in the center, corresponding to the zero on-axis intensity caused by the phase singularity at that point. In turn, a comparison of Fig. 4(b) with Fig. 2(d) shows that the spectra of the laser pulse with OAM and a conventional linearly polarized laser pulse are practically identical.\n\nIn our studies we also considered laser pulses with higher values (up to 10) and different signs of the topological charge \u2113. However, the shape of the resulting distribution of absorbed laser energy did not show any traces of chirality. Thus, we can conclude that the presence of OAM in the laser pulse does not affect the distribution of absorbed laser energy and, therefore, does not change the geometry of LIPSS.# D. Fractal surface structures\n\nAs we saw above, contrary to our expectations, when a laser pulse interacts with a rough surface, the presence of OAM does not change the distribution of the absorbed laser energy. However, how might the surface\u2019s own chirality affect the interaction with the rotating wavefront in a hypothetical context of helical dichroism? From experiment we know that at least individual chiral nanostructures are capable of responding differently to the sign of the OAM in the incoming laser pulse [57]. Therefore, we can expect that the distribution of absorbed laser energy on a surface containing some chiral structures will be different for laser pulses with different OAMs.\n\nBut what kind of chiral structures should we choose to maximize the response to incoming laser pulses? Of course, we could conduct a parametric study by playing with the size, shape, and arrangement of an array of chiral nanoparticles deposited on the surface. However, we decided to simplify the problem by taking a surface with a fractal chiral structure applied to it. As such fractal structure we consider the Julia set J(f) defined from the function f(z) = z\u00b2 + c with c = \u22120.5125 + 0.5213i and protruding 100 nm above the flat surface. Figure 5 shows the resulting geometry of the fractal stainless steel surface used in our simulations. We see that the fractal nature of the Julia set allows us to obtain a surface structure consisting of chiral elements whose scale starts at a few wavelengths and gradually decreases to sub-wavelength sizes.# 8\n\n|Without OAM (\u2113 = 0)|With OAM (\u2113 = +1)|\n|---|---|\n|||\n|||\n\nFIG. 6. The distributions of absorbed energy Q(x, y) (a,b) and their spectra (c,d) for linearly polarized laser pulses interacting with the fractal surface structure. (a) Laser pulse without OAM (\u2113 = 0) and (b) with OAM of topological charge \u2113 = +1. The arrows in (a,b) show the direction of the laser polarization.\n\nscan a whole range of chiral structures of different scales, some of which will have to be in resonance with the incident radiation.\n\nFigure 6 shows the distributions of absorbed energy Q(x, y) and their spectra obtained as a result of the interaction with the fractal surface of linearly polarized laser pulses without (\u2113 = 0) and with (\u2113 = +1) OAM. In Figs. 6(a) and (b) we see that, apart of the central region in Fig. 6(b) with zero losses caused by the phase singularity, both distributions of Q(x, y) do not contain any significant differences visible to the naked eye that could distinguish the cases of \u2113 = 0 and \u2113 = +1. In Fig. 6(c) and (d) we see that the spectra of Q(x, y) contain more high-frequency spectral component compared to the case of a linearly polarized laser pulses interacting with the rough surface (see Fig. 2(d)). This is because, compared to a rough surface, the fractal structure consists of much smaller scatterers. The only visible difference between the spectra in Fig. 6(c) and (d) is observed in the region of zero frequencies and is due to the presence of a zero-loss spot in Fig. 6(b). We verified that changing the sign of the topological charge \u2113 does not lead to any visible changes. We also tested laser pulses with OAM of higher topological charges (up to \u2113 = \u00b110), but did not find any effect of OAM. Additionally, our simulations show that the total absorbed energy Qtot is insensitive to the sign of the topological charge \u2113, independently of the amplitude of \u2113. Thus, we can argue that the presence of chiral structures on the surface does not guarantee that the absorbed energy distribution, and therefore the geometry of resulting LIPSS, will sense the presence of OAM. Furthermore, in the context of positive feedback from repeated laser pulses at the same location, there should be no enhancement of the chiral effect.# E. Spiral intensity distribution\n\nSo far, we have not identified a configuration where a laser pulse with OAM produces chiral distributions of absorbed laser energy that could induce the formation of chiral LIPSS. However, instead of seeking a direct OAM effect, we can exploit OAM indirectly to create a spiral intensity distribution. We can expect that such intensity distribution, being imprinted on the surface, will lead to a spiral arrangement of LIPSS. To obtain the spiral intensity pattern we can superimpose a focused OAM laser pulse with a second one having a plane wave front [58, 59]. To recreate such a combination of laser pulses in our simulations, we use the sum of# FIG. 7. The distributions of absorbed energy Q(x, y) (a,b,c) and their spectra (d,e,f) for linearly polarized laser pulses with spiral intensity distribution obtained using OAMs with topological charges \u2113 = +1 (a,d), \u2113 = +5 (b,e), and \u2113 = +10 (c,f). The arrows in (a,b,c) show the direction of the laser polarization.\n\n|\u2113 = +1|\u2113 = +5|\u2113 = +10|\n|---|---|---|\n||||\n\na plane-wave x-polarized laser pulse (\u03f5 = 0 and \u03b8 = 0\u00b0 in Eqs. (2) and (3)) and a laser pulse with the phase given by \u03d5(x, y) = \u2113 arctan(y/x) \u2212 k0px\u00b2 + y\u00b2 sin \u03b4, where the first term defines the helical wave front associated with the OAM and the second term describes a conical phase intended to simulate tight focusing with a convergence angle of \u03b4. We assume that \u03b4 = 20\u00b0, which corresponds to the focusing with the numerical aperture NA = sin \u03b4 = 0.34.# Absorbed energy (arb.u.) Spectrum (arb.u.)\n\nFigure 7 shows the distributions of absorbed energy Q(x, y) and their spectra for the laser pulses with the spiral intensity distribution obtained with OAMs having the topological charges \u2113 = +1, +5, and +10. In Fig. 7(a,b,c) we see that the spiral intensity pattern of the incident laser pulse is imprinted on the rough stainless steel surface in the form of large-scale spiral regions where absorption occurs. We can control the geometry of these spiral arrangements by changing the amplitude of the topological charge \u2113, which determines the number of spiral arms, and by its sign, which is responsible for the direction of the spiral twist. Within each arm of the spiral regions we see a chaotic distribution of absorbed energy spots aligned along the polarization direction, similar to that observed in the case of linearly polarized laser pulses (see Fig. 2(a)). The spatial spectra of Q(x, y) in Fig. 7(d,e,f) also resemble the spectrum obtained for a linearly polarized laser pulse with a flat phase (see Fig. 2(d)) with the difference that the large-scale spiral formations generate many spectral components at near-zero frequencies. Thus, we see that we can use OAM indirectly to create large-scale controllable arrangements of chiral LIPSS by generating spiral intensity distributions with given parameters.# A. When orbital angular momenta do work\n\nIn the previous section we observed that despite our extensive efforts, we were unable to detect any direct effect of OAM on the distribution of absorbed laser energy during the interaction of laser pulses with rough surfaces. Therefore, there is a high probability of making a premature conclusion that OAM can not provoke the appearance of LIPSS with chirality. As we know from the literature, in certain situations, the laser pulses carrying OAM can sculpture chiral material structures [47, 60].# 10\n\nample, when a surface is irradiated by a nanosecond laser pulse with OAM, nanoscale twisted needles form in the region where the beam has the phase singularity. The twisting direction of these needles can be reversed by altering the sign of the topological charge \u2113. These observations have been made across various materials, including tantalum [45, 48], aluminum [61], copper [60], silicon [62], silver and gold thin films [46] and even azopolymers [63]. Interestingly, chiral surface relief formation was only observed when the handedness of the circular polarization aligned with that of the optical vortex. In contrast, it was suppressed when their signs were opposite [64]. This behavior highlights the effects of constructive and destructive coupling between SAM and OAM to achieve spiral surface reliefs. In these studies, optical radiation force has been widely invoked as the driving force for mass transport that occurs during the melting process.\n\nAccording to Eq. (5) only the first term on the right-hand side contributes to \u20d7ftot, since the time integral of the second term, containing the time derivative, is equal to zero. We can see this from the following simple considerations. Since the electric \u20d7E and magnetic H fields oscillate at \u20d7 the same frequency, both of them can be described by the same harmonic function. In turn, the time derivative of \u20d7E results in a shift of half a period (e.g., the derivative of a sine is a cosine and vice versa). Therefore, \u2202\u20d7E/\u2202t \u00d7 \u20d7H is an odd function of time whose temporal integral has to be zero. Thus, the total force \u20d7ftot can be simply written as \u20d7ftot = \u2212\u03b50\u03c7R \u221e\u2212\u221e(\u20d7\u2207 \u00b7 E)E dt. In what follows we consider only the transverse optical forces given by the first intensity-dependent term in Eq. (5).# B. Expression for optical forces\n\nTo understand in which cases laser pulses with OAM can transfer their vorticity to matter, let us consider the optical forces with which laser pulses act on the medium. In case of a single particle of charge q moving with velocity \u20d7v, the electromagnetic field of a laser pulse acts on this particle with the Lorentz force determined by the expression q(\u20d7E + \u20d7v \u00d7 \u20d7B). Therefore, if we have a material of volume V with the charge density \u03c1, then the overall force acting on this material from the electromagnetic field will be equal to RV \u03c1(\u20d7E + \u20d7v \u00d7 \u20d7B) d3r = RV(\u03c1\u20d7E + \u20d7J \u00d7 \u20d7B)d3r, where \u20d7J = \u03c1\u20d7v is the current. According to this equation, the force \u20d7f acting on a unit volume of the material is given by \u20d7f = \u03c1\u20d7E + \u20d7J \u00d7 \u20d7B. Therefore, if we have a bulk material with an induced polarization \u20d7P, where the density of charges \u03c1 = \u2212\u20d7\u2207 \u00b7 \u20d7P and the current \u20d7J = \u2202\u20d7P /\u2202t, then the force applied to a unit volume of such material will be \u20d7f = \u2212(\u2207 \u00b7 \u20d7P)E + \u2202\u20d7P /\u2202t \u00d7 \u20d7B.\n\nConsidering a medium with a linear response, we can write \u20d7P = \u03b50\u03c7\u20d7E, where \u03c7 is the material susceptibility. Using this expression for the polarization \u20d7P together with the equality \u20d7B = \u03bc0 \u20d7H, we can finally express the force \u20d7f acting on a unit volume of the medium from the electromagnetic field of the laser pulse as:\n\n\u20d7f = \u2212\u03b50\u03c7(\u20d7\u2207 \u00b7 E)E + \u03b50\u03bc0\u03c7 \u2202\u20d7E \u00d7 \u20d7H.# C. Linear and circular polarizations\n\nFirst, to give an intuitive idea of optical forces, let us consider a linearly polarized laser pulse (\u03f5 = 0 and \u03b8 = 0\u00b0 in Eqs. (2) and (3)). Figure 8 shows the x components of the electric field, intensity, the snapshots of the transverse optical force at several points in time and the corresponding total force for the Gaussian and Laguerre-Gaussian beam shapes. The distributions of instantaneous optical force are presented at times t = \u2212T /4.# (a) Linear polarization, Gaussian beam\n\n|(\u03bcm)|(\u03bcm)|(\u03bcm)|(\u03bcm)|(\u03bcm)| | | | | |\n|---|---|---|---|---|---|---|---|---|---|\n| | | | | |Intensity|1.0|0.8|0.6|0.4|\n|0|\u221220| | | | |0.2|0.0|\u221215| |\n|\u221210|\u22125|Time t (fs)|5|10| | | | | |\n|15|t = \u2212T /4|\u2212T /8|0|T /8|T /4| | | | |# Total force\n\n|1.0|0.8|0.6|0.4|0.2|0.0|\n|---|---|---|---|---|---|\n|\u221220|0|20|\u221220|0|20|\n|\u221220|x (\u03bcm)|0|20|\u221220|0|# (b) Linear polarization, Laguerre-Gaussian beam\n\n| | | | | |Intensity|1.0|0.8|0.6|0.4|\n|---|---|---|---|---|---|---|---|---|---|\n|0|\u221220| | | | |0.2|0.0|\u221215| |\n|\u221210|\u22125|0|5|10| | | | | |\n|15|t = \u2212T /4|\u2212T /8|0|T /8|T /4| | | | |# Total force\n\n|1.0|0.8|0.6|0.4|0.2|0.0|\n|---|---|---|---|---|---|\n|\u221220|0|20|\u221220|0|20|\n|\u221220|x (\u03bcm)|0|20|\u221220|0|\n\nFIG. 8. Electric field Ex (isovalues at levels \u00b10.1 V/m), intensity, the snapshots of optical force at times t within half an optical period T (the exact times are shown by minor ticks on the time axis) and the total force \u20d7ftot for linearly polarized laser pulses with the Gaussian (a) and Laguerre-Gaussian (b) beam shapes. The arrows on the total force plot show the direction of laser polarization. The complete time evolution of optical forces is shown in the supplementary movies: movie8a.mp4 for (a) and movie8b.mp4 for (b).\n\n\u2212T /8, 0, T /8, T /4, that is within half the optical period T = \u03bb0/c0. We visualize the force distributions only during one half of the period because, according to Eq. (5), the transverse optical force given by the first term depend on the intensity and thus have a periodicity of T /2. In Fig. 8 we see that the vectors of optical force are directed along the polarization direction, while their magnitude oscillates with the electric field, reaching its maximum at the field crests. We also observe that the optical force vectors, while remaining parallel to the polarization, point away from regions of high intensity. In particular, for the Laguerre-Gaussian beam, there are force components directed toward the dark core of the beam. Despite the oscillation of the force amplitude in time, the total force \u20d7ftot for the both beam shapes mirrors the instantaneous force distributions.\n\nIntensity (arb.u.) Force (arb.u.) Intensity (arb.u.) Force (arb.u.)\n\nNext, we consider the optical forces produced by circularly polarized laser pulses (\u03f5 = 1 and \u03b8 = 0\u25e6 in Eqs. (2) and (3)). Figure 9 presents the x components of the electric field, intensity, the snapshots of the transverse optical force together with the total force for both beam shapes: Gaussian and Laguerre-Gaussian. Here we see that the amplitude of the optical force do not fluctuate with the field, but the force vectors, following the instantaneous direction of laser polarization, rotate along with the electric field vector. As a result of this rotation, the total force \u20d7ftot becomes averaged over all possible angles. For the Gaussian beam shape, the vectors of \u20d7ftot point in all directions from the beam center. In turn, for the# (a) Circular polarization, Gaussian beam\n\n|(\u03bcm)|(\u03bcm)|(\u03bcm)|(\u03bcm)|(\u03bcm)|\n|---|---|---|---|---|\n|Intensity|1.0|0.8|0.6|0.4|\n|0|\u221220|0.2|0.0|\u221215|\n|\u221210|\u22125|Time 0 t (fs)|5|10|\n|15|t = \u2212T /4|\u2212T /8|0|T /8|\n|T /4|Total force|1.0|0.8|0.6|\n|0|\u221220|0.4|0.2|0.0|# (b) Circular polarization, Laguerre-Gaussian beam\n\n|Intensity|1.0|0.8|0.6|0.4|\n|---|---|---|---|---|\n|0|\u221220|0.2|0.0|\u221215|\n|\u221210|\u22125|Time t (fs)|5|10|\n|15|t = \u2212T /4|\u2212T /8|0|T /8|\n|T /4|Total force|1.0|0.8|0.6|\n|0|\u221220|0.4|0.2|0.0|\n\nFIG. 9. Electric field Ex (isovalues at levels \u00b10.1 V/m), intensity, the snapshots of optical force at times t within half an optical period T (the exact times are shown by minor ticks on the time axis) and the total force \u20d7ftot for circularly polarized laser pulses with the Gaussian (a) and Laguerre-Gaussian (b) beam shapes. The arrows on the total force plot show the direction of laser polarization. The complete time evolution of optical forces is shown in the supplementary movies: movie9a.mp4 for (a) and movie9b.mp4 for (b).\n\nLaguerre-Gaussian beam shape, the vectors of \u20d7ftot, which different configurations of such laser pulses, defined using Eqs. (2) and (3): (i) a linearly polarized laser pulse with \u03f5 = 0 and \u2113 = +1, (ii) a circularly polarized laser pulse with \u03f5 = 1 and the same topological charge \u2113 = +1, and (iii) a circularly polarized laser pulse with \u03f5 = 1 and the opposite topological charge \u2113 = \u22121. Figure 10 shows the x components of the electric field together with the laser pulse intensity, as well as the distributions of the optical force (both the time snapshots and the total force) for all three laser pulse configurations. In Fig. 10(a) we see that for the linearly polarized laser pulse the optical force vectors remain parallel to the polarization direction. However, the helical wavefront caused by OAM results in a distinctive rotation of the overall force distribution. Nevertheless, this rotation averages out over time, and# FIG. 10. Electric field Ex (isovalues at levels \u00b10.1 V/m), intensity, the snapshots of optical force at times t within half an optical period T (the exact times are shown by minor ticks on the time axis) and the total force \u20d7ftot for laser pulses carrying the OAM with the topological charge \u2113:# (a) Linear polarization, OAM (\u2113 = +1)\n\n|y (\u03bcm)|x (\u03bcm)|Intensity|\n|---|---|---|\n|20|0.8|1.0|\n|0|0.6|0|\n|-20|0.4|0.0|\n|-15|0.2|0.0|# Time t (fs)\n\n|t|Total force|\n|---|---|\n|-T /4|1.0|\n|-T /8|0.8|\n|0|0.6|\n|T /8|0.4|\n|T /4|0.2|# (b) Circular polarization, OAM (\u2113 = +1)\n\n|y (\u03bcm)|x (\u03bcm)|Intensity|\n|---|---|---|\n|20|0.8|1.0|\n|0|0.6|0|\n|-20|0.4|0.0|\n|-15|0.2|0.0|# Time t (fs)\n\n|t|Total force|\n|---|---|\n|-T /4|1.0|\n|-T /8|0.8|\n|0|0.6|\n|T /8|0.4|\n|T /4|0.2|# (c) Circular polarization, OAM (\u2113 = \u22121)\n\n|y (\u03bcm)|x (\u03bcm)|Intensity|\n|---|---|---|\n|20|0.8|1.0|\n|0|0.6|0|\n|-20|0.4|0.0|\n|-15|0.2|0.0|# Time t (fs)\n\n|t|Total force|\n|---|---|\n|-T /4|1.0|\n|-T /8|0.8|\n|0|0.6|\n|T /8|0.4|\n|T /4|0.2|# 14\n\nIn the figure showing the total force distribution, there is no indication of the presence of OAM (compare with the total force distribution for a simple linearly polarized laser pulse in Fig. 8(b)).# Fig. 10(b)\n\ndepicts the distributions of the optical force for a circularly polarized laser pulse, where the rotation of the polarization and wavefront occurs in the same direction (\u03f5 = 1 and \u2113 = +1). As in the case of a circularly polarized laser pulse with a plane wave front (see Fig. 9(b)), we observe a comparable rotation of the optical force vectors, though with a different force distribution. In particular, we see that the force distribution has four distinct lobes. Also we note that there are no force components directed toward the dark core of the beam.\n\nAs a result, the total force distribution consists only of force vectors directed outward from the beam center. We also note the absence of the net twisting force.# Fig. 10(c)\n\nshows the distribution of optical force for the circularly polarized laser pulse where the helical wavefront rotates in the direction opposite to the direction of laser polarization (\u03f5 = 1 and \u2113 = \u22121). As we can see, such combination of the polarization rotation and the wavefront twist results in the force distribution without the lobes, in contrast to the case of co-rotating polarization and the wavefront shown in Fig. 10(b). This behavior of the force distribution reflects the results of adding and subtracting SAM and OAM. In Fig. 10(c) we also see that the force amplitude oscillates over time. In particular, we observe the appearance of the alternating twisting force (see the force snapshots at times t = \u2212T /8 and T /8). However, because the twisting occurs in opposite directions, the resulting twisting averages out, causing all vectors in the total force distribution to align along the radial direction. Unlike the previous case of co-rotating polarization and wavefront (see Fig. 10(b)), here we see that most of the force is directed towards the beam center. As in the previous case, the distribution of ftot do not contain any net twisting force.\n\nThe above examples show that laser pulses with OAM are capable of twisting matter within the laser pulse duration even in the case of linear polarization. However, they do not leave any twisting force in the wake of the laser pulse as it passes. Therefore, to create chiral material structures with such laser pulses, the pulses need to be long enough for the pulse front to melt the material and the tail to induce a vortex. A multi-pulse configuration, where the first pulse melts the surface and subsequent pulses twist the molten material, could potentially achieve this effect as well.# E. Focused laser pulses\n\nAbove, we found that although laser pulses with OAM can introduce the twisting force, they exert the corresponding torque on the matter only during the pulse duration. However, the formation of chiral material structures would be much more efficient if the laser pulse could create a net twisting force in its wake. In order to find such net twisting forces, let us consider focused OAM laser pulses. Note that in many LIPSS experiments the laser pulses are already focused on the sample surface, so adding external focusing in our analysis seems quite natural. Here we consider the same set of OAM laser pulses introduced previously (see Fig. 10): the linearly polarized laser pulse with \u03f5 = 0 and \u2113 = +1, the circularly polarized laser pulse with \u03f5 = 1 and \u2113 = +1 (co-rotating polarization and wavefront), and the circularly polarized laser pulse with \u03f5 = 1 and \u2113 = \u22121 (counter-rotating polarization and wavefront). Similarly to the case of spiral intensity distribution, to model the external focusing we introduce the phase \u03d5(x, y) = \u2113 arctan(y/x) \u2212 k0px2 + y2 sin \u03b4, where the first term is responsible for the OAM and the second one for the focusing. For better visual appeal we consider a very smooth focusing with the convergence angle \u03b4 = 1\u00b0 corresponding to N A = sin \u03b4 = 0.017.\n\nFigure 11 shows the x component of the electric field, laser pulse intensity, and distributions of optical force (instantaneous and total) for the focused OAM laser pulses introduced above. As we can see from the plots of the electric field, the presence of the phase term responsible for the focusing leads to a distortion of the wavefront: the electric field located closer to the beam center turns out to be lagging in time relative to the peripheral one. Note that the wavefront distortion affects the periodicity with which the optical force changes. Therefore, in the figures with the focused laser pulses, we plot the distributions of the instantaneous optical force at the moments of time t = \u2212T /2, \u2212T /4, 0, T /4, T /2, that is within the full optical period T, rather than its half, as we did for the previous figures. The figures with the instantaneous optical force show that the external focusing results in additional vorticity of the corresponding force distributions. Nevertheless, in Fig. 11(a) we see that despite this additional vorticity, the distribution of the total force ftot for a linearly polarized laser pulse does not indicate the presence of any net twisting force. However, in both cases of a circularly polarized laser pulse, we clearly see that the distributions of ftot have a residual vorticity. Thus, we can conclude that in focused laser pulses with OAM, it is the circular polarization that leads to the emergence of net twisting force. This effect is likely related to the recent observations of clockwise and counterclockwise nanopillar arrays fabricated using left- and right-handed circular polarizations at ZnO surface [66].\n\nIn light of the above observation, the question arises as to whether it is possible to obtain the net twisting force in focused circularly polarized laser pulses without OAM. To answer this question let us consider circularly polarized laser pulses with the Gaussian and Laguerre-Gaussian beam shapes focused by some external focusing element. We model such pulses by setting \u03f5 = 0, \u03b8 = 0\u00b0 and phase \u03d5(x, y) = \u2212k0px2 + y2 sin \u03b4 in Eqs. (2) and (3) together with a corresponding beam shape in Eq. (6).# FIG. 11. Electric field Ex (isovalues at levels \u00b10.1 V/m), intensity, the snapshots of optical force at times t within an optical period T (the exact times are shown by minor ticks on the time axis) and the total force \u20d7ftot for focused laser pulses carrying an OAM with the topological charge \u2113:# (a) linear polarization with \u2113 = +1\n\n|y (\u03bcm)|x (\u03bcm)|Intensity|\n|---|---|---|\n|20|0|1.0|\n|0|0|0.8|\n|\u221220|0|0.6|\n|\u221215|\u221210|0.4|\n|\u22125|0|0.2|\n|0|0|0.0|# (b) circular polarization with \u2113 = +1\n\n|y (\u03bcm)|x (\u03bcm)|Intensity|\n|---|---|---|\n|20|0|1.0|\n|0|0|0.8|\n|\u221220|0|0.6|\n|\u221215|\u221210|0.4|\n|\u22125|0|0.2|\n|0|0|0.0|# (c) circular polarization with \u2113 = \u22121\n\n|y (\u03bcm)|x (\u03bcm)|Intensity|\n|---|---|---|\n|20|0|1.0|\n|0|0|0.8|\n|\u221220|0|0.6|\n|\u221215|\u221210|0.4|\n|\u22125|0|0.2|\n|0|0|0.0|# V. CONCLUSIONS\n\nOur results demonstrate the potential of vortex laser beams to induce unique surface morphologies on rough metallic surfaces. By varying polarization, orbital angular momentum, and initial pre-structures with chiral properties, we assess the conditions under which these beams can generate chiral excitations that result in intricate patterns, such as spiral and helical structures. Unlike conventional beams, the distinctive phase and polarization distributions of vortex beams enable the formation of complex, asymmetrical surface structures, providing new insights into the formation of LIPSS.\n\nContrary to our initial expectations, OAM in a laser pulse does not alter the distribution of absorbed laser energy on a rough surface or induce chiral LIPSS. Additionally, the presence of chiral structures does not ensure that the absorbed energy distribution will reflect OAM effects, making positive feedback from repeated pulses unlikely to enhance these effects. While we did not identify a configuration where OAM directly produces chiral distributions, we demonstrated that OAM can indirectly create a spiral intensity distribution by interfering with a plane wave. However, this approach does not achieve resolution below the laser wavelength. To explore mechanisms effective at the subwavelength scales, we examined the features of twisting optical forces.\n\nLaser pulses with OAM exhibit varying behaviors based on polarization. For linearly polarized pulses with OAM, the optical force vectors align with the polarization but show a rotating distribution due to the helical wavefront, which averages out over time, leaving no net twisting force. Circularly polarized pulses with co-rotating OAM also show a rotation but lack force components directed toward the beam center, resulting in a radial force distribution. For counter-rotating OAM, the twisting force oscillates over time, leading to a net radial force with no residual twisting. Our investigation highlights that focused circularly polarized laser pulses, with or without OAM, are crucial for generating a net twisting force, which could influence subsequent thermo-mechanical processes.\n\nIn conclusion, the combination of orbital and spin angular momenta significantly enhances the flexibility of surface functionalization. Structured light interacting with material surfaces enables advanced material processing with a level of control that surpasses conventional methods. As a key contribution of this work, we highlight the possibility of manipulating LIPSS via spiral intensity distributions and applying optical torque forces, expanding the applicability of LIPSS to areas such as biomimetic design, chiral sensing or enantiospecific surface physical chemistry. While we have shown that structured light can break conventional symmetry and induce rotating surface deformations, the development of self-formed coherent structures with adjustable chiroptical properties will open up new possibilities for diversifying the morphologies of LIPSS and designing advanced nanoarchitectures. To fully realize these potential applications, especially in nano-manufacturing, refining these methods is crucial. Further investigation should focus on how structured light interacts with more sophisticated materials, including those with inherent helical dichroism, to improve control over surface structures and enhance the functional applications of these chiroptical effects.",
        "context_id": 30,
        "question": "What type of angular momentum is associated with circular polarization of light?",
        "answer": [
            "spin angular momentum"
        ],
        "context_length": 47311
    },
    {
        "context": "# 1. INTRODUCTION\n\nThe abundance, spatial distribution, rotation velocity, and sizes of low-mass galaxies (stellar mass M < 109 M\u2609) in the Milky-Way (MW) and similar galaxies in the local Universe (D < 100 Mpc) can inform the fundamental physics of a range of models for galaxy formation and the nature of dark matter (see Bullock & Boylan-Kolchin 2017; Sales et al. 2022 for reviews). A number of physical processes could potentially impact the properties of low-mass galaxies, including cosmic reionization photoionizing gas in low-mass (sub)halos and shutting down their star formation completely since z < 6 (Bullock et al. 2000; Somerville 2002); stellar feedback preventing further star formation and creating dark matter cores that make subhalos more prone to tidal stripping (Governato et al. 2012; Pontzen & Governato 2012; Zolotov et al. 2012; Jiang et al. 2021); alternative dark matter models suppressing the non-linear power spectrum curtailing structure growth on small scales (e.g., Bode et al. 2001; Nadler et al. 2021); the specific assembly history of the host galaxy such as a recently infalling Large Magellanic Cloud (LMC, Nadler et al. 2020) can all lead to significant changes in the abundance and radial distributions of satellites.\n\nHere, we systematically investigate a major contributing effect that could significantly alter the predicted abundance and radial distributions of satellites and their subhalos: the enhanced mass loss due to the strong tidal forces from the baryonic mass content (stars and gas) of their host galaxies. This effect was originally brought forward as a potential resolution to the small-scale challenges of the \u039bCDM cosmological model, i.e. the missing satellite problem (Klypin et al. 1999; Moore et al. 1999) and the too-big-to-fail problem (TBTF, Boylan-Kolchin et al. 2011, 2012; Tollerud et al. 2014), that could mitigate the order-of-magnitude gap between predicted abundances of subhalos in dark-matter-only simulations and observations.# WANG ET AL.\n\n(DMO) simulations (e.g., Springel et al. 2008; Kuhlen et al. 2009; Stadel et al. 2009; Garrison-Kimmel et al. 2014; Mao et al. 2015; Griffen et al. 2016; Nadler et al. 2023b) and the observed number of low-mass satellites in the Milky Way (e.g., McConnachie 2012).\n\nThe enhanced mass loss effect from the host galaxy is primarily studied using pairs of hydrodynamic simulations and DMO counterparts starting out from the same initial conditions, focusing on satellites around MW-mass halos (Sawala et al. 2013; Brooks & Zolotov 2014; Sawala et al. 2015; Wetzel et al. 2016; Zhu et al. 2016; Despali & Vegetti 2017; Sawala et al. 2017; Garrison-Kimmel et al. 2017; Graus et al. 2018; Samuel et al. 2020; Barry et al. 2023; Jung et al. 2024). It is revealed that significant suppression of satellite and subhalo abundances occur, especially near host centers for subhalos with smaller pericenters and more radially-biased velocities due to the addition of baryons (stars and gas) compared to the DMO case. Since this effect is primarily due to the central galaxy\u2019s mass concentration, it is, in principle, agnostic towards the specific treatment of star formation and feedback physics in the hydrodynamic simulations.\n\nTo further isolate the pure gravitational effect of the central galaxy, several simulation works further combined analytic disk potentials with DMO zoom-in simulations (D\u2019Onghia et al. 2010; Errani et al. 2017; Garrison-Kimmel et al. 2017; Kelley et al. 2019). These explorations also discovered similar levels of subhalo abundance suppression as hydrodynamic simulations when compared to their DMO counterparts. These studies interpreted reduced subhalo abundances as complete physical disruption and thus concluded that the central galaxy alone could potentially solve the missing satellites and TBTF problems.1 However, we will argue that reduced subhalo abundances above a peak (or present-day) mass threshold is not evidence of physical disruption; rather, it indicates enhanced mass loss which is in agreement with recent semi-analytic studies (Green et al. 2022). Furthermore, previous studies often implicitly assume that satellite galaxies hosted by subhalos would \u201cdisrupt\u201d along with the subhalo itself, which is also an oversimplification as \u201corphan\u201d galaxies out-surviving their subhalos are required to match observations (Behroozi et al. 2019) and depends on the specific resolution of the simulation used (Mansfield et al. 2023). Thus, we are motivated to revisit how disk potentials affect subhalo populations in both the MW and \u201ctypical\u201d MW-mass halos.\n\nThe MW lives in a halo that has a virial mass of Mvir \u2192 1012 M\u2299 (Bland-Hawthorn & Gerhard 2016; McMillan 2017; Gaia Collaboration et al. 2018; Posti & Helmi 2019; Watkins et al. 2019; Cautun et al. 2020; Labini et al. 2023). All the DMO-embedded disk simulations mentioned above focused on MW-mass halos with Mvir \u2192 1012 M\u2299 and embedded massive disk potentials similar to M\u2299, our Galaxy (Md \u21ad 5 \u2193 1010 M\u2299, Licquia & Newman 2015; Bland-Hawthorn & Gerhard 2016; McMillan 2017; Cautun et al. 2020). Furthermore, in simulations such as D\u2019Onghia et al. (2010) (fixed disk-to-halo mass ratio) and Phat ELVIS (abundance matching M\u2299 to Mvir, Behroozi et al. 2013a), the simplistic disk growth models adopted does not account for diversity in halo assembly and stellar mass growth in a correlated manner. Although Garrison-Kimmel et al. (2017) used more physical disk formation histories from their hydrodynamic runs, their sample size was limited to only two MW-mass hosts. Therefore, a large parameter space of disk-to-halo mass ratios and disk growth histories is yet to be explored for MW-mass halos in terms of their subhalo abundance suppression effects.\n\nIn this work, we aim to fill this gap by better quantifying subhalo abundances over a wider range of disk masses and disk formation histories in MW-mass halos. We present the EDEN (Exploring Disks Embedded in N-body) simulations where we re-simulate 45 Mvir \u2192 1012 M\u2299 MW-mass halos from the Symphony (Nadler et al. 2023b) zoom-in simulation compilation with embedded analytic disk potentials. Two technical highlights of this work are that: i) EDEN evolves disk growth according to the empirical galaxy\u2013halo connection model UNIVERSE MACHINE (UM hereafter; Behroozi et al. 2019; Wang et al. 2021) that explicitly correlates the star formation history with the halo assembly history; ii) EDEN uses the state-of-the-art particle-tracking subhalo finder SYMFIND (Mansfield et al. 2023) to enable more robust subhalo tracking. We investigate the reduction in the abundance of subhalos as a function of host mass, subhalo mass, 3D and pericenter distances to their hosts, and infall times. As the disk mass has a dominant impact on subhalo mass loss rates over other factors like disk radius, scale height, and density profiles (Garrison-Kimmel et al. 2017; Green et al. 2022), we further re-simulate nine of our 45 zoom-in halos with \u21932.5 heavier disk potentials while keeping the form of their disk growth histories fixed. This subset of higher-disk-mass simulations, along with the main set of EDEN simulations, provides good sampling of subhalo abundance suppression over the full range of central galaxy stellar masses for typical Mvir \u2192 1012 M\u2299 MW-mass halos.\n\nBy more realistically modeling subhalos\u2019 tidal evolution compared to DMO simulations, EDEN also provides the groundwork for a better understanding of how halo assembly drives satellite radial distributions and radial quenching trends. In the 101 observed MW-mass hosts from the Satellite Around Galactic Analogs (SAGA) Survey (Geha et al. 2017; Mao et al. 2021, 2024; Geha et al. 2024; Wang et al.\n\n1 The latter problem is addressed because heavier subhalos experience stronger dynamical friction and sink more quickly toward halo centers, where the disk effects are strongest.# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS\n\n2024), the average satellite radial distribution is less concentrated than the typical NFW (Navarro et al. 1996) host dark matter density profile (Mao et al. 2024). The UM-SAGA model (Wang et al. 2024) fit to SAGA satellite data also finds it hard to produce quenched satellite radial distributions as concentrated as the observed satellites. Both of these findings point to the potential effect of the baryonic disk significantly affecting satellite abundance radial distributions. However, in a smaller set of Local Volume (ELVES, Carlsten et al. 2021, 2022; Greene et al. 2023; Danieli et al. 2023) hosts that cover a broader host mass range than SAGA, Carlsten et al. (2020) found that their satellites have a more concentrated radial distribution than many DMO and hydrodynamic simulations, although being largely consistent with the top 25% concentration hosts in SAGA (Mao et al. 2024). This diversity in MW-mass host satellite radial distributions bolsters our emphasis on modeling subhalo populations in a way that accounts for the diversity in disk masses and formation histories, truly placing satellites in our Galaxy (McConnachie 2012; Brown et al. 2014; Weisz et al. 2014, 2015; Savino et al. 2023) in the cosmological context of more diverse MW-mass host environments surveyed in ELVES and SAGA.\n\nThis paper is structured as follows: in Section 2, we introduce the MW-mass halos from Symphony and how we embed disk potentials into these zoom-in simulations; in Section 3, we review the (mis)concept of subhalo \u201cdisruption\u201d and discuss why we refrain from interpreting our results with such a term in this work; in Section 4 we present key results including the effect of disk-to-halo mass ratio on subhalo abundance, subhalo peak mass functions, 3D and peri-centric radial distributions, and subhalo infall time distributions; in Section 5 we discuss the various caveats of EDEN, as well as how to interpret our results in broader theoretical and observational contexts; in Section 6 we summarize our findings. We adopt a flat \u039bCDM cosmology with H0 = 70 km s-1 Mpc-1, \u03a9m = 0.286, \u03a9\u039b = 0.714, ns = 0.96, and \u03c38 = 0.82.# 2. METHODOLOGY\n\nHere, we present the methodology for embedding disk potentials into the Symphony Milky Way DMO simulations. Section 2.1 summarizes basic properties of the Symphony compilation (Nadler et al. 2023b); Section 2.2 introduces how we apply the galaxy\u2013halo model UM to Symphony and retrieve the stellar mass growth histories of the MW central galaxies. Section 2.3 summarizes the disk density components and how we embed them into EDEN host halos at z = 3; in Section 2.4 introduces the nine-host subsample that is re-simulated with 2.5\u00d7 higher disk mass.# 2.1. Symphony Milky Way dark-matter-only zoom-ins\n\nThe Symphony compilation (Nadler et al. 2023b) is a set of cosmological zoom-in DMO simulations that focuses on 262 host halos across four decades in halo mass, covering Large-Magellanic-Cloud-mass (LMC for short), MW-mass (Mvir \u2192 1012 M\u2299 to 1014.5 M\u2299) halos, Group-mass (Mvir \u2192 1011 M\u2299), and Cluster-mass (Mvir \u2192 1013 M\u2299). The zoom-in host halos are mostly isolated, and the simulations have, in principle, enough resolution to resolve all dark matter halos that host observable satellite galaxies in the Universe (down to Mvir,sub \u2192 1.5 \u00d7 107 M\u2299).\n\nIn this work, we focus on the 45 MW-mass host halos from Symphony. Originally, these halos were selected from the Chinchilla c125-2048 cosmological simulation (Mao et al. 2015) with masses Mvir /M\u2299 \u2248 1012.1\u00b10.03. Resolution is increased within \u2018zoomed-in\u2019 Lagrangian regions that contain particles within \u2248 10Rvir of the MW hosts at z = 0. The initial conditions for the zoom-in regions are generated with the public code MUSIC (Hahn & Abel 2011). Halos were then re-simulated from z = 99 using GADGET-2 (Springel 2005). All the simulations in this suite have particle masses of mDM = 4 \u00d7 105 M\u2299 and adopt a comoving Plummer-equivalent force softening scale of \u03b5 = 243 pc in the high-resolution zoom-in region. This mass resolution is equivalent to having 81923 particles in the 125 h-1 Mpc parent box c125-2048. In EDEN, we re-simulate these systems with the same particle resolution and force softening while embedding disk potentials (see Section 2.3).\n\nHalos in Symphony were identified using the phase-space halo finder ROCKSTAR (Behroozi et al. 2013b). We use the Bryan & Norman (1998) virial overdensity definition, which is equivalent to an overdensity of \u0394c = 99.2 at z = 0. Halo merger trees are constructed using CONSISTENTTREES (Behroozi et al. 2013c). We use these CONSISTENTTREES catalogs to track subhalos before they cross into their hosts\u2019 virial radii (Rvir,host). We use the SYMFIND (Mansfield et al. 2023) particle-tracking subhalo finder after subhalo infall for more reliable subhalo tracking, which is a major improvement of this work over previous non-particle-tracking halo finders used to analyze embedded-disk simulations (e.g., AHF, Knollmann & Knebe 2009, for Garrison-Kimmel et al. 2017 and ROCKSTAR for Kelley et al. 2019; Nadler et al. 2023b). All halo catalogs and merger trees for Symphony, including SYMFIND subhalo catalogs, are publicly available at https://web.stanford.edu/group/gfc/symphony/. The halo catalogs of the EDEN simulations presented in this work will be made public on the same website upon publication of this manuscript.# 2.2. UniverseMachine star formation histories\n\nhttps://www-n.oca.eu/ohahn/MUSIC/\n\nhttps://wwwmpa.mpa-garching.mpg.de/gadget/\n\nhttps://bitbucket.org/gfcstanford/rockstar/src/main/\n\nhttps://bitbucket.org/pbehroozi/consistent-trees/src/main/# Figure 1. Properties of EDEN host systems.\n\nLeft: Stellar mass\u2013halo mass (SMHM) relation of the SymphonyMilkyWay hosts (blue dots, DMO) whose stellar masses are obtained using UNIVERSE MACHINE (UM). The blue-shaded regions are the 68% and 95% distributions of the global UM SMHM relation derived in the parent cosmological simulation of Symphony, Chinchilla c125-2048. The MW observations (red error bars) consist of stellar masses from Licquia & Newman (2015) (down triangle), Bland-Hawthorn & Gerhard (2016) (up triangle), McMillan (2017) (left triangle), Cautun et al. (2020) (right triangle) and halo masses from Bland-Hawthorn & Gerhard (2016); McMillan (2017); Cautun et al. (2020); Gaia Collaboration et al. (2018) (diamond), Posti & Helmi (2019) (pentagon), Watkins et al. (2019) (hexagon), Labini et al. (2023) (square). The magenta point shows M31 halo mass (Watkins et al. 2010) and stellar mass (Tamm et al. 2012) measurements. The observations indicate that the MW and M31 are \u21ad 1\u03c9 above the typical stellar mass for an average Mvir \u2192 1012 M\u2191 halo. Wherever only the halo or stellar mass of the MW is provided, we assign an arbitrary value to the other missing mass for clarity. The orange markers show previous embedded disk DMO simulations from Garrison-Kimmel et al. (2017) (crosses) and Phat ELVIS (diamonds, Kelley et al. 2019), which all target MW-like heavy disk systems.\n\nMiddle: UM-predicted stellar mass and stellar half-mass formation scale factor for the SymphonyMilkyWay hosts. Heavier disks also form earlier on average, with a Spearman rS = !0.50. The color bar shows the halo concentration of the MW-mass hosts, with earlier star-forming disks living in more concentrated halos.\n\nRight: Stellar versus halo half-mass scale factor for the 45 SymphonyMilkyWay hosts. Most of the MW halos have stellar masses that form later than their halo masses (the dashed line indicates a one-to-one relation). The color map is the same as the middle panel for halo concentration.\n\nWe use the UM-predicted stellar mass histories of MW host galaxies from Symphony (Nadler et al. 2023b). UM is a flexible empirical galaxy\u2013halo connection model that predicts galaxy star formation rates (SFRs) given their host halo\u2019s mass and assembly history. It is statistically constrained by observational data over a wide range of masses and redshifts (0 < z < 8), including stellar mass functions, galaxy quenched fractions, cosmic SFR densities, UV luminosity functions, and galaxy clustering. The latest version of UM, UM-SAGA, is additionally constrained by low-mass galaxy observations (M \u2192 \u21ab 109 M\u2191) from the SAGA Survey and the Sloan Digital Sky Survey (SDSS, Geha et al. 2012).\n\nHere, MW stellar mass histories are derived using the zoom-in UM application method described in Wang et al. (2021), which uses the UM DR1 model (Behroozi et al. 2019) and joins together all 45 DMO zoom-in simulations for unbiased halo accretion rate ranking. We do not expect the choice to use the UM DR1 model vs. UM-SAGA to have any impact, since here we focus only on modeling the MW-mass host galaxies (M\u2192 \u21ad 1010 M\u2191), and the UM-SAGA update does not impact MW-mass galaxies (Fig. 5 in Wang et al. 2024).\n\nIn the left panel of Fig.1, we show the MW host halo masses (Mvir,host), the UM-predicted stellar (disk) masses for the MW galaxy (M\u2192,Disk), the stellar half-mass formation scale factor (a\u2192,1/2), and the halo half-mass formation scale factor (avir,1/2). We assume that the three-component disk potential (Section 2.3) contains the total stellar mass predicted by UM, and we use M \u2192 and M\u2192,Disk interchangeably. Compared to the latest observational constraints on the MW\u2019s halo mass (Bland-Hawthorn & Gerhard 2016; McMillan 2017; Gaia Collaboration et al. 2018; Posti & Helmi 2019; Watkins et al. 2019; Cautun et al. 2020; Labini et al. 2023), the SymphonyMilkyWay hosts have quite representative virial masses (mean Mvir \u2192 1012.1 M\u2191). However, the MW has an atypically large (\u21ad 1\u03c9) stellar mass (Licquia & Newman 2015; Bland-Hawthorn & Gerhard 2016; McMillan 2017; Cautun et al. 2020) for a halo of Mvir \u2192 1012 M\u2191, and is larger than the average empirical SMHM relation from UM (Wang et al. 2021). We also show observational measurements of the halo mass (Watkins et al. 2010) and stellar mass (Tamm et al. 2012) of M31, which is even higher stellar-to-halo mass ratio. Previous embedded disk DMO simulations marked in orange from Garrison-Kimmel et al. (2017) and Kelley et al. (2019) all focused on MW-like high-disk-mass systems (\u21ad 1\u03c9 up scatter) for MW-mass halos around Mvir \u2192 1012 M\u2191. Therefore, a large part of the M \u2192,Disk /Mvir,host parameter space that the majority of MW-mass galaxies find themselves in has not yet been explored. EDEN aims to fill this important gap and provide a more comprehensive understanding of disk effects on subhalo abundances in MW-mass hosts with a range of galaxy formation histories.# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS# Figure 2. Stellar mass history of the disk in MW-mass systems.\n\nThe median history of the 45 SymphonyMilkyWay hosts (DMO) with UM-predicted stellar mass histories is shown (thick solid grey; shaded band indicates the [16%, 84%] distribution). The median history of nine halos with \u21932.5 higher disk masses, which are chosen randomly from the EDEN fiducial sample, is shown as dashed grey (Symphony M\u2192,Disk \u21932.5); this sample is more consistent than the full Symphony suite with MW observations (left) and previous embedded disk simulations (right). The inferred MW SFHs in the left panel are from Garrison-Kimmel et al. (2017); Kelley et al. (2019), and the z = 0 M\u2192 of the MW is from Cautun et al. (2020). Vertical dotted lines mark z = 3, when the disks are initialized.\n\nLeft panel: The thin gray lines indicate each individual Symphony host. Right panel: Our simulations are compared stellar mass histories in previous work, including two MW hosts in Garrison-Kimmel et al. (2017) (dashed orange curves), and Symphony hosts using abundance matching (AM, Behroozi et al. 2013a) (dashed turquoise). Phat-ELVIS (Kelley et al. 2019) used the same AM model; the dotted turquoise curve shows Symphony AM scaled to their final 10 M\u2191 mass of M\u2192 = 5.9 \u2193 10.\n\nBesides covering a much broader range of disk-to-halo median stellar mass, which is consistent with the MW (M \u2192 = 1010.21 M\u2191 in Snaith et al. 2014 and M \u2192 = 1010.48 M\u2191 in Kruijssen et al. 2019), EDEN also accounts for the intrinsic scatter in disk formation histories due to the diversity in halo assembly. This is naturally achieved with the UM-predicted stellar mass growth histories of the disks, which includes a correlation between galaxy and halo growth.\n\nIn Fig. 1, we also show the wide distribution of star formation half mass scale (a\u2192,1/2) ranging from 0.4 to 0.9, which anti-correlates with stellar mass and halo concentration cvir (middle panel), and are mostly later than their halo half-mass formation time scale (right panel). Disks with larger stellar masses also form earlier, giving them more time to tidally strip subhalos historically.\n\nIn Fig. 2, we show the UM-predicted stellar mass growth histories for each EDEN MW-mass host. We compare the host disk formation histories to observational constraints on the MW (Snaith et al. 2014; Kruijssen et al. 2019) in the left panel. The MW stellar mass growth histories are inferred from the chemical abundances of disk stars (red, Snaith et al. 2014) and the globular cluster age-metallicity relation (blue, Kruijssen et al. 2019). There are differences between the median EDEN star formation history and that of our Galaxy. This is mainly caused by the excessive early (lookback time \u21ad 9 Gyrs) star formation in the real MW, making it more massive and earlier-forming than most EDEN hosts.\n\nIn the most recent nine Gyrs, EDEN hosts formed M \u2192 = 1010.33 M\u2191 values are placed at the population mean given their Mvir,host.# 6 WANG ET AL.\n\nwithout resampling the intrinsic scatter. We also show a \u2018scaled\u2019 version of the Symphony AM model stellar mass growth histories that have their median M \u2192 normalized to the M \u2192,Disk = 5.910 M\u2191 value adopted in Phat ELVIS. Compared to either FIRE-2 or Phat ELVIS, UM stellar mass growth histories cover a much broader range than previously adopted methods and especially better sample the low-M \u2192 and late-forming MW central galaxies (grey band). This crucial aspect of the present work helps us better place the MW\u2019s disk in the context of a broader range of MW-mass halos.\n\nThese comparisons between disk growth models highlight the impact of employing UM stellar mass histories for EDEN: not only is the real MW biased towards higher disk masses and earlier star formation, but previous theoretical work has also left the wide range of disk masses and formation histories unexplored due to specific modeling choices. Therefore, EDEN, with its wider coverage of disk masses and formation histories, should provide a more comprehensive view of subhalo abundance in MW-mass halos.# 2.3. Embedded disk potential setup\n\nWe initialize analytic disk potentials in the z = 3 snapshot of the SymphonyMilkyWay DMO simulations (similar to Garrison-Kimmel et al. 2017; Kelley et al. 2019). We restart the simulations from z = 3 and evolve the total equivalent mass of the disk potential in each host following their UM-predicted stellar mass histories (Fig. 2). We concatenate the output snapshots from the embedded disk simulations at z \u21ac 3 with the z > 3 snapshots in their DMO counterparts for halo finding and merger tree construction.\n\nThe disk potential has three structural components:\n\n$(R, Z) = $ \u2192 (R, Z) + $ g (R, Z) + $ b (r),\n\nwhere $ \u2192 is the axis-symmetric stellar disk, $ g is the axis-symmetric gaseous disk, and $ b is the spherical bulge. Numerical tests in Garrison-Kimmel et al. (2017); Green et al. (2022); Santistevan et al. (2024) have shown that the disk mass is by far the most decisive factor in affecting subhalo abundances, hence our specific choices of disk geometry are sub-dominant compared to the disk mass (see discussions in Section 5.3). We choose the symmetry axis of the stellar and gaseous disks to be both fixed and aligned with the MW host halo\u2019s spin at z = 0 throughout the simulation. In the following, we refer to the combined potential $(R, Z) when stating \u2018disk potential\u2019.\n\nFor the density profile of the stellar ($ \u2192 ) and gaseous ($ g ) disks, we assume that they both follow the commonly used Miyamoto\u2013Nagai potential (Miyamoto & Nagai 1975):\n\n$ d (R, Z) = ! \u221a(\u221a 2+ Z 2 + Rd) 2+ R2.\n\nHere, G is the gravitational constant, M d is the disk mass for stellar (0.60M \u2192 ) or gaseous (0.27M \u2192 ) disk, Rd is the disk scale radius (2.5 kpc for stellar and 7 kpc for gaseous at z = 0), and hd is the disk scale height (0.35 kpc for stellar disk and 0.084 kpc for gaseous at z = 0). The gas fraction is consistent with the latest empirical constraints on the HI to stellar mass ratio (Padmanabhan & Loeb 2020), which yields a gas-to-mass ratio of \u2192 30% at z > 2 and \u2192 15% at z < 1 for MW-mass halos (Fig. 1 in Padmanabhan & Loeb 2020). Our stellar disk scale radius is consistent with the MW\u2019s thin (2.0 \u00b1 0.2 kpc) and thick (2.6 \u00b1 0.5 kpc) stellar disk constraints (Bland-Hawthorn & Gerhard 2016). The MW gaseous disk is less constrained with typically \u2192 2\u2193 the radius of the stellar disk (Bland-Hawthorn & Gerhard 2016; Posti & Helmi 2019), indicating our assumption is reasonable. Our choice of the stellar disk scale height is representative of MW measurements that span from 140 to 430 pc using different stellar populations (McKee et al. 2015), while the gaseous disk scale height assumed is consistent with cold CO disk measurements of \u21ab 100 pc (Heyer & Dame 2015). In addition to the CO disk, there is also a thin and a thick cold HI disk, both having radially varying scale heights (h > 200 pc, Valenti et al. 2016), and we do not model this subtlety in this work.\n\nFor the spherical bulge ($ b ), since it is only \u2192 10% of the total disk mass, its specific density profile would not change our results significantly. Hence, without loss of generality, we assume a Hernquist potential profile (Hernquist 1990) for it:\n\n$ b (r) = ! r + r bGMb.\n\nHere, M b is the bulge mass (0.13M \u2192 ) and rb is the bulge scale radius (0.5 kpc). The stellar bulge mass measurement of the MW has large observational uncertainties (Robin et al. 2012; Nesti & Salucci 2013; Sofue 2013; Portail et al. 2015; Valenti et al. 2016; Portail et al. 2017), which gives a bulge-to-stellar mass fraction of 10 \u2192 30%. Thus, our bulge fraction assumption of 13.1% is consistent with observational constraints. The bulge size choice follows from previous self-interacting dark matter simulations (Robles et al. 2019) that also implemented embedded disks. We note that this scale radius is consistent with the shorter axes of the MW bulge fitted using triaxial models (Launhardt et al. 2002; Robin et al. 2012), but larger than the \u2192 0.1 kpc bulge radius fitted with spherical models (Nesti & Salucci 2013; Piffl et al. 2014).\n\nAt every snapshot, we assume 60%, 27%, and 13% of the total UM-predicted stellar mass to be distributed in the stellar disk, gaseous disk, and spherical bulge, respectively. The gaseous disk here is merely a more extended disk component. The combined disk potential has a mass that is a conservative lower bound of the central galaxy baryon mass as there is, on average, another 15% \u2192 30% of gas mass in true galactic.# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS\n\ndisks. However, it is unclear if the effects of subhalo mass loss due to a true viscous gas disk can be well-approximated by an analytic potential without proper treatment of hydrodynamics. It has also been shown in previous embedded disk simulations (Garrison-Kimmel et al. 2017) and confirmed by our findings (Fig. 5) that a \u21ad 30% change in disk mass does not create sizable differences in subhalo abundance that overcome the host-to-host scatter.\n\nIn our model, all characteristic sizes in the disk potential (Rd, hd, rb) evolve according to the mean mass-size relation evolution from 3D-HST (van der Wel et al. 2014), r \u2197 (1 + z)0.75 M0.72 in their Table 2 for late-type galaxies with M \u2192 1010. Apart from the redshift evolution of the total disk mass and physical sizes, we assume that the three components\u2019 relative mass and size ratios in $(R, Z)$ are fixed throughout the simulation. We provide the detailed formulae of disk acceleration and density distribution in Appendix A.\n\nTo ensure that the embedded disk potentials are anchored to the MW host halo centers, we seed a \u2018sink\u2019 particle (Bate et al. 1995) at z = 3 in every halo when the disks are initialized. We move the low-resolution particle (mDM,low \u2192 108 M\u2609 \u2192 3 Mpc) to the halo center and initialize its velocity as the ROCKSTAR-defined halo velocity. We note that the sink particle mass is comparable to the disk mass at initialization and becomes < 1% of the disk mass at z \u21ab 1. We modify GADGET-2 to self-consistently implement the additional gravity from the disk potential on the dark matter particles. At every time step, the sink particle location is first used to initialize the center of the disk potential. Then, the gravitational force from the disk potential is applied to every particle other than the sink, in addition to the N-body forces in between those particles. Finally, the net gravitational force from all other particles (N-body forces and back reactions on the disk) is exerted on the sink particle to advance its velocity and position. The large particle mass of the sink particle combined with the equivalent mass of the disk potential (already \u21ad 103 than high-resolution particles at z = 3, Fig. 2) will keep the sink particle well-anchored to the MW center due to large dynamical friction (see Fig. 10). This procedure ensures a physical and stable implementation of the disk potential while adding negligible computational costs to the N-body simulation.\n\nUsing the disk potential, we can derive the corresponding density distribution from the analytic disk potential using the Poisson equation. We show an example 2D density distribution cross section in Fig. 3. We assume a total equivalent stellar mass of 2 \u2193 1010 M\u2609; the total disk density is shown in the top panel. In the bottom panel, we show the density ratio between the disk density and the underlying dark matter density to highlight where the disk dominates.\n\nTo explicitly isolate the effect of the disk mass, we randomly choose nine halos out of our suite of 45 EDEN halos and re-simulate them with \u21932.5 higher disk masses (con-# WANG ET AL.\n\nstant multiplicative factor) while fixing the shape of their disk growth histories. By doing this, we not only broaden our halo sample at high disk masses similar to our Galaxy, but we can also isolate the effect of varying disk masses on subhalo abundance. We show in Fig. 2 the median stellar mass growth history of the high-disk-mass subsample. They are more consistent with recent MW observations (top panel) and previous embedded disk simulations (bottom panel) than the full EDEN suite.\n\nIn the following, we refer to the set of 45 EDEN halos with UM-predicted M\u2192 (z) as \u2018EDEN fiducial\u2019 (or EDEN for short) and the nine high-disk-mass subsample as \u2018EDEN M \u2192,Disk \u2193 2.5\u2019; we refer to the DMO counterparts of \u2018EDEN fiducial\u2019, i.e. the 45 SymphonyMilkyWay halos, as \u2018Symphony\u2019; we refer to the nine DMO counterparts of \u2018EDEN M \u2192,Disk \u2193 2.5\u2019 as \u2018Symphony M \u2192,Disk \u2193 2.5\u2019.# 3. THE ALLURING MYTH OF \u201cDISRUPTION\u201d\n\nBefore presenting our results, we discuss some important issues related to their interpretation. The basic question is whether a reduction in subhalo peak (or present-day) mass functions should be interpreted as physical disruption\u2014i.e., a reduction in subhalo abundances that would persist at any resolution level. Based on the recent literature, we argue that the safest and likely most accurate interpretation is instead that the disk enhances subhalo mass loss rates, such that objects are stripped below our resolution limit.# 3.1. Background\n\nThe language that the field uses to describe mass loss in subhalos paints a picture in which subhalos are completely destroyed in a cataclysmic, discrete event: subhalos are said to \u201cdisrupt.\u201d In previous embedded disk simulations (Garrison-Kimmel et al. 2017; Kelley et al. 2019), the abundance suppression in the subhalo peak mass function (SPMF) is used as a diagnostic for \u2018disruption\u2019 of subhalos instead of enhanced mass loss. The peak mass of subhalos do not change after infall, so if the subhalos only experience enhanced stripping and could still be tracked by the halo finder, there would be no suppression in the SPMF in the ideal case; whereas \u2018disruption\u2019 of subhalos would lead to objects no longer detectable and cause a reduction in the SPMF. However, this picture is not correct for DMO CDM subhalos, and the concept of disruption can conflate three distinct scenarios. Colloquially, a subhalo \u201cdisrupting\u201d can mean any of the following:\n\n1. the point at which the subhalo is no longer self-gravitationally bound;\n2. the point at which the subhalo finding tools can no longer identify the subhalo; or\n3. the point at which the satellite galaxy the subhalo hosts has lost so much stellar mass that it would no longer be observable.\n\nAt first blush, meanings 1 and 2 may seem reasonable or even synonymous. Early CDM simulations and analytic arguments seemed to favor a picture in which subhalos could truly disrupt (e.g. Gonzalez-Casado et al. 1994; Hayashi et al. 2003). These arguments generally rely on comparing the characteristic energy injected into the subhalo by tidal shocks at pericenter to the total binding energy of the subhalo or by considering the properties of an isolated halo profile that was suddenly truncated. Although in detail these arguments break down (van den Bosch et al. 2018), even in modern simulations, individual subhalos can suddenly disappear from subhalo catalogs at high resolutions or can be seen to experience runaway mass loss while they are still resolved by the subhalo finder (e.g., Fig. 16 and Fig. 5 in Mansfield et al. 2023, respectively). But this apparent \u201cdisruption\u201d is illusory for most subhalos. While it is true that high-mass subhalos experience so much dynamical friction that they sink to the centers of their host within a few orbits (often remaining self-bound even after the merger finishes: Han et al. 2016; Diemer et al. 2023; Mansfield et al. 2023), lower mass subhalos experience much less dynamical friction (e.g., van den Bosch et al. 2016). Idealized, high-resolution simulations show that low-mass subhalos with the high-internal-density \u201ccuspy\u201d profiles favored by CDM instead stay bound for much longer than the lifetime of the universe (Pe\u00f1arrubia et al. 2010; van den Bosch & Ogiya 2018; van den Bosch et al. 2018; Errani & Pe\u00f1arrubia 2020; Errani & Navarro 2021). This is true even for simulations that include embedded disk potentials (Green et al. 2022). The loss of high-resolution subhalos from simulation catalogs can be explicitly shown to be a combination of subhalo finder failures and numerical non-convergence (Mansfield et al. 2023), while being highly dependent on resolution and subhalo finder.\n\nThe near limitless durability of DMO subhalos in !CDM is caused by their density profiles; models that lower the inner density \u2014 for example, by modifying the nature of the dark matter particle \u2014 can accelerate mass loss rates and even lead to true, resolved unbinding (Nadler et al. 2021; Errani et al. 2023b; Yang et al. 2023; Nadler et al. 2023a; Du et al. 2024). This distinction makes it even more important to be careful about the term \u201cdisruption\u201d in CDM contexts, as the term is a correct description of a physical process that is a hallmark of some alternative dark matter models. That said, even for non-!CDM models, it is worth being careful about what is and is not true disruption. The same processes that can numerically destroy DMO !CDM subhalos can accelerate the destruction of subhalos in other frameworks.\n\nThe third meaning above is on somewhat better theoretical footing for CDM subhalos: satellite galaxies can always# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS# 3.2. Interpreting EDEN Results\n\nThus, when evaluating the impact of the disk potentials on subhalo mass functions in our simulations, we do not say that decreased subhalo mass function amplitudes imply that the disk completely disrupts subhalos. Instead, we interpret this as evidence that disks accelerate subhalos\u2019 mass loss rates.\n\nThis distinction holds even when we compare the subhalo peak mass function between DMO and Disk runs (Section 4.2). The peak halo mass Mpeak is the peak historical value of the halo virial mass Mvir, which reflects the pre-infall mass of subhalos before experiencing significant tidal stripping from their hosts. In the case of a suppressed SPMF, the increased mass loss rate causes subhalos to approach the numerical-driven limits of the sample more quickly. Given the discussion in Section 3.1, one may question the wisdom in evaluating SPMF at all: after all, at low masses, the SPMF is just the infall mass function minus subhalos that have either numerically disrupted or have been lost by the subhalo finder. Skepticism is warranted, but Mpeak-based measurements serve a legitimate purpose. The fact that subhalos must lose large amounts of dark matter mass prior to losing any stellar mass means that infall- and peak-based mass definitions tend to be better predictors of galaxy properties than the current halo mass (e.g., Reddick et al. 2013). In particular, Mpeak-selected subhalo samples have much higher small-radius number densities around their hosts than Mvir-selected samples (see review in Section 5.2 of Mansfield et al. 2023), in line with observations.\n\nIndeed, even when analyzed with SYMFIND, which is capable of tracing \u2198 90% of the subhalos at our minimum mass threshold to the mass scales at which galaxy disruption is likely to begin (Mansfield et al. 2023), the median value of Mvir/Mpeak in our sample is approximately 0.2 in both SymphonyMilkyWay and EDEN. This Mvir/Mpeak threshold is a factor of 2 to 4 higher than the masses that hydrodynamic simulations (Smith et al. 2016) and empirical models (Moster et al. 2018; Behroozi et al. 2019; Wang et al. 2024) predict that stellar mass loss should occur. Thus, the vast majority of subhalos considered in this paper would likely still host visible galaxies (if they had one to begin with). This means that the differences in Mpeak functions discussed below would likely qualitatively extend to stellar mass-selected samples, even if the quantitative translation would require significant modeling work (see Section 5.1 for further discussion).# 4. RESULTS\n\nHere, we present the key results of EDEN. As we have reviewed in Section 3, there are important caveats regarding whether the reduction in subhalo mass functions in our disk simulations represents physical \u201cdisruption.\u201d Due to these complications, we explicitly avoid the term \u201cdisruption\u201d and instead describe how the disk suppresses subhalo peak and present-day mass functions; we also study the spatial and orbital dependence of these effects. In Section 4.1, we present the magnitude of subhalo mass function suppression due to the disk as a function of host stellar-to-halo mass ratio and demonstrate that a larger disk creates more subhalo mass loss; in Section 4.2 we present the subhalo peak mass functions; in Section 4.3 we present the subhalo radial distance and pericentric distance distributions; in Section 4.4 we present the subhalo infall time distributions.# 4.1. Disk-to-halo mass ratio as a critical factor for subhalo abundance\n\nTo intuitively visualize the effect of different disk-to-halo mass ratios on subhalo abundance, we show in Fig. 4 the projected dark matter density maps for four example halos from the EDEN and Symphony comparing their DMO and Disk counterparts. The four example halos fall in four quartiles of disk-to-halo mass ratios; this range creates a visually significant change in subhalo abundance. The larger the disk-to-halo mass ratios, the stronger the suppression of substructures, especially in the central regions of the host. We also note that the host central density becomes higher, and their shapes become rounder due to adiabatic contraction. The rounder halo shapes are qualitatively consistent with results from previous embedded disk simulations (Garrison-Kimmel et al. 2017; Kelley et al. 2019); we show the axis ratios in Appendix B.\n\nTo further quantify the effects of disk-to-halo mass ratio in subhalo abundance, Fig. 5 shows the subhalo count ratios within 100 kpc and Rvir,host between the EDEN fiducial Disk runs and their corresponding Symphony DMO runs.# Figure 4.\n\nDark matter density maps for four hosts in four quartiles of M\u2192,Disk /Mvir,host increasing from top to bottom. The left column shows the SymphonyMilkyWay DMO halos and the right column shows their embedded disk counterparts in EDEN. The DMO and Disk density maps for each halo share the same color scales. The dashed circles mark out the virial radii and the scale bars denote 50 kpc h-1. It is apparent that subhalo abundance suppression becomes stronger with increasing M\u2192,Disk /Mvir,host, especially in the inner regions of each host. The central density of the hosts also increases more from top to bottom due to higher adiabatic contraction with larger embedded disk potentials.# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS# Figure 5\n\nThe top panel shows only subhalos within 100 kpc of their hosts, while the bottom panel shows all subhalos within the virial radii. Subhalo count ratios between EDEN Disk and Symphony DMO simulations for resolved subhalos with Mpeak \u229c 300m DM.\n\nEach colored circle is a pair of Symphony/EDEN hosts, with M\u2192,Disk /Mvir,host values from Symphony. The nine MW-mass hosts were re-simulated with M\u2192,Disk \u2193 2.5 are shown with diamonds and lines connecting them with their EDEN fiducial runs. The color bar indicates the stellar half-mass formation scale factor.\n\nThe Spearman correlation coefficient between NDisk /NDMO and log 10 (M\u2192,Disk /Mvir,host) for EDEN fiducial is r S = !0.60 for r < 100 kpc and r S = !0.74 for r < Rvir,host, which are statistically significant negative correlations. We also provide exponential fits to these relations whose best-fit parameters are shown in Table 1.\n\nThe Spearman rS and exponential fit are carried out on the combined set of EDEN fiducial and EDEN M\u2192,Disk \u2193 2.5 simulations. The cyan crosses are the expected number ratio of subhalos in the absence of disk-induced mass loss due to adiabatic contraction (A.C.) from the disk potential.\n\nThe vertical red (MW, Cautun et al. 2020) and magenta (M31, Watkins et al. 2010; Tamm et al. 2012) bands mark the observational constraints for MW/M31. Orange crosses in the top panel denote the two DMO/Embedded-disk halo pairs from Garrison-Kimmel et al. (2017) whose hydrodynamic counterparts were simulated with FIRE-2 (Wetzel et al. 2023). Disk effects increasingly dominate over adiabatic contraction with increasing M\u2192,Disk /Mvir,host.# 12 WANG ET AL.\n\n(45). In both radial ranges, there are clear trends that subhalo abundance suppression is more effective in halos with larger M \u2192,Disk /M vir,host. Since the halo mass range is quite narrow for the 45 SymphonyMilkyWay hosts (Fig. 1), the primary effect is of changing the disk mass; we have checked that we get almost identical results if we instead used M \u2192,Disk (see also similar trends in hydrodynamic simulation FIRE-2, Fig. 6 in Samuel et al. 2020).\n\nAt low disk mass, M\u2192,Disk /M vir,host \u21ab 0.02, most EDEN halos demonstrate have \u21ab 20% fewer subhalos than their disk-less counterparts within 100 kpc and \u21ab 10% within Rvir,host. There are a few halos with NDisk /NDMO > 1. Physically, the addition of the disk can cause the adiabatic contraction of dark matter (Blumenthal et al. 1986; Ryden & Gunn 1987; Gnedin et al. 2004) and draw more subhalos into the host. Because the disk has two effects, one which increases subhalo abundance and another that decreases it, it is possible that if a low-mass disk\u2019s effect on subhalo mass loss is weak, its host might end up having NDisk /NDMO > 1.\n\nWe apply the Blumenthal et al. (1986) adiabatic contraction model to the Symphony DMO hosts to quantify the expected enhancement in subhalo counts due to adiabatic contraction. This abundance represents the expected number of subhalos within 100 kpc or Rvir,host after including the disk as if the disk potential only contracts subhalo orbits and does not increase their mass loss rates. Assuming spherical symmetry, circular subhalo orbits, and conservation of angular momentum, we have:\n\nM DM (< r0 )r0 =[M DM (< r0 ) + M Disk (< r f )] r f.\n\nHere r0 is the distance to the MW host center in the DMO run, M DM (< r0 ) is the enclosed dark matter mass within r0, which is adiabatically contracted to a smaller sphere with radius r f after adding the disk potential. We assume that the location of the subhalos contract in the same manner as the underlying dark matter density profile of their host, such that subhalos located at r0 in SymphonyMilkyWay are expected to be at r f. We show the contracted subhalo count ratios within 100 kpc and Rvir,host in Fig. 5, which increases with increasing M \u2192,Disk /M vir,host as expected. The EDEN hosts having NDisk /NDMO > 1 within 100 kpc all have low disk-to-halo mass ratios and their subhalo abundance increase are consistent with the predictions of the disk drawing more subhalos inwards due to adiabatic contraction.\n\nAt the massive disk mass end, where M \u2192,Disk /M vir,host \u21ad 0.02, NDisk /NDMO \u2243 1 and disk effects dominate over adiabatic contraction. The two EDEN halos with heavy disks that fall within the observational errors of the MW (blue band, Cautun et al. 2020), have 30 \u2192 40% fewer subhalos within 100 kpc and 20 \u2192 30% fewer subhalos within Rvir,host. We also show the subhalo count ratios for the nine halos in EDEN M \u2192,Disk \u2193 2.5 and connect them with EDEN fiducial values. The average subhalo abundance suppression in the nine halos with heavier disks are more prominent in both radial ranges (side histograms), all having lower NDisk /NDMO with increasing M\u2192,Disk /M vir,host. Three of these EDEN M \u2192,Disk \u2193 2.5 hosts fall within the M31 disk-to-halo mass ratio range and two of them produces \u21ad 50% subhalo abundance suppression within 100 kpc. The one outlier that shows a slight decreased subhalo counts at r < 100 kpc but an increased counts at r < Rvir,host after inserting a M\u2192,Disk \u2193 2.5 disk is undergoing a major merger and experiences a boost in outskirt subhalo counts due to its infalling companion.\n\nApart from the effect of varying disk-to-halo mass ratio on subhalo abundance, a secondary trend with disk half-mass formation scale a\u2192,1/2 is also present in the color coding of the data points in Fig. 5. This is a consequence of heavier disks also growing their disks earlier (Fig. 1), which could impact their subhalo populations longer and lead to more suppression as the disk preferably loses subhalos with earlier infall times (Fig. 8). Therefore, the additional variations in disk formation history could further enhance the suppression in subhalo abundance caused by varying disk masses.\n\nTo summarize our findings, disk effects on subhalo abundance is sensitive to the disk-to-halo mass ratio of the host. The MW and M31 are amongst the most efficient systems in subhalo abundance suppression due to their atypically large M\u2192,Disk /M vir,host. Disk effect on subhalo abundances is weak for small M \u2192,Disk /M vir,host \u21ab 0.02 hosts. For large M \u2192,Disk /M vir,host \u21ad 0.02 hosts, disk effects dominate over adiabatic contraction and become stronger with increasing disk-to-halo mass ratios. We fit the log 10 (M \u2192,Disk /M vir,host )-NDisk /NDMO relation jointly for the 45 EDEN fiducial halos and nine EDEN M \u2192,Disk \u2193 2.5 halos, using an exponential functional form whose best-fit parameters are provided in Table 1. The Spearman rank correlation coefficients for these joint relations are rS = !0.60 within 100 kpc and rS = !0.74 within Rvir,host, both being statistically significant negative correlations between subhalo counts suppression and M \u2192,Disk /M vir,host.\n\n**Table 1. Best-fit parameters for y = NDisk /NDMO as a function of x = log10 (M\u2192,Disk /Mvir,host) shown in Fig. 5.**\n|Range|a|x0|\n|---|---|---|\n|r < 100 kpc|2.64 \u00b1 1.05|0.61 \u00b1 0.10|\n|r < Rvir,host|1.21 \u00b1 0.39|0.67 \u00b1 0.10|# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS# 4.2. Subhalo peak mass functions\n\nFigure 6. The subhalo peak mass function (SPMF) for z = 0 surviving subhalos. Left and right panels show the SPMF functions within 100 kpc and Rvir,host, respectively. The bottom sidebars in each panel show the SPMF ratios relative to the median DMO SPMF in that radial range. In each panel, the solid curve is the median SPMF for Symphony (blue, DMO) and EDEN fiducial (red, Disk). The shaded region in each panel shows the 68% host-to-host scatter in Symphony or EDEN. The red dashed curves are the average SPMF for the nine hosts in EDEN M\u2192,Disk \u2193 2.5, while the blue dashed curves are their DMO counterparts in Symphony. Subhalo abundance suppression is mass-independent only in the high-disk-mass subsample, and high-mass subhalos are preferentially lost in EDEN fiducial. The vertical dashed lines mark out the resolution limit of subhalos in this work (Mpeak \u229c 300m DM \u2192 1.2 \u2193 10).\n\nFurthermore, the SPMF of EDEN M \u2192,Disk \u2193 2.5 is independent of the subhalo masses, in line with previous embedded disk simulations that targeted MW-like heavy disk systems (Garrison-Kimmel et al. 2017; Kelley et al. 2019). Comparing the median SPMF in Symphony and EDEN fiducial, the impact of the disk is stronger within 100 kpc than within Rvir,host, indicating radially dependent suppression (see Fig. 7). In addition to the change in amplitude, the slope of the SPMF is steeper in EDEN than in Symphony, indicating that the disk is more effective at removing mass from high-mass subhalos than it is for low-mass subhalos.\n\nWe also compare the median SPMF of the nine halos in the EDEN M\u2192,Disk \u2193 2.5 subsample with their Symphony DMO counterparts. Their DMO counterparts, labeled as \u2018Symphony M \u2192,Disk \u2193 2.5\u2019, show a similar median SPMF to the full Symphony suite, suggesting that this nine-halo subsample is an unbiased representation of the 45 Symphony halos. However, the SPMF of the EDEN M \u2192,Disk \u2193 2.5 halos are more suppressed than the EDEN fiducial median SPMF at all resolved subhalo masses for both r < 100 kpc and r < Rvir,host, which is another manifestation of subhalo abundance suppression being sensitive to the disk-to-halo mass ratio (Fig. 5).# 4.3. Subhalo radial distribution\n\nAs shown in Section 4.2, subhalo abundance suppression in our disk simulations is radially dependent, with most of the suppression happening at the inner 100 kpc. In this section, we show the subhalo radial distributions in terms of their final distances to their MW hosts\u2019 center as well as their orbital pericenter distances.# Figure 7. Subhalo radial cumulative distributions for resolved subhalos with Mpeak \u229c 300m DM (1.2 \u2193 108 M\u2191).\n\nThe left panels show the subhalo abundance versus the z = 0 distance (r) to the MW host center, and the right panels show the radial cumulative function of their orbit pericenter distances (dperi). Solid curves show average subhalo radial distributions for the Symphony (blue) and EDEN (red); shaded regions with the same color denote the corresponding 68% host-to-host scatter. The dashed red curve shows the average subhalo radial distribution for the nine halos in EDEN M\u2192,Disk \u2193 2.5, and the blue dashed curve shows the corresponding DMO subsample in Symphony. The lower side panels show the CDF ratio of each curve relative to Symphony. The triangles in the right panel denote the scale radii of the \u2018stellar\u2019 (S, 2.5 kpc) and \u2018gaseous\u2019 (G, 7 kpc) disks of the embedded potential. Subhalo abundance suppression becomes weaker for subhalos penetrating into the inner parts of the disk potential. We caution that these radial distributions may change significantly due to numerical resolution (Mansfield et al. 2023).\n\nIn the left panel of Fig. 7, we show the cumulative radial distributions of subhalo distances to their hosts at z = 0. The median EDEN fiducial CDF is more suppressed at all radii than the median CDF for Symphony, with slightly more suppression happening towards smaller radii (r \u21ab 50 kpc). However, the lowering of the CDF median is within the 68% host-to-host scatter of the 45 halos. EDEN M \u2192,Disk \u2193 2.5 magnifies the radial CDF suppression effect, with most of the subhalo abundance suppression happening at r \u21ab 100 kpc relative to their Symphony DMO counterparts. These findings indicate that the surviving subhalo population may not have significantly altered radial profiles due to the disk unless in very high disk-to-halo mass ratio systems like in the MW or EDEN M \u2192,Disk \u2193 2.5.\n\nIn the right panel of Fig. 7, we show the radial distributions for subhalo orbital pericenter distances. To derive pericenter distances, we use cubic splines to interpolate between the lookback time of simulation snapshots and the location of subhalos relative to their hosts. The interpolation is required since the snapshot cadence (\u2192 160 Myr at z = 0 and \u2192 40 Myr at z = 3) is usually too coarse to resolve the time period near each subhalo\u2019s pericenter. The interpolated subhalo and host halo trajectories were then evaluated on 10 Myr cadences during the period when both the subhalo and their host halo existed. The minimum distance between the subhalo and host halo on this 10 Myr grid is then defined as the pericenter distance.\n\nFrom Fig. 7, we can see most of the abundance suppression happens for subhalos with dperi \u21ab 100 kpc. The pericenter distance CDF for EDEN fiducial has a larger suppression than in its radial distance CDF (left panel) and is \u2192 1\u03c9 lower than the Symphony pericenter CDF. This reveals that the disk potential preferentially removes mass from subhalos that came closer to it during pericenter, whereas subsequent orbital motion smears out this signal in the radial CDF. Increasing disk-to-halo mass ratios creates an even larger suppression at fixed r as shown in the EDEN M \u2192,Disk \u2193 2.5 CDF, with most of the additional suppression happening at dperi \u21ab 50 kpc.\n\nTo summarize, halos with disks have fewer subhalos that fall on orbits with pericenter distances dperi \u21ab 100 kpc. Increasing the disk mass increases subhalo abundance suppression strength, especially at dperi \u21ab 50 kpc. We caution that these radial trends in subhalo abundances should be interpreted as qualitative instead of quantitative as subhalo radial distributions can change significantly (Mansfield et al. 2023).# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS# Figure 8. Subhalo counts ratio as a function of their first-infall lookback-time into the MW host for resolved subhalos with Mpeak \u229c 300 m DM (1.2 \u2193 108 M\u2191).\n\nThe left panel shows subhalos within 100 kpc, and the right panel shows subhalos within Rvir,host. The orange circles denote the EDEN fiducial halos, and the turquoise diamonds denote the nine halos in EDEN M\u2192,Disk \u2193 2.5. The shaded bands are Poisson errors on the subhalo count ratios. The subhalos removed from our sample by the inclusion of a disk are preferentially those with earlier infall times.\n\nAs we have shown in the previous sections, subhalo abundance suppression in the presence of a galactic disk is sensitive to the disk-to-halo mass ratio, the subhalo peak mass, and subhalo orbit pericenter distances. As found in Phat ELVIS (Kelley et al. 2019), subhalos that fell in earlier to their hosts are more preferentially stripped than later-infalling subhalos (see also Fig. 1 in Jiang & van den Bosch 2017). The physical intuition behind this effect is that earlier-infalling subhalos fell in when their host halos were smaller and have systematically smaller pericenters (Wetzel & White 2010), leading to stronger tidal stripping from the disk that enhance their mass loss. In the following, we investigate if this effect is generic in EDEN as Phat ELVIS used a MW-like heavy disk mass (Fig. 1).\n\nIn Fig. 8, we show the subhalo counts ratio in EDEN versus Symphony as a function of the lookback time of the first-infall into their MW-mass hosts (r < 100 kpc in the left panel and r < Rvir,host in the right panel). In both radial ranges, subhalo abundance suppression mainly happens for early-infalling subhalos with tfirst,infall > 8 Gyrs, consistent with the findings in Phat ELVIS (Kelley et al. 2019). There are about \u2192 10% of subhalos with r < 100 kpc and fell in from 4 \u2192 8 Gyrs ago that are lost from disk tidal stripping. However, this suppression is not seen considering all subhalos within Rvir,host that fell in 4 \u2192 8 Gyrs ago. This indicates that the additional subhalos brought in by adiabatic contraction (Fig. 5) that have not yet entered the central 100 kpc of the host have not been severely stripped and compensates for the central subhalo abundance suppression (Fig. 7). The nine halos in EDEN M \u2192,Disk \u2193 2.5 further enhance the infall time distribution patterns of EDEN fiducial, with most of the additional suppression of subhalo abundance occurring with tfirst,infall > 8 Gyrs due to the increase in disk-to-halo mass ratio. These findings indicate that the MW disk predominantly loses early-infalling subhalos due to disk tidal stripping, and increasing the disk mass further enhances this effect.# 5.1. A user\u2019s guide to interpreting the scope of theoretical subhalo disruption/mass loss studies\n\nFollowing on the discussion in Section 3, we outline several principles for interpreting the results of studies like our own, which analyze simulated subhalos and depend on how many subhalos have lost so much mass that they have dropped out of the sample (a process often colloquially referred to as \u201cdisruption\u201d). We then outline where our current study fits relative to these points.\n\nQuantitative vs. Qualitative: The most important question in any subhalo study is whether its conclusions require that its results are quantitatively correct, or whether it is sufficient that the results are qualitatively correct. For example, a study claiming that observed satellite population statistics are in conflict with !CDM will almost always require quantitatively correct results, while for a study arguing that two quantities are correlated, qualitatively correct results may be sufficient. This distinction is important because quantitative studies require quantitative attention to the modeling points we bring up below, something that is quite difficult to do. In some cases, we consider such modeling to be an unsolved problem. Qualitative studies are not exempt from these concerns but can usually weather cruder modeling choices.# Sample Selection\n\nWhen studying subhalo populations, one generally needs to select objects based on a mass proxy.7 Popular choices are to select by present-day halo properties such as virial mass Mvir and maximum circular velocity vmax, or to select by pre-infall mass proxies like Mpeak or vpeak, which are the historical maximum of Mvir and vmax.\n\nEach selection choice has its own considerations. First, they target different classes of observations: present-day measures of mass are good tracers of observations that directly probe present-day subhalo mass (e.g., stream gaps, strong lensing, rotation curves, etc.; Bechtol et al. 2022), while pre-infall proxies tend to be better tracers of satellite galaxies selected by stellar mass (e.g., Reddick et al. 2013). Second, population demographics can be very different for samples selected by pre-infall mass proxies than present-day mass proxies. This is particularly true for the radial distribution of subhalos, which is more concentrated in subhalos selected by peak mass than by present mass (e.g., Nagai & Kravtsov 2005; Han et al. 2016). Third, numerical concerns are different for different mass proxies (Mansfield et al. 2023): i) pre-infall selections have more stringent resolution requirements than present-day mass selections and may need to contend with aphysical fluctuations in mass as subhalos disrupt (see also van den Bosch 2017); ii) present-day mass measurements need to contend with the fact that different subhalo finders have different ways of separating the mass of a subhalo from its host and different schemes can bias masses by tens-of-percent; iii) maintaining unbiased vmax values in subhalo velocity profiles for subhalos undergoing mass loss requires almost an order of magnitude more resolution than maintaining unbiased Mvir values.# Resolution\n\nA poorly resolved subhalo may not give accurate predictions for the \u039bCDM model. As subhalos lose mass, they experience more classical two-body scattering (e.g., Power et al. 2003; Ludlow et al. 2019); they may further experience decreased durability due to their simulation\u2019s force softening (van den Bosch & Ogiya 2018; Mansfield & Avestruz 2021), and can eventually encounter runaway mass loss due to discreteness effects (van den Bosch & Ogiya 2018). In a well-calibrated simulation, all three effects affect subhalos at similar resolution levels (Mansfield et al. 2023), but this may not be true in a simulation with excessively large or small force softening scales. Certain subhalo population statistics are strongly dependent on resolution, such as the abundance or radial distribution of subhalos selected at a fixed Mpeak.# Subhalo finder\n\nSubhalo finders are imperfect tools and are often the dominant source of numerical biases in subhalo analysis. For example, the ROCKSTAR subhalo finder (Behroozi et al. 2013b) is one of the most widely used subhalo finders and generally performs at least as well as other major subhalo finders in comparative tests between tools (Knebe et al. 2011; Onions et al. 2012, 2013; Srisawat et al. 2013; Avila et al. 2014; Behroozi et al. 2014; Elahi et al. 2019). However, ROCKSTAR demonstrably loses track of even high-resolution subhalos after relatively modest amounts of mass loss (Griffen et al. 2016; Mansfield et al. 2023) and falsely converges (Mansfield et al. 2023), meaning that even as resolution is increased and the simulation\u2019s subhalos are able to survive for longer, ROCKSTAR will continue losing track of subhalos after they have lost the same amount of mass. The strengths and drawbacks of every halo finder are different and it is important to understand what those weaknesses are in depth, even for qualitative studies. For some subhalo finders, these limitations have never been formally studied. We also recommend that one should generally be skeptical of reliability arguments based on subhalo finder comparisons or internal convergence tests, given that such tests had been performed extensively on the ROCKSTAR subhalo finder without identifying its aforementioned issues.# Galaxy Mass Loss/Orphan Model\n\nTo first order, particles can be stripped from a subhalo when they orbit close to that subhalo\u2019s tidal radius, and a subhalo\u2019s tidal radius gradually shrinks as it orbits its host. Because galaxies are small compared to their halos, low-mass satellite galaxies generally stay intact through their initial mass loss and begin to rapidly lose mass after passing a critical threshold in Mvir/Mpeak (Smith et al. 2013). Whether or not this rapid mass loss is qualitatively consistent with \u201cdisruption\u201d depends on the stellar density profile of the satellite (Errani et al. 2023b). For subhalo finders that lose track of subhalos after this threshold is reached (e.g. SYMFIND, Mansfield et al. 2023), a galaxy mass loss model is needed, lest the finder over predict the abundance of satellite galaxies at a fixed stellar mass. For subhalo finders that lose track of subhalos prior to this threshold (e.g. ROCKSTAR, Mansfield et al. 2023) require both a galaxy disruption model and an \u201corphan\u201d model, i.e., a model that creates post hoc galaxy tracers and evolves them forward based on heuristics. Several points to consider when evaluating these models are (a) that mass loss models calibrated on hydrodynamic simulations likely vastly overestimate mass loss rates due to numerical issues (e.g., Ludlow et al. 2019), (b) that dynamical friction is quite weak in low mass subhalos (van den Bosch et al. 2016) meaning that it is unlikely to be their primary disruption mechanism despite it being a popular prescription, and (c) that the popular choice of generating orphan galaxies during a subhalo\u2019s last snapshot is questionable given that the subhalo recovered during this snapshot is often extremely unreliable.# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS# 5.2. Comparisons with previous embedded disk simulations\n\nWith this context, let us consider the current study. We are studying M peak-selected subhalo samples. The goal of this work is to use these M peak selected samples to qualitatively emulate the behavior of stellar-mass selected samples. We do this because we consider the work associated with producing a quantitatively reliable model of stellar mass loss to be beyond the scope of this study. We consider this a reasonable approximation at this level of accuracy because the majority of our subhalos have M vir /M peak high enough (\u21ad 0.2) that they are unlikely to have started galaxy disruption (see Section 3).\n\nThe interplay between the numerics of our simulations and our chosen subhalo finder, SYMFIND, have been extensively explored in Mansfield et al. (2023). SYMFIND is able to track subhalos well past the point where their galaxies have started to rapidly lose mass and will not be a limiting factor in analysis. However, some subhalos in the n peak \u21ab 5 3 range will experience some degree of accelerated mass loss due to simulation resolution prior to the point when stellar mass loss should begin, although we do not currently have a model that can make quantitative estimates for how many subhalos will be missing in this range. The use of SYMFIND significantly increases the number of subhalos that we find relative to ROCKSTAR (see Appendix D), and the choice between subhalo finders is comparable in effect to the amount of suppression introduced by the disk itself. Much of this difference is due to ROCKSTAR losing track of subhalos early, but some unknown fraction of the difference may come from the fact that SYMFIND is holding onto subhalos so long that their satellite galaxies would have lost enough stellar mass to move outside the equivalent stellar-mass-selected sample. Future work on precise stellar mass loss models will help resolve this issue.\n\nTo begin with, conclusions on subhalo abundance suppression as described in Garrison-Kimmel et al. (2017) and Kelley et al. (2019) are largely consistent with the two EDEN halos with M \u2192,Disk /M vir,host > 0.05 and the EDEN M \u2192,Disk \u2193 2.5 suite that have MW-like heavy disk-to-halo mass ratios (Section 4). Both simulation works have \u2192 10\u2193 higher numerical resolution than EDEN and presented disk effects for subhalos that reached an order-of-magnitude lower masses than our M peak \u229c 1.2 \u2193 108 M\u2191. Therefore, we qualitatively compare these previous simulations with EDEN and emphasize the subhalo count ratios from Garrison-Kimmel et al. (2017), SPMF figure.\n\nIn Phat-ELVIS (Kelley et al. 2019), \u2192 35% subhalos within 100 kpc are lost down to the EDEN resolution limit (see their Fig. 2, peak maximum circular velocity vpeak \u229c 10 km s !1), which is consistent with EDEN (Fig. 5). However, subhalo abundance suppression for one of the two halos in Garrison-Kimmel et al. (2017) is significantly stronger than EDEN (Fig. 5 top panel, data taken from their Fig. 4 at M peak = 1.2 \u2193 10 8 M\u2191). This host in Garrison-Kimmel et al. (2017) has a lower M\u2192,Disk /M vir,host than its sibling but is left only with 30% of subhalos within 100 kpc relative to its DMO run. This is counter intuitive, as its disk formed later than its sibling and always had smaller M \u2192. It is very rare for a MW-mass halo in EDEN to lose > 60% of its subhalos, even when considering the largest M \u2192,Disk /M vir,host hosts in EDEN M \u2192,Disk \u2193 2.5 with \u2192 2\u2193 larger disk-to-halo mass ratios.\n\nWe conjecture two possible factors that can potentially lead to the stronger subhalo abundance suppression seen in Garrison-Kimmel et al. (2017). One possibility is that the subhalo orbits of these two halos had smaller pericenters and were accreted more radially due to the large-scale environment of the host (e.g., accreting subhalos collectively from filaments). Another possibility is that the fragility of AHF halo finder used by Garrison-Kimmel et al. (2017) has even worse subhalo tracking issues than ROCKSTAR (Mansfield et al. 2023), leading to more heavily stripped subhalos being artificially lost track of in the presence of a disk potential. A similar effect could be applicable to AHF (Knollmann & Knebe 2009), the halo finder used in Garrison-Kimmel et al. (2017), such that it numerically enhances the suppression of subhalo abundance. Since only two halos were picked from FIRE-2 and they already happen to yield drastically lower NDisk /NDMO, the average mass-loss for subhalos in FIRE-2 is likely to be statistically stronger than that of EDEN (combining physical and numerical effects). We defer a more careful analysis of this issue to future work.\n\nRegarding subhalo radial distributions, EDEN qualitatively agrees with Garrison-Kimmel et al. (2017) and Kelley et al. (2019). However, there are a few quantitative differences we wish to highlight. Comparing EDEN with Garrison-Kimmel et al. (2017), the latter (see their Fig. 3) has a similar 3D radial distribution to EDEN M\u2192,Disk \u2193 2.5 (Fig. 7 left panel) and loses > 2/3 subhalos within 30 kpc due to the added disk. The differences in 3D subhalo distances mainly come from the cumulative subhalo counts within 100 kpc being \u2192 15% lower than EDEN M \u2192,Disk \u2193 2.5 following discussion above. Comparing Phat ELVIS to EDEN, both Phat ELVIS and EDEN M \u2192,Disk \u2193 2.5 do not have subhalos within 20 kpc of their hosts with the disk added with the suppression ratio decreasing at increasing distances (see their Fig. 3).\n\nThe more significant differences between EDEN and these works are the pericenter distance distributions. Symphony DMO has \u21ab 15% subhalos with dperi < 10 kpc on average, and in EDEN 30% \u2192 40% of subhalos within 10 kpc are lost due to the added disk. However, the two halos in Garrison-Kimmel et al. (2017) have \u21ad 20% subhalos within 10 kpc and reach 50% at dperi \u2192 20 kpc in their DMO run, having much tighter pericenters than EDEN to start with.# 5.3. Caveats with fixing the disk orientation\n\nAs mentioned in Section 2.3, we fix the disk potential axis to the MW host halo spin at z = 0 throughout the simulation. This choice follows previous arguments that the specific shape of the disk potential does not change the quantitative results of subhalo abundance suppression (Garrison-Kimmel et al. 2017; Green et al. 2022) and has been implemented similarly in Phat ELVIS (Kelley et al. 2019). However, this simplistic treatment means that the torque exerted by the disk potential on the subhalos is not back-reacted on the disk, which could otherwise cause the disk to precess and warp. Without this precession, embedded disk simulations set a preferred direction of collapse at halo centers and can create a more ellipsoidal central dark matter density distribution (\u21ab 10 kpc) than their hydrodynamic counterparts (e.g., Fig. 1 lower panel in Garrison-Kimmel et al. 2017).\n\nThe torque on the subhalos due to the assumed rigidity in disk direction may cumulatively influence the spin of the disk that may not keep it aligned with the host halo spin. Additional external torques from nearby structures outside the MW host may also contribute in addition to the subhalos, eventually concertedly shifting the 3D trajectory of the disk. Indeed, as we show in Appendix C, the EDEN hosts end up at locations that are on average \u21ad 15 kpc from their Symphony DMO counterparts at z = 0, sometimes even deviating at \u2192 40 kpc. These shifts in the host halo center do not have significant impacts on our conclusions as the disk (sink particle) is kept within 0.1 kpc of the halo center (Fig. 10) and the subhalo infall times are also largely unchanged as visible through the jumps in host mass growth histories (Fig. 12). This is consistent with the isolation criteria of SymphonyMilkyWay hosts as they are selected to be the most massive object within 4Rvir,host (Mao et al. 2015; Nadler et al. 2023b), which makes the \u21ab 0.1Rvir,host shifts in their locations unlikely to cause significantly different infalling subhalo populations.\n\nThese effects indicate that although the disk direction fixture causes an artificial shift in halo trajectories and more ellipsoidal central densities, its impact on subhalo abundance suppression is negligible. The rigid disk assumption also represents an upper bound on the pure gravitational impact of the disk on subhalos, as the disk in our case cannot absorb orbital energy through precession or warping. This finding is also in line with numerical experiments in Santistevan et al. (2024), which indicate that the orientation of the disk, the density profile of the disk (axisymmetric or spherical), and...# EDEN: EXPLORING DISKS EMBEDDED IN N-BODY SIMULATIONS# 5.4. Effects of numerical resolution\n\nAs mentioned in Section 4.3, the quantitative results in this work, such as the subhalo counts ratio, SPMF, and radial distributions, may be subject to change under different numerical resolutions. We show in Appendix D that, indeed, resolution matters, as well as the specific choice of the subhalo finder. We re-simulate five SymphonyMilkyWay hosts at 8\u2193 higher resolution. Higher resolution leads to a higher SPMF that boosts subhalo counts at all mass scales. The boost in subhalo counts due to higher numerical resolution is comparable to the magnitude of disk effects on subhalo abundances at the same fiducial resolution, further highlighting that the results of disk effects are sensitive to numerical resolution.\n\nTherefore, we advocate for future work quoting quantitative comparisons to EDEN to be conducted at similar numerical resolution and to apply the same halo finder (SYMFIND). As we have checked, the subhalo counts ratios shown in Fig. 5 are more consistent between SYMFIND and ROCKSTAR than the SPMF (Fig. 11) which is more sensitive to the specific halo finder choice. Qualitative comparisons between different simulations using different halo finders should prioritize subhalo abundance ratio (NDisk /NDMO) comparisons rather than absolute subhalo abundance (SPMF).# 5.5. The Milky Way\u2019s disk in a cosmological context\n\nThe most important finding in this work is that subhalo abundance suppression in MW-mass halos is sensitive to the disk-to-halo mass ratio, and thus the MW (as well as M31), with a larger-than-average disk, likely suppresses subhalos more efficiently (Fig. 5). This has profound implications for understanding the galaxy\u2013halo connection of low-mass satellites around MW-mass hosts external to the MW. If one tries to constrain galaxy\u2013halo connection on a broad set of MW-mass objects in the Local Universe from Surveys such as SAGA (Geha et al. 2017; Mao et al. 2021) and ELVES (Carlsten et al. 2022), the overall effect of subhalo abundance suppression across their host samples would be weaker than in the MW itself, due to the rareness (\u2192 1\u03c9 outlier) of a MW-like heavy stellar disk.\n\nFor instance, the MW is a \u21ad 1\u03c9 larger-than-average host compared to the 101 hosts in SAGA (see Fig. 2 in Wang et al. 2024), consistent with EDEN hosts shown in Figs. 1 and 5. For the majority of SAGA-host, that coincides with M \u2192,Disk /M vir,host \u21ab 0.02, disk effects of \u2192 10% within R vir,host and \u2192 20% within 100 kpc are expected for subhalos according to EDEN (Fig. 5). In those cases, constraints on the low-mass galaxy\u2013halo connection based on average number densities of satellites within Rvir,host is rather robust given the expected \u2192 10% disk effects, but radial trends could change significantly especially within the central 100 kpc due to stronger suppression at smaller radii (\u21ad 20, Fig. 7). This indicates that EDEN might mitigate the limitation of UM-SAGA, which predicts fewer quenched satellites than SAGA observations within 100 kpc (Wang et al. 2024), by accounting for the additional tidal forces from the host galactic disk.\n\nHowever, if one focuses exclusively on low-mass satellites in the MW itself (e.g. Nadler et al. 2020), modeling subhalo abundance suppression properly becomes crucial due to the presence of a massive stellar disk that is more effective at stripping subhalos. In fact, Nadler et al. (2020) showed that the subhalo abundance suppression intensity8 of Garrison-Kimmel et al. (2017) is consistent with the luminosity, size, and spatial distribution of MW satellite galaxies but is not precisely constrained by the current data (though future surveys will help reduce uncertainties, e.g. see Nadler et al. 2024). Therefore, our embedded disk simulations, especially the EDEN M \u2192,Disk \u2193 2.5 subsample, establish a novel platform for more realistically modeling subhalo abundances that is particularly important for interpreting the MW.\n\nIn the near future, we plan to extend the EDEN framework to the new \u2018MW-est\u2019 zoom-in simulations (Buch et al. 2024), a suite of 20 MW-like zoom-in simulations that have similar halo masses as SymphonyMilkyWay hosts but also have tailored Gaia-Sausage Enceladus (Belokurov et al. 2018; Helmi et al. 2018) and LMC (van der Marel et al. 2002; Besla et al. 2007; Kallivayalil et al. 2013; Vasiliev 2023) mergers. We also plan to use EDEN as the backbone to model Local Group low-mass galaxies (Weisz et al. 2014; Savino et al. 2023) along with SAGA satellites over the entire observable galaxy stellar mass range (M \u2192 > 102 M\u2191), setting the MW and M31 satellites in a broader cosmological context and constraining the process of cosmic reionization.# 6. CONCLUSIONS\n\nIn this paper, we introduced the EDEN-Symphony simulation suite, a set of 45 MW-mass dark-matter-only (DMO) zoom-in simulations with analytic disk potentials embedded in the host halo centers to capture subhalo abundance suppression due to tidal stripping and mass loss under the influence of a central galaxy baryonic disk. The baseline DMO zoom-in simulations of these 45 MW-mass halos are part of the Symphony compilation (Nadler et al. 2023b). EDEN-Symphony enlarges the number of existing zoom-in simulations with embedded disk potentials in the literature by a factor of three.\n\nA novel feature of the work is that the analytic disk potentials grow self-consistently with their equivalent stellar masses predicted by the empirical galaxy\u2013halo connection.\n\nImplemented using the random forest model introduced in Nadler et al. (2018).# model UNIVERSE MACHINE\n\n(Behroozi et al. 2019) (Sec-tion 2). This ensures that the broad range of disk masses and disk formation histories are well-sampled in a correlated fashion with halo mass and halo assembly. Our simulation suite focuses on Mvir \u2192 1012 M\u2609 halos and covers heavy disk systems like our MW (Fig. 2). We use the particle-tracking subhalo finder SYMFIND (Mansfield et al. 2023) to track subhalos down to Mpeak \u229c 300mDM = 1.2 \u00d7 108 M\u2609 and quantify how the subhalo populations change in the presence of a central disk potential. We have also re-simulated nine embedded-disk halos with MDisk \u2193 2.5 heavier disks (EDEN MDisk \u2193 2.5) to explore the effect of disk mass while fixing the disk growth history. Our main findings are:\n\n1. Subhalo abundance suppression due to the disk is sensitive to the host\u2019s MDisk/Mvir,host such that more suppression occurs in hosts with larger MDisk/Mvir,host (Figs. 4 and 5).\n2. Among MW-mass halos (Mvir,host \u2192 1012 M\u2609), the MW and M31 (MDisk/Mvir,host \u21ad 0.05) have particularly strong subhalo abundance suppression (\u21ad 30% within 100 kpc, Fig. 5) due to their \u21ad 2\u03c9 up-scatter in stellar mass (Fig. 1) and early disk formation (Fig. 2).\n3. In EDEN fiducial hosts, subhalos with large peak masses (Mpeak \u21ad 1010 M\u2609) are more likely to be lost than lower-mass counterparts with the inclusion of the disk. In the more massive disks of EDEN MDisk \u2193 2.5, however, subhalo abundance suppression is independent of subhalo peak mass (Fig. 6).\n4. Subhalos with small pericenters (dperi < 100 kpc, Fig. 7) and early infall times (lookback time \u21ad 8 Gyrs, Fig. 8) are preferentially lost.\n\nThe most important findings of this work are the first two points mentioned above. It is clear from Figs. 1 and 5 that the MW has an atypically large disk for an average Mvir \u2192 1012 M\u2609 halo. Previous simulation work (D\u2019Onghia et al. 2010; Garrison-Kimmel et al. 2017; Kelley et al. 2019) with embedded disk potentials implementing MW-like heavy stellar disks (MDisk \u21ad 6 \u00d7 1010 M\u2609) are potentially biased towards heavy disk systems and overestimates the average subhalo abundance suppression intensities in MW-mass halos. Subhalo abundance suppression is less significant in a broader range of MW-mass halos in the Universe, and suppression ratios obtained in previous models using heavy disk should not be naively generalized to the broader range of MW-mass halos.",
        "context_id": 31,
        "question": "How many MW-mass halos are re-simulated in the EDEN simulations?",
        "answer": [
            "45"
        ],
        "context_length": 81152
    },
    {
        "context": "# Materials and Experimental Methodology\n\nWater-in-oil microemulsions used for droplet evaporation studies throughout this work were prepared using water as the dispersed phase and decane, dodecane, p-xylene, and a surrogate (defined as the mixture of dodecane and p-xylene in the ratio 9:1 by mass) as the continuous oil phase (i. e., base oil). For reference, the properties of the components are tabulated in Table 1. A double-tailed ionic surfactant, sodium bis(2-ethylhexyl) sulfosuccinate, commonly referred to as AOT or Aerosol-OT (\u00b197% purity), was used to stabilize the microemulsions. All the chemicals were procured from Merck and used without any further modification. High purity freshly de-ionized water (resistivity, 18.2 M\u03a9-cm) obtained from a laboratory milli-Q system (ELGA, Purelab Option-Q, DV 25).# Table 1: Properties of Liquids Used to Prepare the Microemulsions\n\n|Component|\u03d6 (mN/m)|\u03bc (mPa.s)|\u201d v H (kJ/mol)|T (\u2103)|p v (kPa)|p v T =85 \u2103 (kPa)|\u03f1 (kg/m\u00b3)|\u03c2| \u03c9=10.6\u03bcm (m\u207b\u00b9)|\n|---|---|---|---|---|---|---|---|---|\n|Water|72.75|0.79|44.0|100|2.30|57.05|997|O(105)|\n|Decane|23.83|0.85|51.5|174|0.19|5.10|730| |\n|Dodecane|25.35|1.36|61.4|216|0.018|0.95|750| |\n|p-Xylene|28.90|0.58|42.4|140|1.16|18.86|860| |\n\nA plot showing temperature dependence of vapour pressure calculated from Antoine equation (log10(pv) = A \u2193 C + TB, where A, B and C are constants tabulated in Table S2) is given in supplementary Figure S1.# Preparation of microemulsions\n\nWater-in-oil microemulsions consist of thermodynamically stable dispersion of nanometer-sized water sub-droplets stabilized by a monolayer of surfactant. The characteristics of these microemulsions are characterized based on two parameters: (1) molar ratio of water to AOT (i.e., \u03c9) and (2) dispersed phase volume fraction (i.e., \u03b5). The \u03c9 and \u03b5 are defined as\n\n\u03c9 = [AOT] / [H2O] and, \u03b5 = VH2O / (VHO + VAOT + VAOT + Voil)\n\nwhere [H2O] and [AOT] denotes the number of moles of water and AOT, respectively, and V denotes the volume of different components. The parameters \u03c9 and \u03b5 can be used as handles to tune the size and number density of water sub-droplets in the microemulsions. As evident from Equation 1, increase in the concentration of water leads to increase in the values of \u03c9 and \u03b5 and is typically used as a handle to tune the size and the number density of the dispersed water sub-droplets.\n\nFor a given value of \u03c9 and \u03b5, the corresponding mass of the continuous oil phase and AOT in the water-in-oil microemulsions can be calculated using Equation 1 by providing the mass of water as an additional input variable. To prepare the microemulsion samples, a pre-determined quantity of AOT is precisely weighed and transferred into pre-cleaned glass vials. Subsequently, an appropriate amount of the oil is added to the vials. The mixture is then shaken gently for a few minutes (depending on the relative quantity of AOT and oil) until the AOT is completely dissolved in the oil phase. Later, a weighed amount of deionized water is added to the oil-AOT mixture and shaken gently until the resulting sample becomes a single-phase optically clear fluid. The samples were stored for 3-4 hours at room temperature (25 \u2103) to ensure equilibration and homogeneity before any experiments or characterization.\n\nTo study the effect of \u03c9 and the nature of the continuous oil phase, microemulsions were prepared using decane as the oil phase at various \u03c9 values: 5, 10, 20, and 40. Furthermore, dodecane microemulsions were formulated at two \u03c9 (= 10 and 20) values, whereas p-xylene# Current Page Content\n\nwhere, k is the Boltzmann constant, T is the absolute temperature, and \u03bc is the solvent viscosity. The water sub-droplet diameter for water-in-decane is found to be DH \u2194 6.2 nm for \u03c9 = 10 and \u03b5 = 0.1. The viscosity (\u03bc) of the microemulsion prepared was measured, and it was found that the viscosity increased with the increase in the value \u03b5.# Figure 1(a)\n\nelucidates the effect of molar ratio, \u03c9 on the size of the water sub-droplets dispersed in a continuous decane phase. The symbols in the plot correspond to the electric field auto-correlation function (g1(\u21bd d)) as a function of delay time (\u21bd d) obtained from dynamic light scattering analysis of decane microemulsions for \u03c9 = 10, 20 and 40 at \u03b5 = 0.1. A single exponential decay of the g1(\u21bd d) versus \u21bd d curve (black continuous lines) indicates the presence of spherical droplets of water dispersed in the decane phase. The shift in the g1(\u21bd d) observed with an increase in the \u03c9, suggests an increase in the size of the water nanodroplets. For the data shown in Figure 1(a), the hydrodynamic diameter (DH) of water sub-droplets is estimated using the fitting procedure described earlier45 and the Stokes-Einstein equation (Equation 2) and is shown in the inset of Figure 1(a).# Figure 1(b)\n\nThe scattering intensity, I(q) versus scattering vector, q, recorded from neutron (\u03b5 = 0.1 and 0.2) and x-ray scattering (\u03b5 = 0.4) captures the effect of change in volume fraction, \u03b5 on the structure of xylene microemulsion at \u03c9 = 10. Since the estimated size of the dispersed water sub-droplets in p-xylene is near the lower limit of the DLS instrument, the use of small angle neutron and X-ray scattering techniques was crucial to gain insight into the structure of these microemulsions. The I(q) versus q data for xylene microemulsions at \u03c9 = 10 and \u03b5 = 0.1 and 0.2 is adapted from Rastogi et al.. The water sub-droplets dispersed in xylene are found to be spherical in shape and had a core diameter of 2.70 \u00b1 0.44 for \u03b5 = 0.1 and 2.92 \u00b1 0.22 nm for \u03b5 = 0.2. More details of the structural analysis can be found elsewhere48.\n\nDue to the limited availability of the beamtime on the SANS beamline, data for \u03c9 = 10 and \u03b5 = 0.4 was recorded on the SAXS instrument and is scaled by a factor of 20 in Figure 1(b) for depiction alongside data from neutron scattering. The experimental details and the fitting procedure for SANS and SAXS data is illustrated previously47,48.# Table 2: SAXS fit parameters for the data shown in Figure 1(c) for microemulsions of different oils at \u03c9 = 10 and \u03b5 = 0.4\n\n|Oil Phase|\u21c02|Diameter, D (nm)|P DI \u2193 D|Length, L (nm)|P DI \u2193 L|\n|---|---|---|---|---|---|\n|p-xylene|29.1|2.5|0.45|-|-|\n|Dodecane|9.8|4.2|0.17|8.9|0.19|\n|Surrogate|24.8|4.1|0.15|9.5|0.07|# Experimental details and post processing\n\nThe schematic of the experimental setup is shown in Figure 2. A droplet of diameter D0 \u2248 850 \u00b1 50 micron is suspended using a single-axis acoustic levitator (Tec5) with a frequency of 100 kHz at standard ambient conditions. The droplet is heated externally at irradiation intensities I\u2193 = 0.05, and 0.1, where I\u2193 = I/Imax and Imax = 1.04 MW/m2 with a 3.5 mm beam diameter continuous CO2 infrared (IR) laser (Synrad 48, wavelength \u03c6 \u2248 10.6 \u03bcm, and maximum power (Pmax = 10W). Droplet evaporation dynamics are recorded using a high-speed camera (Photron SA5) with a high-speed laser light source (CAVILUX\u00ae Smart UHS, 640 nm). A recording rate of 50-125 frames per second (fps) has been used because of the large time scale associated, along with the spatial resolution of 5.2 \u03bcm per pixel. An infrared camera is used for droplet surface temperature measurement at 170 frames per second. At the minimum, six repetitions for each sample were conducted to ensure the reproducibility of data and capture the probability of occurrence of the phenomenon. The droplet images are analyzed and processed using Photron\u2019s proprietary software PFV4(x64) and ImageJ (version 1.53t). For data plotting and figure preparation, Python 3.10 and Inkscape 1.1.2 have been used, respectively. The equivalent diameter of the droplet is calculated as Deq = \u221aDhDv.# This study investigates the vaporization behavior of microemulsion droplets\n\nThis study investigates the vaporization behavior of microemulsion droplets, which usually requires significantly low irradiation intensity depending on the absorption properties of the liquid. The evaporation of the microemulsion droplet can be divided into three distinct stages primarily characterized by the microemulsion composition (\u03b5, \u03c9) and irradiation intensity. Soon after the droplet is exposed to the irradiation, it starts to heat up, and its temperature increases until it reaches a constant value. The temperature rise causes the droplet to undergo evaporation, as a result, the droplet loses its mass and the diameter of the droplet reduces marginally. At the end of evaporation, the formation of a spherical solid residue is observed. As time progresses, this spherical residue undergoes buckling due to the acoustic pressure and stresses generated during the course of evaporation. The formation of the shell is a unique phenomenon observed for microemulsions, primarily due to the non-volatile nature of the surfactant (AOT) used in this work. A detailed discussion about different stages of evaporation is presented in the next section 3.2.# Evaporation of Microemulsion Droplet\n\nTypically, the evaporation of a single component liquid droplet occurs in two stages upon interaction with an IR laser. Stage I is the preheating Stage, where the irradiation energy# Stage II\n\ncorresponds to the steady evaporation stage where the droplet diameter reduces steadily (see Figure 4(a), and (c)) with constant surface temperature.7,53 During the pre-heating phase, the droplet absorbs the irradiation as sensible heat, which increases its temperature. As a result, the droplet\u2019s surface temperature also rises until it becomes constant. Following this, the droplet starts to evaporate, and the irradiation energy absorbed by the droplet during this phase is utilized to vaporize liquid from the droplet surface in the form of latent heat. Consequently, due to the mass loss from the droplet surface, the pure liquid droplet undergoes near-complete evaporation, reaching a final diameter significantly smaller than its initial size (D(t)/D0 \u2194 0.1 \u00b1 0.05). Once the droplet reaches this size, it becomes undetectable within the limitations of the current experimental setup, either due to visualization limitations or loss of stable levitation in the acoustic levitator.\n\nThe fundamental difference between a pure liquid and a microemulsion droplet is that the pure liquid consists of a single component whereas microemulsions are multi-component systems, which typically contain a base oil (i.e., continuous phase), water (i.e., dispersed phase), and a stabilizer or surfactant. In this study, we use microemulsions (water-AOT-oil) droplet, which has nanometer-sized water sub-droplets dispersed into the continuous phase (oil), where a monolayer of surfactant (AOT) stabilizes the interface between the water and oil, as shown in the Figure 5(a). The microemulsions are a ternary system and quaternary for the surrogate microemulsion (water-AOT-dodecane-xylene) and their evaporation characteristics depend on the molar ratio of water to AOT (\u03c9), and dispersed phase volume fraction (\u03b5).44,47\n\nThe evaporation of a microemulsion droplet has been observed to occur in three different stages, where the first two stages (namely preheating and steady evaporation) are similar to that for single-component droplet evaporation. An additional stage (Stage III) referred to as unsteady evaporation stage is observed, where the droplet evaporation rate continuously decreases until the evaporation completely stops, which is attributed to the non-volatile.# (a) Pure decane\n\n|t=0|0.8|10|12|20|28|\n|---|---|---|---|---|---|\n|Stage I: pre-heating|Stage II steady evaporation|Stage II steady evaporation|Stage II steady evaporation|Stage II steady evaporation|Stage II steady evaporation|# Nature of the Surfactant AOT\n\nUpon heating, AOT molecules undergo agglomeration as the evaporation proceeds, and their concentration at the droplet surface increases, leading to the formation of a shell at the end of evaporation. During evaporation, the time scales associated with different stages are affected by the components and compositional parameters (\u03c9 and \u03b5) of the microemulsion and the laser irradiance. The droplet evaporation time (i.e., lifetime) (\u21bd) of a microemulsion droplet, defined as the time period between the start of laser irradiation on the droplet and the end of evaporation, is given as:\n\n\u21bd = \u201dtph + \u201dtse + \u201dtue (3)\n\nwhere \u201dtph, \u201dtse, and \u201dtue denote the preheating, steady, and unsteady evaporation time periods, respectively. The quantitative measures associated with droplet evaporation, such as evaporation rate and time scales, have been calculated from the experimental data and discussed in subsequent sections.# Stage I: Pre-heating\n\nAs discussed earlier, during the first stage (pre-heating), the laser irradiation increases the temperature of the droplet. It is important to note that droplet heating exhibits directional dependence due to the unidirectional irradiation from the laser. Consequently, the droplet experiences non-uniform heating initially. However, for an acoustically levitated droplet, the rotation and internal flow facilitates the transport of energy throughout the droplet. This eventually leads to a more uniform temperature distribution within the droplet, as shown in the Figure 5(a). Following laser activation, the temperature of the droplet exhibits a monotonic increase. This initial heating phase is referred to as the pre-heating period, characterized by a continuous rise in temperature as illustrated in Figure 5. During the pre-heating phase, the irradiation from the laser is used to increase the internal energy of the droplet, which translates to a rise in its surface temperature until it becomes constant.# Decane (\u03c9=5, \u03b5=0.2)\n\n|t|0|0.28|0.64|1.2|1.67|2.40|2.89|3.54|4.15|\n|---|---|---|---|---|---|---|---|---|---|\n|Stage I: pre-heating|4.60|4.84|5.38|5.33|5.87|6.04|8.44|11.06|13.54|\n|Stage II: steady evaporation| | | | | | | | | |\n|Stage II: unsteady evaporation| | | | | | | | | |\n|Shell buckling| | | | | | | | | |# Figure 5\n\n(a) Infrared images showing the droplet surface temperature distribution during different stages of evaporation for decane microemulsion (\u03c9 = 5, \u03b5 = 0.2) droplet. Comparing the evolution of droplet surface temperature for different values of (b) \u03b5 at \u03c9 = 5 and at different (c) \u03c9 values at \u03b5 = 0.1 for decane microemulsion at a laser irradiation intensity of I\u2193 = 0.05. (d) showing droplet surface temperature variation of different oil phase for \u03c9 = 10, \u03b5 = 0.1 at I\u2193 = 0.05.\n\n17# Microemulsion Droplet Surface Temperature\n\nThe microemulsion droplet attains a surface temperature (T) of 89 \u00b1 2 \u2103 for \u03b5 = 0.4, and it is reduced to 85 \u00b1 2 \u2103 for a lower \u03b5 (0.1) at I\u2193 = 0.05. A higher temperature for a higher \u03b5 is expected due to the presence of a larger number of dispersed sub-droplets. For \u03b5 = 0.1, for different \u03c9 values, the droplet surface temperature ranges between 60 \u2193 90 \u2103. The surface temperatures of microemulsions containing different base oils have also been analyzed; the results show that for all the emulsion droplets, the surface temperature ranges between 80 \u2193 85 \u2103.# Stage II: Steady Evaporation\n\nDuring the steady evaporation stage, the droplet undergoes a well-defined, steady-state evaporation phenomenon, which is characterized by a constant rate of decrease in droplet diameter, as observed in Figure 4. The evaporation phenomenon can be attributed to a balance between the energy supplied by the laser irradiation and the energy required for vaporization of the droplet, considering a negligible amount of the energy is lost to the surroundings via conduction. As a result, a cloud of vapor surrounding the droplet surface is created, which then diffuses away due to a concentration gradient.\n\nFigure 6(b) shows the various stages of evaporation for a microemulsion droplet. During Stage II, preferential vaporization occurs between the two volatile components (oil and water), while the surfactant AOT does not evaporate due to its non-volatile nature. Thus, the nature of base oil and its properties significantly influence the evaporation rate for a microemulsion droplet. Although the dispersed water sub-droplets can contribute to the overall evaporation rate, their influence is contingent upon transport to the droplet surface. In essence, the rate of evaporation in microemulsion droplets is dictated by the properties of base oil, and the dispersed water sub-droplets must traverse to the surface to participate in evaporation.\n\nUpon heating a microemulsion droplet, the temperature rise induces an increase in the interfacial tension at the interface between the base oil and dispersed water sub-droplets.\n\n19# Evaporation Rate of Microemulsion Droplets\n\nThen aggregate within the droplet to form larger AOT particles (see Figure 6). For a given \u03c9, the water and AOT volume fraction increases with the increase in \u03b5. This increased concentration of AOT molecules inherently increases the probability of their agglomeration. Consequently, microemulsions with higher \u03b5 values can be expected to exhibit a greater number of agglomerated AOT particles compared to those with lower \u03b5.\n\nThe overall evaporation rate of a microemulsion droplet is anticipated to be influenced by its constituents and properties, contingent on irradiation intensity. While the readily available base oil at the droplet surface is the primary determinant, dispersed water can also contribute to evaporation when transported to the surface. There are three different mechanisms through which the water sub-droplets may get exposed to the surface; (1) receding droplet surface exposing water sub-droplets due to evaporation, (2) diffusive transport, and (3) convective transport due to internal flow.\n\nThe receding time scale of the droplet surface is the same as the evaporation time scale, which is of the order tevp \u2194 O(101) at I\u2193 = 0.05 and \u2194 O(100) at I\u2193 = 0.1. The diffusion time scale (tdif) can be calculated using the formula tdif \u2194 D02/De!, where De! is the effective diffusivity of nanometer-sized dispersed water sub-droplets. The diffusivity of these small sub-droplets is extremely low (\u2194 O(10\u219211)), therefore the diffusion time scales are very large tdif \u2194 O(103) (for decane microemulsion at \u03c9 = 10, \u03b5 = 0.1) compared to evaporation time scale.\n\nThe convection time scale (tcov \u2194 D0/ucov), where ucov is the velocity inside the acoustically levitated droplet. The convection velocity can be calculated using ucov = \u221a(\u03bca \u03f1aumax), where \u03bca and \u03f1a are the dynamic viscosity and density of the air medium surrounding the droplet, and \u03bcd and \u03f1d are the dynamic viscosity and density of the droplet. The umax is the maximum acoustic streaming velocity which is given as umax = c0Ma, where c0 is the speed of sound and Ma is the Mach number, which is related to the acoustic sound pressure level (SPL), as SPL = 197 + 20 log(Ma). The convection velocity due to internal flow inside the droplet is ucov \u2194 250 mm/sec which gives the convective time scale of tcov \u2194 O(10\u21923). Although the convective time scale is much smaller compared to the evaporation time scale, it is important to note that the streamlines.Within an acoustically levitated droplet are azimuthal rather than radial. Therefore, it can be assumed that the convective velocity does not influence water sub-droplet transport towards the surface. As a result, diffusive and convective transport can be assumed to have minimal influence on the overall evaporation of the microemulsion droplet. Therefore, the evaporation of water sub-droplets primarily depends on the receding of evaporating droplet surface. A schematic diagram illustrating the role of different components in microemulsion droplet evaporation is presented in Figure 6(c), however, it is important to note that experimental observation of this phenomenon is extremely difficult due to nanometer size (\u2194 5 \u2193 30 nm45) of these water sub-droplets. The temporal evolution of normalized droplet\n\n|(a)|(b)|(c)|\n|---|---|---|\n|Decane|Decane|Decane-AOT mixture|\n|(d)|(e)|(f)|\n|Dodecane|p-Xylene|Surrogate|\n\nFigure 7: Comparing the temporal evolution of non-dimensional droplet diameter for different values of \u03b5 at (a) \u03c9 = 5 (b) \u03c9 = 10 for decane microemulsion. (c) depicts the droplet diameter regression for the mixture of decane and surfactant (without water, \u03c9 = 0) revealing a similar trend of evaporation for different \u03b5, here \u03b5 solely represents the volume fraction of AOT within the decane-AOT solution. Figures (d), (e), and (f) illustrate the temporal evolution of droplet diameter for dodecane, xylene, and surrogate microemulsion droplets, respectively. All plots shown are at a constant laser irradiation intensity of I \u2193 = 0.05.\n\n22# Evaporation Rate and Microemulsion Droplets\n\nThe diameter is shown in Figure 7, and it is evident that a strong correlation exists between the dispersed phase volume fraction (\u03b5) and the evaporation rate of microemulsion droplets. For all the values of \u03c9 (water to AOT molar ratio), as \u03b5 (dispersed phase volume fraction) increases, the evaporation rate decreases across all the base oils investigated in this study.\n\nThe decrease in evaporation rate is primarily governed by two key factors: (1) lower oil volume fraction (\u03b5oil), (2) increase in the number of water sub-droplets present in the suspended microemulsion droplet when \u03b5 increases. As discussed before, vaporization at the surface of the droplet is the combination of continuous vaporization of bulk liquid (base oil) and intermittent vaporization of water sub-droplets once exposed at the surface. As \u03b5 increases, the corresponding volume percentage of the base oil in the microemulsion droplet decreases (the volume percent of different components in a microemulsion droplet is tabulated in Table S1 for \u03c9 = 10 and 40).\n\nFor instance, a droplet with \u03b5 = 0.1 signifies a 90% oil composition by volume, with the remaining 10% constituting the dispersed phase (comprises of water and AOT dictated by \u03c9 (see Equation 1)). The abundant base oil readily replenishes at the droplet surface, leading to a higher evaporation rate, particularly for a lower value of \u03b5.\n\nTo isolate the effect of non-volatile surfactant on evaporation, samples with an oil-AOT mixture were also prepared without the addition of water. The oil-AOT mixtures were prepared by adding an exact amount of AOT into oil, corresponding to the microemulsion (\u03c9 = 10). Figure 7(c) shows the droplet lifetime history for decane-AOT mixture (\u03c9 = 10), for different values of \u03b5.\n\nIt is observed that an increase in \u03b5 results in a decrease in the evaporation rate of the droplet (oil-AOT). Here \u03c9 = 10 represents the decane-AOT mixture corresponding to decane microemulsions at \u03c9 = 10 and \u03b5 indicates the corresponding \u03b5 of the microemulsion (\u03c9 = 10) which contains the same amount of AOT used to prepare the microemulsion. The prime notation has been adapted in this work to distinguish oil-surfactant mixture from microemulsions.\n\nWhile the evaporation process is primarily driven by the nature of the base oil, the evaporation of water sub-droplets is contingent upon their exposure to the surface. As time# Figure 8\n\n(a) and (c) Shows the droplet evaporation rate (|k|), as a function of \u03b5 for different \u03c9 and base oils, respectively. (b) and (d) Shows the droplet lifetime (\u21bd), as a function of \u03b5 for different \u03c9 and base oils, respectively at I \u2193 = 0.05.\n\n| |p-Xylene|Decane|Dodecane|Surrogate|\n|---|---|---|---|---|\n|(a)| | | | |\n|(b)| | | | |\n|(c)| | | | |\n|(d)| | | | |\n\nThe overall droplet evaporation rate (k) can be estimated using the equation:\n\nk = D(\u21bd)\u00b2 \u2193 D\u2080\u00b2\n\nwhere, \u21bd is the total evaporation time, D(\u21bd) and D\u2080 are the final and initial droplet diameter, respectively. In microemulsions, to estimate the steady evaporation rate (k) from the experimental data, we have calculated the slope of the square of droplet diameter vs the time curve in the steady evaporation phase. The evaporation rate was determined by employing the least squares method implemented within a Python code, which yielded a minimum R\u00b2.# Evaporation Characteristics of Microemulsions\n\nValue of 0.98, indicating a highly linear correlation between the squared diameter of the droplet and time. The evaporation rate for decane microemulsion for different values of \u03c9 as a function of \u03b5 is plotted in the Figure 8(a), which shows that the increase in \u03b5 reduces the rate of evaporation during Stage II. From Figure 7 and Figure 8(a), it is evident that the difference in evaporation rate between \u03b5 = 0.1 and 0.2 is minimal, whereas a significant reduction occurs for \u03b5 = 0.4. The significant decrease in evaporation rate can be attributed to the higher concentration of AOT. At higher \u03b5 (for example, \u03b5 = 0.4), a denser packing of AOT aggregates at the droplet surface occurs due to a higher concentration of AOT, which reduces the effective surface area for evaporation.\n\nThe evaporation lifetime (\u21bd) of the droplet is plotted in the Figure 8(b) and (d) for decane microemulsion and different base oils as a function of \u03b5 at I \u2193 = 0.05. It is seen that the droplet lifetime also decreases with an increase in \u03b5. A higher \u03b5 leads to the inhibition of evaporation due to the reasons explained above, which eventually terminates the evaporation process much earlier. In essence, the evaporation ends much earlier for higher \u03b5, resulting in a shorter evaporation lifetime.\n\nTo further understand the evaporation process in a microemulsion system, we formulate a parameter, \u03d1, defined as the ratio of volume fraction of volatile to non-volatile components of the microemulsion droplet. The \u03d1 can be expressed as:\n\n\u03d1 = 1 \u2193 \u03b5 AOT. (5)\n\nAs shown in the Figure 8(a) and (c), as the value of \u03d1 increases, the evaporation rate of the droplet also increases. This is expected because increasing \u03d1 essentially indicates an increase of volatile components in the droplet and a reduction in the non-volatile AOT, leading to enhanced evaporation, as discussed before. This behavior is consistent for all the microemulsions with different base oils.# Role of the Base Oils (Continuous Phase)\n\nSo far, we discussed the effect of \u03c9 and \u03b5 on the evaporation characteristics. Figure 8(c) shows the evaporation rate of microemulsions.# Figure 9\n\n|(a)|(b)|p-Xylene|\n|---|---|---|\n|Decane|Dodecane|Surrogate|\n\n|(c)|(d)|p-Xylene|\n|---|---|---|\n|Decane|Dodecane|Surrogate|\n\nFigure 9: (a) and (d) Shows the droplet evaporation rate (|k|), as a function of \u03d1 for different base oils at I \u2193 = 0.05 and 0.1 respectively. (b) and (d) Shows that the variation of normalized evaporation time during the unsteady \u201ct ue /\u21bd phase as a function of \u03d1, at I \u2193 = 0.05 and 0.1 respectively.\n\n26# Figure 9\n\n(a) and (c) shows the evaporation rate (k) for different base oils as a function of \u03d1 at I\u2193 = 0.1. It is observed that, at a high heating rate, the evaporation rate of the decane and dodecane microemulsion is similar, and the xylene microemulsion exhibits an even lower.# Evaporation Rate\n\nThis indicates that the combined influence of the vapor pressure and the elevated concentration of AOT near the droplet surface becomes increasingly significant at higher heating rates. It is to be noted that this effect is also expected to occur for the emulsion with higher \u03b5. It is also shown in Figure 9(a) and (c) that, for the lower value of \u03d1, we observe very closely packed data points for all the base oils, indicating similar evaporation rates irrespective of the heating rate. Additionally, the evaporation time also depends on the volatility of the base oils and the relative concentration of the AOT. As shown in Figure 8(d), the dodecane microemulsion exhibits the highest evaporation time followed by surrogate, decane, and xylene. This is also consistent with the trend in their evaporation rate.# Stage III: Unsteady Evaporation and Shell Formation\n\nDuring Stage II, the microemulsion droplet exhibits a constant rate of evaporation, and as time progresses, this process eventually transitions to a stage where the rate of diameter reduction progressively diminishes until it becomes negligible or constant (\u2194 0), as shown in Figure 7 (t/\u21bd = 1 indicates the end of Stage III). Between the end of Stage II and the point where the evaporation rate approaches zero is called the unsteady evaporation phase (i.e., Stage III). Most of the liquid components evaporate during Stage II. As a result, the relative concentration of surfactant inside the droplet increases drastically at this Stage (II). This creates a liquid-deficient droplet with high AOT concentration. As time progresses, the transport of liquid components to the surface becomes even more difficult. Because of these two phenomena, the evaporation rate of the microemulsion droplet decreases continuously until the evaporation completely stops. Figure 9(b) and (d) show the evaporation time during the unsteady evaporation phase of the droplet. It is observed that the droplet with a lower value of \u03d1 exhibits a larger evaporation time (\u201dt ue). This behavior can be attributed to the# Current Observations on Microemulsion Droplets\n\n(diameter) of the microemulsion droplet along with the equivalent droplet diameter (D). It is observed that, during stages II and III, the horizontal and vertical diameters reduce proportionally with the equivalent diameter. For an evaporating droplet, it is expected that both of its characteristic diameters (horizontal and vertical) should decrease with time as a result of a reduction in the overall droplet volume. However, a significant deviation from this anticipated behavior of the droplet diameters has been observed after point C, as shown in Figure 10(a). While the horizontal diameter appears to reach a plateau, indicating minimal change in droplet size with time, the vertical diameter continues to decrease in size. This discrepancy between the horizontal and vertical diameter variations is attributed to the buckling of the spherical shell. The evaporation is considered to stop at the point (C), where the horizontal diameter becomes constant.\n\n|Decane|Surrogate|Xylene|Dodecane|\n|---|---|---|---|\n|=0.4|=0.1|=0.4|=0.4|\n|2|3| | |\n\nFigure 11: SEM images of the residue are shown for different values of \u03b5 and irradiation (I \u2193 ) at \u03c9 = 10. The images show buckling of the spherical shell at low irradiance (I \u2193 = 0.05); however, for I \u2193 = 0.1, the shell shows spherical morphology for the majority of the cases (All the scale bars represent a length of 200 microns).\n\nThe spherical shell exhibits localized buckling, majorly at its north (N) and south poles (S). The deformation of these shells is typically attributed to capillary forces. Liquid evaporation at the droplet surface induces the formation of microscopic pores within the AOT shell. These pores function as tiny menisci, where the Laplace pressure (\u21c1p \u2194 2\u03d6/r pore ) acting just# Final Diameter of the Residual Spherical Shell\n\nThe final diameter of the residual spherical shell at the end of Stage III is measured from the shadowgraph images. The shell size depends on both the evaporation rate and the volume fraction ratio (\u03d1) of the volatile to non-volatile components within the parent droplet. Figure 10(b) and (c) show the variation of the shell size (D) as a function of \u03d1 at I\u2193 = 0.05s and 0.1, respectively. It can be seen that a higher shell size D is inversely proportional to the volume fraction ratio (Ds \u2243 1/\u03d1).\n\nThe shell size of the xylene microemulsion exhibits large shell sizes compared to the other microemulsions. The larger shell size can be attributed to the high vapor pressure of xylene which leads to its faster evaporation. Therefore, the rate of accumulation of AOT particles is much faster than their diffusion/transport throughout the droplet causing an early and larger shell formation. Typically, for xylene microemulsion, no buckling has been observed at a higher heating rate (see Figure 11). The faster evaporation rate of xylene causes the shell to form early such that the solid residual structure can sustain.# Deformation due to the acoustic radiation pressure\n\nA smooth temporal variation of non-dimensional droplet diameter for all microemulsions has been observed at IR irradiation intensity of I\u2193 = 0.05. Similar behavior has been observed at I\u2193 = 0.1 as well, except for xylene and decane microemulsion. The xylene (\u03b5 = 0.4, \u03c9 = 10) and decane (\u03b5 = 0.2, 0.4, \u03c9 = 40) microemulsions show bubble nucleation and breakup at I\u2193 = 0.1. Note that the focus of the present study is to investigate the evaporation characteristics in stable microemulsion droplets. Thus, the study of bubble nucleation and breakup lies beyond the scope of this work and is not addressed herein.# Conclusions\n\nWe explore the evaporation dynamics of stable microemulsion droplets in a contactless environment using an infrared continuous laser. The key evaporation characteristics of stable emulsion droplets are as follows.\n\n1. The microemulsion droplet exhibits three distinct stages of evaporation: pre-heating, steady evaporation, and unsteady evaporation. At the end of evaporation, the droplets result in a spherical residue.\n2. Pre-heating corresponds to the transient heating period of the droplet, where its surface temperature increases and becomes constant after a certain time period, i.e., preheating time (\u201dtph). The ranges from \u201dtph \u2194 1.72 seconds at irradiation intensity of I\u2193 = 0.05, whereas it reduces to \u2194 0.76 seconds at I\u2193 = 0.1.\n3. During Stage II, the droplet undergoes steady evaporation, and the rate of evaporation is observed to decrease with the increase in \u03b5 for a constant \u03c9. It is shown that the evaporation of a microemulsion droplet is governed by the complex interplay between its constituents and their properties. A parameter \u03d1 was introduced, which indicates the cumulative influence of various factors (\u03b5, \u03c9) affecting the evaporation process.Moreover, the nanostructure of microemulsion may have a role in droplet evaporation, however, more evidence is needed to confirm its influence on evaporation.# 4.\n\nAs time progresses, the evaporation rate continuously decreases until the evaporation stops, (Stage III). Finally, the droplet transforms into a solid residual structure. The morphology of this residual shell is spherical, which further undergoes buckling at its south and north poles. This shell size D) is observed to increase as the value of s decreases (Ds \u2243 \u03d1 \u21921).# 5.\n\nAmong all the base oils used in this study, the xylene microemulsion shows a larger shell size in comparison to other microemulsions with a different oil phase. This may be attributed to its high vapor pressure of the xylene.# 6.\n\nAt higher heating, we usually observe a nearly spherical shell in contrast to a buckled non-spherical structure for a lower heating rate.",
        "context_id": 32,
        "question": "What is the resistivity of the water used in the microemulsion studies?",
        "answer": [
            "18.2 M\u03a9-cm"
        ],
        "context_length": 34704
    },
    {
        "context": "# Classical and Quantum Channels\n\n|Classical Channel|Semi-classical Channel|Clients|\n|---|---|---|\n|Classical|Quantum|Classical Channels|# Server\n\nFigure 1: Typical client-server configuration where a limited client delegates a quantum computation to a quantum server. Clients might have a limited quantum capability e.g. the ability to send single qubits [left] or entirely classical channels [right]. In this work we focus on a hybrid proposal where clients have a semi-classical channels [centre] meaning that the client is able to send a coherent state.\n\nis to use delegated solutions where quantum computations are performed by a quantum service provider. This approach is actively being developed in both academic and industry settings.1 This is commonly done via the combination of a code-based interface for describing the computation and classical communications between the client and the server.\n\nUnfortunately, in this scenario there are no information-theoretic guarantees ruling out a malicious provider accessing all information about the client\u2019s desired computation, inputs, and outcomes. Cases where such considerations could be critical include (i) the manipulation of highly sensitive input data, such as confidential medical records or defence-related information, (ii) the execution of proprietary (and confidential) algorithms, and (iii) cases where the output is valuable, e.g. patent-worthy. Furthermore, the client cannot check in general that the service provider has performed its desired computation based only on the received output. The more powerful quantum computers are, the more problematic these issues become, as they grow capable of solving more and more advanced problems. All these considerations sti!en the adoption of quantum computers precisely at the time when they become useful for solving real-world problems. It is therefore vital to protect clients in these scenarii to foster wide-spread use of quantum computing technologies.\n\nThankfully, we can entirely remove the need for clients to trust their quantum service providers, with the help of quantum cryptography. Although quantum computers will be a threat to the security of some classical protocols (e.g. RSA)[1], they also open up# Footnotes\n\n1 Examples of public-accessible cloud platforms include: Quandela https://cloud.quandela.com, IBM https://quantum-computing.ibm.com, IonQ https://ionq.com/quantum-cloud, Rigetti https://docs.rigetti.com/qcs/.# New Possibilities via Cryptographic Protocols\n\nsuch as quantum key distribution [2]. In our particular case, protocols for Secure Delegated Quantum Computing (SDQC) have been a highly active research field in the past few years and represent an extremely promising application [3\u20137]. These protocols allow a client with limited quantum resources to delegate a quantum computation to a server, without the server learning anything about the quantum algorithm, the input which is being processed or the outcome of the computation. During the protocol\u2019s execution, the client can furthermore test that the server is behaving as expected.# Universal Blind Quantum Computation (UBQC)\n\nwho then entangles them. The entanglement operation commutes with the encryption, thus yielding an encrypted graph state. The client instructs the server to measure qubits from the encrypted graph state in a way which both undoes the encryption and performs the computation of the client\u2019s choice. This is the basis of the original Universal Blind Quantum Computation (UBQC) protocol of [5]. The first challenge is therefore to emulate these single-qubit transmissions using only attenuated laser pulses. The second challenge is to reduce the number of entangling gates that need to be performed after the qubits have been generated.# GHZ privacy amplification for rotated states from weak coherent pulses (informal)\n\n1. The server initialises its quantum emitter in the state |+\u2192.\n2. The client sends n randomly-polarised weak coherent pulses, which the server uses to excite its quantum emitter. The energy level structure of the quantum emitter induces as output a polarisation-encoded GHZ state that is Z-rotated by an angle corresponding to the sum of all random angles chosen by the client.\n3.\nThe server emits one photon using a non-rotated coherent pulse and attempts to measure the first n photonic qubits (generated by the client\u2019s pulses) in the |\u00b1\u2192 basis.\n4. If the number of unsuccessful measurements (i.e. no photon was detected) is too high, the client aborts. Otherwise the server applies a correction to the last remaining photon and the quantum emitter which depends on the client\u2019s choice of angles and the measurement results. These corrections are specified in Protocol 3.\n\nAs expressed above, the client will send a sequence of phase-randomised attenuated laser pulses, with the polarisation of each pulse being chosen at random. This polarisation angle corresponds to the secret key which will later be used to encrypt the computation. The server receives these pulses and uses them to excite a quantum emitter with a qubit degree of freedom, such as a spin1/2, that can emit spin-entangled photons. The quantum emitter will have two roles. It will first act as a \u201claser-to-qubit converter\u201d by converting a coherent pulse into a single photonic qubit. This role is somewhat similar to the Remote Blind Qubit State Preparation that was proposed in Ref. [15], but we execute it without having to perform a qubit-preserving photon-number QND measurement. Moreover, it will act as a \u201cphoton entangler\u201d as the emitted photonic qubit will be entangled with the spin. The server exploits this property to produce an entangled state of photonic qubits, removing the need of performing demanding photon-photon entangling gates to produce the encrypted graph state. More precisely, it is possible to produce photonic linear cluster states with a single quantum emitter, with each photon being entangled in polarisation to the next emitted photon in the chain and the last one being also entangled to the spin [20\u201325]. Crucially, the state of each photonic qubit thus created inherits the polarisation angle from the pulse that was used to generate it. These chains of encrypted photonic polarisation-encoded qubits can then be combined to form the graph state supporting the client\u2019s desired computation. Only the links between chains need to be done after the qubits have been emitted, which drastically reduces the amount of entanglement operations. These can furthermore be done via interacting quantum emitters to remove entirely the need for photon-photon entangling gates, by performing the entanglement operations between quantum emitters in between photon emissions.However, as in [15], an attenuated laser pulse necessarily leaks more information about the client\u2019s secret key compared to a single qubit. This is due to the fact that the state of the laser pulse can be described as a redundant encoding of the single qubit state containing the secret key. Taking a simplifying assumption, we can overestimate the information leakage by considering that any multi-photon component in the input laser completely leaks the polarisation information. We therefore need to suppress this information leakage in a way which meshes nicely together with the qubit generation technique described above. Fortunately, it is possible to easily switch between generating chains of qubits and GHZ states simply by selectively applying a Hadamard operation on the quantum spin. The full protocol then generates chains of encrypted GHZ states instead of chains of single qubits. Each qubit in the GHZ states is generated with a different random polarisation angle. The result is a chain of encrypted GHZ states whose encryption key is the sum of the polarisation angles. Each GHZ state can easily be collapsed into a single qubit while preserving the encryption key via measurements on all qubits but one in the GHZ state in the |\u00b1\u2192 basis, thus recovering the chain of single qubits used for the rest of the blind protocol. Intuitively, the server can learn the final encryption key of a given qubit only if all the angles used to generate the corresponding GHZ state have leaked. Therefore the final leakage probability is suppressed exponentially in the number of qubits per GHZ state.\n\nThis protocol exploits low-intensity lasers |\u03c9\u2192 which generally have a non-negligible vacuum component, | \u21910|\u03c9\u2192 |2. In that case, even with an honest server and without other sources of photon loss, the quantum emitter sometimes cannot emit a photonic qubit. Our protocol accounts for this scenario by introducing a threshold on the number of detected photons during the aforementioned collapse of the GHZ states. If the losses reported by the server are too high, the client simply aborts.\n\nThe end result of the protocol in this simplified example is the state \u219212 (|00\u2192 + ei\u03c9 |11\u2192) for a random angle \u03b5 known only to the client. The first qubit corresponds to the quantum emitter\u2019s qubit degree of freedom, while the second is the state of the remaining polarisation-encoded photon. This gives a lot of flexibility in how the server can use this protocol to construct a larger graph state since the emitter and photon are still entangled. The server can easily extend this into a linear cluster state by repeating the protocol, or continue growing this rotated GHZ state so that it has multiple photons available for performing graph fusion operations.\n\nWe show that both the protocol\u2019s security error and correctness error decrease exponentially with the number of pulses sent by the client (stated formally in Th. 5 and proven in App. C).# Theorem 1 (Blind state preparation from weak coherent pulses, informal).\n\nThe probability of obtaining an incorrect state and the probability that information about the angle \u03b5 leaks to the server both exponentially decrease in the number of pulses n, as proven in a composable security framework.# Theorem 2 (Blind Delegated Quantum Computation (BDQC) from weak coherent pulses, informal).\n\nThere exists an efficient composably secure BDQC protocol with an exponentially-low security error in which the client only sends weak coherent pulses and the server only manipulates interacting quantum emitters.\n\nWe then use this to build a protocol for SDQC. In addition to BDQC\u2019s blindness, SDQC also guarantees that the server performs the client\u2019s computation as instructed.\n\n6# Theorem 3\n\n(Secure Delegated Quantum Computation from weak coherent pulses, informal). There exists an efficient composably secure SDQC protocol for delegating BQP computations with an exponentially-low security error in which the client only sends weak coherent pulses and the server only manipulates interacting quantum emitters.\n\nFinally, we study the experimental feasibility of our protocol by computing the trade-off between correctness and security in the presence of loss. Recent experimental developments in that regard show extremely promising results [22\u201325, 27] and hold the current record for the largest entangled photonic state produced [25]. Our scheme is thus not only secure but also experiment-ready.# Comparison with Dunjko et al. [15]\n\nThe protocol from [15] works by having the server non-destructively measure the client\u2019s coherent light pulses in photon number, and post-select only the light pulses containing at least one photon. Then, the server must isolate one photon from each pulse. It applies a privacy amplification step which requires as many CZ gates as there are photons. After that, the server also has to entangle the privacy-enhanced photons into the graph state supporting the client\u2019s computation by using photon-photon CZ gates. All four of these steps are challenging technological requirements.\n\nThe Remote State Preparation (RSP) in Ref. [15] \u2013 performing a polarisation-preserving QND measurement to count the photons in each pulse and extracting a single qubit from pulses containing at least one photon \u2013 is separated from the privacy amplification and the graph state generation. This first step has been implemented by [18], requiring a number of pulses from the client which is around 108 per qubit in the graph in the best case scenario where there is virtually no spatial separation between the client and server. They solve the issue of performing a QND measurement and qubit extraction \u2013 which previously required a much more complex process [28, 29] \u2013 by using a linear optical setup performing teleportation with EPR pairs. Although this is rather fast \u2013 their experiment generates 1011 leaky qubits via this RSP technique in two hours \u2013 they still need to entangle these polarisation-encoded base qubits using photonic gates to perform the privacy amplification, which is not done in [18]. Since they claim to want to use these 1011 leaky qubits to generate 103 leak-free qubits, this implies a multiplicative overhead of 108 in terms of entangling photonic gates compared to the client\u2019s base computation. The only viable option in this setting is to use deterministic non-post-selected photon-photon entangling gates since anything else will degrade far too rapidly to be useful. Unfortunately, the fidelities of such gates which have been demonstrated [30\u201332] are still insufficient to apply such a number of operations while preserving the integrity of the quantum state, even with error-correction techniques.\n\nOn the other hand, our RSP technique using a quantum emitter is intrinsically intertwined with the privacy amplification and graph state generation steps: the entanglement required for these two processes is directly generated by the structure of the quantum emitter and no additional operations are required. The result is that the final qubits are# Comparison with Takeuchi et al. [35]\n\nA recent paper [35] also proposed a protocol for information-theoretically secure SDQC using only semi-classical light. There are however significant differences both in terms of results and techniques. Their protocol is based on the SDQC protocol from [36] and requires the client to send states sampled from the same set as in our construction. The privacy amplification technique is similar to ours and that of Dunjko et al. However, their construction requires the same complex operations as Dunjko et al as well: QND measurements and photon-photon entangling gates. As such they do not resolve the issues stemming from these operations and our protocol is therefore much more suitable for implementation. Furthermore their security is not proven in a composable framework such as the one used in the present paper, meaning that their security guarantees do not automatically hold if an outcome of their protocol is later reused in another cryptographic construction.# Organisation of the paper\n\nWe start by giving preliminaries on (i) the generation of photonic graph states, and (ii) delegated quantum computing protocols. In Section 3, we present our protocol for securely preparing encrypted graph states from semi-classical light and its application in blind and secure delegated protocols. Later in Section 4 we analyse the success probability of our protocol on realistic hardware, taking into account multiple effects leading to photon loss. Finally, we discuss the merits of and possible improvements to our protocol in Section 5.# 2 Preliminaries\n\nSection 2.1 presents how to generate graph state from quantum emitters and weak coherent pulses. Then Section 2.2 describes the UBQC protocol that allows a client to blindly delegate quantum computations to a server. Additional preliminaries can be found in Appendix A, in particular Appendix A.2 presents the composable Abstract Cryptography.# Figure 6: Generation of a cluster state of n = 3 rows and m = 4 columns.# (a) First step in the cluster state emission.\n\nStarting from n = 3 quantum emitters, we apply a CZ gate between each pair of neighbours, followed by an emission operator and Hadamard on all emitters.# (b) Result after repeating the process in Figure 6a m = 4 times.\n\nThe quantum emitters are then measured in the Z basis. This yields the state from Figure 3 encoded in the polarisation of the emitted photons.# 3 Remote state preparation with semi-classical clients\n\nThe security of the delegated quantum computation protocols presented in the previous section rests on the fact that Bob cannot access information about the secret angle \u03b5 v with# 3.1 Blind graph state generation using quantum emitters\n\nInstead of sending single qubits, in our protocol Alice transmits coherent states to hide the phases \u03c2. The main advantage is that a photonic attenuated coherent state source is simply a laser light which is already commercially available and likely to remain much more economical than a source of single qubits. Such a coherent state |\u03c9\u2192\u03c9 with \u03c9 = |\u03c9|ei\u03d6 is given by:\n\n|\u03c9\u2192\u03c9 = e\u2197|\u03f1|2/2 (8)\n\nwhere a\u03c9 \u2020 := cos(\u03b5)aR + sin(\u03b5)aL is the rotated polarisation creation operator and |\u21bc\u2192 is the vacuum state. Using a phase-randomised laser, Alice instead sends the state:\n\n2\u03c2\u03d6=0\u2223\u2223\u2223|\u03c9|ei\u03d6 \u232a|\u03c9|ei\u03d6 \u2223\u2223\u2223 \u03c9 d\u21c0 = e\u2197|\u03f1|2 (9)\n\nOn the other hand, Bob must convert these coherent states into single qubits while preserving the polarisation in order to use them in the UBQC protocol. This can be done using the scheme presented in Section 2.1 with a slight modification of the emission operator in Eq. (2). If Alice\u2019s light source sends the state \u21bd\u03f1,\u03c9 such that it is able to drive the optical transition of the quantum emitter from Figure 4, the production of spin-entangled photon states is done via the following global operation:\n\nEqe(\u03b5) = |\u2197, L\u2192 \u2191\u2197| + ei\u03c9 |\u2198, R\u2192 \u2191\u2198| = RZ (\u03b5)Eqe. (10)\n\nThe polarisation of the input coherent laser therefore imprints a phase on the photon after emission, corresponding to a RZ (\u03b5) rotation described in Section 2.1, then the resulting state is the rotated cluster state \u2223\u2223\u2223 G(\u03b5)\u232a, which is the required resource for both the UBQC and SDQC protocols as shown in Eq. (6). Intuitively, so long as all the angles \u03b5v are hidden from Bob, the UBQC protocol which uses this strategy for generating the blind graph remains secure. We now introduce a new resource in the Abstract Cryptography security framework and then describe a protocol which formalises this idea. For unitary U, we write U(\u21bd) to mean U\u21bdU\u2020. Furthermore, when applying CNOT operations, we always assume that the first qubit mentioned is the control while the second one is the target.# Resource 1 Blind Graph State Extender\n\n|Inputs:|Computation by the resource:|\n|---|---|\n|Alice inputs an angle \u03b5 \u2194 \u201d.|The resource samples a bit b \u2194 {0, 1} uniformly at random and sends to Bob the two-qubit state RZ ((\u21931)b \u03b5)CNOT (\u21bdB \u21d1 |0\u2192\u21910|), where RZ is applied to the second qubit.|\n\nThis resource captures the perfect setting where the rotated emission operator Eqe(\u03b5) is applied to Bob\u2019s quantum emitter without leaking any information about the state.# Protocol Security\n\nThe blind graph state generation protocol (Protocol 2) perfectly constructs the Blind Graph RSP Resource 2, meaning that the security error is exactly 0. This is captured by Theorem 4, whose proof can be found in Appendix B.# Theorem 4 (AC security of Protocol 2)\n\nProtocol 2 perfectly constructs in the Abstract Cryptography framework the Blind Graph RSP Resource 2 from |V| instances of the Blind Graph State Extender Resource 1.# 3.2 Security amplification via rotation aggregation\n\nIt would seem like simply exciting a quantum emitter via a pulse in the state \u21bd\u03f1,\u03c9 would be enough to construct the Blind Graph State Extender Resource 1 since it is equivalent to applying the emission operator Eqe(\u03b5). However this is not the case since a laser pulse leaks a non-negligible amount of information.\n\nDue to the phase randomisation step, Bob has access to a probabilistic mixture of states |k\u2192\u2191k|\u03c9. These states consist of k photons which each contain the same information about \u03b5. If he performs a polarisation-preserving photon number resolving QND measurement, he can learn the number k of incoming photons for each vertex of the blind graph state. Bob can then choose to attack the vertex where he received the most photons.\n\nIf we assume that Bob is capable of manipulating each of these photons in isolation, the amount of knowledge that he can extract from the execution of the protocol directly |\u03c9|2 increases with the intensity of the coherent state via the probability of obtaining multiple photons. On the other hand, a relatively high value for |\u03c9|2 is desirable in order to excite the quantum emitter generating the blind graph state.\n\nIn order to recover the ideal scenario described by the Blind Graph Extender Resource 1, we make use of a security amplification gadget which exponentially suppresses the information leakage using only a linear number of pulses.\n\nThis protocol makes use of the GHZ-generation capability of the quantum emitter paired with the rotated emission operator Eqe(\u03b5). Alice sends to Bob a certain number of phase-randomised rotated weak coherent pulses \u21bd\u03f1,\u03c9i for a fixed intensity |\u03c9|2 and randomly chosen values of \u03b5. Bob uses these pulses to drive the transition of his quantum emitter, effectively applying the rotated emission operator Eqe(\u03b5i). These consecutive pulses will generate a rotated GHZ state whose angle is the sum of all angles used by Alice for laser pulses containing at least one photon. Bob emits one last photon using Eqe and then measures all qubits generated by Alice\u2019s pulses in the X basis. He can thus detect which laser pulses contained no photons. If too few photons have been detected, Alice aborts the protocol. This threshold, set by the security proof, prevents Bob from discarding too many pulses from which he would not get enough information. If Alice has not aborted, she communicates to Bob a correction which depends on her desired angle and the parity of the measurement outcomes. After applying this correction, Bob\u2019s device contains the random rotated state chosen by Alice. This process is presented in Figure 8.\n\n||$%#\u27e9||$%\"\u27e9||$%!\u27e9|\u0398!|\u2211\u0398$|\n|---|---|---|---|---|\n|\"(\u03b8)|\"(\u03b8)|\"(\u03b8)|\"(\u03b8)|\"(\u03b8)|\n|!!|\u0398\"|\u0398#| | |\n\nFigure 8: Example of privacy amplification with three weak coherent pulses in the lossless case. The laser with polarisation control on the left is in the hands of Alice while the quantum emitter (square) is handled by Bob. Alice sends three pulses with randomly rotated polarisations. Bob uses these pulses to drive the quantum emitter and then emits one more photon. The result is a five-qubit rotated GHZ state after the first arrow. Bob then measures (in orange) all polarisation qubits in the GHZ state but one in the |\u00b1\u2192 basis. After correction, the result is a rotated Bell state containing the spin qubit and one polarisation qubit. This is the same state \u2013 up to rotation \u2013 as in the middle step of the basic linear cluster generation process from Figure 5.\n\nWe describe formally this process in Protocol 3.\n\n17# Protocol 3 GHZ privacy amplification for rotated states from weak coherent pulses# Public information:\n\nLaser pulse intensity |\u03c9|2, number n of pulses sent per GHZ gadget, threshold t.# Inputs:\n\nAlice inputs an angle \u03b5 \u2194 \u201d. Bob inputs a single-qubit spin state \u21bdqe.# Protocol:\n\n1. Alice samples values (\u03b51, . . . , \u03b5n) uniformly at random from \u201d.\n2. For 1 \u21d4 i \u21d4 n:\n1. Alice sends the phase-randomised rotated weak coherent pulse \u21bd\u03f1,\u03c9i to Bob.\n2. Bob uses this pulse to apply the rotated emission operator to his spin qubit Eqe(\u03b5i).\n3. Finalisation:\n1. Bob emits another photon using the emission operator Eqe. This qubit is indexed 0.\n2. Bob attempts to measure all photonic qubits i \u2196 = 0 in the GHZ state in the |\u00b1\u2192 basis. Let S \u2199 {1, . . . , n} be the set of indices for which the measurement succeeded (i.e. a photon has been detected) and let b = \uf8fbi\u2191S bi be the parity of the corresponding measurement outcomes. Bob sends S to Alice.\n3. Alice aborts if |S| \u21d4 t, sending Abort to Bob and setting it as her output. Otherwise, she samples uniformly at random a bit mx and the correction angle \u00af = (\u21931)mx \u03b5 \u2193 \uf8efi\u2191S \u03b5i and bit mx to Bob.\n4. Bob applies the correction RZ(\u00af + b\u03f1) to photonic qubit 0 and sets it as his output together with bit mx.# On the final correction operation.\n\nThe final correction might seem like a superfluous step, but it is necessary for proving the security of the protocol. In particular, the simulator built as part of the security proof requires this flexibility to adapt its behaviour to the set S returned by the distinguisher. However, this does not mean that this correction must necessarily be implemented as an additional quantum operation by Bob. We use this protocol within Protocol 2 to build a resource state for the UBQC protocol, which already requires Alice to send measurement angles to Bob. Alice can merge the correction with these measurement angles so that both are applied simultaneously as a single operation. Therefore, this correction does not imply an additional classical communication step, nor an additional operation by Bob, if Protocol 3 is used together with UBQC.# Protocol security.\n\nThe privacy amplification protocol presented above constructs the Blind Graph State Extender Resource 1, with a security error which decreases exponentially in the number of pulses n sent by Alice. This analysis holds in the lossless regime, assuming that each laser pulse has a probability \u21c11 of applying the rotated emission operator to the spin qubit. The next section gives arguments for this assumption and discusses more realistic settings when taking losses into account. The security of our protocol is given by Theorem 5, whose proof can be found in Appendix C.# 3.3 Post-selecting on receiving all photons\n\nIt is possible also for Alice to reject the state as soon as a single photon is lost. We describe this alternative formally in Appendix D as Protocol 6, along with a correctness and security analysis.\n\nThe threshold protocol presented above will abort significantly less often than the one which requires Bob to recover all photons. This is because Protocol 3 tolerates some losses due to a photon not being emitted. On the other hand, the security bound is slightly degraded. Combined together, the threshold protocol\u2019s security and successful completion probability both increase with the number of emitted photons, as expressed in Theorem 5. It would seem like it is therefore the superior protocol since the success probability of the post-selected protocol actually decreases exponentially with the number of photons which need to be collected.\n\nHowever this analysis holds only if we assume that (i) there are no losses after the photons have been emitted, and (ii) the emission operator correctly describes the result of the interaction between the laser pulse and the quantum emitter.\n\nIf a photonic qubit is lost after it has been generated in Protocol 3, the state is perfectly mixed from the point of view of both Alice and Bob. Discarding the state as soon as a single photon is not detected by Bob during the measurement prevents this issue and as a consequence the correctness of the output state of Protocol 6 is unaffected by losses which happen after the qubit generation procedure. This includes the case where the photon is emitted while the interaction between the laser and the quantum emitter is still taking place and is filtered out as described in Section 4.1.\n\nBoth protocols on the other hand may suffer if the laser-spin interaction deviates from the ideal emission operator described here. If for instance a photon is emitted while the laser interaction is ongoing and the laser re-excites the quantum emitter resulting in a second photon being emitted, neither protocols will count this as a loss but the end result will be a perfectly mixed state in both cases. However, if the operator instead applies a slightly different angle for some or all values of \u03b5, this can be mitigated if the overall protocol can tolerate this noise level.# 3.4 Delegated quantum computation protocols with semi-classical client\n\nWe describe in this section how to build delegated quantum computation protocols with a semi-classical client. To this end we compose our bind graph RSP protocol and privacy amplification protocol with the protocols from [5] and [26] to obtain respectively BDQC and SDQC protocols.# Blind delegation from semi-classical client RSP.\n\nWe use the composition theorem from the AC framework to replace the call to the Blind Graph RSP resource in the UBQC protocol from Section C with our Protocols 2 and 3. The general composition Theorem 8 allows us to combine Theorems 9, 4 and 5 into the following result.# Theorem 6 (BDQC with semi-classical client).\n\nLet \u03c9, \u03bd \u03f1, n be defined as in Theorem 5 and let |V| be the number of vertices in the graph supporting Alice\u2019s desired MBQC computation. Consider the protocol obtained by replacing the blind graph state generation in the UBQC Protocol 4 (steps 1 and 2) with Protocol 2 composed with Protocol 3 with parameters (\u03c9, n). This protocol |V| \u00b7 exp(\u2193\u03bd\u03f1 n)-constructs in the Abstract Cryptography framework the BDQC Resource 3.# Testing the server\u2019s honesty with semi-classical light\n\nThere are two challenges when trying to use our semi-classical client technique together with protocols for SDQC which require the client to produce states in the computational basis, such as those from [8, 50, 51].\n\nFirst of all, generating a computational basis state with the energy structure presented in Figure 4 would project the spin state to the corresponding computational basis state. This would then need to be reinitialised by the Server to continue the entanglement generation. The information of whether or not the spin needs to be reinitialised in a superposition would therefore leak whether the state produced is rotated or in the computational basis.\n\nThen, there is the issue of amplifying the security of both computational basis and rotated states using a single gadget which does not introduce new attack vectors or leak information. This has been attempted in [54] but a new attack on their scheme has been shown in [26]. Thankfully, this second work also fixes the issue by proposing a novel SDQC protocol which makes use of the generalised tests introduced in [55]. This corresponds to the protocol presented in Section 2.2.3 and more formally as Protocol 5.\n\nThe composable security of their protocol scales exponentially with the number of repetitions of the UBQC protocol, and the security of our blind graph RSP protocol degrades linearly in the number of repetitions. We again use the general composition Theorem 8 to combine Theorems 10 and 5 into the following result, yielding an SDQC protocol in which Alice only needs to send weak coherent states.# Theorem 7 (SDQC with semi-classical client)\n\nLet \u03c9, \u03bd \u03f1, n be defined as in Theorem 5, let N be the number of executions of the UBQC Protocol 4 in Protocol 5 and let |V| be the number of vertices in the graph supporting Alice\u2019s desired MBQC computation. Consider the protocol obtained by replacing the blind graph state generation step in each UBQC execution in the SDQC Protocol 5 with Protocol 2 composed with Protocol 3 with parameters (\u03c9, n). This protocol N |V| \u00b7 exp(\u2193\u03bd\u03f1 n) + -S -constructs in the Abstract Cryptography framework the SDQC Resource 4, for -S exponentially decreasing in N being the security bound of the SDQC Protocol 5.\n\nFinally, as shown in [26, 55], this SDQC protocol tolerates a constant level of global honest noise, meaning that slight defects will be corrected and Alice is able to recover the correct output without aborting even if her apparatus or Bob\u2019s have a low-enough level of noise.# 4 Fine-tuning laser intensity for correctness/security trade-off\n\nIn the previous section we have shown via Theorem 5 that, so long as the total single-photon efficiency \u21c11 is greater than the probability of receiving two or more photons p \u03f1,2, we can amplify the security of blind state generation to arbitrary levels by increasing the number of photons in the GHZ gadget.\n\nThe vacuum probability of the incoming coherent pulse imposes an upper bound on= 1 \u2193 e\u2197|\u03f1|2. \u21c11 of p \u03f1,1. In reality, if the average number of photons |\u03c9|2 is not large enough, the population of the emitter will not be sufficiently inverted to achieve single-photon emission with a high efficiency and the criteria for security amplification may not be satisfied. The exact value of \u21c11 can depend on the energy-level structure of the device, the excitation scheme, and the pulse parameters. Thus, an important question is: can any practical single-photon generation schemes and parameter regimes allow for \u21c11 \u221d p \u03f1,2?# Figure 9: A resonantly driven two-level emitter for producing single photons from a weak coherent state.# 4.2 Saturating the upper-bound via alternative energy level structures\n\nThe scheme analysed in the previous section gives a concrete idea of how to satisfy the criteria for security amplification using an excitation scheme that can be directly applied to protocols for deterministic graph state generation, but it is not the only approach to generating a single photon from a weak coherent state nor is it the most efficient. It may be possible to improve \u21c11 by using a cavity to control the light-matter coupling or by altering the shape of the coherent pulse. But it is not easy to see how to attain the upper bound \u21c11 = p 1,\u03f1 using just a two-level emitter, or that it is even possible in general. Interestingly, it is possible to reach the upper bound by increasing the complexity of the emitter energy level structure.\n\nThe upper bound p 1,\u03f1 can be reached by considering a system with just one additional ground state |s\u2192 that is detuned from |g\u2192, so that we have a three-level % type energy level structure (see Figure 11(a)). If the % system is initially in the ground state |g\u2192, a coherent pulse can be used to excite the system to state |e\u2192. As soon as a single photon is emitted by the system due to the decay of |e\u2192 to |s\u2192, the system is no longer coupled by the excitation pulse and thus cannot be re-excited. Therefore, by filtering the emission in frequency, the",
        "context_id": 33,
        "question": "Who is the corresponding author for the paper discussing classical and quantum channels?",
        "answer": [
            "Luka Music"
        ],
        "context_length": 34170
    },
    {
        "context": "# 1. Introduction\n\nWith the rapid development of large language models (LLMs) (Brown, 2020; Achiam et al., 2023; Touvron et al., 2023; Dubey et al., 2024; Jiang et al., 2023; Bai et al., 2023b; Yang et al., 2024; Anthropic, 2024; OpenAI, 2024), code-specific language models have garnered significant attention in the community. Built upon pretrained LLMs, code LLMs such as the StarCoder series (Li et al., 2023; Lozhkov et al., 2024), CodeLlama series (Roziere et al., 2023), DeepSeek-Coder series (Guo et al., 2024), CodeQwen1.5 (Bai et al., 2023a), and CodeStral (team, 2024), have demonstrated superior performance in coding evaluations (Chen et al., 2021; Austin et al., 2021; Cassano et al., 2022; Jain et al., 2024). However, in comparison with the recently state-of-the-art proprietary LLMs, Claude-3.5-Sonnet (Anthropic, 2024) and GPT-4o (OpenAI, 2024), the code LLMs are still falling behind, either open-source or proprietary models.\n\nBuilding upon our previous work, CodeQwen1.5, we are excited to introduce Qwen2.5-Coder, a new series of language models designed to achieve top-tier performance in coding tasks at various model sizes. Qwen2.5-Coder models are derived from the Qwen2.5 LLMs, inheriting their advanced architecture and tokenizer. These models are pretrained on extensive datasets and further fine-tuned on carefully curated instruction datasets specifically designed for coding tasks. The series includes models with 1.5B and 7B parameters, along with their instruction-tuned variants. We are committed to fostering research and innovation in the field of code LLMs, coding agents, and coding assistant applications. Therefore, we are open-sourcing the Qwen2.5-Coder models to the community to support and accelerate advancements in these areas.\n\nSignificant efforts have been dedicated to constructing a large-scale, coding-specific pretraining dataset comprising over 5.5 trillion tokens. This dataset is sourced from a broad range of public code repositories, such as those on GitHub, as well as large-scale web-crawled data containing code-related texts. We have implemented sophisticated procedures to recall and clean potential code data and filter out low-quality content using weak model based classifiers and scorers. Our approach encompasses both file-level and repository-level pretraining to ensure comprehensive coverage. To optimize performance and balance coding expertise with general language understanding, we have carefully curated a data mixture that includes code, mathematics, and general texts. To transform models into coding assistants for downstream applications, we have developed a well-designed instruction-tuning dataset. This dataset includes a wide range of coding-related problems and solutions, sourced from real-world applications and synthetic data generated by code-focused LLMs, covering a broad spectrum of coding tasks.\n\nThis report introduces the Qwen2.5-Coder series, an upgraded version of CodeQwen1.5, featuring two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. Built on the Qwen2.5 architecture and pretrained on over 5.5 trillion tokens, these code-specific models demonstrate exceptional code generation capabilities while maintaining general versatility. Through rigorous data processing and training techniques, Qwen2.5-Coder achieves state-of-the-art performance across more than 10 code-related benchmarks, outperforming larger models in various tasks. The release of these models aims to advance code intelligence research and promote widespread adoption in real-world applications, facilitated by permissive licensing.# Architecture\n\nThe architecture of Qwen2.5-Coder is the same as Qwen2.5. Table 1 shows the architecture of Qwen2.5-Coder for two different model sizes: 1.5B and 7B parameters. Both sizes share the same architecture in terms of layers, having 28 layers and a head size of 128. However, they differ in several key aspects. The 1.5B model has a hidden size of 1,536, while the 7B model has a much larger hidden size of 3,584. The 1.5B model uses 12 query heads and 2 key-value heads, whereas the 7B model uses 28 query heads and 4 key-value heads, reflecting its larger capacity. The intermediate size also scales with model size, being 8,960 for the 1.5B model and 18,944 for the 7B model. Additionally, the 1.5B model employsTechnical Report\n\n embedding tying, while the 7B model does not. Both models share the same vocabulary\n size of 151,646 tokens and have been trained on 5.5 trillion tokens.\n\n Tokenization        Qwen2.5-Coder inherits the vocabulary from Qwen2.5 but introduces\n several special tokens to help the model better understand code.                        Table 2 presents an\n overview of the special tokens added during training to better capture different forms\n of code data. These tokens serve specific purposes in the code-processing pipeline. For\n instance, IM__END_OF_TEXT_TOKEN marks the end of a text or sequence, while the <|fim_prefix|> ,\n <|fim_middle|>, and <|fim_suffix|> tokens are used to implement the Fill-in-the-Middle\n(FIM) (Bavarian et al., 2022) technique, where a model predicts the missing parts of a code\n block. Additionally, <|fim_pad|> is used for padding during FIM operations. Other tokens\n include <|repo_name|> , which identifies repository names, and <|file_sep|> , used as a\n file separator to better manage repository-level information. These tokens are essential in\n helping the model learn from diverse code structures and enable it to handle longer and\n more complex contexts during both file-level and repo-level pretraining.\n\n                 Configuration             Qwen2.5-Coder 1.5B             Qwen2.5-Coder 7B\n                 Hidden Size                         1,536                        3,584\n                 # Layers                              28                           28\n                 # Query Heads                         12                           28\n                 # KV Heads                             2                            4\n                 Head Size                             128                          128\n                 Intermediate Size                   8,960                        18,944\n                 Embedding Tying                      True                        False\n                 Vocabulary Size                    151,646                      151,646\n                 # Trained Tokens                     5.5T                         5.5T\n\n                                Table 1: Architecture of Qwen2.5-Coder.\n\n                         Token                  Token ID        Description\n                         IM__END_OF_TEXT_TOKEN          151643          end of text/sequence\n                         <|fim_prefix|>         151659          FIM prefix\n                         <|fim_middle|>         151660          FIM middle\n                         <|fim_suffix|>         151661          FIM suffix\n                         <|fim_pad|>            151662          FIM pad\n                         <|repo_name|>          151663          repository name\n                         <|file_sep|>           151664          file separator\n\n                                Table 2: Overview of the special tokens.\n\n 3    Pre-training\n 3.1   Pretraining Data\n\n Large-scale, high-quality, and diverse data forms the foundation of pre-trained models. To\n this end, we constructed a dataset named Qwen2.5-Coder-Data. This dataset comprises\n five key data types: Source Code Data, Text-Code Grounding Data, Synthetic Data, Math\n Data, and Text Data. In this section, we provide a brief overview of the sources and cleaning\n methods applied to these datasets.\n\n 3.1.1   Data Composition\n Source Code        We collected public repositories from GitHub created before February 2024,\n spanning 92 programming languages. Similar to StarCoder2 (Lozhkov et al., 2024) and\n DS-Coder (Guo et al., 2024), we applied a series of rule-based filtering methods. In addition\n\n                                                         4# Technical Report\n\nto raw code, we also collected data from Pull Requests, Commits, Jupyter Notebooks, and Kaggle datasets, all of which were subjected to similar rule-based cleaning techniques.\n\n|Tokens (B)|Average Performance|\n|---|---|\n|600|50|\n|500|48|\n|400|45|\n|300|42|\n|200|40|\n|100|38|\n|0|35|\n\nStage 1    Stage 2   Stage 3    Stage 4\n\nFigure 1: Number of data tokens across different cc-stages, and the validation effectiveness of training Qwen2.5-Coder using corresponding data.# Text-Code Grounding Data\n\nWe curated a large-scale and high-quality text-code mixed dataset from Common Crawl, which includes code-related documentation, tutorials, blogs, and more. Instead of the conventional URL-based multi-stage recall method, we developed a coarse-to-fine hierarchical filtering approach for raw data. This method offers two key advantages:\n\n1. It enables precise control over each filter\u2019s responsibility, ensuring comprehensive handling of each dimension.\n2. It naturally assigns quality scores to the dataset, with data retained in the final stage being of higher quality, providing valuable insights for quality-driven data mixing.\n\nWe designed a cleaning pipeline for the Text-Code Grounding Data, where each filter level is built using smaller models, such as fastText. Although we experimented with larger models, they did not yield significant benefits. A likely explanation is that smaller models focus more on surface-level features, avoiding unnecessary semantic complexity.\n\nIn Qwen2.5-Coder, we applied this process iteratively. As shown in Figure 1, each iteration resulted in improvement. Through 4-stage filtering, the average scores on HumanEval and MBPP increased from 41.6% to 46.8% compared to the baseline, demonstrating the value of high-quality Text-Code Grounding Data for code generation.# Synthetic Data\n\nSynthetic data offers a promising way to address the anticipated scarcity of training data. We used CodeQwen1.5, the predecessor of Qwen2.5-Coder, to generate large-scale synthetic datasets. To mitigate the risk of hallucinations during this process, we introduced an executor for validation, ensuring that only executable code was retained.# Math Data\n\nTo enhance the mathematical capabilities of Qwen2.5-Coder, we integrated the pre-training corpus from Qwen2.5-Math into the Qwen2.5-Coder dataset. Importantly, the inclusion of mathematical data did not negatively impact the model\u2019s performance on code tasks. For further details on the collection and cleaning process, please refer to the Qwen2.5-Math technical report.# Text Data\n\nSimilar to the Math Data, we included high-quality general natural language data from the pre-training corpus of the Qwen2.5 model to preserve Qwen2.5-Coder\u2019s general capabilities. This data had already passed stringent quality checks during the\n\n5Technical Report\n\ncleaning phase of Qwen2.5\u2019s dataset, so no further processing was applied. However, all\ncode segments were removed from the general Text data to avoid overlap with our code\ndata, ensuring the independence of different data sources.\n\n3.1.2    Data Mixture\nBalancing Code, Math, and Text data is crucial for building a robust foundational model.\nAlthough the research community has explored this balance before, there is limited evidence\nregarding its scalability to large datasets. To address this, we conducted empirical experi-\nments with different ratios of Code, Math, and Text data, designing multiple experiments to\nidentify an optimal combination rapidly. Specifically, as shown in Table 3, we compared\nthree different Code: Text ratios \u2014 100:0:0, 85:10:5, and 70:20:10.\nInterestingly, we found that the 7:2:1 ratio outperformed the others, even surpassing the\nperformance of groups with a higher proportion of code. A possible explanation is that\nMath and Text data may positively contribute to code performance, but only when their\nconcentration reaches a specific threshold. In future work, we plan to explore more efficient\nratio mechanisms and investigate the underlying causes of this phenomenon. Ultimately,\nwe selected a final mixture of 70% Code, 20% Text, and 10% Math. The final training dataset\ncomprises 5.2 trillion tokens.\n\n      Token Ratio                Coding                  Math                         General                  Average\n  Code     Text    Math     Common        BCB     MATH       GSM8K       MMLU        CEval     HellaSwag\n   100       0       0         49.8       40.3      10.3        23.8        42.8      35.9         58.3          31.3\n    85      15       5         43.3       36.2      26.1        52.5        56.8      57.1         70.0          48.9\n    70      20       10        48.3       38.3      33.2        64.5        62.9      64.0         73.5          55.0\n\n   Table 3: The performance of Qwen2.5-Coder training on different data mixture policy.\n\n3.2    Training Policy\n\n     Qwen2.5            File-Level Pretrain     Repo-Level    Qwen2.5-Code-Base                     Qwen2.5-Code-Instruct\n                                                 Pretrain           (1.5B, 7B)       Code SFT             (1.5B, 7B)\n      (1.5B, 7B)             5.2T Tokens         300B Tokens\n\n                                \u2460                  \u2461                                    \u2462\n\n                  Figure 2: The three-stage training pipeline for Qwen2.5-Coder.\n\nAs shown in 2, we employed a three-stage training approach to train Qwen2.5-Coder,\nincluding file-level pretraining, repo-level pretraining, and instruction tuning.\n\n3.2.1    File-Level Pretraining\n\nFile-level pretraining focuses on learning from individual code files.                            In this stage, the\nmaximum training sequence length is set to 8,192 tokens, covering 5.2T of high-quality data.\nThe training objectives include next token prediction and fill-in-the-middle (FIM) (Bavarian\net al., 2022). The specific FIM format is shown in Figure 3.\n\n  File-Level FIM format.\n\n   <|fim_prefix|>{code_pre}<|fim_suffix|>{code_suf}<|fim_middle|>{code_mid}IM__END_OF_TEXT_TOKEN\n\n                                       Figure 3: File-Level FIM format.\n\n                                                            6Technical Report\n\n3.2.2    Repo-Level Pretraining\nAfter file-level pretraining, we turn to repo-level pretraining, aimed at enhancing the model\u2019s\nlong-context capabilities. In this stage, the context length is extended from 8,192 tokens to\n32,768 tokens, and RoPE\u2019s base frequency is adjusted from 10,000 to 1,000,000. To further\nleverage the model\u2019s extrapolation potential, we applied the YARN mechanism (Peng et al.,\n2023), enabling the model to handle sequences up to 131,072 (132K) tokens.\nIn this stage, we used a large amount of high-quality, long-code data ( \u2191 300B) and extended\nfile-level FIM to the repo-level FIM followed by methods described in Lozhkov et al. (2024),\nwith the specific format shown in Figure 4.\n\n  Repo-Level FIM format.\n\n   <|repo_name|>{repo_name}\n   <|file_sep|>{file_path1}\n   {file_content1}\n   <|file_sep|>{file_path2}\n   {file_content2}\n   <|file_sep|>{file_path3}\n   <|fim_prefix|>{code_pre}<|fim_suffix|>{code_suf}<|fim_middle|>{code_fim}IM__END_OF_TEXT_TOKEN\n\n                                     Figure 4: Repo-Level FIM format.\n\n4    Post-training\n4.1    A Recipe for Instruction Data\n\nMultilingual Programming Code Identification                         We fine-tune a CodeBERT to perform\nthe language identification model to categorize documents into nearly 100 programming\nlanguages. We keep the instruction data of the mainstream programming languages and\nrandomly discard a portion of the instruction data of the long-tail languages. If a given\nsample contains very little code data or even no code snippets, the sample will possibly be\nclassified into \u201cNo Programming Language\u201d tag. We remove most of the samples without\ncode snippets to keep the code generation capability of our instruction model.\n\nInstruction Synthesis from GitHub                 For the unsupervised data (code snippets) massively\nexisting in many websites (e.g. GitHub), we try to construct the supervised instruction\ndataset. Specifically, we use the LLM to generate the instruction from the code snippets\nwithin 1024 tokens and then we use the code LLM to generate the response Sun et al. (2024).\nFinally, we use the LLM scorer to filter the low-quality ones to obtain the final pair. Given the\ncode snippets of different programming languages, we construct an instruction dataset from\nthe code snippets. To increase the diversity of the instruction dataset. Conversely, we first\ngenerate the answers from the code. Then we use the LLM scorer to filter the low-quality\nto obtain the final triplet. Similarly, given the code snippets of different programming\nlanguages, we can construct an instruction dataset with the universal code from the code\nsnippets. To fully unleash the potential of our proposed method, we also include the open-\nsource instruction dataset in the seed instruction dataset. Finally, we combine three parts\ninstruction dataset for supervised fine-tuning.\n\nMultilingual Code Instruction Data                   To bridge the gap among different programming\nlanguages, we propose a multilingual multi-agent collaborative framework to synthesize\nthe multilingual instruction corpora. We introduce language-specific agents, where a set of\nspecialized agents are created and each dedicated to a particular programming language.\nThese agents are initialized with language-specific instruction data derived from the limited\n\n                                                           7# Technical Report\n\nExisting multilingual instruction corpora. The multilingual data generation process can be split into:\n\n1. Language-Specific Intelligent Agents: We create a set of specialized agents, each dedicated to a particular programming language. These agents are initialized with language-specific instruction data derived from curated code snippets.\n2. Collaborative Discussion Protocol: Multiple language-specific agents engage in a structured dialogue to formulate new instructions and solutions. This process can result in either enhancing existing language capabilities or generating instructions for a novel programming language.\n3. Adaptive Memory System: Each agent maintains a dynamic memory bank that stores its generation history to avoid generating similar samples.\n4. Cross-Lingual Discussion: We implement a novel knowledge distillation technique that allows agents to share insights and patterns across language boundaries, fostering a more comprehensive understanding of programming concepts.\n5. Synergy Evaluation Metric: We develop a new metric to quantify the degree of knowledge sharing and synergy between different programming languages within the model.\n6. Adaptive Instruction Generation: The framework includes a mechanism to dynamically generate new instructions based on identified knowledge gaps across languages.# Checklist-based Scoring for Instruction Data\n\nTo completely evaluate the quality of the created instruction pair, we introduce several scoring points for each sample:\n\n1. Question & Answer Consistency: Whether Q&A are consistent and correct for fine-tuning.\n2. Question & Answer Relevance: Whether Q&A are related to the computer field.\n3. Question & Answer Difficulty: Whether Q&A are sufficiently challenging.\n4. Code Exist: Whether the code is provided in question or answer.\n5. Code Correctness: Evaluate whether the provided code is free from syntax errors and logical flaws.\n6. Consider factors like proper variable naming, code indentation, and adherence to best practices.\n7. Code Clarity: Assess how clear and understandable the code is. Evaluate if it uses meaningful variable names, proper comments, and follows a consistent coding style.\n8. Code Comments: Evaluate the presence of comments and their usefulness in explaining the code\u2019s functionality.\n9. Easy to Learn: Determine its educational value for a student whose goal is to learn basic coding concepts.\n\nAfter gaining all scores (s1, ..., sn), we can get the final score with s = w1s1 + ... + wnsn, where (w1, ..., wn) are a series of pre-defined weights.# 4. Code Execution Engine:\n\n- Provides isolated environments for executing code snippets securely\n- Supports parallel execution of multiple test cases\n- Handles resource allocation and timeout mechanisms# 5. Result Analyzer:\n\n- Compares the output of code snippets against expected results from unit tests\n- Generates detailed reports on test case successes and failures\n- Provides suggestions for improvements based on failed test cases# Coarse-to-fine Fine-tuning\n\nWe first synthesized tens of millions of low-quality but diverse instruction samples to fine-tune the base model. In the second stage, we adopt millions of high-quality instruction samples to improve the performance of the instruction model with rejection sampling and supervised fine-tuning. For the same query, we use the LLM to generate multiple candidates and then use the LLM to score the best one for supervised fine-tuning.# Mixed Tuning\n\nSince most instruction data have a short length, we construct the instruction pair with the FIM format to keep the long context capability of the base model. Inspired by programming language syntax rules and user habits in practical scenarios, we leverage the tree-sitter-languages1 to parse the code snippets and extract the basic logic blocks as the middle code to infill. For example, the abstract syntax tree (AST) represents the structure of Python code in a tree format, where each node in the tree represents a construct occurring in the source code. The tree\u2019s hierarchical nature reflects the syntactic nesting of constructs in the code and includes various elements such as expressions, statements, and functions. By traversing and manipulating the AST, we can randomly extract the nodes of multiple levels and use the code context of the same file to uncover the masked node. Finally, we optimize the instruction model with a majority of standard SFT data and a small part of FIM instruction samples.# 5 Decontamination\n\nTo ensure that Qwen2.5-Coder does not produce inflated results due to test set leakage, we performed decontamination on all data, including both pre-training and post-training datasets. We removed key datasets such as HumanEval, MBPP, GSM8K, and MATH. The filtering was done using a 10-gram overlap method, where any training data with a 10-gram string-level overlap with the test data was removed.# 6 Evaluation on Base Models\n\nFor the base model, we conducted a comprehensive and fair evaluation in six key aspects, including code generation, code completion, code reasoning, mathematical reasoning, general natural language understanding and long-context modeling. To ensure the reproducibility of all results, we made all evaluation codes publicly available2. For comparing models, we chose the most popular and powerful open source language models, including the StarCoder2 and DeepSeek-Coder series. Below is the list of artifacts used in the evaluation for this section.# 6.1 Code Generation\n\nHumanEval and MBPP Code generation serves as a fundamental capability for code models to handle more complex tasks. We selected two popular code generation benchmarks.\n\n1 https://pypi.org/project/tree-sitter-languages/\n\n2 https://github.com/QwenLM/Qwen2.5-Coder# Technical Report\n\n|Artifact|Public link|\n|---|---|\n|Qwen2.5-Coder-1.5B|https://hf.co/qwen/Qwen/Qwen2.5-Coder-1.5B|\n|Qwen2.5-Coder-7B|https://hf.co/qwen/Qwen/Qwen2.5-Coder-7B|\n|CodeQwen1.5-7B-Base|https://huggingface.co/Qwen/CodeQwen1.5-7B|\n|StarCoder2-3B|https://hf.co/bigcode/starcoder2-3b|\n|StarCoder2-7B|https://hf.co/bigcode/starcoder2-7b|\n|StarCoder2-15B|https://hf.co/bigcode/starcoder2-15b|\n|DS-Coder-1.3B-Base|https://hf.co/deepseek-ai/deepseek-coder-1.3b-base|\n|DS-Coder-6.7B-Base|https://hf.co/deepseek-ai/deepseek-coder-6.7b-base|\n|DS-Coder-33B-Base|https://hf.co/deepseek-ai/deepseek-coder-33b-base|\n|DS-Coder-V2-Lite-Base|https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Base|\n|DS-Coder-V2-Base|https://hf.co/deepseek-ai/DeepSeek-Coder-V2-Base|# Table 4: All artifacts released and used in this section.\n\n|Model|Size|HumanEval| | |MBPP| |BigCodeBench| |\n|---|---|---|---|---|---|---|---|---|\n| | |HE|HE+|MBPP|MBPP+|3-shot|Full|Hard|\n|StarCoder2-3B|3B|31.7|27.4|60.2|49.1|-|21.4|4.7|\n|DS-Coder-1.3B|1.3B|34.8|26.8|55.6|46.9|46.2|26.1|3.4|\n|Qwen2.5-Coder-1.5B|1.5B|43.9|36.6|69.2|58.6|59.2|34.6|9.5|\n|StarCoder2-7B|7B|35.4|29.9|54.4|45.6|-|27.7|8.8|\n|StarCoder2-15B|15B|46.3|37.8|66.2|53.1|-|38.4|12.2|\n|DS-Coder-6.7B-Base|6.7B|47.6|39.6|70.2|56.6|60.6|41.1|11.5|\n|DS-Coder-V2-Lite-Base|2.4/16B|40.9|34.1|71.9|59.4|-|30.6|8.1|\n|CodeQwen1.5-7B-Base|7B|51.8|45.7|72.2|60.2|61.8|45.6|15.6|\n|Qwen2.5-Coder-7B-Base|7B|61.6|53.0|76.9|62.9|68.8|45.8|16.2|\n|DS-Coder-33B-Base|33B|54.9|47.6|74.2|60.7|66.0|49.1|20.3|# Table 5: Performance of various models on HumanEval, MBPP and the \u201ccomplete\u201d task of BigCodeBench.\n\nTo evaluate Qwen2.5-Coder, namely HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). HumanEval consists of 164 manually written programming tasks, each providing a Python function signature and a docstring as input to the model. MBPP, on the other hand, comprises 974 programming problems created by crowdsource contributors. Each problem includes a problem statement (i.e., a docstring), a function signature, and three test cases.\n\nTo further ensure accurate evaluation, EvalPlus (Liu et al., 2023) extends HumanEval into HumanEval+ by adding 80 times more unique test cases and correcting inaccurate ground-truth solutions in HumanEval. Similarly, MBPP+ offers 35 times more test cases than the original MBPP.\n\nAdditionally, we should notice that MBPP 3-shot is particularly suitable for monitoring model convergence during training. Early in the convergence process, the model tends to be unstable, causing significant fluctuation in metrics, and simple 3-shot examples effectively mitigate it. Therefore, we also report the results of MBPP 3-shot performance.\n\nAs shown in Table 5, Qwen2.5-Coder have shown impressive performance in basic code generation, achieving state-of-the-art results among open-source models of the same size and surpassing even larger models. In particular, Qwen2.5-Coder-7B-Base outperforms the previous best dense model, DS-Coder-33B-Base, across all five metrics.# BigCodeBench-Complete\n\nBigCodeBench (Zhuo et al., 2024) is a recent and more challenging benchmark for code generation, primarily aimed at evaluating the ability of tool-use and complex instruction following. The base model generates the expected code through a# Performance of different models on MultiPL-E\n\n|Model|Size|Python|C++|Java|PHP|TS|C#|Bash|JS|Average|\n|---|---|---|---|---|---|---|---|---|---|---|\n|StarCoder2-3B|3B|31.7|30.4|29.8|32.9|39.6|34.8|13.9|35.4|31.1|\n|DS-Coder-1.3B-Base|1.3B|34.8|31.1|32.3|24.2|28.9|36.7|10.1|28.6|28.3|\n|Qwen2.5-Coder-1.5B|1.5B|42.1|42.9|38.6|41.0|49.1|46.2|20.3|49.1|41.1|\n|StarCoder2-7B|7B|35.4|40.4|38.0|30.4|34.0|46.2|13.9|36.0|34.3|\n|StarCoder2-15B|15B|46.3|47.2|46.2|39.1|42.1|53.2|15.8|43.5|41.7|\n|DS-Coder-6.7B-Base|6.7B|49.4|50.3|43.0|38.5|49.7|50.0|28.5|48.4|44.7|\n|DS-Coder-V2-Lite-Base|2.4/16B|40.9|45.9|34.8|47.2|48.4|41.7|19.6|44.7|40.4|\n|CodeQwen1.5-7B-Base|7B|51.8|52.2|42.4|46.6|52.2|55.7|36.7|49.7|48.4|\n|Qwen2.5-Coder-7B-Base|7B|61.6|62.1|53.2|59.0|64.2|60.8|38.6|60.3|57.5|\n|DS-Coder-33B-Base|33B|56.1|58.4|51.9|44.1|52.8|51.3|32.3|55.3|50.3|# HumanEval Infilling\n\nMany developer aid tools rely on the capability to autocomplete code based on preceding and succeeding code snippets. Qwen2.5-Coder utilizes the Fill-In-the-Middle (FIM) training strategy, as introduced in Bavarian et al. (2022), enabling the model to generate code that is contextually coherent. To assess its code completion proficiency, we utilize the HumanEval Infilling benchmark Allal et al. (2023). This benchmark challenges the model to accurately predict missing sections of code within tasks derived from HumanEval. We use the single-line infilling settings across Python, Java, and JavaScript, focusing on predicting a single line of code within given contexts. Performance was measured using the Exact Match metric, which determines the proportion of the first generated code line that precisely match the ground truth.\n\nThe table 7 illustrates that Qwen2.5-Coder surpasses alternative models concerning model size. Specifically, Qwen2.5-Coder-1.5B achieves an average performance improvement of 3.7%, rivaling the majority of models exceeding 6 billion parameters. Moreover, Qwen2.5-Coder-7B-Base stands as the leading model among those over 6 billion parameters, matching the performance of the formidable 33 billion parameter model, DS-Coder-33B-Base. Notably, we excluded DS-Coder-v2-236B from comparison due to its design focus not being on code completion tasks.# 6.4 Math Reasoning\n\nMathematics and coding have always been closely intertwined. Mathematics forms the foundational discipline for coding, while coding serves as a vital tool in mathematical fields. As such, we expect an open and powerful code model to exhibit strong mathematical capabilities as well. To assess Qwen2.5-Coder\u2019s mathematical performance, we selected five popular benchmarks, including MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), MMLU-STEM (Hendrycks et al., 2020) and TheoremQA (Chen et al., 2023). As shown in Table 9, Table 3 highlights Qwen2.5-Coder\u2019s strengths in mathematics, which likely stem from two key factors: first, the model\u2019s strong foundation built on Qwen2.5, and second, the careful mixing of code and mathematical data during training, which has ensured a well-balanced performance across these domains.\n\n|Model|Size|MATH|GSM8K|MMLU STEM|TheoremQA|\n|---|---|---|---|---|---|\n|StarCoder2-3B|3B|10.8|21.6|34.9|12.1|\n|DS-Coder-1.3B-Base|1.3B|4.6|4.4|24.5|8.9|\n|Qwen2.5-Coder-1.5B|1.5B|30.9|65.8|49.0|21.4|\n|StarCoder2-7B|7B|14.6|32.7|39.8|16.0|\n|StarCoder2-15B|15B|23.7|57.7|49.2|20.5|\n|DS-Coder-6.7B-Base|6.7B|10.3|21.3|34.2|13.6|\n|DS-Coder-V2-Lite-Base|2.4/16B|39.0|67.1|58.5|29.3|\n|CodeQwen1.5-7B-Base|7B|10.6|37.7|39.6|15.8|\n|Qwen2.5-Coder-7B-Base|7B|46.6|83.9|67.6|34.0|\n|DS-Coder-33B-Base|33B|14.4|35.4|39.5|17.5|\n\nTable 9: Performance of various models on four math benchmark, named MATH, GSM8K, MMLU STEM and TheoremQA respectively.\n\n|Model|Size|MMLU Base|MMLU Redux|\n|---|---|---|---|\n|StarCoder2-3B|3B|36.6|37.0|\n|DS-Coder-1.3B-Base|1.3B|25.8|24.5|\n|Qwen2.5-Coder-1.5B|1.5B|53.6|50.9|\n|StarCoder2-7B|7B|38.8|38.6|\n|StarCoder2-15B|15B|64.1|48.8|\n|DS-Coder-6.7B-Base|6.7B|36.4|36.5|\n|DS-Coder-V2-Lite-Base|2.4/16B|60.5|58.3|\n|CodeQwen1.5-7B-Base|7B|40.5|41.2|\n|Qwen2.5-Coder-7B-Base|7B|68.0|66.6|\n|DS-Coder-33B-Base|33B|39.4|38.7|\n\nTable 10: MMLU results of different models, a general benchmark for common knowledge.# Technical Report\n\n|Model|Size|ARC-Challenge|TruthfulQA|WinoGrande|HellaSwag| |\n|---|---|---|---|---|---|---|\n|1B+ Models|StarCoder2-3B|3B|34.2|40.5|57.1|48.1|\n|DS-Coder-1.3B-Base|1.3B|25.4|42.7|53.3|39.5| |\n|Qwen2.5-Coder-1.5B|1.5B|45.2|44.0|60.7|61.8| |\n|6B+ Models|StarCoder2-7B|7B|38.7|42.0|57.1|52.4|\n|StarCoder2-15B|15B|47.2|37.9|64.3|64.1| |\n|DS-Coder-6.7B-Base|6.7B|36.4|40.2|57.6|53.8| |\n|DS-Coder-V2-Lite-Base|2.4/16B|57.3|38.8|72.9|-| |\n|CodeQwen1.5-7B-Base|7B|35.7|42.2|59.8|56.0| |\n|Qwen2.5-Coder-7B-Base|7B|60.9|50.6|72.9|76.8| |\n|20B+ Models|DS-Coder-33B-Base|33B|42.2|40.0|62.0|60.2|\n\nTable 11: General performance of different models on four popular general benchmark, ARC-Challenge, TruthfulQA, WinoGrande and HellaSwag.# 6.5 General Natural Language\n\nIn addition to mathematical ability, we aim to retain as much of the base model\u2019s general-purpose capabilities as possible, such as general knowledge. To evaluate general natural language understanding, we selected MMLU (Hendrycks et al., 2021) and its variant MMLU-Redux (Gema et al., 2024), along with four other benchmarks: ARC-Challenge (Clark et al., 2018), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2019), and HellaSwag (Zellers et al., 2019). Similar to the results in mathematics, Table 11 highlights Qwen2.5-Coder\u2019s advantage in general natural language capabilities compared to other coders, further validating the effectiveness of Qwen2.5-Coder data mixing strategy.# 6.6 Long-Context Evaluation\n\nLong context capability is crucial for code LLMs, serving as the core skill for understanding repository-level code and becoming a code agent. However, most of current code models still have very limited support for length, which hinders their potential for practical application. Qwen2.5-Coder aims to further advance the progress of open-source code models in long context modeling. To achieve this, we have collected and constructed long sequence code data at the repository level for pre-training. Through careful data proportioning and organization, we have enabled it to support input lengths of up to 128K tokens.# Needle in the Code\n\nWe created a simple but basic synthetic task called Needle in the Code, inspired by popular long-context evaluations in the text domain. In this task, we inserted a very simple custom function at various positions within a code repo (we chose Megatron3 to honor its contributions to open-source LLMs!) and tested whether the model could replicate this function at the end of the codebase. The figure below shows that Qwen2.5-Coder is capable of successfully completing this task within a 128k length range.# 7 Evaluation on Instruct Models\n\nFor the evaluation of the instruct models, we rigorously assessed six core areas: code generation, code reasoning, code editing, text-to-sql, mathematical reasoning and general natural language understanding. The evaluation was structured to ensure a fair and thorough comparison across models. All evaluation code is publicly accessible for reproducibility.\n\n3 https://github.com/NVIDIA/Megatron-LM\n\n4 https://github.com/QwenLM/Qwen2.5-Coder# Table 12: All artifacts released and used in this section.# 7.1 Code Generation\n\nBuilding on the performance improvements of the Qwen2.5-Coder series base models, our Qwen2.5-Coder series instruct models similarly demonstrated outstanding performance in code generation tasks.# HumanEval and MBPP\n\nWe also evaluated the code generation capabilities of the Qwen2.5-Coder series instruct models using the EvalPlus (Liu et al., 2023) dataset. As illustrated by the experimental results in Table 13, our Qwen2.5-Coder-7B-Instruct model demonstrated superior accuracy, significantly outperforming other models with a comparable number of parameters. Remarkably, it even exceeded the performance of larger models with over 20 billion parameters, such as CodeStral-22B and DS-Coder-33B-Instruct. Notably, Qwen2.5-Coder-7B-Instruct was the only model in our evaluation to surpass an 80% accuracy rate on HumanEval+, achieving an impressive 84.1%.# BigCodeBench-Instruct\n\nThe instruct split provided by BigCodeBench (Zhuo et al., 2024) is intended for assessing the code generation abilities of instruct models. We assessed the Qwen2.5-Coder series instruct models on the BigCodeBench-Instruct. As shown in Table 13, the Qwen2.5-Coder-7B-Instruct outperformed other instruct models with similar parameter sizes, achieving higher accuracy scores on both the full and hard subsets, reaching 41.0% on the full subset and 18.2% on the hard subset, demonstrating the Qwen2.5-Coder series instruct models\u2019 powerful code generation capabilities.# LiveCodeBench\n\nLiveCodeBench (Jain et al., 2024) is a comprehensive and contamination-free benchmark designed to evaluate the coding capabilities of LLMs. It continuously# Table 13: The performance of different instruct models on code generation by HumanEval, MBPP, bigcodebench and livecodebench. For bigcodebench here, we report \u201cinstruct\u201d tasks score.\n\n|Model|Size|HumanEval HE|HumanEval HE+|MBPP|MBPP+|Full|Hard|Pass@1|\n|---|---|---|---|---|---|---|---|---|\n|DS-Coder-1.3B-Instruct|1.3B|65.2|61.6|61.6|52.6|22.8|3.4|9.3|\n|Qwen2.5-Coder-1.5B-Instruct|1.5B|70.7|66.5|69.2|59.4|32.5|6.8|15.7|\n|DS-Coder-6.7B-Instruct|6.7B|78.6|70.7|75.1|66.1|35.5|10.1|20.5|\n|DS-Coder-V2-Lite-Instruct|2.4/16B|81.1|75.0|82.3|68.8|36.8|16.2|24.3|\n|CodeQwen1.5-7B-Chat|7B|86.0|79.3|83.3|71.4|39.6|17.2|20.1|\n|Qwen2.5-Coder-7B-Instruct|7B|88.4|84.1|83.5|71.7|41.0|18.2|37.6|\n|CodeStral-22B|22B|78.1|74.4|73.3|68.2|47.1|20.6|32.9|\n|DS-Coder-33B-Instruct|33B|79.3|68.9|81.2|70.1|46.5|17.6|27.7|# Table 14: The performance of different approaches on instruct format MultiPL-E.\n\n|Model|Size|Python|Java|C++|C#|TS|JS|PHP|Bash|Average|\n|---|---|---|---|---|---|---|---|---|---|---|\n|DS-Coder-1.3B-Instruct|1.3B|65.2|51.9|45.3|55.1|59.7|52.2|45.3|12.7|48.4|\n|Qwen2.5-Coder-1.5B-Instruct|1.5B|71.2|55.7|50.9|64.6|61.0|62.1|59.0|29.1|56.7|\n|DS-Coder-6.7B-Instruct|6.7B|78.6|68.4|63.4|72.8|67.2|72.7|68.9|36.7|66.1|\n|DS-Coder-V2-Lite-Instruct|2.4/16B|81.1|76.6|75.8|76.6|80.5|77.6|74.5|43.0|73.2|\n|CodeQwen1.5-7B-Chat|7B|83.5|70.9|72|75.9|76.7|77.6|73.9|41.8|71.6|\n|Qwen2.5-Coder-7B-Instruct|7B|87.8|76.5|75.6|80.3|81.8|83.2|78.3|48.7|76.5|\n|CodeStral-22B|22B|78.1|71.5|71.4|77.2|72.3|73.9|69.6|47.5|70.2|\n|DS-Coder-33B-Instruct|33B|79.3|73.4|68.9|74.1|67.9|73.9|72.7|43.0|69.2|\n|DS-Coder-V2-Instruct|21/236B|90.2|82.3|84.8|82.3|83|84.5|79.5|52.5|79.9|\n\ngathers new problems from leading competitive programming platforms like LeetCode, AtCoder, and CodeForces, ensuring an up-to-date and diverse set of challenges. Currently, it hosts over 600 high-quality coding problems published between May 2023 and September 2024.\n\nTo better demonstrate our model\u2019s effectiveness on real-world competitive programming tasks, we conduct evaluation of the Qwen-2.5-Coder series instruct models on the Live-CodeBench (2305-2409) dataset. As illustrated in Table 13, the Qwen-2.5-Coder-7B-Instruct model achieved an impressive Pass@1 accuracy of 37.6%, significantly outperforming other models of comparable parameter scales. Notably, it also surpassed larger models such as CodeStral-22B and DS-Coder-33B-Instruct, underscoring the Qwen-2.5-Coder series\u2019 exceptional capabilities in handling complex code generation challenges.# Multi-Programming Language\n\nThe Qwen2.5-Coder series instruct models have inherited the high performance of the base model on the Multi-Programming Language. To further evaluate their capabilities, we tested the instruct models on two specific benchmarks: MultiPL-E (Cassano et al., 2022) and McEval (Chai et al., 2024).# MultiPL-E\n\nAs demonstrated by the evaluation results in Table 14, Qwen2.5-Coder-7B-Instruct consistently outperforms other models with the same number of parameters, including DS-Coder-V2-Lite-Instruct, in code generation tasks across eight programming.\n\n5 https://leetcode.com\n\n6 https://atcoder.jp\n\n7 https://codeforces.com# Technical Report\n\nWith an average accuracy of 76.5%, Qwen2.5-Coder-7B-Instruct surpasses even larger models such as CodeStral-22B and DS-Coder-33B-Instruct (despite having over 20 billion parameters), highlighting its powerful code generation capabilities in multiple programming languages.# 7.2 Code Reasoning\n\nTo assess the code reasoning capabilities of the Qwen2.5-Coder series instruct models, we performed an evaluation on CRUXEval (Gu et al., 2024). As illustrated by the experimental results in Table 15, the Qwen2.5-Coder-7B-Instruct model achieved Input-CoT and Output-CoT accuracies of 65.8% and 65.9%, respectively. This represents a notable improvement over the DS-Coder-V2-Lite-Instruct model, with gains of 12.8% in Input-CoT accuracy and 13.0% in Output-CoT accuracy. Furthermore, the Qwen2.5-Coder-7B-Instruct model outperformed larger models, such as the CodeStral-22B and DS-Coder-33B-Instruct, underscoring its superior code reasoning capabilities despite its smaller size.\n\nFigure 7 illustrates the relationship between model sizes and code reasoning capabilities. The Qwen2.5-Coder instruct models stand out for delivering superior code reasoning performance with the fewest parameters, surpassing the results of other open-source large language models by a significant margin. According to this trend, we expect that code reasoning performance comparable to GPT-4o could be achieved with a model around the 30 billion parameters scale.# Model Performance\n\n|Model|Size|Pass@1|Pass@2|\n|---|---|---|---|\n|DS-Coder-1.3B-Instruct|1.3B|17.3*|21.1*|\n|Qwen2.5-Coder-1.5B-Instruct|1.5B|30.1|33.1|\n|DS-Coder-6.7B-Instruct|6.7B|34.6*|42.1*|\n|DS-Coder-V2-Lite-Instruct|2.4/16B|48.9*|55.6*|\n|CodeQwen1.5-7B-Chat|7B|23.3|33.1|\n|Qwen2.5-Coder-7B-Instruct|7B|50.4|57.1|\n|CodeStral-22B|22B|35.3*|45.1*|\n|DS-Coder-33B-Instruct|33B|49.6*|59.4*|\n|DS-Coder-V2-Instruct|21/236B|73.7|-|\n\nTable 16: The code editing ability of different instruct models evaluated by Aider benchmark. * indicates that the experimental results have been reproduced in our experiments, and the whole edit-format was consistently applied across all experiments.\n\nPASS@1 accuracy of 50.4%, significantly outperforming comparable models. Notably, it also surpasses larger models such as CodeStral-22B (22 billion parameters) and DS-Coder-33B-Instruct (33 billion parameters), showcasing its remarkable efficiency and effectiveness in code editing tasks.# 7.4 Text-to-SQL\n\nSQL is one of the essential tools in daily software development and production, but its steep learning curve often hinders free interaction between non-programming experts and databases. To address this issue, the Text-to-SQL task was introduced, aiming for models to automatically map natural language questions to structured SQL queries. Previous improvements in Text-to-SQL focused primarily on structure-aware learning, domain-specific pre-training, and sophisticated prompt designs.\n\nThanks to the use of finely crafted synthetic data during both pre-training and fine-tuning, we significantly enhanced Qwen2.5-Coder\u2019s capability in Text-to-SQL tasks. We selected two well-known benchmarks, Spider (Yu et al., 2018) and BIRD (Li et al., 2024), for comprehensive evaluation. To ensure a fair comparison between Qwen2.5-Coder and other open-source language models on this task, we used a unified prompt template as input, following the work of Chang & Fosler-Lussier (2023). As shown in Figure 8, the prompt consists of table representations aligned with database instructions, examples of table content, optional additional knowledge, and natural language questions. This standardized prompt template minimizes biases that may arise from prompt variations. As shown in Figure 9, Qwen2.5-Coder outperforms other code models of the same size on the Text-to-SQL task.# Model Performance in Math and General\n\n|Model|Size|MATH|GSM8K|GaoKao2023en|OlympiadBench|CollegeMath|AIME24|\n|---|---|---|---|---|---|---|---|\n|DS-Coder-V2-Lite-Instruct|2.4/16B|61.0|87.6|56.1|26.4|39.8|6.7|\n|Qwen2.5-Coder-7B-Instruct|7B|66.8|86.7|60.5|29.8|43.5|10.0|\n\n|Model|Size|AMC23|MMLU|MMLU-Pro|IFEval|CEval|GPQA|\n|---|---|---|---|---|---|---|---|\n|DS-Coder-V2-Lite-Instruct|2.4/16B|40.4|42.5|60.6|38.6|60.1|27.6|\n|Qwen2.5-Coder-7B-Instruct|7B|42.5|45.6|68.7|58.6|61.4|35.6|\n\nTable 17: The performance of math and General.# Figure 8: Prompt template of Qwen2.5-Coder for text-to-SQL tasks.\n\n|Model|Score 1|Score 2|\n|---|---|---|\n|Qwen2.5-Coder-7B-Instruct|51.1|82.0|\n|CodeStral-22B|46.2|76.6|\n|DS-Coder-33B-Instruct|45.6|73.8|\n|DS-Coder-V2-Lite-Instruct|41.6|74.6|\n|DS-Coder-6.7B-Instruct|39.8|70.0|\n|DS-Coder-1.3B-Instruct|Bird|22.0|\n| |59.0|Spider|# Figure 9: The text-to-SQL evaluation on various instruct code models.# 7.5 Math Reasoning and General Natural Language\n\nIn this section, we present a comparative analysis of the performance between our Qwen2.5-Coder-7B-Instruct model and the DS-Coder-V2-Lite-Instruct model, focusing on both mathematical computation and general natural language processing tasks. As indicated in Table 17, the Qwen2.5-Coder-7B-Instruct model outperforms the DS-Coder-V2-Lite-Instruct in 11 out of 12 tasks. This result underscores the model\u2019s versatility, excelling not only in complex coding tasks but also in sophisticated general tasks, thus distinguishing it from its competitors.# 8 Conclusion\n\nThis work introduces Qwen2.5-Coder, the latest addition to the Qwen series. Built upon Qwen2.5, a top-tier open-source LLM, Qwen2.5-Coder has been developed through extensive pre-training and post-training of Qwen2.5-1.5B and Qwen2.5-7B on large-scale datasets. To ensure the quality of the pre-training data, we have curated a dataset by collecting public code data and extracting high-quality code-related content from web texts, while# Technical Report\n\nFiltering out low-quality data using advanced classifiers. Additionally, we have constructed a meticulously designed instruction-tuning dataset to transform the base code LLM into a strong coding assistant. Looking ahead, our research will focus on exploring the impact of scaling up code LLMs in terms of both data size and model size. We will also continue to enhance the reasoning capabilities of these models, aiming to push the boundaries of what code LLMs can achieve.",
        "context_id": 34,
        "question": "Which model scored the highest on the Performance Metrics chart?",
        "answer": [
            "Qwen2.5-Coder 7B-Instruct"
        ],
        "context_length": 45226
    },
    {
        "context": "# 2 Introduction\n\nWhat is the path to a flagship mission? Many astronomers and engineers spent the years leading up to the 2020 decadal survey providing worked examples of what could come next2,3. The Astro2020 Decadal Survey prioritized a Large IR/optical/UV space telescope (\u201cHabitable Worlds Observatory\u201d, HWO) to pursue an ambitious program of exoplanetary discovery and characterization, as well as cosmic origins astrophysics1. The decadal survey report recognized that many of the galactic ecosystem, exoplanet, and stellar science goals of the HWO require high-throughput imaging and spectroscopy at ultraviolet (UV) to optical wavelengths. The Astro2020 Panel on Electromagnetic Observations from Space1 recommended that \u201c. . . [the] mission will also need focal plane instrumentation to acquire images and spectra over the range of 100 nm to 2 \u03bcms with parameters similar to cameras and spectrometers proposed for the Large Optical UV Infrared Telescope (LUVOIR)...\u201d\n\nThe decadal survey endorsed an early program of technology maturation ahead of the pre-Phase-A start of HWO to reduce cost and schedule risks to the mission. This recommendation, the Great Observatories Mission and Technology Maturation Program, was adopted by NASA in the form of the Great Observatories Maturation Program (GOMAP)4 to finalize the science requirements from Astro2020 and prepare the requisite technologies for HWO (from stable telescopes to starlight suppression to ultraviolet-enabling mirror coatings).\n\nIn this white paper, we have collected key science cases that drive the UV baseline for our next flagship mission and captured the current state of UV technology development - combining the current state of the art with the steps required to reach the projected instrument capabilities of HWO. Our expectation is that the details will evolve as the work of the GOMAP progresses. However, this document provides a foundational understanding of UV development work, accelerating the ability of our community to move quickly \u2014 a necessity with the tight HWO timelines.\n\nThe reader might approach this paper in a variety of ways. First, we present a summary in Section 1. This provides a map to the rest of the text for those who have a particular area of question.\n\n32 https://www.jpl.nasa.gov/habex/documents/\n\n4 https://asd.gsfc.nasa.gov/luvoir/\n\nhttps://science.nasa.gov/astrophysics/programs/gomap/\n\n5# Motivating Science\n\nAt the heart of every flagship mission NASA builds and flies sits the scientific boundaries we plan to explore and surpass. HWO focuses on two key areas: discovery and characterization of habitable worlds, and the broad remit of transformational astrophysics. The primary purpose of HWO is to directly determine whether planets in nearby stellar systems exhibit potential for habitability and, if so, to search for signatures of extra-terrestrial life. Such an investigation begs answers to the question of what fundamental processes lead to the formation and evolution of life. Below we outline specific science questions in the two relevant topic areas from the Astro2020 Decadal: \u201cWorlds and Suns in Context\u201d and \u201cCosmic Ecosystems.\u201d Both areas will stretch our understanding of the formation and evolution of stars, planets, and galaxies.\n\nBeing able to identify multiple Earth-analog systems would change our understanding of our own Solar system and the many systems we have found over the last decade. Beyond discovery, which will be accomplished with coronagraphy, characterization is crucial for us to differentiate planets, especially those that might have atmospheres that could support life as we know it. Biosignature identification requires spectrographic capability and a spectrograph operating down into the deep UV provides access to crucial diagnostic spectral lines in exoplanet atmospheres. Besides biocompatibility, UV spectroscopy will reveal the impact of UV radiation on the chemistry and heating of planetary atmospheres, an understanding that is critical as we move forward to truly understand the excitingly broad range of exoplanets in our nearby universe.\n\nOur understanding of galaxy evolution has shifted drastically in the last decade. Work with line-of-sight absorption systems and a small number of integral field emission maps has exposed the circumgalactic medium (CGM) to be a highly active and interactive ecosystem, one which is seemingly crucial to our understanding of both the evolution of galaxies themselves as well as the larger cosmological history of the Universe. The CGM feeds gas into galaxies to fuel star formation while simultaneously receiving energy and metal enriched gas as explosions and winds drive escape from the galaxy. We are just beginning to understand the mechanisms that govern the CGM and the overall role that this dynamic regime plays in how the history of matter in our universe unfolds. The UV is crucial to understanding the CGM in local systems where we are able to resolve them on physical scales that match the dominant processes.\n\nIn the sections below, we focus on particular science cases that drive ultraviolet observational requirements. We call out lower boundary wavelength ranges alongside each topic heading. We use the bandpass convention of extreme ultraviolet (EUV) defined as 10.0 \u2013 91.1 nm, far ultraviolet (FUV) as 91.2 \u2013 180.0 nm, and near ultraviolet (NUV) as 180.0 - 320.0.# 3.1 Worlds and Suns in Context\n\nExoplanetary science has witnessed an explosion of discovery in the last three decades, with over 5000 exoplanets now known and planetary occurrence rates of 10 \u2013 60% for F, G, and K stars and 80% or more for M dwarf stars.2, 3 Now that large numbers of planets with basic properties (e.g., mass and radius) are known, the next task is to characterize the composition and evolution of their atmospheres, and ultimately to determine which of these worlds may support habitable conditions.\n\nUV radiation from protoplanetary and exoplanetary systems provides unique tracers of atomic and molecular gas in these environments, making spectral coverage at UV wavelengths (100 \u2013 320 nm) essential to completing the exoplanet science goals of the HWO.\n\nThe Astro2020 panel on Exoplanets, Astrobiology, and the Solar System highlighted two key research questions where UV diagnostics and environmental context are critical:\n\n1. How does a planet\u2019s interaction with its host star and planetary system influence its atmospheric properties over all time scales? (E-Q2d)\n2. What is the range of potentially habitable environments around different types of stars? (E-Q3c)\n\nThese questions are addressed by (1) comprehensive characterization of the radiation environment of exoplanet host stars as a function of stellar mass and age. UV photons and high-energy particle outputs regulate the chemical and physical state of exoplanetary atmospheres (Section 3.1.1). (2) Measurements of the chemical inventory and radial distribution of molecular gas in protoplanetary disks set the initial conditions for the planets that we will characterize with HWO (Section 3.1.2). In order to understand the full life cycle of habitable planets, we need to place observational constraints on the physical and chemical conditions where they are born, including mapping the molecular abundances and physical structure of the inner regions (&lt; 3 AU) around young stars.# 3.1.1 UV emission from exoplanet host stars: 102 \u2013 160 nm\n\nIn parallel with the discovery of thousands of planets, it has become clear that the planetary mass and effective surface temperature alone do not predict the characteristics of a planet\u2019s atmosphere. The physics and chemistry of all types of planetary atmospheres are driven by the ultraviolet radiation (100 \u2013 320 nm; see Figure 1) from their parent stars. From the importance of the stellar FUV and NUV radiation on our ability to accurately predict and interpret biosignature gases on rocky exoplanets6\u20139 to the young Neptune-mass planets being sculpted by their host star\u2019s EUV radiation (a leading theory to explain the \u201cradius gap\u201d;10, 11), the stellar UV radiation inputs are critical. Indeed, with the very first James Webb Space Telescope (JWST) transiting planet observations, we are seeing direct evidence of UV-induced photochemistry on the Hot Jupiter WASP-39b,12 moving this field from model predictions to empirical studies of the influence of stellar photons on exoplanetary atmospheres.\n\nAccess to the FUV bandpass, specifically coverage from 102 \u2013 115 nm, is critical for calculating the EUV inputs into exoplanetary atmospheres. The EUV radiation is the key forcing function in determining the atmospheric escape timescale of all types of planets. The 102 \u2013 115 nm range includes unique spectral tracers of portions of the host star chromosphere and coronae; this formation temperature (Tform) coverage greatly reduces the uncertainty of EUV reconstruction techniques.15# Homopause\n\nFig 1: A sketch demonstrating the interactions of UV light with different layers of a planetary atmosphere. The ultraviolet stellar spectrum drives photochemistry (FUV and NUV photons, 100 \u2013 320 nm) and atmospheric escape (EUV photons, 10 \u2013 90 nm) on orbiting planets. The EUV spectrum is calculated from chromospheric and coronal lines in the FUV spectrum, with the 100 \u2013 115 nm band providing unique diagnostics for temperature and abundances. The host star spectra are used to predict the most promising habitable planet candidates and to interpret atmospheric spectra.4, 5# LUMOS T Tauri Star Simulation\n\n2.0x10-16\nE.Sx10-%\n2 8E L.OxlO-6\n5.0x10-17\nLUMOS\nH2\n\n15 Wavelength (A 125120 1130\n\nFig 2: Spectral simulation of the 111.1 \u2013 113.2 nm spectrum of an edge-on protoplanetary disk, a spectral region containing strong lines of H2 and H2O.13, 14 Multi-object spectroscopy from 100 \u2013 170 nm enables simultaneous detection of up to 30 young stars in a region like the Orion Nebula Cluster (ONC). A multiplexed instrument on a \u21926 m space observatory would eclipse the total UV disk archive of HST in 2 \u2013 4 pointings in the ONC.As there is no EUV spectroscopy observatory on the horizon, these techniques will remain our best means of calculating atmospheric escape rates from temperate, terrestrial planets in the HWO era. The 102 \u2013 115 nm region includes emission lines of S IV (106.3, 107.3 nm) [log(Tform) = 4.9], O VI (103.2, 103.8 nm) [log(Tform) = 5.5], Ne V (113.6, 114.5 nm) [log(Tform) = 5.4], and Fe XIX (111.8 nm) [log(Tform) = 7.0]. In concert with the intermediate- and high-temperature lines found in the FUV redward of 115nm: C II and Si II [log(Tform) = 4.5], Si III [log(Tform) = 4.7] Si IV [log(Tform) = 4.8], C IV [log(Tform) = 5.0], N V [log(Tform) = 5.2], O V [log(Tform) = 5.3], Fe XII [log(Tform) = 6.2], and Fe XXI [log(Tform) = 7.1], this suite of lines provides the comprehensive temperature cover to calculate the EUV irradiance from all nearby planet-hosting stars without the need for coordinated X-ray measurements.# 3.1.2 Characterizing Protoplanetary Disks: 100 \u2013 115 nm\n\nHow do planets form? To answer this question, it is crucial to trace the composition, distribution, and evolution of planet-forming material in protoplanetary disks. UV spectroscopy is a unique tool for observing the molecular gas in the inner regions of protoplanetary disks: the strongest electronic band systems of H2 and CO reside in the 100 \u2013 170 nm wavelength range. UV-fluorescent H2 spectra are sensitive to gas surface densities lower than 10-6 g cm-2, making them an extremely useful probe of remnant gas at r < 10 AU during the disk dispersal stage after planet cores have formed. In cases where mid-IR CO spectra or traditional accretion diagnostics (e.g. H\u03c9 equivalent widths) suggest that the inner gas disk has dissipated, far-UV H2 observations can offer unambiguous evidence for the presence of a remnant molecular disk and ongoing protostellar mass accretion.\n\nA high-sensitivity space observatory with a multi-object, high-resolution capability (R &gt; 30,000, &gt; 4 square arcminutes per field) enables transformative absorption-line studies of high-inclination (i &gt; 60 degrees) disks. Absorption-line spectroscopy through high-inclination disks, currently limited on HST to a small number of bright stars, is important because the strongest molecular absorption systems of key disk volatile species such as CO, OH, H2O, CO2, and CH4 have their peak absorption cross-sections in the 100 \u2013 170 nm range. Access to wavelengths from 100 \u2013 115 nm is particularly critical as (1) absorption from cool H2 (T < 500 K) is restricted to \u03b5 < 111 nm, (2) the strongest absorption bands of H2O reside between 111 \u2013 113 nm, and (3) the CO-dissociation bands reside at \u03b5 < 108 nm. UV absorption line spectroscopy is the only direct observational technique to characterize co-spatial populations of these molecules with H2, offering direct measurement of the absolute abundance and temperature of inner disk gas without having to rely on molecular conversion factors or geometry-dependent model results (as is required with emission-line spectroscopy).# 3.1.3 Cometary Emissions: 100 \u2013 205 nm\n\nUV spectroscopy has been instrumental in analyzing cometary emissions, especially when detecting molecules with subtle emission features. A pivotal advancement using the Rosetta Alice FUV instrument (70 \u2013 205 nm) on comet 67P/C-G was the simultaneous measurement of both parent molecules like H2O, CO, CO2, and O2 ratios in a comet, from Rosetta, such as the high O2/H2 and their dissociative products (e.g., H, O, and C). Findings were groundbreaking. By assessing both the parent molecules and their subsequent products, we gain a deeper insight into comet composition, the conditions of the solar nebula where comets originated, and the dynamics within the.# 3.2 Cosmic Ecosystems\n\nAstro2020 identified \u201cCosmic Ecosystems\u201d as a key scientific priority for the 2020s. How do galaxies acquire the gas they use to form stars? How do they sustain star formation over billions of years when they appear to contain much less gas than this requires? How does feedback from star formation and active galactic nuclei (AGN) expel gas and metals, and to what extent is this feedback recycled into later star formation? What happens to a galaxy\u2019s gas when it quenches? Is it used up, ejected, or hidden? Inflows and outflows of gas likely shape the evolution of star formation within a galaxy. All of these flows meet in the CGM, a diffuse gaseous medium spanning roughly 30 times the radius and 10,000 times the volume of the visible stellar disk. This complex medium is diffuse, multi-temperature and multiphase medium, and encodes the histories of star formation, galaxy interactions, and other processes that shape galactic evolution. How can we measure this medium? It requires observations of the 100 \u2013 200 nm bandpass.\n\nFUV wavelengths allow access to a host of species that trace gas with a wide range of ionization states and densities. These lines can be used to trace matter in emission or absorption and are ideal for probing the interstellar medium (ISM), CGM, and intergalactic medium (IGM). The wide range of lines, especially at wavelengths <100 nm, provide strong diagnostics to trace the physical, chemical, kinematic, and ionization state of the gas and enable the study of the baryon cycle that supports cosmic ecosystems. Figure 3 shows the abundance of lines and their expected strengths as a function of wavelength adapted from. The density of line tracing gas over four orders of magnitude in temperature (102\u21926 K) makes this wavelength range ideal for probing a range of physical processes (e.g., wind-driven feedback, gas accretion into galaxies, turbulence in the CGM and IGM, etc.) that have remained elusive so far. Spectroscopic capabilities throughout the UV bandpass allow access to a significant range of local redshifts to better understand the evolution of the galaxy-CGM-IGM relationship over time.# 3.2.1 Mapping multiphase gas in the CGM: 100 \u2013 300 nm\n\nFigure 4 shows why UV coverage is essential to understanding the intrinsically \u201cmultiphase\u201d galactic gas flows. The shaded map at lower left shows the distribution of gas in a simulated L* galaxy from the EAGLE project, which spans multiple \u201cphases\u201d from 103 - 106 K and eight orders of magnitude in density. In such ionized gas, the quantum-mechanical rules of electron orbits dictate that the gas will emit and absorb energy predominantly at UV wavelengths, up to 80% according to detailed simulations. Many of the transitions appear as strong UV absorption and emission lines. This inescapable physics means that access to UV wavelengths in space is essential if we are to resolve questions about how galaxies acquire, process, eject, and recycle their gas over the last 10 Gyr of cosmic time.\n\nSolving these problems requires pushing the boundaries of CGM characterization far beyond the limits of today\u2019s measurements, to z = 1 \u2013 2, for two major reasons. First, this period 7 \u2013 10# 6.5 Temperature and Density of Galactic Gas\n\nHigher Temperature\n\n|Temperature (K)|HI|Mg II|CIV|O VI|Ne VIII|\n|---|---|---|---|---|---|\n|0|1216|2800|1550 A|1032 A|775 A|\n\nRedshift: 0.5, 1.5, 2, 3, 4, 6# Fig 3:\n\nThe distribution of resonance lines as a function of the rest-frame wavelength in the ultra-violet regime (figure adapted28). The strength of the line represents the elemental abundance and strength of the transitions. The colors indicate the ionization state of the gas the lines are expected to trace. The line density increases as we go deeper into the UV, demonstrating a wealth of scientific targets there.# Fig 4:\n\nDiffuse gas in and around galaxies requires UV capability for most of cosmic time. The top row shows a simulated galaxy at z = 0.7 from the FOGGIE suite,29 rendered in some key diagnostic ions. The temperature and density regimes probed by these ions are marked in the \u201cphase diagram\u201d of this galaxy\u2019s gas. In upper right, we show how these lines, ranging from Mg X at 68nm to Mg II at 280 nm, vary in observed wavelength with redshift. Even with redshift, most of this diffuse gas is visible only in the UV for the last 10 Gyr of cosmic time. X-ray lines such as O VII and O VIII (both around 2 nm) probe gas at \u21921 million K but not the cooler phases where accretion and recycling occur. The 100 \u2013 120 nm range marked \u201cFar-UV\u201d is critically important to capture O VI (103.2 nm) at z > 0.1 and the EUV ions Ne VIII and Mg X at z > 0.5 rather than z > 1.# 3.2.2 Lyman continuum observations across cosmic time: 100 \u2013 120 nm\n\nQuantifying the physical conditions that allow radiation emitted shortward of the hydrogen ionization edge at 91.18 nm (the Lyman Continuum; LyC) to escape the first collapsed objects and ultimately reionize the universe is a compelling problem for astrophysics. The escape of LyC emission from star-forming galaxies and active galactic nuclei is intimately tied to the emergence and sustenance of the metagalactic ionizing background that pervades the universe to the present day and in turn is tied to the emergence of structure at all epochs. The James Webb Space Telescope (JWST) was built in part to search for the source(s) responsible for reionization, but it cannot\n\nGyr ago encompasses the peak of cosmic star formation. Second, at z > 0.5 we gain UV access to a wide range of extreme UV lines such as Ne VIII, O II to IV, and Mg X (&lt; 80 nm in the rest frame, see Figure 4) that enable a much broader set of diagnostics of physical state and metal content that are only available with redshift. A large wavelength grasp (100 \u2013 300 nm) provides coverage of critically important rest-frame extreme-UV ions that redshift into the FUV for z > 0.5. This includes nearly every ionization state of the most abundant heavy element, oxygen, from O I (cold gas), through O VI (warm ionized gas), which could be covered simultaneously for sightlines at z \u2193 1. The OVI line (103 nm) is a particularly important diagnostic. As can be seen from Figure 4, this line is a probe of environments between hot gas at T > 106 K (probed by Mg X and NE VIII) and cooler gas at T &lt; 105 K probed by other lines. As such this line can elucidate the mechanisms that maintain these temperature differentials, and help us understand the detailed histories of galaxies. Fortunately, very high ionization lines like Ne VIII (77.5 nm), Mg X (61 nm), and Si XII (50 nm) become available in (optically thin) IGM and CGM gas at z > 0.5, reaching a temperature regime (T > 106 K) that is usually thought to be the exclusive domain of X-ray telescopes.\n\nThe same broad wavelength coverage that enables a robust baryon census will also complete a multiphase metals census over a wide range of galaxy masses, star-formation rates, and environments. This is only possible with UV coverage and redshift that places EUV lines of O II, O III, and O IV into FUV. Using this capability, HWO\u2019s users can complete the low-z metals census, constrain the physics of feedback from galaxies, and assess the importance of galactic recycling with robust statistics.\n\nThe combination of large UV wavelength coverage and the potential to include a UV integral field unit (IFU) instrument allows for truly transformational studies of the CGM and IGM. Observational evidence for enormous circumgalactic halos has historically come in the form of absorption lines in the spectra of bright background objects (e.g., quasars) that intersect the halos of galaxies. These studies have defined the current paradigm and taught us a great deal about the physical properties of the CGM, including the average covering fraction of gas and the average ratios of gas phases. But because these studies require bright background objects from which absorption lines can be measured, we have only seen single, narrow sightlines through the halo gas for the vast majority of galaxies. Qualities derived from these studies are averages, not individual views of particular galaxies. A complete gas census can only be developed by eventually moving to volume-filled mapping of the ultra-diffuse gas residing in the CGM and IGM. Since the smallest spatial scales are most easily observed in nearby galaxies, it is advantageous to initiate CGM mapping campaigns in nearby galaxies that target the aforementioned suite of diagnostic lines at \u03b5 &lt; 200 nm.# Fig 5:\n\nLeft - Mean IGM transmission for z = (0.1, 0.3, 0.5, 0.7, 1.0, 2.0, 2.9). Vertical bars mark the Lyman edge and indicate the level of expected variation found in a Monte Carlo study of IGM transmission. Right - same for z = (2, 3, 4, 5, 6). The red line on the right marks the mean transmission at Ly\u03c9.\n\nObserve LyC escape directly because of the progressive increase in the mean transmission of the intergalactic medium towards the epoch of reionization (Figure 5).\n\nThe low-redshift universe (0.1 < z < 1) is the ideal place to study the physics of LyC escape because corrections for the mean transmission of the IGM are modest. In particular, the very lowest redshifts offer the opportunity to examine direct escape from H II regions within nearby galaxies.\n\nDirect quantification of the escape fraction of LyC photons fLyC at low- and intermediate-z require space-based facilities with extreme sensitivity to the EUV restframe, as estimates for L\u2191 150 nm in Figure 6 show. Access to EUV wavelengths down to 100 nm is essential for this science.# 4 Reflective Coatings\n\nAn environmentally robust and reflectively uniform broadband (UV/O/IR) reflective coating has been a goal of coating technology development for many years. The 2020 Astrophysics Decadal Survey states of HWO that \u201cThe mission will also need focal plane instrumentation to acquire images and spectra over the range of 100 nm to 2 \u03bcm\u201d (Page I-2). The coating used to deliver this broadband performance must be resilient to the space environment, as well as compatible with the high-contrast imaging system and other potential science instruments for HWO. Recent advances in deposition technologies have provided new possibilities for coatings that not only provide high reflectively, but improve ease of handling and decrease environmental constraints.\n\nWhen we discuss broadband mirror coatings that include ultraviolet reflectance, two components must be considered - the reflective coating and the protective overcoat. There is also the possibility of applying a second overcoat or \u2018capping\u2019 layer, often a small number of individual layers to improve durability. The primary material provides the underlying reflectance, while the overcoat improves the durability and environmental stability of the coating (but also modifies its reflectance).\n\nPast and current broadband mirror coatings operating in the UV typically utilize reflective aluminum as the base material, as other common coatings like silver and gold become partially transparent in this range. This aluminum layer is capped with a thin dielectric fluoride that is transparent to the UV through NIR but prevents aluminum oxidization, which degrades far-UV reflectance. The choice of the dielectric capping layer sets the bandpass, reflectance, and environmental stability/resilience of the coating. Magnesium fluoride (MgF2; HST, GALEX) and lithium# Fig 6:\n\nLeft - flux estimates for L 150nm \u2191 galaxies with escape fractions of f LyC =(1, 0.57, 0.36, 0.17, 0.04,e 0.003, 0) at redshifts of z = (0.3, 0.5, 0.7, 1.0).34 The recovery of flux towards the shortest wavelengths suggests the possibility of LyC drop-ins.\n\nFluoride (LiF; Far Ultraviolet Spectroscopic Explorer - FUSE) are both flight qualified options in this bandpass. Both of these approaches use physical vapor deposition (PVD) to coat optics. This requires a vacuum chamber to hold the optic while deposition material is heated and then evaporated onto the surface. The optic is usually unheated, but some techniques in development have found improved performance through heating the optic during deposition.\n\nA key technical challenge, historically, for wavelengths above 100 nm, the reflectance of protected Al is limited by the residual absorption and the eventual degradation of the fluoride overcoats. The Al/LiF technology was discovered about 60 years ago, and up until recently, there have been few but significant advances.37 reported the first aluminum mirrors protected with LiF in the 1960s, and a few years later38 were first in optimizing Al/LiF mirrors at the Hydrogen Lyman \u03d1 line (102.6 nm). Then,39 studied the effect of deposition rate and substrate temperature for Al/LiF mirrors. The fact that high substrate temperatures enhanced the mirror reflectivity was further explored several decades later by,40 who demonstrated the highest R at 122 nm wavelength with Al/LiF mirrors fabricated using the 3-step hot process.41 The results of these hot LiF deposition are shown in Figure 7). Optimizations in pre- and post-annealing conditions have been explored, and these enhanced LiF (eLiF) coatings have been demonstrated on several sounding rocket missions.\n\nThere are multiple viable potential coating choices - as can be seen summarized in Table 1. There are a collection of coatings that have been flight qualified, and two newer approaches that have provided promising coatings with better FUV performance that still require investment to be physically scaled up. For example, the estimated TRL levels for Al+MgF2 and Al+LiF are the highest given their HST and FUSE heritages respectively. Likewise, eLiF coatings have flown on.\n\n14# Wavelength (nm)\n\nFig 7: The measured reflectance of enhanced lithium fluoride (eLiF) deposited at elevated substrate temperature versus the performance of conventional LiF developed for the FUSE mission.40\n\n|Reflectance|Wavelength (nm)|\n|---|---|\n|1.0|100|\n|0.0|1000|\n\nFig 8: Comparison of experimental UV/Optical/IR reflectance of representative mirror coatings. \u201cRT\u201d indicates a room temperature deposition process.\n\nThe SISTINE sounding rocket and the SPRITE CubeSat and Aspera Astrophysics Pioneer mission will use an eLiF mirror coating that is capped with an ALD MgF2 layer given the maximum sensitivity this coating provides to rest-frame O VI emission lines (103 nm; Section 3.2). The two emerging technologies (XeLiF and e-beam passivated Al+AlF3) have been shown to be at the TRL 3 and 4 respectively.\n\nFigure 8 displays reflectance data for a variety of coatings across the NUV, optical and near-infrared (200 \u21ad \u03b5 \u21ad 2500 nm) bands. The main feature to note from this figure is that the reflectance performance of all these Al-based mirror coatings is very similar in the UV/O/IR spectral regions. The most notable exceptions are from the bare Al sample (which actually exhibits a slightly higher reflectance in the NUV) and the curve labeled as \u201cAl+eLiF (265 \u00b0C, no cap layer)\u201d that shows a reduction in reflectance of 70% at 200 nm (when compared to an average value of 80% at this wavelength for all the other samples). The main reason for this reduced reflectance value for this sample is due to the fact that the thickness of the LiF layer was optimized to provide maximum performance at the shortest wavelength of interest for SPRITE of 103 nm.42While the simplest description of a fluoride protected aluminum coating is that the fluoride provides a transmissive overcoat to the reflective aluminum, the nuances of that overcoat, especially the thickness, have a great deal of influence on the final coating performance. In general, a thinner overcoat of fluoride shifts the peak reflectance of the coating in the FUV to shorter wavelengths, while also suppressing the amplitude of that peak FUV reflectance. A thicker coating shifts the peak to longer wavelengths while also enhancing the amplitude of that peak. A mission that prioritizes very short wavelengths, such as Ly \u03d1 at 1026 \u02daA, for instance, would therefore have a reflectance at Ly \u03c9 and longer FUV wavelengths lower than an identical coating formula with a thicker fluoride layer. This is most apparent when evaluating the F U SE and SPRITE LiF and protected eLiF coatings (the SPRITE coating curves are included in Figure 11,43). Beyond 2000 \u02daA all LiF+Al curves rapidly approach 90% reflectance. For this paper, it is essential that the reader consider that the comparison of coatings is not entirely straightforward, especially as enhanced performance over a narrow bandpass at short wavelengths is not always apparent in a plot.\n\nWe present two NASA funded on-going efforts to advance ultraviolet reflective coating performance, sometimes used in combination. The first is improving physical layer deposition using either room temperature (eLiF) or elevated temperature (XeLiF) deposition, described in Section 4.1. The second is focused on atomic layer deposition (ALD), described in Section 4.2. We then compare some example coatings.# 4.1 eLiF and XeLiF - Physical Vapor Deposition (PVD) Coatings\n\nEnhanced Lithium Fluoride (eLiF) Conventional deposition techniques for LiF have not achieved predicted theoretical performance, with reflectances that peak between 70-75% across the 100 \u2013 200 nm bandpass before rising to >85% from the NUV through NIR. Over the past decade, the community has developed several enhanced deposition techniques for LiF+Al that have drastically improved the reflectance in the UV and resilience to humidity with no known detrimental impact on the visible and NIR performance.40, 44\u201346 As mentioned previously, eLiF refers to deposition of LiF protecting layer onto a substrate at elevated temperature.# Coating Properties\n\n|Coating Technology|Value|TRL|Largest Optics Coated|Substrate Temperatures Required?|Max Relative Humidity for Coating Stability|Dielectric Layer Process|H-roughness|\n|---|---|---|---|---|---|---|---|\n|Bare Al|>150 nm|> 1 meter|No|~70-100%| | |~0.78 nm|\n|Al+MgFz|>111 nm|> 1 meter|No|~70%|PVD| |~1.84 nm|\n|Al+LiF|>101 nm|~0.5 meter|No|30%|PVD|Fresh 1.5-2.5 nm|Aged >3 nm|\n|AlteLiF+MgFz|>102 nm|~5-6|0.3 meter|Yes|~60 %|eLiF (PVD)|1.5-2.5 nm|\n|Al+XeLiF|>103 nm|~3|Sx5 cm2|No|~60%|Reactive PVD|~1-1.5 nm|\n|Al+AIFz (e-beam)|>105 nm|~4|Sx5 cm?|No|~60%|E-beam Plasma|~0.81 nm|\n\nTable 1: Parameter comparison for various protected-Al FUV coatings.# The most developed of these new deposition techniques is known as protected enhanced lithium fluoride, or \u201cprotected eLiF\u201d.\n\nThis coating combines two recent advancements pioneered under NASA\u2019s APRA, SAT, and Roman Technology Fellowship (RTF) programs: post-coating annealing of the LiF+Al to enhance the reflectance, and then capping with an ultra-thin overcoat of MgF2 to reduce hygroscopicity. The overcoat process was developed at JPL using atomic layer deposition, which enables ultra-thin and highly uniform capping layers that have a minimal impact on the reflectance of the underlying LiF-based coating.# The eLiF process\n\nwas pioneered at NASA-GSFC in the Thin Films Coating Lab (TFCL), first as a high-temperature deposition process where the mirror was heated to &gt;250\u00b0C prior to LiF deposition, but has since evolved to only require heating for post-coating annealing. This change significantly reduces the complexity of the process, while also increasing repeatability. The first usage of eLiF (as well as an early form of protected eLiF) was on the SISTINE sounding rocket, which employed the first-generation version of the coating (deposited onto a heated substrate) on optics as large as 0.5-m in diameter. While several SISTINE optics had excellent reflectance (&gt;80% at 105 nm), the results were mixed, due most likely to non-uniform heating of the larger optics. The SISTINE grating was not coated with eLiF, as it was a replica grating with a photoresist that would not withstand such temperatures. The secondary mirror of SISTINE was capped with a thin layer of AlF3 using the JPL ALD process for added resilience to humidity.\n\nOver the course of three launches, five years of student-led development and testing in the laboratory, as well as over 6 months of over-ocean transit from Australia to the United States in 2022 in a partial pressure of nitrogen, the SISTINE secondary mirror had only minimal degradation from 100 \u2013 120 nm. It has since been determined that an MgF2 capping layer can provide even better protection, although there is a slight loss in reflectance of 2-7% between 100 \u2013 110 nm due to absorption in the MgF2 that should be traded against added risk in AlF3 capping or bare eLiF.# The second generation of protected eLiF\n\nis to be flight tested on the SPRITE CubeSat, which is expected to launch in mid-2024 to low Earth orbit (LEO). SPRITE is designed as a HWO technology testbed and carries a calibration channel that will aid in dissociating any on-orbit degradation of the coatings from losses due to contamination or detector degradation. Three of the SPRITE optics are coated in MgF2 protected eLiF, while the SPRITE primary mirror (PM) is coated in MgF2 protected conventional LiF+Al. The SPRITE PM was not coated in eLiF as the annealing process has not yet been installed in the large 2-m coating chamber at the GSFC TFCL. Testing carried out during the SPRITE coating development effort shows that MgF2 protected eLiF is resilient to humidity exposure, with only minimal degradation after four weeks of exposure to 60% RH.\n\nSPRITE has already demonstrated this resilience, with the SPRITE telescope assembled over two weeks in a modestly controlled environment in Maryland where the ambient humidity varied from 25% to 50% and nitrogen purging was employed only overnight and when the optics were not being handled. Witness sample testing showed no significant degradation in the coating reflectance over this time. The SPRITE grating has also been coated successfully with protected eLiF, as it is a master grating and therefore has no photoresist.# Several other programs\n\nare now baselining eLiF and protected eLiF, including the INFUSE (Integral Field Ultraviolet Spectroscope Experiment) sounding rocket (eLiF; launched October 2023) and the Aspera Pioneer (protected eLiF; launch circa 2025).# 1.0 Mirror Reflectance\n\n| |0.8|0.6|0.4|0.2|0.0|\n|---|---|---|---|---|---|\n| |Pristine eLiF (SPRITE Witness)|MgF2 Protected eLiF (SPRITE Witness)|Protected eLiF Aged 4 Weeks at 60% RH|FUSE LiF+Al| |# Wavelength (\u00c5)\n\n1000 1200 1400 1600\n\nFig 9: The measured reflectance of a SPRITE test sample with just eLiF (dashed), after the addition of the capping layer (solid) and after four weeks of aging at 60% RH and elevated temperature. It is important to note that SPRITE is specifically optimized for reflectance at 1050 \u02daA, which requires a thin LiF layer and therefore lower reflectance at \u03b5 > 1150 A than is possible with other formulations. FUSE (dotted) was also optimized for short wavelength reflectance, which is why its peak reflectance does not match other conventional LiF+Al optics.# XeLiF\n\nThis section will describe a new reactive process that offers protection of Al coatings with a more stable and transparent LiF protection layer, along with unprecedented reflectivity. The process for preparing these Al+LiF mirror (or XeLiF) coatings utilizes exposure to XeF2 gas before and after the Physical Vapor Deposition (PVD) of the LiF metal-fluoride dielectric coating is applied. The technical description of the process is as follows.\n\nFirst, bare optically smooth glass is coated with Al in a high vacuum chamber by PVD. This process, along with the chamber in which the process is carried out, has been extensively described elsewhere where the only difference is that in the present process, the chamber was baked out for several days at 100\u2194105 \u2193 C before the deposition. Then, the bare Al mirror is immediately exposed to a reactive XeF2 gas before and after the application of the final flash evaporated LiF layer by conventional PVD. Therefore, the Al is exposed to XeF2 within a few seconds after the aluminum evaporation, and typically the exposure lasts for up to 3 mins. The quick exposure of bare Al to XeF2 grants a thin ( \u2192 2.5 \u2194 3.2 nm, typical values measured through ellipsometry) protective layer of AlF3. Once the mirror is complete, it is exposed again to a high partial pressure of XeF2 for a few seconds. An important detail is that all are carried out at ambient temperature.\n\nAfter the mirror coating fabrication, optical characterizations are performed with a spectrometer and measurement configurations that have been described elsewhere. The estimated absolute reflectance error is \u00b1 1%. FUV reflectance data were measured immediately after deposition.# 4.2 Atomic Layer Deposition (ALD) Coatings\n\nAdditional improvements in LiF stability with respect to long-duration storage and possible exposure to elevated relative humidity levels have been explored by capping LiF mirror structures with thin layers of more stable fluoride materials like MgF2 or AlF. If the capping layer is thin enough3 the short wavelength performance of LiF can be maintained while sufficiently altering the surface chemistry to avoid deleterious interactions with water vapor. This general approach has been explored in a number of previous studies.\n\n57, 58 reported on samples of eLiF encapsulated with ALD AlF3 that were subjected to long duration elevated humidity exposure where it was observed that the capping layer was able to reduce, but not completely eliminate, reflectance degradation over year-long exposure conditions. The secondary mirror of the SISTINE sounding rocket was coated with this eLiF/AlF3 approach59 and showed \u21ad 5% reflectivity degradation over 4.5 years including.# Fig 10: Reflectivity of two XeLiF samples (ID 09 and 11) both fresh and aged (18 and 12 weeks, respectively).\n\n| |0.8|0.7|0.6|0.5| | | |\n|---|---|---|---|---|---|---|---|\n|Fresh/Aged| |121.6 nm| |Aged after humidity tests| | | |\n| | |Wavelength (nm)|100|120|140|160|180|\n\nThese samples were also characterized with ellipsometry to derive thickness and optical properties (in the 200\u21942500 nm range) of the ALF3 layers as well as Atomic force microscopy (AFM) to study the surface roughness and topography of each sample in two fields of 500 nm \u2197 500 nm and 5 \u03bcm \u2197 5 \u03bcm size. Figure 10 displays FUV reflectance for two examples of Al/LiF mirrors fabricated with the rPVD described above. The LiF thicknesses of samples 09 and 11 are \u2192 22.9 nm and are optimized to provide the higher reflectivity at the Hydrogen Lyman \u03c9 line (121.6 nm), which is one important diagnostic for astronomy and often used as a reference.\n\nAmong the highest reflectance values reported at 122 nm wavelength:41 and52 demonstrated \u2192 0.90 \u2194 0.91 with Al/MgF2,53, 54 and55 demonstrated \u2192 0.90\u21940.91 with Al/AlF3, and40 showed \u2192 0.90 with Al/LiF. Most of these works used high-substrate temperatures;50 are the exceptions in which room-temperature e-beam generated plasma in SF6/Ar gas mixtures was used to passivate bare Al and56 in which the high reflectance was obtained through optimization of the PVD parameters such as a high deposition rate for both Al and MgF2. The results shown in Figure 10 with the rPVD process are unprecedented reflectivity of 0.926 at 122 nm, which is the highest value ever reported at this wavelength for the Al/LiF mirror coatings.# 3.3 Coating Performance\n\nThree rocket integration and launch campaigns and commercial freight shipping across the ocean. Further stability enhancement demonstrations have been made by using ALD capping layers of MgF2 on eLiF mirrors.42 Such coatings have been deposited on the flight optics of the SPRITE CubeSat which anticipates a 2024 launch. SPRITE includes a calibration channel with MgF2/Al optics that will allow for a direct comparison of the on-orbit degradation of both systems. The same eLiF/MgF2 coating is also planned to be implemented on the Aspera Pioneers Mission, with flight optic coatings expected to be completed by the end of 2023.\n\nALD methods can also be utilized to deposit the entire protective layer on aluminum mirror coatings. In addition to the MgF2 and AlF3 processes noted above, ALD processes have also been established for LiF coatings including protected Al mirrors.60 The possibility of using ALD methods for the deposition of the protective coating is largely related to the general ability of ALD for precise film thickness uniformity. For a protected-Al mirror coating, the uniformity of the protective layer largely determines the ultimate reflectance uniformity of the mirror coating. Achieving film thickness uniformity with physical vapor deposition (PVD) methods often requires motion control of the substrate and/or source, or requires physical masking of the vapor species. ALD is a non-line-of-sight technique and is therefore generally agnostic of both the spatial scale of the substrate but also the substrate\u2019s surface shape. This may be relevant for the coating of a many-segmented system with individual segments of varying figures.\n\nCommercial ALD solutions exist for the coating of meter class substrates, although these have largely been focused on flat-panel substrate processing or batch processing.61 Custom systems designed for the coating of telescope optics at the meter-scale and have been demonstrated for protected silver mirror coatings at longer wavelength applications.62 The key challenge associated with the adoption of ALD approaches for future UV mirrors centers around the combination of PVD Al with the ALD fluoride protective coating. There currently exists no promising method for the ALD of Al films with sufficient optical quality in the UV/vis. This restriction has been overcome by investigating dual-use coating chambers where evaporation and ALD can occur without breaking vacuum. It has also been explored via atomic layer etching methods that would allow coating to be performed in two separate chambers, for example by allowing the aluminum to oxidize and then remove the native oxide with a self-limiting chemical etching process prior to the deposition of the protective coating.# 4.3 Coating Performance\n\nThe field of UV coatings has produced numerous solutions for materials and techniques capable of producing high-reflectivity and environmentally robust overcoats. The most common dielectric coating material for protected aluminum is MgF2, as was used on HST, GALEX, and many other UV-sensitive missions. MgF2 is a common overcoat available at many commercial coating vendors. It is resilient to most environmental impacts, including humidity, light touch/contact, and the space environment. It is birefringent, however, with angle-of-incidence dependent differences in the optical indices that could provide an extra factor for consideration in high-contrast imaging instrument design for HWO. Al+MgF2 also has very limited reflectance in the bluest portion of the UV (91.2 nm < \u03b5 < 120 nm), a bandpass highlighted by the 2020 Decadal Survey as an objective for HWO sensitivity (\u00a73.1.1 and \u00a73.2.2).\n\nHowever, very thin layers of MgF2 (and AlF3; 1 \u2013 2 nm) can now be applied as a second overcoat, or capping layer, to impart the protective and environmentally stable benefits of these.# Comparison of Experimental FUV Reflectance of Representative Mirror Coatings# Figure 11\n\n|Coating Material|Wavelength (nm)|\n|---|---|\n|Bare Al (RT)| |\n|Al+Cryolite (RT)| |\n|Al+MgF2 (RT)| |\n|Ti+Al+MgF2 (225 C)| |\n|Al+AlF3 (RT)| |\n|AlteLiF (265 C)| |\n|AlteLiF+MgF2 (265 C)| |\n|Ti+Al+XeLiF (RT)| |\n\n0.0\n\n100 110 120 130 140 150 160 170 180 190 200\n\nThis figure compares a broad range of coating materials and approaches. It is important to keep in mind that these coatings were not made to be compared - they are samples from different projects which each had their own sets of requirements and optimized bandpasses.\n\nFluorides on top of the primary reflectance and protective layers without imposing spectral restrictions on the final throughput. These thin depositions typically applied through an Atomic Layer Deposition (ALD) process, have been demonstrated to add strong environmental resiliency to the final optical surface at a modest cost in UV reflectance. These \"capped overcoats\" have been deployed in the lab and on suborbital missions and will be flown on small orbital astrophysics missions in 2024 and 2025, as described below.\n\nLithium fluoride protected aluminum (LiF+Al) has the highest energy bandgap of any fluoride overcoat, with high reflectance to \u2192102 nm and flight heritage on OAO-3 Copernicus, FUSE, and many sounding rockets.\n\nAs can be seen in Figure 11, when one zooms in on the UV portion of the reflectance curves originally shown in Figure 8, there is a wide variation in reflectivity with material and application technique.\n\nThe curve that exhibits the highest reflectance (> 96% at the Lyman-alpha (Ly\u03c9) wavelength of 121.6 nm is the one labeled as \"Al+Cryolite (RT)\". The cryolite overcoat is a salt that has a conventional PVD process done at ambient or room temperature (RT) that was deposited by a chemical composition of sodium, aluminum, and hexafluoride (Na3AlF6).\n\nThis coat would seem as an attractive choice, given that it provides the highest reflectance at Ly\u03c9 and longer wavelengths. However, it has the lowest band-gap energy (when compared to the other metal-fluoride materials) and this limits its use to wavelengths longer than about 118-120 nm. The cryolite salt is also a hygroscopic material that would have to be protected from degradation even in moderate levels of relative humidity (> 30 \u2194 40%).\n\nThe curve with the second highest reflectance at Ly\u03c9 (\u2193 91%) in Figure 11 is the one labeled as \"Ti+Al+MgF2\". This sample was made on a glass substrate that had a seed layer of titanium (Ti) under the Al coating and the MgF2 deposition was done at an elevated temperature of 225 C. It should be noted that this sample provides better performance when compared to the curve with legend \"Al+MgF2 (RT)\" where the MgF2 was deposited at ambient temperature.# FUV reflectance performance\n\nThis sample consists of Al protected with AlF3, and was done at room temperature by using the e-beam plasma process. This sample exhibits a more balanced reflectance of around 90% (at 121.6 nm), while it is over 80% at wavelengths as short as 110 nm (on account of the larger band-gap energy of AlF3 when compared to cryolite or MgF2).\n\nWhen performance is required at wavelengths as short as 100 nm, the metal-fluoride material of choice is LiF. The two sets of samples with the LiF protection shown in Figure 11 are labeled as \u201cAl+eLiF (265 C)\u201d and \u201cAl+eLiF+MgF2 (265 C)\u201d. In reality, these two curves correspond to the same sample where the dashed blue line is the original sample (after the hot or enhanced LiF deposition done at 265 C), whereas the solid blue curve is the reflectance after the application of a thin MgF2 layer done via the ALD process. This thin ALD layer only reduces the reflectance by 1-2% from the peak value of 80% at 103 nm. A most important consideration is the fact that the ALD overcoat of MgF2 renders the sample much more environmentally stable when compared to the unprotected eLiF one.\n\nAn exciting recent coating possibility is shown in the \u201cTi+Al+XeLiF (RT)\u201d curve. It was produced with the process described in Subsection 4.1. This XeLiF sample is made with the Al and LiF depositions followed by the fluorination of a XeF2 precursor gas. This sample shows a modest reflectance of 70% at 103 nm. However, the reflectance is over 80% for wavelengths longer than 110 nm. Another remarkable feature of this sample is the fact that reflectance has been shown to be tolerant to relative humidity of around 60% for over a 7-days period.# 4.4 Polarization Sensitivity\n\nThe results of the calculations based on ellipsometric measurements of optical constants are shown in Figure 12. The left panel of this figure shows the calculated average phase retardance for bare Al coatings as well as protected with various metal fluoride overcoats (such as AlF3, LiF, or MgF2). These results show an average retardance at the angle of incidence, AOI=12 that increases with decreasing wavelength in the spectral range shown in the figure. It is worth pointing out that there were no requirements for this specified in the LUVOIR/HabEx reports. Moreover, the average calculated coating-induced polarization is 0.11% at AOI=12 in the 200-3000 nm spectral range. This number compares very favorably with the requirement specified in LUVOIR/HabEx reports of < 1%. The coating-induced polarization measured at AOI=12 agrees with calculations, and this will represent a beam speed limit (and related packaging challenge) for the overall design. Overall, the choice of the protective metal-fluoride layer does not add much to the polarization aberration as long as this fluoride layer is not very thick.# 4.5 Coating Development towards HWO\n\nAs has been demonstrated, several coatings have reasonable broadband performance in reflectance in the UV through NIR. To reach TRL 6 for HWO, these coatings will need to be successfully deposited onto a meter-class optic and then measured on the optic itself to demonstrate the required uniformity. This will require scaling the process through a focused development effort, as there are no other missions likely that will have meter-class UV optics on the horizon.\n\n65 https://asd.gsfc.nasa.gov/luvoir/reports/\n\n66 https://www.jpl.nasa.gov/habex/pdf/HabEx-Final-Report-Public-Release-LINKED-0924.pdf# Fig 12: Polarization Sensitivity\n\nRetardance (left) and normalized diattenuation (right) at AOI=12\u2193, which is the maximum acceptance angle stated in LUVOIR final report, of bare Al and Al protected with different fluorides (LiF, AlF3, and MgF2), calculated with experimentally derived optical constants.\n\n|%|Al+22.7 nm AlF3 (Red)|Al+18 nm LiE (green)|L|Al+22.7 nm AlF3 (Red)|\n|---|---|---|---|---|\n|415|Al+24 nm MgF2 (blue)|1|0.15|Bare Al (black)|\n| |Bare Al (black)| |0.05| |\n\ndeal value (no polarization aberration) - Ideal value (no polarization aberration)\n\n|Wavelength (nm)|200|700|1200|1700|2200|2700|\n|---|---|---|---|---|---|---|\n|Wavelength (nm)|200|1200|2200| | | |# Development of a Test Facility\n\nDevelopment of a test facility for mapping the reflectivity of a meter-class optic. These coatings should also then be deployed on feed optics for ground-based high-contrast imaging testbeds to demonstrate compatibility with the coronograph. We propose these paths while we await the work of GOMAP to define the exact requirements needed for the observatory.\n\nAny new deposition method should also require on-orbit testing for resilience in the relevant environment. CubeSats and SmallSats are ideal platforms for such testing; sounding rockets provide the most rapid path to space, but not long-term exposure. The following list presents current and recommended advancement steps for UV coatings in the next 3 \u2013 5 years to advance the technology in support of a late-2020s Phase A start for HWO:\n\n- Scaling, SPRITE = 16 x 18 cm; the next steps for capped Al+eLiF coatings should be half-meter then meter-scale optics\n- Scaling of XeLiF process to 1 meter diameter (via a current SAT grant)\n- Extend current ALD work especially for larger optics (ongoing collaboration between JPL and UCSC for larger chamber (Private Communication; J. Hennessy)\n- Demonstration of the most promising UV/O/IR coating candidate (to be evaluated in 2025-2026) in the space environment\n- Demonstration of repeatable uniformity segment-to-segment\n- Demonstration of appropriate coating uniformity and polarization properties to meet the coronograph driven requirements\n- Demonstration of steps between state of the art and 1m, likely requiring new test facilities to demonstrate these from UV to NIR.\n- Development of the characterization chain for HWO optical segments, including handling/shipping protocols and test facilities.\n\n23# 5 Detectors\n\nDetectors are the heart of any telescope. In addition to being the end of the optical path, the sensor type, size of pixel, sensitivity, count rate limits, and other detector characteristics set many of the parameters for the instrument or telescope: e.g., a fast optical system typically requires small pixels, and vice versa. The limits and capabilities of detectors place strict limits on the science that can be achieved by any mission. UV detector development has lagged behind IR, due in part to a lack of commercial and military funding sources. In the last 15 years, NASA has invested in UV detector technology development via the APRA and SAT programs. As a result of these and other investments, there are several compelling UV detector technologies. Since at least the 2010 Decadal survey, UV astrophysics has worked towards a detector that can achieve the \u201ctriple crown\u201d of sensors: Large format, high quantum efficiency, and low noise/high SNR properties. We also consider power requirements, out of band sensitivity and rejection. For spectroscopy, this often translates to a photon counting device. For imaging, photon counting is typically not needed. Below, we detail the current state of the art for microchannel plates and solid state detectors.# 5.1 Microchannel Plates\n\nMicrochannel Plates (MCP) are widely used as an amplification stage in particle and photon detectors. They are comprised of parallel micron-diameter glass capillaries bundled into a two-dimensional array. Amplification occurs when a \u201cprimary\u201d electron or other ionized particle, after being accelerated in an electric field, strikes the wall of a pore and excites multiple \u201csecondary\u201d electrons within the material. \u201cSecondary electrons\u201d escaping the pore surface are again accelerated in the electric field, eventually striking the wall, and generating a cascade effect as the process continues down the pore. Typical MCP based photodetectors incorporate a photocathode to convert photons to photoelectrons, a stack of 2 or 3 MCPs to achieve gains > 106, and a position sensitive anode.\n\nDetectors based on MCPs have flown on dozens of missions including sub-orbital, shuttle, space station, low Earth orbit satellite, geosynchronous satellite, and planetary missions, with hundreds of successful operational years accumulated. MCP detectors have been implemented for many successful UV astronomy missions and instruments, such as Extreme Ultraviolet Explorer (EUVE), Far Ultraviolet Spectroscopic Explorer (FUSE), Galaxy Evolution Explorer (GALEX), Hubble Space Telescope (HST) Space Telescope Imaging Spectrograph (STIS), and HST\u2019s Cosmic Origins Spectrograph (COS). They are also on the recently launched NASA Ionospheric Connection Explorer (ICON) SMEX mission, Global-scale Observations of the Limb and Disk (GOLD) Mission of Opportunity, and are widely used on NASA UV suborbital sounding rocket and CubeSat investigations. SwRI\u2019s Alice/UVS line of FUV spectrographs have a cumulative 50+ years of failure-free operation, with five instruments in flight and a sixth due to launch in 2024. Much of this success is partially attributable to the robustness and durability of MCP detector construction, their adaptability to a wide range of formats (including curved focal planes), choice of the sensitivity bandpass (utilizing different photocathodes as seen in Figure 16), lack of red end sensitivity, very low background noise and no requirements for cooling. MCP detector technology using imaging readouts, electronics, MCPs, and photocathodes, in open-faced and sealed-tube packages has continued to make significant advancements in the last few years.\n\n7 https://nap.nationalacademies.org/catalog/12951/new-worlds-new-horizons-in-astronomy-and-astrophysics# Improvements in MCP Technology\n\nImprovements in background rate, quantum efficiency, spatial resolution, event handling rates, maximum size formats, thermal and mechanical robustness, lifetime stability, and low power/mass electronic readouts are all in various stages of implementation and infusion.# Traditional MCPs\n\nA traditional MCP is implemented as a wafer of lead glass tubes forming a microcapillary array of semi-conductive tubes with a secondary emissive layer on the glass surface. Conventional manufacturing techniques are limited to formats less than 150 mm. The gain of traditional Pb-glass MCPs degrades with usage, requiring extended \u201cburn-in\u201d periods to stabilize the gain. Pb-glass MCPs also have background rates set by radioactive materials present in the glass (e.g., 40 K) and have approximately 2% detection efficiency for gamma-rays because of the Pb in the glass.# Advancements with ALD\n\nThe advent of atomic layer deposition (ALD) has opened the door to new MCP manufacturing techniques which have shown improvements to MCP performance. ALD is ideal for coating high aspect ratio parts, such as MCPs, and is compatible with materials demonstrating high secondary electron yield. These coatings have been applied to Pb-glass MCPs, for example, the MCPs used in the JUICE-UVS, to achieve better gain and detector lifetime, but the MCPs still retain the mechanical properties and background characteristics of conventional MCPs.# Borosilicate Glass MCPs\n\nNew technology Borosilicate glass MCPs utilize ALD to provide both resistive and emissive surfaces. They have many advantages over traditional Pb glass MCPs, including more mechanical robustness (larger formats of 200mm have been successfully flown), very low intrinsic background (minimal radioactive content), a factor of 3 reduction in gamma-ray detection efficiency (no lead), and significantly improved gain stability. ALD MCPs with large-area formats (12 \u00d7 12 cm2 rates (<0.05 events/cm2/s), extended lifetimes (>4 \u00d7 1020 \u03bcm pores), and very low background with 10 \u03bcm pores, up to 20 \u00d7 20 cm2 with 13 events/cm2) without degradation have been made. Historical mission data shows that the in orbit background (for LEO) is strongly affected by the mass of the satellite converting galactic cosmic rays to local radiation background via Bremsstrahlung.# Background Rates\n\nALD borosilicate MCP technology should also help reduce these contributions for both the pre-launch and in orbit backgrounds by a factor of 2.\n\n|Filled|Open|Total on-orbit background|Pre-launch background|\n|---|---|---|---|\n|HST-COS|8|EUVE|GALEX-FUV|\n|ALEXIS|l 0.6 8|Average pre-launch background for conventional MCP stacks|32 cntsecicm'|\n\nFig 13: Background for conventional MCP detectors in low Earth orbit. The trend suggests an on-orbit background rate increase consistent with the log of the satellite mass. ALD technology should significantly reduce this effect.# Photocathodes\n\nThe conversion of photons to electrons by the photocathode and photoelectron detection by the MCP determine the MCPs\u2019 quantum efficiency (QE).86 The photocathode can be deposited on a window directly in front of the MCP (a proximity-focused semitransparent cathode) or directly onto the MCP (opaque mode).87\u201391 Opaque alkali-halide photocathodes are widely used for EUV/UV sensors, as are semitransparent multi alkali photocathodes for NUV detectors. EUV/UV photocathodes have broadband response with efficiency peaks at wavelengths where photoelectron emission maximizes. These reach to more than 50 percent detective quantum efficiency (DQE) around 100 nm, more than 60 percent at about 500 nm, and over 70 percent around 12 nm depending on the material chosen. NUV DQE is somewhat lower at about 30 percent at 180 nm for bialkali photocathodes. Alkali-halide (CsI, KBr) opaque photocathodes on MCPs obtain high QE (50% at \u2193110 nm) and have broadband sensitivity from 10 nm to 160 nm. We compare photocathode material performance in Figure 16.# Readout Techniques\n\nCross delay line, and more recently cross strip conductive patterned anodes are often the choice for UV MCP imaging detectors. These anode schemes derive photon event centroid positions from the charge distribution that an event produces across a set of strips in an MCP amplified detector. The incoming photon produces a primary electron(s) from the photo-# Figures\n\nFig 14: 20 x 20 cm Microchannel Plate with XDL readout for the DEUCE rocket program.\n\nFig 15: INFUSE (2023) 100 mm XS Detector with 25 \u03bcm resels.# Fig 16: Detective quantum efficiency (DQE) as a function of wavelength for different photocathode materials as shown in the key.\n\n|KBr|Bialkali|CsBr|Csl|GaN|\n|---|---|---|---|---|\n|500|500|500|500|500|\n|1000|1000|1000|1000|1000|\n|1500|1500|1500|1500|1500|\n|2000|2000|2000|2000|2000|\n|2500|2500|2500|2500|2500|\n\nCathode. Each photoelectron is then multiplied within the pores of a microchannel plate pair and the resulting electron cloud is collected on two orthogonal sets of metal strips that form the anode. To obtain an accurate event position the size of the electron cloud is optimized so that the charge impinges on several neighboring strips. The cross delay line derives positions from the difference in pulse arrival times at opposing ends of serpentine delay lines in each axis formed from connecting the strips. The cross strip anode provides signal amplitudes on each strip allowing sub-strip accuracy centroiding to be accomplished. The cross strip scheme achieves high spatial resolutions (sub 20 \u03bcm) at relatively low overall MCP gain (less than 1,000,000). Recently, cross strip anode formats of 100 mm have been implemented in both open face and sealed vacuum tube compatible (UHV/500 deg C) configurations as a significant step towards large area devices suitable for HWO spectroscopy detectors.# Imaging electronics\n\nCandidate MCP detector photon event readout systems include both cross delay line (XDL) and cross strip (XS) anodes. Low power, high performance electronics for cross delay lines have been implemented in a number of forms for existing missions and can be finessed for future application using current state of the art components. However, XS readouts are more appropriate for large area, high performance detectors for HWO. XS electronics implemented with discrete components are working well for recent instruments, but recognizing the need for low mass/power electronics for future applications an effort is underway to produce compact ASIC electronics. Prototypes (GRAPH - Gigasample Recorder of Analog waveforms from a Photodetector) have already demonstrated low power consumption and promise the ability to process events at high rates (more than 10 MHz) with power consumption of less than 7 W for a 50mm detector. GRAPH ASICs are also configured to be used in parallel so that larger format XS detectors are addressable with adapted firmware.\n\n27# 5.2 Solid State Detectors\n\nSolid-state detectors offer significant advantages in size, mass, and manufacturability compared to the vacuum tube based technology of MCPs. Using solid-state detector arrays improves instrument compactness and reduces instrument complexity.\n\nThere are intrinsic advantages to the fabrication of solid-state UV detectors in wide bandgap material such as gallium nitride and its alloys. Their bandgaps, ranging from 4.6 eV (GaN) to 6.2 eV (AlN), offer a broad range of out of band rejection thresholds, while their direct bandgap for the entire range of alloys enables strong absorption of UV photons, and their wide bandgap could enable higher operating temperatures. There are, however, practical material challenges to fabricating high quality detectors in III-N materials. The lack of native substrate in the III-N family creates challenges for producing high quality crystalline material and doping of these semiconductors\u2014the first step of creating a semiconductor junction and detectors\u2014remains challenging.\n\nSilicon carbide is another wide bandgap (3.26 eV) material with direct bandgap and intrinsic properties for visible rejection and higher operating temperatures. While SiC is not as tailorable as III-N material for the cutoff wavelength, it can be grown on silicon wafers, a clear advantage for manufacturability.\n\nThere is enormous past and continuing investment in silicon imaging devices which can be leveraged for scientific UV imaging. Silicon imaging and detector arrays, especially charge coupled devices (CCDs) and complementary metal oxide semiconductor (CMOS) arrays, are ubiquitous in different fields of imaging applications. This is due in part to the steady advancement of silicon VLSI (very large-scale integration) technology and the consumer market for imaging. Shortly after the invention of CCDs in 1969 at Bell Labs, extensive programs were established at JPL to advance early CCDs for imaging systems aboard NASA\u2019s Flagship mission Galileo and its first Great Observatory, HST. For a more in-depth discussion of the history of CCDs and their use in space-based applications, we refer the reader to and.\n\nCMOS-based imaging started around the same time in the 1960s, but it was not until the 1990s that their development began in earnest, due in part to fabrication advancement and to focused effort of CMOS-Active Pixel Sensor development at JPL and other CMOS images elsewhere. CMOS imaging has had an enormous impact on the consumer field and in recent years has become viable for scientific applications.\n\nIn the UV, the short photon absorption distance coupled with the formation of carrier traps (electrons in n-channel and holes in p-channel devices) in the Si-SiO2 result in negligible QE below 400 nm in the front illuminated silicon devices or even unpassivated back illuminated silicon devices.\n\nThe naturally-occurring traps due to positive charge trapping in the Si-SiO2 interface, can be countered by the addition of charges of opposite polarity, e.g., a thin crystalline layer of silicon with embedded high boron density in a single atomic sheet. This delta-doping process results in silicon detectors that have reflection-limited response, i.e., near 100 % internal QE from very soft x rays through near infrared, \u2193 1 \u03bcm, at which point silicon becomes transparent to photons. Because delta doping is a back surface process, it is agnostic to the readout structure and silicon detector architecture and essentially any silicon detector can become UV sensitive, allowing the mission to benefit from the very large commercial pool and latest designs in silicon detectors.\n\nWith photo-electrons efficiently collected in silicon detectors through delta doping, the QE can be further tailored by the addition of anti-reflection coatings or filters for out of band rejection.# (AR-coated)\n\n|Wavelength (nm)|QE (%)|\n|---|---|\n|200|70|\n|300|60|\n|400|50|\n|500|40|\n|600|30|\n|700|20|\n|0|10|# Front-illuminated\n\n|Wavelength (nm)|QE (%)|\n|---|---|\n|200|70|\n|300|60|\n|400|50|\n|500|40|\n|600|30|\n|700|20|\n|0|10|\n\nFig 17: Delta-doping involves a doping layer to a back-illuminated silicon detector, which eliminates an electron well in the detector surface. The result is silicon detectors sensitive in the UV. The QE can be further increased by the addition of anti-reflection coatings. Left: solid lines are models, markers are measurements, showing early results of delta-doped and AR Coated devices. Right: single layer AR Coatings using atomic layer deposition (ALD) and thermal deposition (MgF2 only) are applied for higher throughput in the NUV and FUV.# Layer MDF\n\n|Material|Type|\n|---|---|\n|AlO3|aluminum|\n|Al2O3|aluminum|\n|AlO3| |# silicon APD\n\nFig 18: Left: Schematic of the implementation of a metal dielectric filter (MDF). Left center: Linear scale QE of two example MDFs optimized for 225nm. Right Center: Alternative MDF example, with peak performance at 190 nm. Right: MDF optimized for FUV Bandpass, showing both reflectance from detector with integrated filter and transmission into detector.\n\nAs large as 50% to 60% are possible, with nearly 3-4 orders of magnitude out of band rejection with bandpasses whose location can be arbitrarily chosen. Further tailorability can be achieved by detector-integrated filters. Three to four orders of magnitude rejection has been obtained by incorporating metal dielectric filters directly on delta doped detectors. The thick aluminum layer provides blocking of the visible light while allowing UV photons to reach the detector surface. Depending on the spectral range of interest, the dielectric material of choice would be an oxide or fluorides. Adding each metal-dielectric bilayer will provide additional out of band rejection but because the metal layer is not entirely transparent, there will be tradeoff between the out of band rejection and in band peak quantum efficiency.# Fig 19: A range of Delta doped and AR coated devices spanning various formats (1kx1k, 2kx1k, 2kx2k, 2kx4k, etc) and architecture (EMMCD, CCD, Full depletion CCD, etc) and polarity (n-channel and p-channel CCDs) from JPL which have been used for suborbital and ground-based missions.\n\nBecause the processes developed are independent of the frontside architecture and format, the end-to-end post fabrication processing including delta doping and special coatings.# Architecture variants\n\nSolid-state Silicon devices all rely on similar architectures- a pixel for photo-electron collections, either directly connected to a readout amplifier in the case of CMOS devices or coupled to neighboring pixels in the case of CCDs. For a CCD, at least one and up to tens of readout amplifiers are used for reading out the charge in all pixels in a single device. The CCD architecture has been the dominant detector of choice for visible wavelength and ground based astronomy. Due to their serial readout scheme, CCDs have longer readout times than CMOS devices but have historically had lower noise in the amplifier, and fewer amplifiers per device make image processing more straightforward. In recent years, CMOS devices have advanced such that the read noise in each amplifier is low enough for scientific use and variations between pixels are no longer a significant disadvantage. Depending on how they are designed and operated, CMOS devices can have low fill factors (area of a pixel that is photo-sensitive) and are sometimes matched with microlenslet arrays to improve throughput. In recent years, low noise and back illuminated scientific CMOS imaging arrays have been developed. The recently selected Explorer UVEX is planning the use of 4kx4k delta doped and low noise CMOS arrays. The FUV version of the UVEX detectors plan to use detector-integrated filters.# Solid state photon counting detectors\n\nWhen using a normal silicon CCD, it is impossible to tell the exact number of photo-generated electrons in a pixel because the added detector noise is usually too large; with conventional CCD read noise of a few electrons, a single sample will be unable to distinguish between individual electrons in a well. Several different technologies enable photon counting in a solid state detector, which we describe briefly below.\n\nA variant of CCD is the Electron Multiplying CCD (EMCCD), which implements an additional set of pixels between the normal CCD serial register and the readout amplifier. They consist of a normal CCD image area with an additional serial register added after the normal serial register. This addition to the serial register of an EMCCD contains serial pixels that replace one clock of the pixel with a DC level pixel and a high voltage (HV) clock. These multiplication pixels have a high voltage applied to them, creating deep wells for electrons to impact and ionize as they enter the pixel. This impact ionization generates more electrons, yielding a significant gain over the course of several hundred multiplication pixels. This allows for photon counting with a CCD since the multiplication gain will increase the number of electrons in a pixel to well above the read noise. EMCCDs need to be operated carefully, however, to limit only a single photon event per pixel.# 5 Layer\n\n| |175|195|215|235|\n|---|---|---|---|---|\n|Wavelength (nm)| | | | |\n\nFig 20: Delta doped and AR coated 1kx2K EMCCD prepared for FIREBall-2. LEFT: quantum efficiency of a three-layer AR coated device (red) and five layer AR coated device (blue). RIGHT: photographs of delta-doped 2KX1K EMCCDS. Different colored detectors have different AR coatings applied.\n\nhave other noise sources that are added by the multiplication process. The advantage of the EM gain process is that it increases the signal from a single photo-electron to a value much larger than the on-chip amplifier read-noise. This process means single events can be detected by a threshold process. When operated in photon counting mode, pixels with counts greater than 5 times the read-noise are considered to have had 1 event. Pixels with counts less than this threshold are considered to have zero events. Some complications come along with this process. Not all electrons in the device will be amplified above the 5 sigma threshold. The multiplication process is stochastic and depends on the gain applied. Additionally, noise comes from clock induced charge (CIC) and dark current. For an EMCCD, both of these noise sources will be amplified in the same manner as photo-electrons and need to be addressed in analysis.\n\nEMCCDs are currently used on several missions at a range of mission sizes: FIREBall-2 (a balloon borne UV multi-object spectrograph), the Roman Space Telescope, SHIMCO (an astrophysics UV Rocket). EMCCDs are also baselined for several missions in planning stages: a New Frontier\u2019s SILENUS, PRISM (a lunar mission), a lunar DALI (Development and Advancement of Lunar Instrumentation) instrument, and was the baseline detector for HabEx.\n\nOne of the emerging technologies that may provide promise for astrophysical photon counting is the skipper CCD architecture. A skipper CCD uses a non-destructive read-out amplifier and is able to sample a single pixel multiple times. This effectively reduces the read noise by several orders of magnitude depending on the number of samples. The reduction in noise is equal to the square root of the number of samples. This type of device had been described previously but has finally been made workable. As an example, using a 3.5e- read noise amplifier and 4000 samples per pixel, the effective read noise is reduced to 0.068, well below the typical value of 0.16 usually required to clearly distinguish between numbers of electrons. Thus a skipper CCD can distinguish between 0 and 1 electrons in a pixel, which is the typical expectation for photon counting, and matches what an EMCCD or MCP can do. But it can also distinguish between 1 and 2, 2 and 3, 3 and 4, and so on up to the saturation limit of the well (see Figure 21).# 6 Gratings\n\nUV spectroscopy has historically suffered from low-performance hardware compared to other wavebands, in part due to the inherent challenges in material properties at UV wavelengths (absorption in common VIS/IR substrates, degradation over time, etc). In particular, UV high-resolution spectroscopy is limited by low efficiency gratings (typical efficiencies of <40%; e.g.,105, 106) and scattered diffracted light that severely increases background levels (e.g.,106\u2013108). The biggest limitation to building high-sensitivity, high-resolution UV spectrographs is the low performance of UV blazed gratings. Advances must be made to achieve the high-efficiency blazed gratings that will be necessary to enable a high-resolution (R>30,000) UV spectrograph on HWO.\n\nHistorically, UV gratings used in astronomical instruments have been ruled either mechanically or holographically. Mechanically ruled gratings are produced using a diamond stylus to cut and shape grooves, while holographic gratings are produced using interference lithography, resulting in a sinusoidal groove profile at the specified density. Mechanically ruled gratings are capable of good diffraction efficiency as a result of their sharp, blazed facets, but typically suffer from groove period errors that induce scatter and ghosting. Conversely, holographic gratings are capable of precise groove placement, which minimizes stray light, but generally lack the diffraction efficiency achievable with a mechanical ruling process. While it is possible to improve upon holographic grating efficiency by directly recording a blazed facet in the photoresist using sophisticated holographic recording techniques, these techniques are resource intensive. In addition, for both ruling techniques, fabrication processes limit the achievable groove spacing; mechanical ruling is not# Fig 21\n\nHistogram obtained in the visible showing the photon counting and photon number resolving capability of Skipper CCD. Each peak represents an integer number of electrons in a bin. The main figure shows the clear delineation between pixels with zero electrons and pixels with one electron. The inset figure shows that this clear separation is present at higher electron counts, 775 to 779 electrons per pixel. Figure from104\n\nmeans a skipper can perform photon counting and is not limited by the typical rate requirements of an EMCCD (which can only distinguish between 0 and 1 electrons in a pixel, if the frame rate is correct for the observation) or an MCP. An example of the ability to detect faint sources is shown in Figure 22, where various detector types are contrasted for an observation of faint emission from the CGM of a nearby galaxy.# Fig 22\n\nSimulated CGM around a Milky Way type galaxy shows faint, extended, filamentary structure. The panels show mock observations and the resulting SNR distribution (random-noise effect included) from an IFU with the simulated performance of a:\n\n- (Top) Skipper CCD\n- (Middle) EMCCD\n- (Bottom) MCP\n\nThe white rectangle shows that the Skipper CCD can detect a very faint diffuse gas signal with greater fidelity compared to an EMCCD or MCP for otherwise the same conditions.\n\n33# 6.1 UV gratings with electron-beam lithography\n\nIn response to NASA\u2019s Astrophysical Strategic Technology Priorities and Gaps program, which recognized the need to improve all aspects of UV hardware, enormous progress has been made on the crucial task of improving UV gratings. Electron-beam lithography (EBL), enabled by semiconductor nanotechnology, offers a flexible method for patterning customized, minute features over large areas and, is therefore well-suited to patterning UV gratings. EBL achieves isolated feature sizes of \u219210 nm, an order of magnitude smaller than required for most UV gratings.\n\nRecent development in EBL gratings for UV applications has leveraged years of progress in X-ray grating fabrication, where EBL and complementary nanofabrication techniques are used to manufacture small-period gratings capable of high spectral resolution and high diffraction efficiency. These nanofabrication processes allow for customizable groove layouts, densities, and facet angles, all of which are defined prior to fabrication. The desired groove pattern is then exposed with an EBL tool in an electron-beam-sensitive resist, which coats the grating substrate. The pattern is then further developed and etched to embed the desired groove pattern into the substrate itself. The etches that transfer the pattern into the substrate retain the groove layout exposed with the EBL tool such that the overall groove placement is largely determined by the quality of the EBL exposure.# Current state of the art in EBL gratings\n\nThe state of the art in UV gratings fabricated via EBL relies on precise groove placement on the nm scale from advanced EBL tooling, pattern transfers into the grating substrate via a series of dry etches, and blazing the grooves to a custom facet angle with potassium hydroxide (KOH) etching. An EBL grating blazed with KOH etching was fabricated for the ESCAPE small-explorer mission concept. The ESCAPE prototype was coated with Au and achieved >75% peak absolute diffraction efficiency. Further compared the performance of a prototype, blazed EBL grating to a previously flown mechanically ruled grating, finding that the EBL grating achieved a 50% increase in absolute diffraction efficiency and a factor of \u21925 improvement in scatter. Showed that a 50 mm \u2197 50 mm grating with a 1-\u03bcm period written with EBL is expected to reach a spectral resolution R \u2192 35,000, matching grating performance onboard HST/COS when taking into account the difference in groove density.\n\nUV gratings have also been fabricated at a range of groove densities and patterns, ranging from \u219210-\u03bcm down to \u2192110-nm groove spacing and from parallel to aberration-correcting groove profiles.\n\nThere have also been substantial advances in patterning large areas with University-grade EBL tools. For example, the fabrication of a 100-mm \u2197 107-mm (>100 cm2) X-ray grating patterned using EBL in just a few days has been reported, and UV gratings as large as 36 cm2 have been manufactured and tested. EBL tools are automated, requiring no human input to complete a patterning run once initiated. EBL tools maintain focus over long duration runs by checking alignment patterns outside of the pattern boundaries, so are robust against changes in environmental.# Ongoing EBL Development\n\nWith promising early results on UV echelles, flat substrates, and a range of groove densities and facet angles, ongoing development is focused on maximizing customizability, one of the primary advantages of EBL processing. Active focus areas for curved gratings include scaling current EBL techniques to larger areas and the placement of curved grooves on non-planar substrates, both of which are critical for a broad range of aberration correction capabilities. Additional development efforts currently funded under NASA APRA and SAT programs include strategies for consistent, repeatable groove placement over large write areas, characterizing achievable resolving power and diffraction efficiency over a broad range of groove profiles, and maturing additional techniques to produce blazed facets as alternatives to KOH etching.\n\n110 showed that, while KOH etching yields atomically smooth groove facets capable of high diffraction efficiency, etch fidelity begins to deteriorate for groove directions that deviate \u21ab2\u2193 from a nominal layout direction. This degradation is due to the etch\u2019s reliance on the crystal structure of the silicon substrate; the KOH etches to a specific family of crystal planes, the {111} planes, and sets of {111} planes are parallel to each other. As the relative groove curvature exceeds \u21922 \u2193, the etch conditions evolve such that the quality of the groove size and surface degrades. For applications that require aberration-correcting profiles with a large amount of curvature and where diffraction efficiency is paramount, alternative blaze techniques may be capable of improved performance over the existing state of the art with KOH etching.# Alternative Techniques for Blazed UV Gratings\n\nTwo alternative techniques are being developed to produce blazed UV gratings capable of high diffraction efficiency and low scatter. In each case, the techniques rely on EBL to expose the overall groove layout and therefore retain the resolving power capabilities possible with EBL gratings. Thermally activated selective topography equilibration (TASTE) has been in development for EUV and X-ray grating applications and uses \u201cgreyscale\u201d EBL and thermally activated EBL resist to sculpt blazed facets.120 Greyscale EBL uses a modulated electron dose across individual grating groves to generate a variable-height groove profile, which is then thermally reflown to create a continuous, blazed facet. TASTE has been demonstrated on gratings with densities as high as 2500 grooves/mm, with early results showing EUV diffraction efficiency of \u219260%.120 At higher groove densities, however, greyscale EBL becomes more challenging; further development and process optimization is needed for such applications.\n\nGratings blazed using ion-beam etching (IBE) were recently developed for X-ray applications and the processes are transferable to UV gratings.121 IBE, which has also been used to blaze holographically ruled gratings (e.g.122), uses EBL to expose the desired groove layout, a series of.# 7 Spectral Multiplexing Technologies\n\nSpectral multiplexing offers the simultaneous acquisition of spectroscopic information from multiple objects or regions within a telescope\u2019s field of view (FOV) within a single pointing. Ground-based spectral multiplexing techniques rely on fiber optics, multilenslet arrays (MLAs), or reflective image slicers (KCWI, MUSE). These devices are used to dissect a target region into non-overlapping spatial regions to obtain large numbers of spectra ideally at the resolution limit of the telescope and spectrograph. MOS technologies used in ground-based instruments deploy individual fiber optics to target locations, historically by hand plugging plates, and most recently by implementing robotic positioners (i.e. MOONS, SDSS-V, DESI). IFUs will often use close packed fiber optic bundles along with a lenslet array to dissect a contiguous target field. Fibers have not historically had high transmission in the ultraviolet, although sub-orbital testing was done during the first Faint Intergalactic Redshifted Emission Balloon flights and development is ongoing. MLAs for astronomical applications are made of fused silica, but are limited in transmission by anti-reflective coatings of limited bandpass. An FUV reflective image slicer has flown on the Integral Field Ultraviolet Spectrograph (INFUSE), a sounding rocket-borne instrument and the first FUV IFU to fly with access to wavelengths below 1150 \u02daA.\n\nHere we review micro-electro mechanical (MEMs) technologies that employ either arrays of clear aperture micro-slits, reflective micro-mirror arrays, or reflective \u201cslit-farms\u201d to partition the field prior to spectral acquisition as well as reflective image slicers, as potential effective flight qualified multiplexing approaches that have some space-based heritage and potential in the ultraviolet.# 7.1 Microshutter Arrays\n\nMicroshutter arrays (MSA) are programmable multi-aperture micro-slit devices that were developed by GSFC for use on the JWST Near Infrared Spectrograph (NIRSpec). MSAs allow for the simultaneous acquisition of spectra from multiple objects in the FOV, enabling spectroscopic analysis on an industrial scale equivalent to that long enjoyed by multi-object fiber optic fed spectrographs in ground-based applications. Their use on JWST is ushering a revolution in space-based spectroscopic analysis. These arrays can be programmed to provide any pattern of slits.# MSA Slitlet Configuration\n\ncorresponding to sparsely distributed sources on the sky. It can also be programmed to provide shaped slits on extended sources. A NIRSpec MOS example is shown in Figure 23.\n\n|MSA Slitlet|MSA Quadrant 3|MSA Quadrant|\n|---|---|---|\n|Configuration Spectra|Failed Open Single Shutters|(Fixed Slits Always Open)|\n|Zero Order Images of Right quadrant slitlets|Associated Spectra|Detector NRST|\n| |MSA Quadrant|MSA Quadrant|\n| |Detector NRS2| |\n\nFig 23: Example of NIRSpec MOS spectra acquired under illumination of a test slit pattern with a calibration lamp.\n\nThe 1st generation MSAs flying on NIRSpec were optimized to provide high contrast in the cryogenic environment required for IR observation. The operation involved a combination of electrostatic and scanning magnet actuation that requires a heavy complex mechanical assembly. GSFC has recently developed Next Generation MSAs (NGMSAs) that retain much of the original.# MSA Egg-Crate Detail\n\n|FEn|FEn+1|FEn|\n|---|---|---|\n|SiO2|SiO2|SiN43|\n|GE|GE|GE|\n|GE|AlO32|AlO3|\n|GE| |2|\n|HumlinHar|BEn+2| |\n| |BEn+1| |\n| |BEn|BEn+1|\n\nFig 24: Left \u2013 MSA egg-crate detail. Middle \u2013 Shutter blades viewed from light-shield side. Right \u2013 Shutter unit cell with material layers (light-shield not shown). The egg-crate frame of Si is held at ground (GE) and coated with an insulating layer of ALD grown Al2O3. It is isolated from the Si3N4 shutter that is coated with Front Electrode (FE). A Back Electrode is deposited at an angle over the wall of the Al2O3. A potential difference between FE and BE attracts the shutter to the wall to latch it in the open position. Removal of the potential difference causes the shutter to relax to the closed position.\n\n37# MSA Format Development Trend for Cosmic Origins\n\nachievable with micromirrors (Kutyrev et al., 2008; Travinsky et al., 2017). The ease in which NGMSAs can be integrated into a wide range of instrument optics designs ensures broad science community return on investment in this technology. The wide range of Decadal Survey mission concept studies that incorporated the MSA (The Large UV/Optical/IR Surveyor (LUVOIR), The FORTIS, JWST).# Figure 1.2\n\nMSA format development trend for Cosmic Origins is shown. Our pilot (FORTIS) and JWST formats are shown on the left. A concept and manufacturing development array produced by GSFC fiscal year FY18 Strategic Astrophysics Technology (SAT) project is shown in the center. Our FY18 SAT project also produced a ruggedized design of an array mounted on a ceramic carrier with supporting grid that is suitable for spaceflight is shown on the right.\n\n|Format|Dimensions|Development|\n|---|---|---|\n|FORTIS|128 x 64|Manufacturing|\n|JWST|365 x 171|Development (840 x 420)|\n|Qualified|736 x 384|FY18 SAT|\n\nThe design shown on the right has been demonstrated through tests to general environmental verification standard (GEVS) vibration levels for evolved expendable launch vehicle (EELV) launch. We propose this format for the Decadal Survey IROUV strategic mission application going forward.# Figure 26\n\nLeft - FORTIS 128x64 NGMSA on PCB carrier. Right - NGMAS assembly installed at the FORTIS prime focus.\n\nArchitecture but have eliminated the need for a scanning magnet. This advance was enabled by the combination of thinner shutters and improvements in electrical isolation between shutters and ground. Together these improvements permit purely electrostatic actuation, greatly decreasing the time to open/close or address selected shutters from several seconds to a fraction of a second.\n\nShutters are etched into a silicon nitride (Si3 N4) layer, forming an array with a pitch of 200 \u03bcm. Each shutter is connected to the array by a torsion hinge that is suspended above a 100 \u03bcm thick support grid called the \u201cegg-crate\u201d (Figure 24 \u2013 Left). Complete 90 deflection and latching open of the shutter is provided by a potential difference between electrodes deposited on.\n\n38# 7.2 Digital Micromirror Devices\n\nMultiobject and integral field spectroscopy can also be achieved using a digital micromirror device (DMD) - an array of many small mirrors (\u2192 10\u03bcm on a side) (Figure 27), developed by Texas Instruments and currently widely used in video projection and 3D printing applications. Each mirror of the DMD can be tilted into one of two orientations, usually \u00b112 \u2193 with respect to the device plane\u2019s normal orientation. In this way, light incident on the DMD can be directed into one of two directions, or \u201cchannels\u201d. In the most straightforward implementation, a DMD can be used to create hundreds of \u201cslitlets\u201d in the field of view and direct light from those locations to a spectrograph, while blocking the rest of the FoV. DMDs serve a similar function to MSAs in a spectrograph, but with a key fundamental difference: DMDs are reflective devices, whereas MSAs are transmissive. This allows DMD and MSA-based spectrographs unique auxiliary capabilities. For example, in a typical MOS observation, the slitlets only cover \u2192 1% of the field of view (by area). A DMD MOS can be designed with a parallel channel, to utilize the light not sent to the primary spectrograph (to do deep imaging, slitless spectroscopy, etc). However, a DMD-based spectrograph must contend with the reflectivity and scatter of the DMD mirrors, while MSA shutters are completely transmissive at all wavelengths.# Spring Tip\n\nFig 27: Left: a 1024\u2197768 mirror DMD with a custom NUV-transmissive window. Middle: a close-up of several micromirrors, each 13\u03bcm on a side, with a 92% fill-factor. Right: A schematic of the micromirror hinge and tilt mechanism; note DMDs tilt about their diagonal.\n\nSeveral spectrographs based on DMDs have been built for ground-based telescopes, including: RITMOS, 141 IRMOS, 142 BATMAN, 143 and SAMOS. 144 To demonstrate the suitability of DMDs for deployment in space, a NASA SAT program was developed to perform flight qualification activities and to investigate the prospects for window replacement. Overall, it was determined that DMDs are not particularly sensitive to shock and vibration loads145, 146 or the radiation environment, except in the most extreme conditions.147\u2013151\n\nAlthough commercially available DMDs are designed for use in the visible regime, their throughput can be extended into the NUV regime, to 170 nm by replacing the stock BK7 window with a suitable material (such as fused silica, sapphire, or MgF2). For use in the shorter wavelengths, it may be possible to re-coat DMDs with the kinds of coatings that have been developed for monolithic optics in the FUV (see Section 4). However, because only commercially available DMDs are sufficiently mature for use in space-based instruments, there are size limitations. In general, larger missions allow for telescopes with more fine spatial resolution and effective area; however, the fixed size of the DMD results in a decreasing field of view, with increased focal length. To enable a larger field of view, multiple groups are exploring the possibility of mosaicing DMDs.# 7.3 Integral Field Units\n\nIFUs powered by image slicers provide a way to obtain imaging spectroscopy in the FUV while avoiding the low transmission of fiber bundles. These systems consist of a slicer cube and a pupil array. The slicer cube is a stack of thin mirrors rotated at slightly different angles to reflect light onto a pupil array. Traditional image slicers use a pupil array of mirrors to realign the beams from each slicer element so that the image, when refocused, maintains a specific shape, generally an array of \u201clong-slit\u201d fields of view (defined by the image slicer) arranged end-to-end to avoid spectral confusion. The system is then diffracted by a grating and re-imaged by a set of camera optics. An image slicer first flew on the JWST Mid-Infrared Instrument (MIRI).152 The first FUV IFU, an image slicer, flew on NASA sounding rocket flight 36.375 UG Fleming aboard INFUSE. The INFUSE slicer cube, provided by Canon, Inc., consisted of 26 slices micromachined out of invar and copper to 7.1 \u02daA RMS surface roughness and a \u03b5/20 flat figure.153 It was coated in conventional Al+LiF through a partnership with GSFC.130 There are currently plans to coat the flight spare in Al+eLiF.# olwnich pioducesa\n\nFig 28: Left: INFUSE image slicer concept. Cygnus Loop image credit:.154 raytrace with pictures of the slicer cube, grating array, and MCP detector inset.# Dispersion Directions (opposi0 for toplbottom pupE gralings# Right: INFUSE\n\nA traditional image slicer system is too inefficient for the FUV. In order to improve throughput, INFUSE replaced the pupil mirrors with aberration correcting, Type IV holographic gratings from Horiba JY (Figure 28). These gratings are identical replicas, blazed to 1300 \u02daA. Twenty-five are coated in conventional Al+LiF as the photoresist used to create replica gratings cannot tolerate the heat of a hot coating deposition. All of the gratings are arranged in two sets. Thirteen of the gratings are arranged so that the slicers are imaged end-to-end, with the dispersion going from the top of the large format MCP detector toward the bottom while the other thirteen are arranged with the dispersion going from the bottom to the top. These spectra were then reassembled into a 3D data cube after flight. This permitted moderate resolution integral field spectroscopy over a 2.48\u2019 \u2197 2.57\u2019 FOV.# 8 Contamination Reduction\n\nThe UV is particularly sensitive to the presence of contaminants because of amplified scatter and the preferential absorption of this bandpass by molecular hydrocarbons. As such, strict practices of contamination control will need to be implemented to maintain an acceptable level of throughput through all critical surfaces of the observatory to ensure performance metrics are met at launch. Once attained, this performance can be maintained through the lifetime of the mission by measures such as baking, choice of low-outgassing materials, and vent path designs. This section will address the approaches that need to be taken to provide access to the far ultraviolet for HWO.# 8.1 Contamination Budgets\n\nThe objectives of contamination control for space-borne optical instruments are a) to determine the maximum allowable contaminant quantities, or budgets, for the spacecraft or science instrument, and b) to select and implement the appropriate controls to prevent unacceptable contaminant-induced degradation.\n\nA crucial early step in contamination control for highly contamination-sensitive systems is to establish the degradation allowables. In this step, some percentage of the performance margin of the system is reserved for contaminant-induced degradation. By analysis of degradation vs. contaminant quantity, ie., obscuration, absorption, and scatter, these degradation allowables form the basis to determine the maximum allowable lifetime contaminant quantities, called the contaminant budgets, for both particulates and molecular contaminants. Having accurate absorption coefficient.\n\n41# 8.2 Contaminants and their Effects\n\nThe contaminants that plague spacecraft and space science instruments are categorized as either particulate contaminants or molecular contaminants. Particulate contaminants are almost always microscopic and are relatively easy to describe. They may range from sub-micron size dust and smoke particles to visible garment fibers. Molecular contaminants cover the range from single atoms to complex molecules of several hundred atomic mass units (amu). Those particles which settle onto surfaces, and those molecular species which are adsorbed onto surfaces, and which change the properties of those surfaces, are the contaminants that degrade optical systems as well as other spacecraft surfaces and that require controls.\n\nSettled particles obscure portions of optical beams and scatter light out of these beams as stray light (Figure 29, left panel), while molecular deposits absorb as well as scatter light (Figure 29, right panel). Depending on the optical wavelengths and the contaminant quantities, a combination of particles and molecular species can reduce the optical efficiency of each optical surface by a few hundredths of a percent to several percent, the latter being unacceptable for optical systems such as the HWO ultraviolet instruments.# 8.3 Contamination Controls\n\nThe primary factors of contamination control include initial and precision cleaning of physical components, cleanroom quality, personnel garmenting and operations, tool and work surface cleanliness, selection of low outgassing materials, cleanliness monitoring, maintenance cleaning, and specific controls during environmental testing. The contamination budgets derived for the HWO and the science instruments will lead to the specific contamination controls necessary for the observatory. Modern cleanrooms are quite capable of maintaining classes ISO 5 and ISO 68 airborne particle Cleanliness Levels, and even ISO 4 for periods when optical components are exposed even for lengthy time periods. The use of airborne particle counters and the collection of settled particles.\n\n8 ISO 14644-1:2015 Cleanrooms and associated controlled environments, Part 1: Classification of air cleanliness by particle concentration.# for single pass of beam through contaminant deposit\n\n|Particle Cleanliness Level to IEST-STD-1246|wavelength, nm|\n|---|---|\n|10%|200|\n|8%|400|\n|5%|600|\n|3%|100|\n|0%|150|\n| |200|\n| |250|\n| |300|\n\nFig 29: Left - Change in surface obscuration as the particle Cleanliness Level increases (as set by IEST-STD-1246, referencing non-volatile molecular cleanliness levels). Right - Absorption loss of a UV light beam versus wavelength and contaminant deposit equivalent thickness. Combined, these plots motivate contamination control when working in the ultraviolet regime.\n\nTo determine particle settling rates during critical operations will be necessary for HWO optics. It is anticipated that, for most Assembly, Integration, and Test (AIT) operations with HWO optics, personnel garmenting will require full coverall suits with attached hood, booties, and face covers.\n\nAll flight and flight-spare optical elements, including detectors, must always be accompanied by optical witness samples (OWS) having surface coatings applied simultaneously with those of the optical elements. The only times the OWSs leave the optical elements would be during UV reflectance or other measurements to verify the optical element performance.\n\nThroughout AIT operations the cleanliness of all hardware, tools, and work surfaces must be monitored either directly using high-precision Non-Volatile Residue (NVR) sampling and particle collection, including by tape-lift, either directly from hardware when possible, or from companion witness plates. Monitoring for molecular contaminant accretion must include real-time Quartz Crystal Microbalances (QCM) during all AIT operations. QCMs with 10 megahertz crystal frequency have sensitivity to detect equivalent contaminant equivalent thickness changes of 0.04 nm. Finally, the contamination controls must include maintenance cleaning procedures to be applied as necessary to maintain the hardware cleanliness within the derived budgets.\n\nAll work surfaces and tools should be maintained to the same cleanliness requirements as the hardware that will be in contact with the tools and surfaces. Unique controls must be developed for transfer of contamination-sensitive hardware between the cleanrooms and the vibration and thermal vacuum test facilities. And sufficient cleanliness of those facilities must be verified prior to their use.\n\nFor example, prior to a vacuum test of the hardware, a chamber bakeout must be performed and an abbreviated \u201cdry run\u201d of the chamber, with contamination monitoring, must be made covering the temperature range of the pending test. The monitoring should include Residual Gas Analysis (RGA), QCM, and high precision NVR sampling to verify that the chamber and all internal test apparatus are not a contamination threat to the hardware. It can be shown by modeling that partial pressures of volatile-condensables at < 10-14 Torr at critical hardware temperatures do not present a significant contamination threat. The RGAs to be used should have at least this sensitivity.# 8.4 Maintenance Cleaning\n\nAn extremely crucial control will be the selection of non-metallic materials; the paints, adhesives, pottings, wire insulation materials, and other plastics that must be used. Only materials that meet the low outgassing and molecular contamination requirements to be derived for the contamination budget can be used in HWO hardware. Traditionally the testing of materials for outgassing of molecular contaminants has included the basic test by ASTM-E595 which requires that Total Mass Loss (TML) during 24 hours in vacuum at 125\u00b0C not exceed 1%, nor shall Collected Volatile-Condensable Material (CVCM) exceed 0.1% on a collector at 25\u00b0C. However, this test alone certainly is insufficient for acceptance of materials for use on HWO hardware. A more appropriate test is MSFC-SPEC-1443. This test has the same requirements for TML and CVCM as ASTM-E595, plus it includes a MgF2/Al optical witness sample for ultraviolet reflectance measurements before and after the vacuum exposure. The basic UV requirement of MSFC-SPEC-1443 is that the reflectance of the mirror shall not change more than 3% at 121.6 nm, 125 nm, 130 nm, and at 10 nm increments to 200 nm wavelengths. If this test proves sufficient for HWO, these reflectance criteria will require revision consistent with the wavelength and degradation requirements of the observatory.\n\nContaminant transport modeling will be a useful tool for predicting the molecular contamination accretion on critical surfaces during vacuum testing, helping to determine the necessary cleanliness of the vacuum test facilities, and on orbit to help with the allocation of the lifetime molecular contamination budget and specific materials choices.\n\nFinally, on-board means of heating all optical elements should be provided to deplete molecular contaminants that may adsorb onto surfaces during space operations.# 9 References\n\nASTM-E595, Standard Test Method for Total Mass loss and Collected Volatile Condensable Materials from Outgassing in a Vacuum Environment\n\nMSFC-SPEC-1443, Outgassing Test for Nonmetallic Materials Associated with Sensitive Optical Surfaces in a Space Environment# 8.5 Venting\n\nAdequate venting of all enclosures, particularly of enclosed optics, must be sufficient to allow rapid depletion of outgassing species during vacuum bakes and tests. The venting rates are functions of the heats of adsorption of the molecular contaminants, the temperatures, and the vent dimensions. The difference between large vents and launch-only sized vents can mean weeks rather than a few days to deplete molecular contaminants from an enclosed assembly. This is discussed in detail in Appendix B.# 8.6 Future Testing\n\nA significant number of the non-metallic materials tests reported in the ASTM-E595 database are 30 to over 40 years old and many of the materials are no longer available. As noted above, acceptance of the ASTM-E595 criteria is certainly insufficient to select a material for HWO. Very few of the currently available materials used on spacecraft have been tested to MSFC-SPEC-1443. This is a very difficult test to pass, and at the same time, it remains to be seen if MSFC-SPEC-1443, even with extended wavelength range measurements, is the correct test method to be used for HWO materials, or if some modification of MSFC-SPEC-1443 or some other test method is more appropriate. In any case all non-metallic materials, and in some cases multiple lots of these materials, will have to be tested.\n\nAdditionally, the UV absorption coefficients as well as the heats of adsorption, of the volatile-condensables of the materials expected to be used in the HWO must be determined. This should include determinations of multiple specimens of each material for statistical significance.# 9 Development Priorities\n\nPrevious UV technology development priorities can be found in Astrophysics Biennial Technology Report 2022 (https://apd440.gsfc.nasa.gov/technology.html). Here we offer the following updated listing of UV technology development priorities as a path towards raising the TRL of components and systems to enable FUV science objectives.\n\n- Reflective coating development and characterization towards high reflectivity Broadband FUV to NIR Mirror Coatings.\n- Characterization of coating polarization and uniformity\n- Large format, low dark count, high efficiency, high dynamic range, photon counting, Solar blind FUV and NUV Detectors.\n- High throughput, large format technology for integral field and multi-object spectroscopy\n- High efficiency diffraction gratings for high and low spectral resolution from FUV to NIR wavelengths\n- FUV Imaging bandpass filters\n- Testbed development to raise systems level TRL for UV coronagraph, spectrograph and imaging systems.# 10 Path to HWO\n\nNew UV science and technology developments have been progressing at the speed that the Astrophysics Research and Analysis (APRA) & Strategic Astrophysics Technology (SAT) programs allow. To accelerate the pace to match the timeline proposed by the GOMAP for HWO, we need tools that go beyond the individual/component-level technology developments offered by APRA and SAT.\n\nWe require a combination of:\n\n1. Process level (\u201cmaterials physics\u201d) development.\n2. Scaling existing technologies to the sizes needed for HWO.\n3. Shifting from individual component level proof-of-concept development to the production line development of optical elements required for HWO electro/optical systems.\n\nWe also need to invest in system-level prototype/test instruments for both the laboratory and space. These instruments provide powerful inputs to Phase A-level trades and decisions (2029), so we need to do this soon. We require a combination of:\n\n1. Investment beyond APRA-levels in suborbital missions (balloons and rockets).\n2. Development of laboratory prototype testbed instruments.\n3. In-space technology demonstration missions, analogous to the Earth Science InVEST program, that can combine process development/scaling with systems-level testing and early-career training (Section 10.1).# 10.1 Training\n\nInstrumental astrophysics is an unusual field, in the sense that it is populated by people with a broad range of backgrounds, and it is supported by a broad range of roles. We might train as engineers and then move to astrophysics, or train as astrophysicists and then move into labs. We might be faculty members or professional engineers. The breadth makes this subfield unique. The project scale is also very large, compared to many subfields in astrophysics (and on par with subfields in physics like particle physics) although this has shifted as mission and telescope size has grown. This makes training a particularly crucial aspect of our human infrastructure.\n\nA great deal of knowledge is passed person to person during hands-on lab experiences. A peer reviewed journal specifically for instrumentation (SPIE JATIS) has existed for less than a decade, with a great deal captured in local observatory or project documentation or in conference proceedings. It is crucial as we shape the future of UV hardware and space missions we recognize that we not only need the right hardware - we need pathways to train and support early career researchers so they can take leadership roles in the mission in Phase A \u2194\u2198 E. As we move to increasingly large space-based missions and surveys on the ground and in space, many astronomy programs focus on the data handling skills necessary to navigate the enormous results. We must also keep pathways open for training builders to sustain projects for the upcoming decades. We present below one potential pathway to increasing the ranks of builders for the ultraviolet.# 10.2 Smallsats for Accelerated Technology and Workforce Maturation\n\nMany of the key technologies in the development queue for HWO require the combined activities of 1) facility and process development for validation of technologies at the scale required for HWO and 2) deployment in the \u2018real world\u2019 environment of mission Integration & Test followed by on-orbit operations. Many of these goals could be accomplished simultaneously through the development of a dedicated program combining laboratory advancement and process scaling conducted in parallel with the deployment of these technologies to space. Parallel and closely-linked facility, laboratory, and instrument prototype development program that can be applied to any of NASA\u2019s Future Great Observatories (FGOs), and an initial framework for this program has been outlined as the Smallsat Technology Accelerated Maturation Platform (STAMP).157 Advanced broadband optical coatings, high-sensitivity ultraviolet detector systems, and multi-object selection technology could all be brought to TRL 6 and flight demonstrated through such a program. STAMP advances HWO technology on an accelerated timescale, building on current ROSES SAT+APRA programs, reducing cost and schedule risk for HWO, while conducting a compelling program of preparatory science and workforce development with direct cost and schedule risk reduction for HWO mission implementation in the 2030s.\n\nThe STAMP science and instrument teams would be made up of &gt; 50% early-career researchers; this builds the critical relationships between observers, theorists, and instrumentalists that enable open communication and a cohesive development environment for HWO. The organizational layout of the mission would feature \u2018deputies\u2019 in all key mission science roles, including deputy-PI (dPI), deputy-Project Scientist (dPS), deputy-Instrument Scientist (dIS), and in all the major engineering and program management positions (Program Manager, Project Systems Engineer, Instrument Systems Engineer, etc). These positions would be fully funded and provide a direct route for the training of the scientists and engineers with the expertise to serve in science, instrument, and mission leadership roles for HWO.# 11 Summary\n\nWe have presented here a summary of the current status of UV technology, establishing a baseline for the development of the upcoming Habitable World Observatories instrumentation. This representative sample of the last several decades of development demonstrates not only the thriving UV technology but also illustrates the areas for advancement in preparation for HWO. Multiple pathways exist for successful instrumentation across the ultraviolet bandpass, and rapid investment will support the scaling up of technology that has been flight qualified on smaller scales. Coordination across the community will provide a path for rapid subsystems level flight qualification along with full integrated systems level tests. This is an exciting time to work in UV hardware, and we hope the knowledge gathered here provides a strong foundation for the exciting challenges posed by HWO.# ACKNOWLEDGMENTS\n\nThis work has been supported by the following NASA grants: NNX11AG54G, NNX14AI78G, NNG15PF22P, NNG16LI16P, NNX17AC26G, 80NSSC19K1040, 80NSSC22K0940, 80NSSC22K1698, NNH08ZDA001N, NNH12ZDA001N-RTF, NNX16AG28G, NNX12AG06G, 80NSSC20K0412, 80NSSC21K2016, 80NSSC21K1667, 80NSSC19K0661, NNX17AI84G, 80NSSC23K0754, 80NSSC22M0081. A portion of this research was done at Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration (80NM0018D0004).This work was supported in part by the NASA Cosmic Origins Program Office under the auspices of the Cosmic Origins UV Working Group.# Appendix A: UV Technology Frequently Asked Questions\n\n- What are some of the challenges associated with combining ultra-high-contrast exo-planet observations and a UV-sensitive telescope?\n- Wavefront Control: Going to shorter wavelengths means that the same wavefront error in physical dimension becomes a larger fraction of wavelength. This places more stringent requirements on the deformable-mirror system.\n- Polarization aberration: Although effects of polarization aberration are more pronounced in the UV, this degradation is concentrated at small angles in terms of \u03b5/D. Habitable zones of the best UV target stars tend to be at larger angular separations and angular separation per \u03b5/D is also larger in the UV compared to longer wavelengths can mitigate the science impact of this change.\n- Coating uniformity: A recent study (Krist, priv. comm.) using the USORT pupil (which has 19 hexagonal segments in the primary-mirror assembly) found that reflectance differences at the segment-to-segment level of up to 3% is tolerable in terms of contrast performance.\n- Scattered light: Scattering, due to surface roughness and particle contaminants, is stronger in the UV. Accurate estimates of near-angle scattering (NAS) is currently lacking, mainly due to lack of measurement data. NASA Engineering and Safety Center is funding a study (PI: Gaskin) to better quantify the impact of NAS on ultra-high-contrast imaging, which can lead to improved mirror-PSD and contamination-control requirements.\n- Will UV coating surface roughness be capable of working with coronagraphy at the level needed for HWO?\nThe current roughness requirements driven by the nominal coronagraphy design are applied to the full wavelength range. Work is in process to test and quantify roughness, but the requirement as currently understood is not met by any coating at any wavelength range.\n- What contamination control requirements does UV sensitivity impose on the observatory?\nContamination control is necessary for the reflective coatings in the ultraviolet. However, the contamination control is well understood and defined as shown in Section 8 and Appendix B.\n- What are some of the key science cases that need wavelength coverage down to 100 nm?\nWe highlight key science cases in Section 3, and summarize them below.\n\n- 102 \u2014 115 nm: FUV inputs into exoplanetary atmospheres, including emission lines like SIV, OVI, NeV, and FeXIX# Current Page Content\n\n- 100 \u2014 115 nm: Direct measurement of absolute abundance and temperature of inner disk gas of protoplanetary disks.\n- 103 nm: Provides access to CGM OVI at z = 0\n- 100 \u2014 110 nm: CGM high ionization lines like Ne VIII (0.29 < z < 0.41) and Si XII (1 < z < 1.2)\n- 100 \u2014 120 nm: Lyman continuum escape fraction at very low redshift (0.1 < z < 1)# Technological Considerations\n\n- MKIDS and nanowire detectors have made a lot of progress - can we use those technologies?\n- Not for this. Currently, these detectors are both at a relatively low TRL and not yet achieving desired QE in the FUV for potential UV spectrographs. Detectors beyond those discussed here are being considered for other instrumentation on HWO.\n- Does pushing the observatory\u2019s wavelength coverage below the short wavelength cutoff of Hubble (115 nm) increase the technical challenges?\n- No. In the decades since Hubble was built, technology development has pushed forward performance below 115 nm, including in optical coatings and detectors. Sections 4 and 5 demonstrate the significant progress that decreases the technical risk of extending UV coverage beyond the Hubble cutoff.\n- Are MCP detectors limited in their ultimate count rate and susceptible to gain sag that limits their lifetime?\n- High rate real time gain sag is due to the local recharge time of a pore. During instrument design, strategies can be used to improve the detector dynamic range, such as using lower resistance MCPs and low gain compatible readouts such as the cross strip to achieve 1000 events/sec for a resolution element. The MCP lifetime is related to total charge extraction. Traditional MCPs drop in gain substantially with extracted charge. The lifetime of new technology borosilicate MCPs with ALD functionalization have been shown to maintain stable high gain during extended life testing (>7 C cm\u22122) to more than 5 x 1013 events per cm2 which is substantially more than most missions accumulate.\n- Are MCP detectors at risk when exposed to high count rates?\n- In practice, no. The rate would have to be extremely high (focused beam of sunlight) to physically heat the MCP. This is usually avoided (and proven at TRL 9) by multiple layers of hardware, software, and observation planning restrictions. Newer MCPs also tolerate much higher count rates (Siegmund 2023 NIM) as demonstrated by full recovery after exposure to laser illumination.# Appendix B: Contamination Control\n\nParticles typical particles, including fibers, that contaminate space hardware range from approximately 1 \u03bcm to 1 mm in maximum dimension. Their sources include air-borne dust in the local environment and fibers and particles released from tools, work surfaces, garments, human hair and skin, etc.\n\n49# Settled particle distributions\n\n|1E+8|Level 100|Level 400|\n|---|---|---|\n|1E+7|Level 200|Level 500|\n|1E+6|Level 300|Level 600|\n|1E+5| | |\n|1E+4| | |\n|1E+3| | |\n|1E+2| | |\n\n10 particle size, microns 100\n\nFig 30: Cumulative particle size distributions for selected Cleanliness Levels\n\nThe dominant effects of settled particles for optical systems are light scatter and obscuration (sometimes parametrized as percent area covered). IEST-STD-124611 establishes particle Cleanliness Levels listing the maximum number of particles per 0.1 m2 surface area for ranges of particle maximum linear dimensions.\n\nTypical criteria for contamination-sensitive optical instruments are often in the range of particle Cleanliness Levels of 200 to 300 (Figure 30) which are not unusually difficult to achieve, even for large optical systems. Even with 3 or 4 optical surfaces in series, this represents less than 0.1% optical obscuration for the optical system (Figure 29). However, for scatter-sensitive optical instruments such as coronagraphs, complex scatter analyses, involving measurements of the actual size distribution, may be necessary to determine acceptable particle deposition.# Molecular Contaminants\n\nIt is difficult to bound the sources and variety of molecular contaminants to which space hardware can be exposed. Even after thorough initial and precision cleaning, the molecular contaminants found on space hardware in cleanrooms include volatile-condensable molecular species of masses up to approximately 300 amu. These molecular contaminants may exist on surfaces as randomly spaced single molecules, coalesced nanometer sized \u201cdeposits,\u201d monolayer to multilayer films less than 0.5 nm to a few nm in thickness, or even micro droplets, all possibly consisting of a mix of molecular species.\n\nThe dominant effect of these molecular contaminants for optical systems is light absorption, which can be quantified using Lambert\u2019s Law when the absorption coefficients of the contaminants are known. In a worst-case of molecular contaminant deposits as micro-droplets, light scatter will also occur.\n\n11 IEST-STD-CC1246, Product Cleanliness Levels and Contamination Control Program, Institute of Environmental Sciences and Technology\n\n12 The strange notation of # / 0.1 m2 is a result of the historical attempt to equate Imperial unit values which were given in # / ft2 to metric nomenclature. Of course, this leads to an error of about 9.3% in comparing Imperial data to metric data.# IEST-STD-1246\n\nIEST-STD-1246 defines non-volatile molecular cleanliness levels over a very broad range from 10 ng/0.1m2 to 25 mg/0.1m2 (alternatively, 10 pg/cm2 to 25 \u03bcg/cm2). Molecular contaminant lifetime allowables for spacecraft and space science instruments have typically been in the range of 50 to 500 \u03bcg/0.1m2 (50 to 500 ng/cm2; equivalent to thicknesses of 0.5 to 5 nm for contaminants with specific gravity = 1.0). However, dependent on the results of contamination degradation analyses of the HWO telescope and instruments, the molecular contaminant allowables for HWO may be significantly lower13.# Molecular Contaminant Transfer\n\nIn controlled environments such as cleanrooms and vacuum chambers troublesome molecular contaminants are transferred to and accrete onto surfaces primarily by desorption from a source surface and subsequent adsorption onto another surface. These are called volatile-condensable species. The standard term for this desorption is outgassing and is quantified as mass loss per unit area, typically in micrograms per cm2 (\u03bcg/cm2) or nanograms per cm2 (ng/cm2). These outgassing species can be both molecules of, or molecular fragments of the base material, foreign contaminants from the surface of the base material, or contaminants diffused from the interior of the base material.\n\nAnother source of contaminant transfer between surfaces is contact transfer. As the name implies, contact transfer is contacting one surface with another and the transfer of contaminants from one to the other. An example is to touch a clean surface with a contaminated glove and leave some of the contaminants on the clean surface.\n\nMolecular contaminant transfer, or transport, is discussed further in Appendix B.# Molecular Contaminant Effect\n\nThe magnitude of the optical absorption losses are predictable using Lambert\u2019s Law: the intensity of a light beam inside an absorbing medium decreases exponentially as a function of the distance the beam has traveled into the medium and as a function of the absorption coefficient, \u03c9, of the absorbing material.\n\nThe absorption coefficients, \u03c9, given in units of inverse thickness (1/t), or inverse mass per areal density (i.e., cm2/ng), have widely ranging values over several orders of magnitude for wavelengths from the far UV through the IR for the various molecular contaminants typically found.\n\nThe available data on absorption coefficients for molecular contaminant deposits is very limited, as these values are exceptionally difficult and expensive to measure, particularly for UV wavelengths. In support of the Wide Field Planetary Camera (UV wavelength range: 200-1000 nm) and beneficial to the Faint Object Spectrograph (UV wavelength range: 115-850 nm) which were flown on Hubble, \u03c9 values of contaminant deposits were measured at several UV wavelengths and for multiple thicknesses by Dr. Joseph Muscari, Martin Marietta Corp., for several adsorbed contaminants.14 Other limited compilations of \u03c9 values have been produced showing a broad range of values vs. wavelength for the few contaminant source materials which have been tested. Detailed information about the test methods and test sample variations are unknown and may contribute to the data variations.\n\n13 For example, with a design consisting of 5 surfaces (primary, secondary, pick-off mirror, instrument reflecting surface, detector surface) there will be 9 passes through contaminant deposits. For deposits 0.5 nm thick, the total degradation between 100 nm and 150 nm will be 30% to 50%.\n\n14 Nonmetallic Materials Contamination Studies, Final Technical Report, J A Muscari, Martin Marietta Corp., JPL contract 955426, December 16, 1980# Wide range of absorption coefficients vs wavelength with estimated TYPICAL curve\n\n| |1E-3| |1E-4| |1E-5| |1E-6| |1E-7| |1E-8|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|100|200|300|400|500|600|700|800|900|1000| | |\n\nFig 31: Range of Absorption Coefficients vs. Wavelength with estimated TYPICAL Curve.\n\nData currently available can be used to bracket the probable range of UV \u03c9 values. Figure 31 shows a summary of the available absorption coefficient data compiled mostly by Joseph Hueser, Ball Aerospace (unpublished), over the wavelength range of 150 to 1000 nm. A \u201ctypical value\u201d curve has been included in the Hueser plot. Many of the data points shown in the Hueser plot were derived from Muscari measurements. Great caution should be used in applying this data as in most cases the precision of unknown, was typically measured on relatively thick contaminant deposits, and may not be representative of currently available materials.\n\nCautiously using the estimated \u201ctypical value\u201d curve given in Figure 31 for molecular contaminant absorption values, and for want of more and better data, the predicted absorption over the range of 122 nm to 300 nm for contaminant equivalent thicknesses of 0.25, 0.5 and 1 nm was shown in Figure 29. This illustrates how very important it is to minimize molecular contamination in the vacuum UV and the need for precise adsorption coefficient data on materials to be used on the HWO. For a telescope and UV science instrument in series, there will be a minimum of 4 optical surfaces and 7 passes of the optical beam through contaminant deposits, which can severely degrade instrument performance. \u03c9 values are orders of magnitude higher for most molecular contaminants in the UV vs. the near IR and Mid-IR, making HWO far more contaminant-sensitive than JWST.# Molecular Contaminant Cleanliness Levels\n\nTable 2 is a listing of selected molecular Cleanliness Levels from IEST-STD-CC1246E with level names shown in column 1. Column 2 is the defined maximum allowable Non-Volatile Residue (NVR) per 0.1 m2 for each level. When collecting contaminant samples from surfaces for quantifying, the measurement results are often given in nanograms per cm2 (ng/cm2). Column 3 shows this equivalent areal density, while the Column 4 shows the approximate equivalent thickness were the molecular contaminant to be uniform in thickness and to have a density of 1 g/cm3.\n\n15 The density of most contaminants is expected to be in the range of 0.8 to 1.2 g/cm3 based on the densities of the typical source plastics.# Table 2: Non-volatile molecular cleanliness levels from IEST-STD-1246\n\n|Molecular Contaminant Level|Maximum Allowable NVR Mass / 0.1 m2 Surface Area|Equivalent Areal Density (ng/cm2)|Approx. Equivalent Thickness (nm)|\n|---|---|---|---|\n|R5E-3|5 \u03bcg|5|0.05|\n|R1E-2|10 \u03bcg|10|0.1|\n|R2E-2|20 \u03bcg|20|0.2|\n|R5E-2|50 \u03bcg|50|0.5|\n|R1E-1|100 \u03bcg|100|1|\n|R2E-1|200 \u03bcg|200|2|\n|R5E-1|500 \u03bcg|500|5|# Molecular Mass Transport\n\nOutgassing is not just a vacuum phenomenon, it occurs in all environments. As molecular contaminants transfer between surfaces at room temperature, such as during exposed periods in cleanrooms, they desorb from source surfaces, adsorb onto exposed surfaces, and come to rest for predictable time periods, with the mean molecular rest times dependent on their heats-of-adsorption16 and the local temperature. Often the total adsorbed mass can be reduced or essentially depleted by elevating the temperature. However, this must be done in high vacuum. The species with lower heats of adsorption, less than about 23 kc/mol deplete readily in vacuum even at near room temperature. The molecular species which are the most difficult to deplete are those with heats of adsorption greater than about 24 kc/mol. And the species with heats of absorption greater than about 27 kc/mol are so stable that they are affected very little at spacecraft bakeout temperatures, and may more likely be chemisorbed rather than physisorbed as is the norm for volatile-condensable molecular contaminants.\n\nCleanrooms are far cleaner than laboratories and workshops, but still, they are not perfect. They will have measurable quantities of airborne particles and volatile-condensable molecular species leading to settling of particles and adsorption of molecular contaminants. Historical evidence from measurements on witness plates of volatile-condensables has shown that molecular contaminants do adsorb onto these surfaces with adsorption rates and maximum quantities being dependent on the cleanroom quality and assembly and integration operations that are occurring. Such evidence has shown that the time for the adsorbed species to reach some apparent equilibrium with the gaseous phase species to be on the order of a few weeks to a couple of months for the few cleanrooms that were monitored.\n\nFigures 32 and 33 show the contaminant accretion rates onto clean surfaces for an important range of volatile-condensables. For the analysis, it was assumed that the areal density of each adsorbed contaminant at equilibrium was 50 ng/cm2 (roughly equivalent to a single monomolecular layer of water) and does not represent a particular cleanroom. Figure 34 shows that the predicted time for a clean, exposed surface to equilibrate to the cleanroom quantity of species with heats of adsorption of 23 kc/mol is on the order of about 15-20 hours, whereas Figure 35 shows it could take up to 120 hours for a 26 kc/mol contaminant to reach cleanroom equilibrium. Additional analyses show that the volatile-condensibles having heats of adsorption less than about 23 kc/mol move very quickly through the cleanroom, while volatile-condensables having heats of adsorption greater than about 27 kc/mol are more stable.\n\nHeats of adsorption can differ for a specific molecular species on different surfaces, dependent on other contaminants present and the unique Van der Waals forces for the contaminant-surface pair.# Mass transfer from cleanroom to article for\n\nH = 23 kclmole; Qo = 50 nglcm2 = 0.5 nm 1E-10\n\n|1|5040| |9|\n|---|---|---|---|\n|30|1|20|incident Iux|\n|article quantity|1|outgassing rate|1|\n|10| | |1E-13|\n\nsource temp 22 C; article temp 22 C\n\ncleanroom environment\n\ntime, hours\n\nFig 32: Mass transfer contaminant accretion onto clean surfaces at an H of 23 kc/mol over 15-20 hrs.# Mass transfer from cleanroom to article for\n\nH = 26 kclmole; Qo = 50 nglcm2 = 0.5 nm 1E-13\n\n|1|5040| |9|\n|---|---|---|---|\n|30|1|20|quantity|\n|incident Iux|article quantity|1|outgassing rate|\n|1E-15| | | |\n\nsource temp 22 C; article temp 22 C\n\ncleanroom environment\n\ntime, days\n\n90 120 150\n\nFig 33: Mass transfer contaminant accretion onto clean surfaces at an H of 26 kc/mol over 120 hrs.\n\ngreater than 26 kc/mole travel so slowly from source surfaces at room temperature that they are not a significant problem in the cleanroom. This is shown in Figure 34.\n\nThe message is that molecular contaminants do transfer throughout cleanrooms with those with low heats-of-adsorption reaching some equilibrium on exposed surfaces in hours to days, while those with higher heats-of-adsorption may take so long to transfer that they do not present a significant contaminant risk to hardware that is sealed or bagged and purged for the majority of the time in AIT.\n\nFigure 35 shows that when a plane, fully exposed contaminated surface is placed in vacuum# Mass transfer from cleanroom to article\n\nH = 27 kclmole; Qo = 50 nglcm2 = 0.5 nm 1E-13 L\n\nTime, days\n120\n240\n360\n480\n600\nFig 34: Mass transfer contaminant accretion onto clean surfaces at an H of 27 kc/mol indicating low levels of contamination.\n\nThe molecular contaminants desorb relatively quickly in a predictable manner as a function of temperature, heats of adsorption, and elimination of the original contaminant sources. However, consider as an example, a short telescope tube with dimensions of length/diameter = 4 and open on the aperture end. The probability of a molecule desorbing from a random spot inside this open-ended tube venting out the aperture in each desorb-adsorb cycle is only 0.06, based on view factors. In this case, the time to depletion of the contaminant is increased from about 12 hours to roughly 200 hours. This has important implications for vacuum baking cleanup of complex assemblies.\n\nVenting is an extremely important factor for efficient vacuum bake maintenance cleaning of assemblies. Vents of enclosures such as electronics boxes are normally designed to prevent pressure differentials during vacuum pumpdown and during launch from mechanically damaging the enclosure. A normal launch vent design criterion is given by:\n\nA &gt; 2000mm2 (1)\n\nwhere A = cross sectional area of the vents, and V = volume of the vented enclosure. This is sufficient for launch venting, but not for vacuum bake cleaning. The internal surface area of even an empty enclosure with vents to this criterion is approximately 1000 greater than the area of the vents only, leading to a \u201cvent factor\u201d (defined as the ratio of the area of the vent aperture to the total surface area inside vented cavity) of 0.001. Vacuum baking of such articles for even weeks would be very ineffective, leaving these articles as potential contaminant sources when in assemblies adjacent to optical elements, particularly during vacuum tests.\n\nThe maximum temperature for vacuum bakes of spacecraft assemblies is often limited by the batteries to about 60 \u2193 C. Figure 36 shows that the predicted time to essentially deplete a heat of adsorption 26 kc/mol contaminant is on the order of 10 to 12 days at 60 \u2193 C for surfaces with a vent factor of 0.06. Increasing the vacuum bake temperature to 82 \u2193 C will reduce the time to deplete the 26 kc/mol species to about 1 day. This can result in significant savings when vacuum.# Vacuum desorption of admolecules from article\n\nH= 23 kclmole; Q'0 = 50 nglcm2 = 0.5 nm 1E-11\n\n| |50|40|30|20|10|\n|---|---|---|---|---|---|\n|aruclequanlity.0\"|1E-12|9|L 2030|1E-13|1|\n|source temp|vacuum environment 22 C; article temp 22 C|1E-14|1E-15|11| |\n| |12|16|20| | |\n\nFig 35: Analysis showing that when a plane contaminated surface is placed in vacuum the molecular contaminants desorb relatively quickly as a function of temperature and heats of adsorption.\n\nbaking subassemblies. And the contaminants with lower heats of adsorption will be depleted much quicker. However, regardless of the heat of adsorption, the times to deplete will be far longer for surfaces inside boxes with only small launch vents.\n\nIt is shown by analysis that the most troublesome volatile-condensable contaminants to deplete with vacuum baking are those with heats of absorption in the range of 23 to 26 kcal/mol. Molecular contaminants with lower heats of absorption are readily depleted in vacuum at room temperature and above. However, the contaminants with high heats of absorption require higher temperatures and perhaps long times in vacuum bake to deplete.\n\nThe above data show that volatile-condensable molecular contaminants can be depleted by vacuum baking, however adequate venting of enclosed volumes is necessary. Enclosures of articles other than optical components should be opened for vacuum bake cleanup and should be vacuum baked separate from the optics. The troublesome contaminants must be depleted to acceptable levels prior to top assemblies of optics. Mechanical and electronic assemblies must be exposed in vacuum together, apart from optical elements, to avoid the optics being contaminated.\n\nTo take scheduling advantage of this knowledge requires monitoring all vacuum bakes with high-sensitivity, broad-range residual gas analyzers (RGA), and determining the heats of adsorption of the specific higher mass (100 \u2013 300 amu) species. In some cases, a temperature-controlled quartz crystal microbalance (TQCM) can be used to determine approximate heats of adsorption simultaneously while monitoring the RGA-identified mass peaks.# Vacuum bake of article at 60 \u00b0C\n\n|H|Q'0|anicle quantily|anicle outqassino|\n|---|---|---|---|\n|26 kclmole|50 nglcm\u00b2 = 0.5 nm 1E-12|50|1E-13|\n| | |40|1E-14|\n| | |30| |\n| | |20|chamber temp 60 \u00b0C; article temp|\n| | |10|vaCuiom (bake environment 1E-15|\n| | | |1E-16|\n\ntime, days at 60 \u00b0C\n\nFig 36: Vacuum bake efficacy: the predicted time to essentially deplete a heat of adsorption 26 kc/mol contaminant is on the order of 10 to 12 days at 60 \u00b0C for surfaces with a vent factor of 0.06.",
        "context_id": 35,
        "question": "What is the wavelength range deemed critical for calculating the EUV inputs into exoplanetary atmospheres?",
        "answer": [
            "102 \u2013 115 nm"
        ],
        "context_length": 139880
    },
    {
        "context": "# 1 INTRODUCTION\n\nIn the last decade, continuous human activities increased\u2014both in private and working life\u2014thus raising the necessity for automation. Consequently, the adoption of bots [134, 150]\u2014i.e., software systems designed to automate a specific function or set of activities\u2014has grown and available for a plethora of purposes [22, 141]. In particular, conversational agents (CAs) [141, 149, 150]\u2014also known as chatbots\u2014are bots that communicate with users, by natural language or similar, using a communication channel. Recently, CAs\u2014and, more generally, bots\u2014have started to be adopted.# Authors\u2019 addresses:\n\nStefano Lambiase, slambiase@unisa.it, SeSa Lab \u2013 University of Salerno, Salerno, Italy; Gemma Catolino, gcatolino@unisa.it, SeSa Lab \u2013 University of Salerno, Salerno, Italy; Fabio Palomba, fpalomba@unisa.it, SeSa Lab \u2013 University of Salerno, Salerno, Italy; Filomena Ferrucci, ferrucci@unisa.it, SeSa Lab \u2013 University of Salerno, Salerno, Italy.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\n\u00a9 2018 Association for Computing Machinery.\n\nManuscript submitted to ACM# Lambiase et al.\n\nIn the software development field [134, 146]. For example, practitioners use bots to automate software maintenance and evolution activities. Similarly, they adopt bots for collaboration, speeding up communication and knowledge-sharing. Besides proposing them, the research community worked on frameworks to ease the development process of conversational systems.\n\nThe exploration of bots and conversational agents in systematic literature reviews reveals a focus on both technological and strategic applications within software engineering. Lewandowski et al. [135] and Suhaili et al. [151] emphasize the technical construction and advocate for considering interaction design and natural language capabilities. Motger et al. [141] and Del Carpio and Angarita [115] discuss integration into user interactions and software development. Moguel-S\u00e1nchez et al. [139] examine practical applications, such as project management and static code analysis. As a common finding, despite their potential, bots, especially with Generative AI and Large Language Models, present development and integration challenges.\n\nTo support both the research community\u2014providing new research opportunities\u2014and practitioners\u2014delivering knowledge for adopting bots and CAs\u2014we carried out a multivocal literature review (MLR) [120, 142], following the guidelines of Garousi et al. [120]. Specifically, we aimed to understand the current role and impact of bots and CAs in the software engineering (SE) field, focusing on the (1) motivations for their adoption, (2) challenges, (3) best practices, and (4) benefits coming from their adoption.1\n\nOur work complements and diverges from existing literature by reviewing both academic and grey literature and identifying best practices for bot design and adoption. By focusing on engineering and interaction design challenges, we provide guidelines to enhance the practical application of bots in the industry. Additionally, this research aims to be useful to both researchers and practitioners. Moreover, the decision to conduct a Multivocal Literature Review (MLR) rather than an SLR for studying bots in software engineering is supported by Garousi et al. [120]. The rapid technological evolution led by practitioners and the lack of extensive academic publications necessitates incorporating grey literature (GL) for current insights [144, 151]. The practical application of bots further underscores the need for GL to understand real-world impacts. Aligning academic research with practical experiences provides a comprehensive synthesis of knowledge [120], and including practitioner-driven insights is crucial to avoid publication bias [144, 151].\n\nWe collected 107 literature items\u201479 formal studies and 28 grey ones. Results show how CAs are complex to design and deploy despite the diffusion of tools to support their development. Moreover, we noticed that these systems are prone to several problems that undermine their adoption. In particular, interaction\u2014like interruption and noise\u2014emerged as the most critical issue. Nevertheless, we discovered that bots and CAs benefit the practitioners\u2019 community considerably, justifying putting effort into increasing their adoption. In terms of contributions, we provide the following:\n\n- A taxonomy to categorize and describe bots and conversational agents based on the motivation of their use for software development purposes.\n- A list of challenges arising from adopting bots and conversational agents.\n- A set of best practices to deal with bots and conversational agents and an association between such practices and challenges (reported in the Discussion section).\n- A list of benefits when adopting bots and conversational agents in the software development context.\n- An online appendix [133] containing all our findings and all the data gathered during the steps of our multivocal literature review.\n\n1 From now and for the rest of the paper, we will use the term \u201cbot\u201d to refer to both bots without and with a conversational component (CAs) when it is not necessary to put in evidence the conversational component.\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering\n\nThese findings can be helpful for researchers, proposing new research opportunities and standardizing the terminology for referring to bots and CAs, and practitioners, reporting best practices and knowledge to ease the adoption and development of conversational systems.# 2 BACKGROUND AND RELATED WORK\n\nAccording to Storey and Zagalsky [22, 85, 150], a software bot is \u201ca conduit or an interface between users and services, typically through a conversational user interface\u201d [150], that can be exploited for automating processes/activities, thus improving productivity [22, 150]. For this reason, software bots are widely used for different purposes. For instance, in the context of software development, they have been adopted for automating application deployment\u2014e.g., build, test, and report\u2014or for supporting developers during communication activities\u2014e.g., agenda and information retrieval [22, 32, 40].\n\nConversational agents (CAs)\u2014also known as chatbots\u2014are particular types of software bots that use communication channels like Slack, Teams, or Discord to interact with users, generating human-like conversations [3, 156].\n\nThe properties of bots and conversational agents, other than their potential applicability to various fields, have attracted the interest of multiple research communities\u2014e.g., Software Engineering and Human-Computer Interaction\u2014over the last few years. Despite being a young research field, we have identified six systematic literature reviews that synthesize state of the art on the usage of bots and CAs in the software engineering context [135, 139, 141, 144] and in related fields [151].\n\nLewandowski et al. [135] conducted a systematic literature review aiming to provide a structured overview of how conversational agents might be used from a strategic standpoint, namely how they can be used within work and company processes, other than governance structures. The authors found 21 relevant scientific records from 2015 to 2020. The key findings of the systematic literature review suggested that most of the papers focused on the technical aspects behind conversational agents. At the same time, other perspectives should be considered when designing more practical and usable CAs. These aspects concern (1) natural language capabilities and training and (2) interaction design, namely, the way a conversational agent can interact with its stakeholders. According to Lewandowski et al. [135], the missing analysis of these aspects may prevent the broader adoption of conversational agents in the industry. Concerning this literature review, our work shares the goal of eliciting the best practices to design conversational agents. Moreover, our analysis also considers themes connected to motivations, challenges, and benefits of conversational agents and bots in software engineering. Finally, our scope is larger since we also include resources from the grey literature, thus including complementary perspectives. As a result, we provide a more comprehensive analysis of the topic of interest.\n\nSuhaili et al. [151] investigated the technology employed to develop conversational agents. Indeed, they primarily focus on the components and techniques used to design those tools, other than on the datasets for their training, evaluation metrics, and application domains. Their review covered the scientific literature from 2011 and 2020. The authors reported that deep and reinforcement learning represent popular methods for understanding users\u2019 intents and generating appropriate responses. In addition, they found that T!\"##$%2 and A\"%&\"\u2019$ T%()$& I\u2019*+%,(#\"+\u2019 S-.#$,. 3 represent the most widely used datasets to train and test the capabilities of conversational agents. Finally, metrics such as accuracy, F1-score, and BLUE index are more frequently used to evaluate the built agents. The work by Suhaili et al. [151] is complementary to ours; besides their goal, we aim to elicit the best practices for designing bots and conversational agents.\n\n32 The T!\"##$% dataset: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis.\n\nThe A\"%&\"\u2019$ T%()$& I\u2019*+%,(#\"+\u2019 S-.#$,. dataset: https://www.kaggle.com/datasets/hassanamin/atis-airlinetravelinformationsystem.\n\nManuscript submitted to ACM# 4 Lambiase et al.\n\nMotger et al. [141] conducted a systematic literature review of secondary literature, reviewing 28 articles. Their goal was to analyze the anatomy of CAs by providing a taxonomy regarding (1) the human-computer interaction features more relevant for users when interacting with CAs, (2) which techniques are used for the design and implementation of CAs, and (3) which approaches are used for the training phase of the NLP and NLU modules of bots. Our work can be considered complementary to the one by Motger et al. [141]. First, we focus on the design of both bots and conversational systems, hence providing a more extensive overview than Motger et al., who only analyzed CAs. More importantly, we approach the literature review from the software engineering perspective rather than artificial intelligence. As such, we are interested in providing guidelines and recommendations on how to engineer bots and CAs rather than understanding which are the best practices to train their internal artificial intelligence components. Moreover, we elaborate on additional concerns that may inspire further research in software engineering, such as the definition of a taxonomy to characterize bots and a list of benefits from their adoption. Last but not least, our work is based on a multivocal review [142] and, therefore, we analyze both white and grey literature by following the guidelines by Kitchenham et al. [131] and Garousi et al. [120].\n\nMoguel-S\u00e1nchez et al. [139] conducted a systematic literature review to identify (1) the activities for which bots are used in software development, (2) the benefits, and (3) the problems. They reviewed 83 primary studies and conducted a thematic analysis to organize their quantitative and qualitative findings. They found that bots are mainly used for project management, to automate tasks with a low level of abstraction, such as tagging pull requests and commits, assigning team members to code reviews, performing static code analyses, and tracking changes in project repositories. Divergently from Moguel-S\u00e1nchez et al. [139], we included in our research also the grey literature, resulting in a more comprehensive knowledge of the topic other than a larger quantity of literature. Moreover, we also investigated the best practices associated with using bots in SE and created a mapping between challenges and best practices as an ulterior contribution to our work. Finally, regarding the challenges, we rely on already published taxonomy to enhance it and continuously extend the state of the art.\n\nDel Carpio and Angarita [115] conducted an SLR studying the support given by software assistants (SAs)\u2014including bots\u2014to the entire software development process. Specifically, they studied (1) what processes are supported by SAs, (2) how practitioners interact with SAs, (3) what techniques are used to implement SAs, and (4) what challenges face SAs. They reviewed 40 studies and found that most SAs are\u2014actually\u2014conversational agents, and their interaction with users is far from optimal. The work by Del Carpio and Angarita [115] is complementary to ours; besides their goal, we aim to elicit the best practices for designing bots and conversational agents, and we focus on the benefits of using bots. Moreover, we included the grey literature, which helped us identify a set of practices to complement the findings of the white literature.# 3 RESEARCH STUDY DESIGN\n\nThe goal of the study is to understand the current role and impact of bots and conversational agents in software engineering, focusing on the (1) motivations for their adoption, (2) challenges, (3) best practices, and (4) benefits provided to software engineers. The purpose is to provide a comprehensive overview of the matter, which might be useful to learn about the current state of the art and of the practice. Moreover, we want to stimulate further research to be pursued. The perspective is of both researchers and practitioners: the former are interested in having a unique resource to use as a starting point to deepen their knowledge of the research field; the latter are interested in understanding best practices, challenges, and benefits of using bots in practice.\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering\n\nTo achieve our goal, we performed a Multivocal Literature Review (MLR) [142, 145], namely, a research process where past published (formal) literature (e.g., journal and conference papers) and grey literature (e.g., blog posts, videos, and white papers) on the topic investigated are systematically identified, selected, and critically assessed. An MLR builds on top of the concept of Systematic Literature Review (SLR) [130, 131]\u2014where only the formal literature is taken into account\u2014to provide benefits not only to the research community but also to practitioners.\n\nWe relied on two guidelines when defining the protocol and conducting the review. In particular, concerning the formal literature analysis, we adopted the well-established approaches by Kitchenham et al. [130, 131]. In addition, we followed the guidelines defined by Garousi et al. [120] when it comes to the grey literature. Finally, when organizing and reporting the results, we followed the \u201cGeneral Standard\u201d and \u201cSystematic Reviews\u201d guidelines provided by the ACM/SIGSOFT Empirical Standards. In the following subsections, we describe the main steps of each phase.# 3.1.1 Initiation Phase\n\nThe Initiation phase consisted of three sub-phases.# Initial Examination of Previous Studies\n\nTo collect insights and basic knowledge about the adoption of bots and conversational agents in software engineering, we conducted a preliminary investigation in which we reviewed a few existing studies on the matter, e.g., [135, 151]. In particular, we used the Repository for Bot-related Research [108]\u2014i.e., a publicly available repository of formal literature maintained by the software engineering research community. After reading the studies, we discarded them in a \u201cthrow-away prototyping\u201d\u2014as suggested in the literature [120, 131]\u2014to start the main review process with basic knowledge, avoiding possible bias or influence from no systematic intervention.# Motivation and Needs Identification\n\nBefore choosing to conduct an MLR, Garousi et al. [120] suggested investigating the current state of the art on the topic of interest to identify limitations that grey literature could address. For this reason, Garousi et al. [120] defined a checklist that consists of seven \u201cYes\u201d /\u201cNo\u201d questions related to the complexity and interest of the topic. A high number of \u201cYes\u201d justifies the inclusion of grey literature in the systematic analysis. This step is crucial to prevent the inclusion of grey literature from being of no benefit to research and resulting in a futile effort. As for our study, Table 1 reports the filled checklist. To fill such checklist, we used the knowledge obtained during the initial phase (described in the previous paragraph) and the SLRs on the topic [135, 143, 144, 151], as suggested by Garousi et al. [120]. Six out of seven responses were \u201cYes,\u201d which allowed us to decide to proceed with including grey literature in our study.\n\nThe first two questions were about the topic itself rather than the work. We answered the first question positively since the use of bots in software engineering is intrinsically a complex topic from both a technical and social perspective [A1\u2013A3]. Moreover, bot technology advances primarily on the practitioner side rather than the research side, thus leading to a fast evolution of technology that is difficult to capture by research studies alone, often requiring slow publication times to facilitate systematic review and process. Furthermore, the broad adoption of bots, coupled with the recent advent of generative AI, leads to a wide range of issues to analyze, many of them from both a research and practitioner perspective. Regarding the second question, since the topic of bots in software engineering is young, there is not enough literature to determine the presence of a lack of consensus. Nevertheless, the youthfulness of the topic further poses the need to conduct studies involving the gray literature as well.\n\n54 ACM/SIGSOFT Empirical Standards: https://github.com/acmsigsoft/EmpiricalStandards.\n\nThe entire process is depicted in Appendix A\u2014Fig. 1 [133].\n\nManuscript submitted to ACM# Table 1. Questions to decide whether to conduct an MLR [120].\n\n|#|Question|Answer|\n|---|---|---|\n|1|Is the subject \u201ccomplex\u201d and not solvable by considering only the formal literature?|Yes|\n|2|Is there a lack of volume or quality of evidence, or a lack of consensus of outcome measurement in the formal literature?|No|\n|3|Is the contextual information important to the subject under study?|Yes|\n|4|Is it the goal to validate or corroborate scientific outcomes with practical experiences?|Yes|\n|5|Is it the goal to challenge assumptions or falsify results from practice using academic research or vice versa?|Yes|\n|6|Would a synthesis of insights and evidence from the industrial and academic community be useful to one or even both communities?|Yes|\n|7|Is there a large volume of practitioner sources indicating high practitioner interest in a topic?|Yes|\n\nNote: The possible answers to each question are \u201cYes\u201d or \u201cNo\u201d. One or more \u201cYes\u201d responses suggest that it could be useful to conduct an MLR.\n\nThe third question is about the subject under study in the work. Literature has repeatedly shown that the study context is central to software engineering research [118, 147]. This becomes even more true when it comes to bots and their impact on practitioners. The present work aims to elicit the motivations, challenges, strategies, and benefits of practitioners\u2019 use of bots for different purposes and in different conditions. Hence, the contribution of grey literature to characterize this context is essential. For such a reason, we answered the third question positively.\n\nQuestions four, five, and six are about the work objectives. As mentioned above, the goal of the work is to provide knowledge suitable for (1) opening new avenues of research and (2) improving the use and adoption of technology by practitioners. Given this goal, corroborating academic research with practitioners\u2019 knowledge is essential. In fact, where formal research can provide a systematic and robust set of information, grey literature can augment that information with concrete and applicable strategies useful to both worlds [120]. On the other hand, academic research may shine precisely in disproving false beliefs born in the practitioner world. In summary, the goal of this paper is certainly to provide knowledge that is useful to both worlds, and that is as true and reliable as possible.\n\nRegarding question seven, we found significant evidence about the extensive adoption of bots by practitioners, consequentially increasing the importance of the context on the matter [144, 151]. Beyond that, the professional world\u2019s commitment to process automation has led to the development of many frameworks to support the use, development, and adoption of bots.\n\nLast, including grey literature can provide current perspectives and address the gaps in formal academic literature. First, it helps to avoid publication bias, although it is noted that the grey literature accessed may not represent all unpublished studies. Moreover, excluding grey literature would risk losing critical insights and perspectives on the topic, a conclusion also observed in the case study presented in the guidelines papers [120, 137].\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# Goals and Research Questions Definition\n\nOur main objective was to provide an overview of the use of bots and conversational agents for software engineering purposes. We investigated four aspects that led to multiple research questions to reach our goal.\n\nFirst, we aimed to investigate the reason behind using bots in software engineering; our goal was to create a taxonomy to categorize bots based on their use. Hence, we formulated our first research question.# RQ 1 \u2014Motivations and Goals\n\nWhich motivations and goals cause the use of bots and conversational agents in software engineering?\n\nWhile the prevalence of bots and conversational agents has been growing significantly in the recent past [144, 151], developing, adopting, and interacting with them is still problematic [A1, A156]. For this reason, the second research question aimed at categorizing and mapping the challenges faced by practitioners, along with possible solutions.# RQ 2 \u2014Challenges\n\nWhat are the challenges related to using bots and conversational agents in software engineering?\n\nBased on the considerations above, we proceeded with our analysis by extracting a list of known best practices from the formal and grey literature, with the goal of helping practitioners adopt and develop bots for engineering purposes. Moreover, we aimed to identify possible solutions to the limitations elicited while answering the second research question.# RQ 3 \u2014Best Practices\n\nWhat are the best practices for using bots and conversational agents in software engineering?\n\nFinally, the last research question aimed to investigate the impact of adopting bots, collecting and reporting the benefits of their usage. This led us to the definition of our last research question.# RQ 4 \u2014Benefits\n\nWhat are the benefits of using bots and conversational agents for software engineering?# 3.1.2 Search Phase\n\nThe Search Phase consists of two sub-phases.# Data Sources Selection\n\nThis phase aims to identify the most reliable databases to extract the literature and start our process. This step differs between formal and grey literature.\n\n- Formal Literature. To collect the formal literature, we selected Scopus, IEEE Xplore, and ACM Digital Library as data sources. These databases are considered the top three among the computer science formal literature sources, other than have been used in various other literature reviews [119, 121, 125, 131]. Furthermore, they were recommended in the guidelines [120, 130] since they can provide a complete overview of the published research. As recommended by [120, 130], we took all the results extracted from the first research and filtered them using eligibility criteria.\n\n76 Scopus website: https://www.scopus.com/home.uri\n\n8 IEEE Xplore website: https://ieeexplore.ieee.org/Xplore/home.jsp\n\n9 ACM Digital Library website: https://dl.acm.org/\n\nThe top list of computer science research databases: https://paperpile.com/g/research-databases-computer-science/\n\nManuscript submitted to ACM# Grey Literature\n\nTo search the grey literature, we used the Google search engine. The reason behind our choice is that Google (1) has been used by other MLRs on Software engineering [119, 121, 125], and (2) is recommended in various MLR guidelines [109, 120]. However, it is worth noting that deciding when to stop the search process is a complex problem\u2014mainly caused by the high volume of available items [120, 142]. For this reason, we adopted two different choices\u2014as recommended by Garousi et al. [119\u2013121]:\n\n- We stop the research when no new concepts emerge from the search results anymore, i.e., Theoretical saturation;\n- We applied the Effort Bounded that regards the inclusion of the top N search engine hits only. We analyzed the first ten pages produced by Google, observing that its algorithm retrieves and shows the most relevant results in the first few pages.# Search String Design\n\nIn this phase, we defined different search strings to collect the formal and grey literature. In particular, we used the knowledge obtained during the initial examination to draft the search string. Its definition was based on (i) key terms on the topic of interest\u2013gathered from relevant papers, (ii) synonyms and alternative terms, and (iii) logical connectors to combine different terms in different ways.\n\nThe initial version of the search string included basic terms such as \u201cbots\u201d and \u201csoftware engineering.\u201d During the preliminary exploration phase (Section 3.1.1), we began to use the string and improve it in accordance with the results obtained. We focused on the individuation of synonyms, which led first to the expansion of the search string for formal literature and then to the detailed definition of the different strings for grey literature.\n\nAs for the formal literature, the final search string is shown in the box below. As it is possible to see, we did not include key terms of our research questions, e.g., \u201cgoals\u201d, since we aimed to collect as much formal literature as possible, even if the phase led to high effort.# Formal Literature Search String\n\n(\u201cbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchatbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchat bot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cconversational agent(s)\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201csoftware engineering\u201d)\n\nAs for the grey literature, following the guidelines provided by [121, 132], we started from a general search string to define a more specific one based on the goal of each research question. The reason behind our choice arose from the noise present in the output that characterizes informal databases, like the Google search engine. Table 2 reports the search strings for the grey literature based on the research questions of our study.# 3.1.3 Eligibility Criteria Phase\n\nIn this phase, the aim is to define a series of criteria to apply after collecting the literature for filtering and identifying those that provide direct evidence based on our research questions [120, 130]. The phase consisted of two sub-phases: (1) Defining the formal literature eligibility criteria and (2) Defining the grey literature eligibility criteria.\n\nAs for the formal literature, the eligibility criteria are divided into three sets that had to be evaluated for each paper extracted from the search phase.\n\n1. Exclusion Criteria, namely, criteria that, if met for at least one of them, decree the elimination of the study from the analysis set;\n2. Inclusion Criteria, namely, criteria that, if met for at least one of them, decree the inclusion of the study into the analysis set;\n3. Quality Criteria, namely, criteria that are used to grade studies and may lead to their elimination if the grade is below a certain threshold.\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# Exclusion and Inclusion Criteria\n\nTable 3 shows exclusion and inclusion criteria. We defined such criteria according to the guidelines by Kitchenham et al. [120, 130] and past literature in the Software Engineering field [125, 132]. Each criterion could be only True or False. Using these filters, we could exclude all preliminary research results, e.g., workshops or posters, and avoid considering duplicated papers derived from the combination of three data sources. Nevertheless, we decided not to exclude workshop papers reported in the Repository for Bot-related Research [108] since these could be highly related to this work\u2019s main reference community. In addition, we included EC6 in order to ensure that the collected papers were concerned with the topic of interest. Moreover, as for other LRs [121, 125, 135, 144, 151], we defined our inclusion criteria to match our objective and research questions. Regarding our exclusion criteria, it is worth discussing the EC5\u2014Studies written before 2014. We decided to apply such criteria based on the findings of previous LRs [144, 151], which report that literature on software bots\u2014particularly in their communication-oriented form\u2014started growing after 2014. The other exclusion criteria are widespread among other LRs, and some have been applied using the filters integrated into the selected databases for data source selection.# Grey Literature Eligibility Criteria Evaluation\n\nConcerning the grey literature eligibility criteria evaluation, we followed the guideline proposed by Garousi et al. [120] combining inclusion and exclusion criteria with quality assessment criteria (Appendix A\u2014Table 1).# Search Strings for Grey Literature\n\n|#|Search String|RQ|\n|---|---|---|\n|0|(\u201cbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchatbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchat bot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cconversational agent(s)\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201csoftware engineering\u201d)|General String|\n|1|(\u201cbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchatbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchat bot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cconversational agent(s)\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201csoftware engineering\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201cgoal(s)\u201d \ud835\udc3f\ud835\udc40 \u201cmotivation(s)\u201d \ud835\udc3f\ud835\udc40 \u201cobjective(s)\u201d)|RQ1|\n|2|(\u201cbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchatbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchat bot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cconversational agent(s)\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201csoftware engineering\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201cchallenge(s)\u201d \ud835\udc3f\ud835\udc40 \u201climitation(s)\u201d \ud835\udc3f\ud835\udc40 \u201cnegative impact(s)\u201d)|RQ2|\n|3|(\u201cbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchatbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchat bot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cconversational agent(s)\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201csoftware engineering\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201cbest practice(s)\u201d )|RQ3|\n|4|(\u201cbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchatbot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cchat bot(s)\u201d \ud835\udc3f\ud835\udc40 \u201cconversational agent(s)\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201csoftware engineering\u201d) \ud835\udc41\ud835\udc42 \ud835\udc43 (\u201cbenefit(s)\u201d \ud835\udc3f\ud835\udc40 \u201cpositive impact(s)\u201d)|RQ4|\n\nNote: The search string #0 was the same as the search string used for the formal literature and was used as a base for the search strings for the grey literature.\n\nManuscript submitted to ACM# Table 3. Exclusion and Inclusion Criteria for Formal Literature.\n\n|Exclusion Criteria|Inclusion Criteria|\n|---|---|\n|EC1 Studies not written in English.|IC1 Studies that report about the uses of bots in Software Engineering.|\n|EC2 Posters.|IC2 Studies that report about the challenges of bots in Software Engineering.|\n|EC3 Workshop papers.|IC3 Studies that report about the best practices in the use of bots in Software Engineering.|\n|EC4 Study not entirely available for free.|IC4 Studies that report about the benefits of using bots for Software Engineering.|\n|EC5 Studies written before 2014.| |\n|EC6 Studies not related to the topic of interest.| |\n|EC7 Duplicated studies.| |# 3.2.1 Studies Selection Phase.\n\nIn the studies selection phase, the aim was to apply the planned strategy to collect the literature [120, 130]. The analysis included all studies published from 2014 to 4/4/2024 (Appendix A\u2014Fig. 2 [133]).\n\nAs for the extraction and selection of the formal literature, we started by running the search string on the three selected databases, e.g., Scopus, obtaining a set of 2108 studies. We imported the results of the three datasets in Excel, and the first two authors of the paper started the inclusion and exclusion process. As a first filtering step, we started deleting duplicates by the three datasets merged. Then, the remaining set of papers was divided in two, and the first two authors started applying inclusion and exclusion criteria on the two separate sets; the authors determined if they include or exclude the paper starting by reading the title, abstract, and\u2014if necessary\u2014the entire body of the studies.\n\nAfter analyzing 20 papers each, the two authors reviewed corresponding work and discussed; no conflict arose, and then the authors continued applying the exclusion and inclusion criteria, resulting in 173 studies included and 1935 excluded. Next, an ulterior session of filtering was applied jointly by all the authors on the 173 studies that remained; this aimed at identifying potential errors in the inclusion process. After this step, we excluded other 64 studies, obtaining a final set of 109 studies. Finally, the first two authors of the paper applied the quality criteria, obtaining a final set of 79 studies from the white literature.\n\nRegarding the grey literature, the first author started using the search strings on the Google Search Engine to collect the literature. For each search string, the first 10 pages were checked, and the items judged related to the work objectives were imported into an Excel file until theoretical saturation was reached [120]. By doing so, we obtained a set of 52 items. Then, the first two authors of the paper started applying all the eligibility criteria simultaneously, as Garousi et al. [120] suggested. Such a process led to identifying 28 items for the grey literature.\n\nAfter completing the extraction and selection process for both the formal and grey literature, we combined the two sets. This final step resulted in a comprehensive collection of 107 studies, which formed the basis for the subsequent data extraction phase.\n\nRegarding the distribution of literature for type\u2014i.e., blogs (for grey literature), conferences, workshops, and journals\u2014most of the works are conference papers (52), while 28 are blogs, 13 are workshop papers, and 14 are journal papers. This is not a surprise, seeing that the research topic specifically concerning conversational agents\u2014is extremely recent and tends to be more relegated to conferences and live discussion settings. Moreover, as expected, a large amount of the literature comes from the International Workshop on Bots in Software Engineering (BotSE); it is the main research community on bots and CAs for Software Engineering purposes. In addition, there are also top venues on Software.\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# 3.2.2 Data Extraction Phase\n\nIn the data extraction phase, we started from our objectives and research questions to define a series of data extraction forms, i.e., sets of questions to identify useful elements to answer a research question in the studies [120, 130]. Aligning with other similar works, we defined the extraction forms jointly\u2014through brainstorming and discussion\u2014and then the first author applied them. We identified and extracted the relevant data using a pre-defined data extraction form from each selected source that we needed to answer the research questions. We also extracted some general information\u2014e.g., authors names, published year, and keywords\u2014as recommended by guidelines on literature review [120, 130]. To validate and test our forms, we conducted a preliminary extraction on a set of 15 randomly selected sources\u2014like in other literature reviews [119, 121, 125, 138].\n\nFrom a practical standpoint, we imported all the papers into a new Excel sheet and added a column for each data form identified. Then, we began extracting information from the papers in random order. If the extracted information was different from what had been extracted previously, we would create a new column in the file under the data form group (following a hierarchical structure). However, if the information extracted from a paper had already been indicated by another paper, meaning the column already existed, we would simply mark the corresponding cell with a sign and add any new information. For example, one of the data forms for RQ1 was \u201cwhat are the reasons for using bots in SE?\u201d Through the paper by Van Tonder and Le Goues [A4], we extracted the information \u201crepair bots, to identify issues in the code and automatically resolve them.\u201d Subsequently, the paper by Monperrus [A5] also reported the same information, so an \"X\" was added to the corresponding cell in the repair bots column.\n\nAll the extracted data are in our online appendix [133]. We maintained traceability between the forms and the research questions as recommended by MLR guidelines [120].# 3.2.3 Data Synthesis and Analysis Phase\n\nIn the data synthesis phase, we synthesized data previously collected to present them to the target audience in a readable and usable way. After extracting and dividing the data between the various form groups, the first two authors analyzed them. In this way, we avoided losing important information or misunderstanding data [120, 130]. If the first two authors were unsure how to synthesize a study\u2019s results, the plan was to ask the remaining two authors for help; this case never occurred.\n\nTo conduct a correct data synthesis, we relied on two qualitative analysis methods, namely, narrative synthesis\u2014i.e., the description and ordering of primary evidence with commentary and interpretation of a set of data\u2014and thematic analysis\u2014i.e., the summary of studies under a set of recurrent themes in literature [114]. First, we used narrative synthesis to perform an initial analysis of the gathered data from the previous phase. Then, we used thematic analysis to classify and categorize the data.\n\nConcretely, we started with the data extracted in the previous phase and began annotating them with descriptions and labeled comments using keywords, such as \u201cRepair bots\u201d and \u201cMaintenance bots\u201d. Subsequently, we identified descriptions that closely resembled each other or appeared to be related and began aggregating them into individual categories. Each category became a new column in the Excel file, and through iterative repetition of this process, some columns were grouped into thematic similarity groups. For instance, \u201cRepair bots\u201d and \u201cBots used for identifying flaky tests\u201d were grouped under the category test bots. To maintain traceability, within the file, all papers contributing to the generation of a category were marked with an \u201cX\u201d corresponding to the paper and the column dedicated to that category.\n\nManuscript submitted to ACM# Lambiase et al.\n\nThe first two authors jointly conducted the steps mentioned above\u2014at least for a subset of the items\u2014to identify possible discussion points to strengthen the final results. At the end of the process, we obtained a new set of insights and information categorized under four macro categories\u2014one for each research question. We reported all our findings in the online appendix for our work [133].# 3.3 Results Reporting Phase\n\nBased on the target audience of this review, i.e., practitioners and researchers, we relied on the guidelines provided by Garousi et al. [120] to enhance the readability\u2014particularly from the practitioners\u2019 side. For such a purpose, we (1) decided to include a practical-oriented Discussion section (Section 5.3), (2) asked practitioners for feedback on our results, and (3) adopted various tools to enhance results summaries.# 4 ANALYSIS OF THE RESULTS\n\nIn the following section, we describe the results achieved for each research question and report some statistics of our papers collection.# 4.1 RQ 1: On the use of bots and conversational agents for software engineering purposes\n\nAs a first goal, we aimed to identify the motivations and objectives behind the adoption of bots for software engineering purposes. Our purpose was to gather general and fundamental pieces of information to (1) define a taxonomy to categorize bots based on their use and (2) better perform the following research step.\n\nOur investigation into answering the first research question (described in Section 3) led to the identification of 13 fundamental motivations for adopting bots and conversational agents in software engineering. In order to provide a more comprehensive and understandable taxonomy, we categorize each of the fundamental motivations into 4 macro categories of bots (Depicted in Appendix B\u2014Fig. 1 [133]). The identified motivations are not mutually exclusive; the same bot can be used for different motivations according to a plethora of reasons, e.g., working roles, needs, and types of projects. In the following section, we elaborated on each category and associated motivations, corroborating the discussion with the primary studies and grey literature supporting them.# 4.1.1 Software Product Bots\n\nIn this category, we group all the bots and associated motivations of use used to perform automatic operations on source code or related artifacts and activities. We identified the following sub-categories:# Development\n\nThis sub-category refers to bots that are used to generate source code instead of developers or other practitioners. In our investigation, we found 11 studies, 9 from the white literature [A6\u2013A13] and 2 from the grey one [B80, B81] (\u2191 10.28% of the entire dataset).\n\nPart of the literature focuses on the use of bots (in most cases empowered by Generative AI) to automatically write source code [A9, A13][B80, B81] and code for machine learning purposes [A9]; in this way, practitioners can save time and increase productivity. Nevertheless, Generative AI is still raw in this task, and software engineers do not rely heavily on it [A9, A13][B80, B81]. Another part of the literature combined bots and models-based language to generate source code and entire applications [A6, A7, A10\u2013A12, A14]; in this context, bots act like facilitators in creating models representing complex systems and modules. Interestingly, bots to generate smart contracts [A7, A12] and recommendation systems [A11] (thus, bots used to generate other bots) are being proposed.# Refactoring\n\nThis sub-category refers to bots that are used to (1) improve source code and (2) avoid and/or resolve technical debts by means of refactoring operations, i.e., Refactoring Bots. Our dataset has 11 studies, 8 from the white.# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# Testing\n\nThis sub-category refers to bots that support testers and similar roles during the testing activity and for executing test code. Our dataset has 11 studies, 10 from the white literature [A4, A5, A17, A19, A22\u2013A27] and 1 from the grey one (\u2191 10.28% of the entire dataset).\n\nMost of the literature regards the use of bots for automation of test cases execution [A4, A5, A17, A22, A23, A26, A27]. Furthermore, we found that many bots are used to identify bugs and perform automatic fixes [A4, A5, A17, A19, A22, A23][B82], i.e., Repair Bot. Last, some works propose bots to (i) support developers in writing test code [A27] and (ii) implement heuristics to detect flaky tests [A22]\u2014i.e., tests exhibiting both a passing and failing behavior when run against the same code [136].# Configuration\n\nThis sub-category concerns bots used to support practitioners during the setup and configuration processes of new software projects. Our dataset has 6 studies, 4 from the white literature [A19, A22, A28\u2013A30] and 1 from the grey one [B84] (\u2191 5.61% of the entire dataset).\n\nFrom the identified literature, two studies propose bots for automatically creating/modifying software configuration files or similar [A28, A29], and four studies [A19, A22, A30][B84] report about bots used to manage and monitor project dependencies.# CI/CD\n\nThis sub-category regards the usage of bots to support everyday activities in the context of CI/CD. In our investigation, we found 8 studies, 4 from the white literature [A22, A25, A31, A32] and 4 from the grey one [B82, B84\u2013B86] (\u2191 7.48% of the entire dataset).\n\nSome studies analyze the usage of bots for monitoring software systems (for example, on GitHub) and performing automatic operations if certain conditions are met (e.g., automatically build the system on branch merge) [A22, A25]. Finally, five works study the use of bots to automatically trigger actions when specific criteria\u2014related to no-code platforms\u2014are met [A22, A31, A32][B82, B85]. For example, automatically creating an issue on GitHub when a bug is reported, or a task is assigned in the collaboration platform of the team (e.g., Trello and Jira). Moreover, some bots to support DevOps-related activities started to emerge [B86].# 4.1.2 Software Process Bots\n\nUnder this category, we group all the bots used to improve and enhance communication, collaboration, and management activities. We identified the following three sub-categories:# Team\n\nThis sub-category refers to bots used to support practitioners in handling team quality attributes like turnover and socio-technical aspects. We found 6 studies, 3 from the white literature [A33\u2013A35] and 3 from the grey one [B82, B85, B87] (\u2191 5.61% of the entire dataset).\n\nRegarding the motivations, some bots can be used to support management activities such as risk management [B87] and community smells management [A34], i.e., anti-patterns precursors of social debt [152]. Moreover, a study proposes to use bots specifically to reduce turnover\u2014the phenomenon of continuous influx and retreat of human resources in a team [116]\u2014in an open-source software community by improving the reviewer selection process in code review [A33]. More related to the open-source communities, bots are used to help onboard new contributors [B82] and alleviate burnout and stress [A35][B85] by\u2014for example\u2014supporting core contributors\u2019 activities.# Communication and Collaboration\n\nThis sub-category concerns bots used to help communication and collaboration between practitioners. During our investigation, we found 8 studies: 7 from the white literature [A6, A8, A33, A36\u2013A39] and 1 from the grey one [B85] (\u2191 7.48% of the entire dataset).\n\nTwo studies reported that practitioners use bots to facilitate meetings and communication in community chats, encouraging team participation [A37][B85]. Bots can also assist managers with scheduling and planning meetings [A36]. Additionally, bots help detect when two individuals work on the same artifact, preventing code conflicts [A38]. Bots can record past decisions to avoid information loss and support future decisions [A6, A8]. They also identify experts to reduce turnover and assist newcomers [A33, A39].# Task\n\nPractitioners adopt bots to support activities related to task management, e.g., evaluation, assignation, and planning. During our investigation, we found 5 studies: 3 from the white literature [A36, A40, A41] and 2 from the grey one [B87, B88] (\u2191 4.67% of the entire dataset).\n\nSome of the bots are adopted to find the right person for the right task by using metrics and developers profiling [A40, A41][B87]. More related to the managers, a work proposed an all-round bot for task management assistance; the focus of such a bot is task monitoring, issues management, planning, and also discussion of task characteristics to assigning it to the right person [A36].# Code Review\n\nThe review revealed that practitioners use bots to support the code review process, often associating it with pull requests. Our dataset has 6 studies on this topic, 5 from the white literature [A33, A42\u2013A45] and 1 from the grey one [B85] (\u2191 5.61% of the entire dataset).\n\nIn general, researchers report that the adoption of such bots could effectively benefit software projects in different ways [A33, A42, A44]. For example, turnover decreases, and communication becomes more effective; moreover, pull requests increase in quality, and contributions to open-source projects are more rapid than without the bots. Nevertheless, the risk of introducing noise exists; some studies proposed the adoption of bots to orchestrate code review bots and summarize their activity and outputs more concretely and cleanly [A43, A45].# 4.1.3 Knowledge Bots\n\nUnder the category Knowledge Bots, we group all the bots used to store, share, and manage information and knowledge about the software project. We identified the following sub-categories:# Documentation\n\nPractitioners use bots to support documentation writing. Our dataset has 10 studies regarding this type of bot, 9 from the white literature [A6, A7, A22, A24, A28, A29, A46\u2013A48] and 1 from the grey one [B85] (\u2191 9.35% of the entire dataset).\n\nSeveral works focus their attention on bots used to automatically create and update documenting artifacts [A6, A28, A29][B85] (e.g., code comments, documents, and reports). The state of the art [A7, A22] has proposed bots to automatize the writing of notes for new software product releases, using NLP to synthesize developers\u2019 commit messages and comments. Moreover, practitioners started to use Generative AI to write code commits messages [A47, A48]. Other studies propose bots to improve and speed up agile processes by automatizing the agile retrospective step by collecting metrics from different sources [A7, A46]. Last, bots are also used to record and document design decisions and, for some of them, generate static or behavioral models on-the-fly [A6, A7] (e.g., UML graphs).# Metrics\n\nBots in the Metrics category are used to monitor the team environment (both product and process) and collect metrics. We found 7 studies related to this category, 4 from the white literature [A22, A25, A49, A50] and 3 from the grey one [B86\u2013B88] (\u2191 6.54% of the entire dataset).\n\nSome studies are related to collecting and analyzing product and process metrics, allowing practitioners to make informed decisions [A49, A50][B87, B88]; this is particularly important when dealing with services and micro-services.# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# 4.1.4 Emergent Bots\n\nIn this last category we have included three reasons for adopting bots that have been emerging in recent times and that we believe are worthy of mention although still in an embryonic stage.# Digital Twins Bots\n\nBots in this category are integrated into the development workflow to support specific developers and act as companions. We identified 19 studies about this type of bot, 16 from the white literature [A17, A22, A24, A25, A35, A51\u2013A61] and 3 from the grey one [B82, B85, B88] (\u2191 17.76% of the entire dataset).\n\nThe majority of works report that bots are used to support the retrieval of information regarding general implementation questions and, particularly, API documentation and guidelines [A22, A52, A53, A56\u2013A58][B85]. Then, some studies focus on using bots to enforce the application of formatting guidelines or requirements [A17, A59][B82] and good programming practices [A51][B82]. Furthermore, some bots are also used like actual peers during the programming activity (pair-programming bots) [A60, A61]; among these, some are used to detect possible implementation choices that could lead to defects in the code and propose solutions [A55]. Related to the open-source field, some bots act like peers to recommend useful tools to solve a problem [A54], support elite developers in general tasks, and help them stay focused on the work by eliminating noise [A35].# Orchestration Bots\n\nDespite being useful for practitioners, a well-known reported problem with bots is the noise arising from their use. In short, since bots are still unable to behave according to the operational context, if their interaction is not well designed, it could result in them providing too much information to practitioners, often unhelpful, causing distraction and discomfort [A1, A25, A63, A134][B85]. To solve this problem, some practitioners operationalize bots specifically designed to orchestrate and summarize the results of other bots. In such a sense, these Orchestration Bots act like bot managers. We identified 2 studies about this type of bot, both in the white literature [A43, A45] (\u2191 1.87% of the entire dataset).\n\nWessel et al. [A43] proposed a series of guidelines for developing these types of bots, while Ribeiro et al. [A45] created F/\u2019\u2019$&B+#, a bot designed using the previously mentioned guidelines to orchestrate pull-request bots.# Technology Transfer Bots\n\nLast but not least, and more attractive for researchers, some bots are starting to be used to support the technology transfer between research and the practitioner field. Indeed, the limit of research-developed tools is often the poor effort invested in developing their non-functional requirements, resulting in poor quality. Some bots are starting to be developed specifically to surpass such a limitation. We identified 3 studies about this type of bot, 2 in the white literature [A31, A34] and 1 in the grey one [B85] (\u2191 2.80% of the entire dataset).\n\nIn this sense, Beschastnikh et al. [A31] and Voria et al. [A34] proposed an ecosystem of bots specifically designed to integrate other research-originated bots and make them more easily available to and usable by practitioners.# 4.2 RQ2: On the challenges of bots and conversational agents for software engineering purposes\n\nThe diffusion of bots and conversational agents for supporting software development processes has also led to the emergence of new challenges for computer scientists. For this reason, we investigated the challenges related to the interaction with, adoption, and development of bots for software engineering purposes.\n\nRegarding the existing literature, Wessel et al. [156] identified a series of challenges in the interactions with software bots in the open-source development context, with a particular focus on the concept of \u201cnoise\u201d\u2014namely, annoying bot behaviors, e.g., verbosity and unsolicited actions, which lead to deterioration of communication in teams.# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# 4.2.1 Interaction Challenges\n\nMost of the challenges identified by the literature regard the interaction between practitioners and bots. Such interaction can be intended as unidirectional\u2014the user receives a message from the bot and reacts to it\u2014and bidirectional\u2014the user and the bot actively exchange messages. During our investigation we found 27 studies; 17 from the white literature [A1, A3, A19, A25, A39, A43, A61, A63\u2013A71, A134] and 10 from the grey one [B81, B85, B86, B89\u2013B95] that report on this challenge (\u2191 25.23% of the entire dataset).# Interruption and Noise\n\nOne of the most frequently noted challenges involves certain bots exhibiting a tendency towards verbosity or producing irrelevant output [A1, A25, A43, A63, A70, A134][B85]. This issue, commonly referred to as \u201cnoise\u201d in the literature, can significantly affect bot usability as well as team communication and collaboration. The origin of this behavior often lies in the bot\u2019s design\u2014sometimes, bots are intentionally programmed to \u201charass\u201d developers. On the other side, errors in the bot\u2019s operation can also result in unexpected and disruptive noise.# 4.2.2 Adoption Challenges\n\nSeveral studies reported challenges in adopting bots and chatbots for development reasons. We found 19 studies: 12 from the white literature [A1, A4, A5, A20, A22, A25, A30, A54, A60, A72\u2013A74] and 7 from the grey one [B81, B84\u2013B86, B92, B94, B95] that report on such a category (\u2191 17.76% of the entire dataset).# Motivation and Trust\n\nAccording to various studies, the first challenge involves motivating practitioners to use bots [A1, A5, A25, A74][B84\u2013B86, B92]. In fact, various practitioners are skeptical of bots because of bias\u2014they think they can implement malicious behaviors or are useless and noisy, underlining a lack of trust [A5, A20, A25, A30, A54, A60, A72\u2013A74][B84\u2013B86, B92] of developers in bots\u2019 recommendations. Furthermore, the problem of trust becomes harder when dealing with Generative AI-powered bots. In fact, such AI is notoriously subject to hacking actions performed by malicious persons and aimed at influencing their behaviors [B94, B95] by introducing bugs or defects in the source code [B86]. Furthermore, the risk of unexpected privacy violations [B86] and the lack of transparency in personal data management [B81, B94] constitute a severe limit to the adoption of Generative AI-powered bots.# Discoverability and Customization\n\nFurthermore, identifying the correct bot to solve a problem is challenging [A1, A22] for two reasons: first, there are limited search mechanisms to identify appropriate bots given a problem; second, the lack of a unified standard to characterize bots leads to difficulties in their description and categorization. This becomes even more problematic when practitioners identify the correct bot but have difficulty configuring it [A1, A4, A30][B86]. In fact, such systems often have few configuration options, even if they have to be used in significantly different contexts.# Terms of Services\n\nThe adoption of bots can be hindered by the specific terms of service of the platforms they are intended to operate on, which may restrict their functionalities [B84]. This legal and regulatory environment can severely limit the actions that bots are allowed to perform, thereby reducing their effectiveness and potential benefits. As a result, practitioners often hesitate to integrate bots into their workflows due to concerns over compliance and the potential legal ramifications of bot activities that violate these terms. This fear of inadvertently breaching platform policies can lead to a reluctance to adopt bot technology despite its potential to enhance productivity and efficiency.\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# 4.2.3 Development Challenges\n\nThe last category of identified challenges concerns with the development of bots. Specifically, we found 19 studies, 13 from the white literature [A1\u2013A3, A26, A39, A48, A61, A64, A68, A72, A75\u2013A77] and 6 from the grey one [B90, B96\u2013B100] that report on such a category (\u2191 17.76% of the entire dataset).# Framework, Architecture, and Integration\n\nFirst of all, various actors have proposed various frameworks to support developers in the development of bots and chatbots (e.g., Microsoft Azure Bot Framework, Google DialogFlow, Amazon Lex, and Rasa). Although such variety can provide practitioners with a large plethora of possibilities to reach their objectives, the literature identifies a difficulty in identifying the correct framework to start development [A1, A48, A64, A68, A75, A77][B96\u2013B99]. Such a challenge arises for three main reasons. First, the enormous plethora of frameworks that implement the same functionalities makes it hard to select the better one. Second, the lack of readable documentation for each framework ulteriorly hardens the decision process. Third, bots are complex systems to design from an architectural point of view [A3, A48, A72, A77][B100] because, to improve modularity, engineers push to adopt client-server architectures in which the core functionalities are implemented on the server and the interaction with the user on the client side. This, in addition to adding complexity due to the management of communication protocols between the various components (e.g., HTTPS, API Call, and Service Calls), often makes it necessary to use different technologies for the same system, and not all bot frameworks allow easy integration with them [A48, A64, A68]. Furthermore, all the previously mentioned problems become harder when practitioners deal with AI-powered bots; in this type of bot, in addition to the integration of the already numerous technologies involved, it is necessary to integrate artificial intelligence systems in a way that meets the required nonfunctional requirements [A48].# Testing\n\nFurthermore, despite such a large amount of frameworks, a significant minority support developers in testing activities of bots and CAs [A3, A26, A64, A76][B90]. This is a significant problem because the lack of testing for such a system contributes to the mistrust of practitioners when they have to use them. Underlying this lack is the difficulty in automating the interactions between a human user and the bot. Furthermore, the large number of frameworks for bot development makes it difficult to provide a single tool capable of testing for all.# Maintenance\n\nSuch a challenge in terms of testing leads to the emergence of another one: the maintenance\u2014and consequent evolution\u2014of bots is challenging [A1, A2, A26]. In fact, the lack of tests and the difficulty in monitoring such systems can impact maintenance activities, making it hard to identify and fix a bug.# Training\n\nTraining AI-powered bots for natural language understanding and processing is a complex endeavor, especially within the specific context of software engineering. Obtaining suitable datasets for training these tools is a significant challenge [A3, A39, A61, A68, A69]. Not only are these datasets scarce, but they are also particularly difficult to acquire when tailored to the nuanced requirements of software engineering [A48, A61]. This scarcity and specificity of data lead to considerable difficulties in effectively training AI modules to perform their intended tasks.# 4.3 RQ3: On the best practices to use bots and conversational agents for software engineering purposes\n\nBesides collecting limitations and challenges for software bots, we were also interested in the possible best practices to overcome them. For such a purpose, we formulated our third research question to extract a list of best practices from the formal and grey literature to help practitioners adopt and develop bots for engineering purposes. We decided to divide the identified best practices into two categories based on the approach\u2019s scope. In the following, we elaborated on each best practice individually.\n\nManuscript submitted to ACM# 4.3.1 Development and Design Best Practices\n\nAs already reported in Section 4.2.3, the large number of available technologies and the need for standardized methodologies make developing and designing bots challenging. In the following, we report the best practices identified by researchers and practitioners that can facilitate the implementation of bots and conversational systems (Appendix B\u2014Table 1 [133]).# Follow a modular architecture\n\nDespite their significant heterogeneity, bots\u2014particularly conversational agents\u2014are characterized by the need to work similarly on different platforms. For example, CAs should be used on different communication platforms\u2014e.g., Slack and Discord\u2014but still implement the same functionalities. In these situations, a client-service architecture\u2014or derived ones\u2014represents the best choice [A2, A65]. Srivastava and Prabhakar [A3] defined a reference architecture for conversational agents which specifies that these systems should be divided into two macro-components:\n\n1. Conversational Subsystem, which implements the input analysis, the extraction of user intent, and the response;\n2. Business Component, which contains the logic to fulfill user intents and is implemented like an API.\n\nAnother possible architecture [B100] can organize chatbots into five modules:\n\n1. Environment represents the core Natural Learning Process (NLP) engine and context interpretation.\n2. Question and Answer System is where the system interprets the question and responds with relevant answers from the knowledge base.\n3. Plugins is the module that uses API calls to interrogate other systems for executing bot operations.\n4. Node Server is the module that handles the traffic requests from users and routes them to appropriate components.\n5. Front-end Systems are the possible client-facing platforms users can use to interact with the system.# Make bots adaptive and able to learn over time using AI\n\nSoftware bots, specifically in the figure of CAs, need to communicate with a large plethora of users characterized by different cultural and language characteristics. Therefore, in most cases, such interaction and the bot\u2019s user understanding rely on an NLP module\u2014that uses datasets composed of possible phrases and assertions in different languages\u2014to interpret the user\u2019s intent. However, the dataset employed should increase over time based on the user\u2019s usage. For this reason, implementing a machine learning system able to learn over time and modify the dataset accordingly to new conditions is needed\u2014namely, active learning systems [148]\u2014becomes necessary [B85, B89, B90, B94].# Adopt a specific lifecycle process for bots\n\nBots are complex systems to design, both from a technical and logical side, especially if integrated with a conversational module, i.e., like for CAs. Consequently, practitioners focused their attention on defining rigorous steps to orchestrate these systems from a development and maintenance perspective [B101\u2013B103]. In the following, we reported the elicited steps for conversational agents development:\n\n1. Clearly defines the process to be automated or supported by the CA;\n2. Catch the target audience\u2019s needs by engaging with software developers through interviews and usability testing sessions to identify common challenges they face during the development cycle;\n3. Use existing sources to analyze competitors by conducting surveys of similar tools already existing or interviewing the potential audience;\n4. Define audience intents using use cases and journey maps after conducting individual interviews with the audience;\n5. Design the interaction workflow for each intent by using modeling languages, e.g., UML;\n6. Choose technologies for the CA development and operation platforms according to company needs;# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering\n\n1. Design the CA architecture to be modular as much as possible and adherent to the guidelines proposed by researchers [A2, A3, A65][B100];\n2. Develop a measurement framework to evaluate the CA by establishing metrics such as reduction in time to fix bugs, frequency of use, and developer satisfaction to assess the impact of the CA on the development process [A35, A49, A78][B85, B100];\n3. Implement an artificial intelligence (NLP, NLU, and active learning) module capable of understanding technical language specific to software development, using, for example, libraries that can parse and make sense of code snippets and technical queries [A61];\n4. Implement the CA application and interaction logic in a way that can handle diverse inputs from developers, such as textual descriptions, code blocks, or even spoken commands, and provide appropriate responses or actions [A35, A54, A60, A67, A79, A134][B85, B101\u2013B103];\n5. Deploy the CA and adopt a continuous improvement process.\n\nCarefully select the correct framework based on bot requirements. As mentioned in Section 4.2.3, the increasing adoption of bots and conversational agents in software development leads diverse actors to develop frameworks for supporting their development. Some examples are Azure Bot from Microsoft\u2014able to provide a single environment to develop bots for different communication channels\u2014or Rasa\u2014an open-source solution to develop bots and train NLP models all in the same place. Nevertheless, determining which framework best fits a project\u2019s requirements is far from easy [A1, A64, A68, A69, A75][B96\u2013B99]. To address this challenge, practitioners identified criteria and key motivations that should guide while selecting the development framework, i.e., core technology, the target platform of operation, and the distinctive characteristics [B96\u2013B99]. We reported the main framework with associated motivations in Appendix B\u2014Table 2 [133].\n\nAdopt Domain Specific Language Models to develop bots. Developing bots requires expertise in several areas, such as software engineering and natural language processing. Although frameworks and platforms simplify the implementation of CA, there is still a significant obstacle caused by the inability of such frameworks to support all the developers\u2019 desires. Some studies suggest approaches based on domain-specific languages (DSLs) for model-driven development, reducing the workload of developers and designers [A77]. Beyond that, such studies indicate that there are already bots developed to support the developer in building models that can then be used to develop systems\u2014e.g., recommendation systems\u2014in an automated manner [A11].\n\nMake the bot able to save and use contextual information using RAG and Vector DBs. Maintaining contextual information is critical for adopting bots in software engineering [A1, A19, A67, A70][B85, B90]. Integrating Retrieval-Augmented Generation (RAG) models and Vector Databases can address this challenge [A71, A79][B93]. RAG models combine retrieval and generation capabilities to access and use context-specific information effectively. Vector Databases store and query contextual data embeddings, enabling precise and context-aware responses. This synergy ensures chatbots provide more meaningful and context-rich conversations, enhancing the overall user experience.\n\nConduct Wizard of the Oz studies to create datasets for NL modules. Wizard of Oz studies create datasets for training NLP and NLU models for developer-support bots [A61]. Human operators simulate AI responses, allowing researchers to collect nuanced language data from developers. This method captures realistic insights into the support developers need and the queries they pose. Despite the complexity of these studies, collecting extensive data is essential for developing effective models [B94].\n\nManuscript submitted to ACM# 4.3.2 Interaction and Adoption Best Practices\n\nInteraction for bots is an important\u2014if not the most important\u2014aspect to take care of during their development and design. For such a reason, it is not surprising that the state of the art focused more on best practices to improve how bots communicate with users (Appendix B\u2014Table 3 [133]).# Integrate bots actions in a way that does not interrupt developer workflow.\n\nBots are naturally designed to automate some processes\u2014or part of them\u2014to allow users to waste less time and focus on other tasks [150]. Such automation comes with no problems: first, bots\u2019 actions should be secure to allow users to trust them; second, bots should not interrupt users in their operation, or the advantage of the automation loses its value and usefulness [A1, A5, A25, A43, A45][B84, B85]. For example, many users found tool-recommender-bot interrupted existing processes\u2014most notably by breaking software builds\u2014and were discouraged from merging pull requests from our system [A54]. For the reasons mentioned above, software bots should be designed so that the results of their actions should not slow down or hinder the user\u2019s actions, at least not unexpectedly and covertly [A25, A30, A35, A54, A60][B85, B101\u2013B103]. In recent years, orchestrator bots, i.e., bots designed to manage and guide the actions of other bots, have emerged as an efficient way to integrate a large number of bots easily without interrupting the practitioner\u2019s workflow [A43, A45].# Allow developers to edit bot configuration easily.\n\nAs mentioned in Section 4.2.2, configuring bots to allow their operation in different contexts is a crucial challenge [A1, A4]\u2014both for their adoption and development. Therefore, developing the system providing users with easy ways to configure it\u2014e.g., editable configuration files, graphical interfaces for configuration, and exhaustive documentation\u2014could be extremely useful in increasing bot adoption [A4, A16, A25, A30, A35]. Moreover, Erlenhov et al. [A25] demonstrated that such a best practice could mitigate the Interruption and Noise problem [A1]\u2014elaborated in Section 4.2.1\u2014through a design or configuration that is mindful of whom to notify of which events.# Provide bots with personality.\n\nUser perception is essential for bots. Various studies [A5, A60, A134] and grey literature [B101\u2013B103] reported that this perception can be highly influenced by the bot\u2019s language register\u2014as well as by its name and graphical elements. Specifically, ensuring that the bot is recognizable and distinguishable, i.e., providing it with a distinctive personality, can help in its adoption and use [A60, A134]. Therefore, the bot\u2019s language should be casual, accessible, and fun; in a way that is not dull but friendly. Nevertheless, it should be paid attention to not trigger the uncanny valley effect, i.e., the phenomenon whereby a computer bears a near-identical resemblance to a human being and evokes a sense of unease or aversion in the person interacting with it [129, 140].# Design a Smooth and frictionless interaction.\n\nVarious works identified and analyzed the phenomena of \u201cnoise\u201d\u2014i.e., annoying bot behaviors that lead to deterioration of communication in teams [156]. Consequently, designing a smooth and frictionless interaction becomes mandatory, and this could be achieved by carefully planning conversational flows, especially for conversational bots. This can be achieved by (1) allowing bots to detect dead ends in conversations and prompt users by giving hints on how to continue the interaction and (2) implementing mechanisms to provide continuous feedback to the user that the bot is listening [A35, A54, A60, A67, A134][B85, B101\u2013B103]. Moreover, designing bots to be proactive\u2014maybe integrating Generative AI in them\u2014is a successful way to please practitioners and make their interactions smooth [A79]. However, this can be achievable if the natural language comprehension of the bot is well enough designed and developed [A67]. Moreover, designers should evaluate the use of graphical user interface elements\u2014e.g., cards, buttons, and boxes\u2014to facilitate the interaction and make the conversation more efficient [A134].\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering\n\nEnforce transparency in bot actions and outputs. Trust is a vital aspect of adopting bots for software development since such systems are often designed to automate product development and release operations, which are considered vital to the success of a software project. The first way to improve it is related to the uncanny valley effect: when users interact with a machine that seems too human to them, they begin feeling a sense of unease and revulsion [129, 140]. To avoid such a situation, developers should ensure that the bot\u2019s software nature is always apparent and that situations perceived as ambiguous by the user are never created [A5, A30, A35, A134][B85]. Moreover, it should always be evident when the user is asked to take action and why the request is made. For example, implementing graphical elements that are activated only at the moment when the user has to give input can be an excellent way to request an action overt [A5, A134][B85].\n\nIntegrating concepts from behavioral science in interaction flow. As already said, recommendation bots\u2014namely, bots designed to provide user tips and recommendations\u2014are largely adopted for software development purposes. Nevertheless, such bots\u2019 recommendations seem to be ineffective if the interaction with the users violates social contexts within software engineering and interrupts the development workflow of programmers. For this reason, Brown and Parning [A72] proposed adopting behavioral science concepts to design and guide the interaction flow of conversational systems\u2014e.g., nudging [153] and choice architecture [154]. Notably, elaborated in their work, they use 11 practical tools\u2014provided by Johnson et al. [126]\u2014to integrate choice architecture [154]\u2014the way options are organized and presented to humans\u2014in recommendation bots and consequentially improve the recommendation process. From a practical point of view, bots should be designed according to three interaction principles:\n\n- Actionability. It refers to the ease with which users can act on recommendations. To improve developers\u2019 perception of recommendations, better choices should be easier to take and implement.\n- Feedback. It refers to the way pieces of information are provided to the user. To improve developers\u2019 perception of recommendations, useful and essential information should be provided to the users.\n- Locality. It refers to the setting\u2014both physical and temporal\u2014surrounding the context of developer recommendations. To improve developers\u2019 perception of recommendations, they should be presented during their working hours and when they are in the workplace.\n\nMoreover, a study performed by Robe et al. [A60] demonstrated that practitioners are more inclined to adopt and use bots that demonstrate adaptability, motivation, and social presence skills; such characteristics support developers\u2019 trust.\n\nProvide users with a high level of control over bot. Another way to improve interaction is through nudges\u2014i.e., any aspect of the choice architecture that alters people\u2019s behavior in a predictable way without forbidding any options or significantly changing their economic incentives [153]. As a way to use nudge for conversational systems, Brown et al. [A54] reported that making easy and cheap recommendations\u2014instead of forcing updates and changes to developers\u2014could improve the users\u2019 perception of recommendation bots [A16, A19, A25, A35, A54, A67][B85]. For example, in the context of a bot that analyzes the source code to find quality problems, making comments above the incriminated line of code instead of automatically refactoring it could be a better choice. Another way to provide a sense of control to the user\u2014as proposed and reported by Erlenhov et al. [A25]\u2014is to make the bot ask for confirmation before starting to work on a task or at the end of interactions\u2014in a transaction-fashion\u2014to avoid misunderstanding.\n\nCreate a reliable test infrastructure. An alternative way to make developers trust a development bot is to provide it with a reliable and public test infrastructure [A25][B90]. In such a way, the potential audience of the tool can evaluate it in advance and feel more comfortable with it. Nevertheless, testing bots can be challenging due to the need to simulate.# Lambiase et al.\n\nAll possible user interactions [A3, A26, A64, A76][B90]. In order to facilitate such activity, practitioners propose to (1) use one of the provided frameworks for bots\u2014and chatbots\u2014testing\u2014e.g., testYourBot10 and Dimon11 \u2014and (2) develop and perform manual testing of bots based on their interaction flow [B90].# Integrate a feedback system allowing users to evaluate bot choices.\n\nMost problems that characterize software bots regard how systems interact with users. In most cases, these issues could be fixed by asking users what they want and what the specific problem is; therefore, implementing a system for practitioners to provide feedback and evaluate bots\u2019 actions is highly recommended. Although an automatic approach based on heuristics would be the most appropriate solution\u2014but not the easiest one to implement\u2014in which the user is directly asked to provide opinions can also reach the goal [A35, A49, A78][B85, B100].# Adapt the bots to the privacy policies of organizations.\n\nTo enhance trust in bots using generative AI, developers should implement adapters at the bot interfaces. These adapters control the bot\u2019s access to information and responses, aligning operations with privacy policies [B94] and ensuring confidentiality and user consent. Configured to limit sensitive data collection, adapters reduce data breach risks and support data minimization [B94]. This integration enhances compliance with privacy regulations and bolsters trust in the bot\u2019s application.# 4.4 RQ 4: On the benefits of bots and conversational agents for software engineering purposes\n\nIn this RQ, we aimed to identify the benefits for development teams of adopting bots for software engineering purposes. Specifically, our goal was to determine which areas are most impacted by bots to identify possible fields yet to be explored and propose future steps for research to deepen (Appendix B\u2014Table 4 [133]).# 4.4.1 Productivity Benefits.\n\nMost of the benefits provided by bots regard the productivity aspects of software development. This is because bots have been designed initially to automate processes considered unimportant and tedious for practitioners [A8, A27][B104 \u2013B106]. For this reason, it does not look surprising that the most positive impact of their adoption\u2014recorded by researchers and practitioners\u2014is related to the productivity.\n\nFurthermore, bots can help developers complete tasks related to meaningful goals for the project [A6, A8, A22, A35, A40, A42, A45, A134][B87, B88]. Such a benefit is defined by Storey et al. [A22] under the title of effectiveness and can be achieved in various ways. For example, by automatically capturing data and disseminating them to the entire team, bots can increase the developers\u2019 awareness of the project situation [A22, A134]. An ulterior way to improve developers\u2019 effectiveness is through conversation moderation\u2014i.e., providing information during meetings or bringing the discussion back into focus when participants tend to digress [A6].\n\nBots can also impact productivity by helping developers complete tasks more quickly [A6, A8, A13, A15, A19, A22, A27, A35, A42, A44, A45, A47, A51, A52, A57][B86, B104, B105]\u2014this is identified by efficiency [A22]. Such a benefit can be reached in many ways: (1) bots can spare developers from reading large amounts of documentation by understanding what they search for and providing it immediately [A22, A52]; (2) bots can support during source code writing by providing code snippets or tips [A15, A51][B105]; (3) bots can also help in documentation phases, by collecting pieces of information from different sources and summarizing them [A6][B104].\n\nIn practice, bots are also used to perform simple tasks, e.g., building a new version of a software project, executing test suites, and preparing execution reports, thus implying an increase in workers\u2019 free time and optimizing the execution.\n\n10 testYourBot site: https://shankar96.github.io/testYourBot/\n\n11 Dimon site: http://dimon.co/\n\nManuscript submitted to ACM# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# 4.4.2 Collaboration Benefits\n\nBots also provide benefits when it comes to knowledge sharing and collaboration in development teams [A6, A8, A37, A44, A46, A134][B86, B87, B104]. For example, if correctly used, bots can distribute to the team the status of other tasks, consequently building team trust and cooperation [A134]. Furthermore, bots can enhance the recording of decisions, performing: (1) automatic fulfillment of artifacts templates (e.g., user stories), (2) summary build from discussions, (3) automatic generation of diagrams, and (4) monitoring of conversations to automatically refer previous decisions. Moreover, bots can enhance developer collaboration by performing (1) recording and retrieving technical discussions, (2) fetching and updating models based on conversation, and (3) parsing committed messages. By doing all of these, bots can support the decision-making process of different figures involved in software engineering [A43][B86].\n\nFurthermore, by abstracting practitioners from executing repetitive and tedious tasks, there is less chance of mistakes [B104\u2013B107]. This is since (1) humans tend to commit errors if constantly repeating the same task, and (2) software can apply quality standards pre-defined. Moreover, such an abstraction can be used by managers to improve the risk management activity [B104, B105, B107] and facilitate the adoption of new methodologies and technologies in their team [B104, B106]. For example, by using a chatbot to improve code review, managers could provide developers with easy tips and good practices without assigning human effort to such a purpose.# 4.4.3 Technical Benefits\n\nLastly, bots and conversational agents can also provide many benefits from a technical and product view. For example, bots can improve the maintainability and quality of source code by detecting technical debt\u2014e.g., code smells and test smells\u2014or by enforcing good programming practices [A8, A15\u2013A19, A25, A27, A51][B82, B104\u2013B107]. Moreover, bots can collect information from different artifacts\u2014e.g., source code, documentation, and reports\u2014to spread to the entire team and reduce socio-technical problems, e.g., turnover. Furthermore\u2014it is always related to the knowledge-sharing side\u2014bots can also be used to collect information about API and new technologies and provide them \u201con demand\u201d to the users. Such communication can occur in any communication channel used by the development team [A8, A25, A31, A50, A56, A57, A134][B82, B86, B106].# 5 DISCUSSION, IMPLICATIONS, AND FUTURE STEPS\n\nThe results of our study pointed out several observations worth further discussion. We started from them to identify potential future research lines that the research community can explore. Moreover, since the multivocal literature review also comprises gray literature, practitioners can benefit from our observations. In the following paragraphs, we report the observations and implications from our review, organized by topic.# 5.1 Conversational Agents and Human Aspects of Software Development\n\nConversational Agents (CAs) are de facto software systems in which interaction with users represents the main characteristic, thus implying that making the potential audience more involved during design phases is crucial to project success [A1, A3, A19, A25, A39, A63, A64, A66\u2013A68, A134][B85, B89\u2013B92]. Moreover, since most of the reported software bots operate with different stakeholders simultaneously, the interaction patterns between different actors also acquire importance. For this reason, representing collaboration and communication patterns in software communities in a quantitative way could help facilitate the CAs\u2019 interaction workflow design.# 5.2 Conversational Agents for Training and Support\n\nBesides practical uses, the adoption of bots also revolves around knowledge sharing [A22, A24, A25, A31, A41, A49, A50, A52, A53, A56\u2013A58][B82, B85, B87, B88]. Specifically, practitioners use bots to (1) share information between team members and (2) record decision rationale. Moreover, conversational agents are used to provide information on software modules to new developers in order to facilitate training phases. Based on our findings, we can provide the following considerations:\n\n- Bots and Turnover. Bots can be used to collect expert knowledge and provide it to newcomers [B82, B85]. In this way, the consequences of turnover [116]\u2014i.e., the continuous influx and retreat of human resources in a team\u2014can mitigate the progressive loss of knowledge on a software module. From here, researchers should focus on how (i) bots should collect such knowledge and (ii) how this should be provided to newcomers.# 5.3 Addressing Limitations with Best Practices\n\nOur literature survey identified challenges and best practices related to bots and conversational agents [156]. We tried to map such challenges to best practices based on the various papers to provide solutions and mitigation strategies supported by evidence in research and practitioners\u2019 contexts. We reported our findings in Figure 2.\n\n- Challenges and Best Practices. Our mapping\u2014based on literature evidence\u2014should be validated through qualitative studies and could be a valuable contribution for practitioners facing bot challenges.# 5.4 Conversational Agents for Project Management: How far are we?\n\nProject Management is the practice of applying knowledge, skills, methods, and tools to meet the requirements of a specific project, achieving quality and performance goals [124]. In the context of software development, project management is a highly complex and hard-to-standardize activity due to the volatile nature of software and the extreme heterogeneity of stakeholders involved\u2014both in terms of background and competencies [111, 112, 122]. Moreover, according to the Standish Group\u2019s CHAOS Report [127], approximately 70% of software projects fail\u2014some of them with catastrophic consequences [117, 123, 128] and state of the art demonstrated that management aspects significantly impact project outcomes, leading management activities as a priority to consider [110, 113]. Software bots\u2014and particularly Conversational Agents (CAs)\u2014could help improve and support management activities:# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering# Interruption and Noise\n\n- Provide users with high level of control over bot\n- Integrate bots actions without interrupting developer workflow# Usability\n\n- Allow developers to edit bot configuration easily# Contextual Information\n\n- Make the bot able to save and use contextual information using RAG and Vector DBs\n- Design a Smooth and frictionless interaction# Motivation\n\n- Make bots adaptive and able to learn over time using AI# Lack of Trust in Recommendation\n\n- Enforce transparency in bot actions and outputs# Long Response Latency\n\n- Provide bots with personality# Natural Language\n\n- Integrating concepts from behavioral science in interaction flow# Integration\n\n- Conduct Wizard of the Oz studies to create datasets for NL modules# Testing\n\n- Carefully select the correct framework based on bot requirements# Architecture\n\n- Adopt a specific lifecycle process for bots\n- Adopt Domain Specific Language Models to develop bots# Framework\n\n- Adapt the bots to the privacy policies of organizations# Terms of services\n\nFig. 2. Challenges (on the right) and Associated Best Practices (on the left).# CAs as Managers\u2019 Twins\n\nProject managers (PM) are responsible for ensuring that the project meets the requirements. In order to achieve this objective, managers should have various skills and\u2014most critically\u2014manage various factors simultaneously [124]. To support this goal, having a community of bots collecting different metrics and monitoring different activities could support PMs in their daily tasks [A34]. Moreover, making a hierarchical bots structure in which pieces of information are filtered and summarized differently at each level of the organization could help managers focus on essential tasks, resulting in increased productivity and mental health quality [A43].\n\nManuscript submitted to ACM# 5.5 Open Issues for bots and conversational agents\n\nDespite the significant attention reserved to the topic by researchers and practitioners, using bots for software development is still a young and unexplored field. For such a reason, there are a series of open issues and exploration opportunities that the research community should care of:\n\n- Usability Research for CAs. Although CAs put the interaction with users at their center, to the best of our knowledge, there needs to be more research on the usability of such systems. Moreover, practitioners should put more effort into defining standards and guidelines to design interaction workflows in conversational systems.\n- Testing of Bots. Testing bots and conversational agents arose as a complex topic [A3, A26, A64, A76][B90]. Most of the testing strategies for these systems fall under the category of system testing, and only a few have been proposed at lower levels of testing (e.g., unit and integration).\n- Behavioral Science and Bots. Motivating users to adopt bots is a widely documented problem [A1, A5, A25][B84, B85, B92]. Various works proposed adopting behavioral science concepts\u2014e.g., nudge and interaction principles\u2014to improve the design phases of these systems [A72]. For this aim, researchers from both fields\u2014computer science and social science\u2014could join forces to open new research opportunities.\n- Interruption and Noise: an Open Challenge. Interruption and noise are significant and documented problems for conversational agents and bots [A1, A25, A63, A134][B85]. Despite the work already done to propose mitigation strategies for such phenomena, more should be done to increase the adoption and effectiveness\u2014in terms of accomplished contribution\u2014of bots for development purposes.\n- Generative AI for Software Development. The recent emergence of large language models in the consumer market has inevitably led software engineers to experiment with integrating these tools into their workflow [A9, A13]. When such LLMs are used in their chatbot versions (e.g., ChatGPT), they constitute software bots. Despite their impressive capabilities and the potential they could have in all aspects of software development, some clear limitations continue to exist, likely due to the technology\u2019s still early stage.# Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering\n\ncommon words\u2014e.g., bot\u2014but also on similar terms used by different authors for pointing to a similar concept\u2014e.g., conversational agents. We followed a formal search using defined keywords, followed by a manual search using the references of the initial pool in our field of study. As for the search engine, we included official comprehensive academic databases, e.g., Scopus, but also fewer ones like Google Scholar. While for the grey literature, we relied on Google Search Engine\u2014like other Grey Literature and Multivocal Literature Reviews [121, 125]. Therefore, we believe that we collected all the relevant studies in the field. Finally, as for the inclusion/exclusion criteria, they can usually suffer from researchers\u2019 judgment and experience. For this reason, all the paper\u2019s authors were involved in checking criteria by applying joint voting for study selection. As a result, only those with high scores were selected for this study.# Construct Validity\n\nThreats in this category regard the extent to which the object of study truly represents the theory behind the study and are mainly due to imprecision in performed measurements [157]. One threat in this category concerns the lack of empirical evidence in the studies coming from grey literature. Indeed, this literature reports facts based on opinion or experience. Since the grey literature represents the voice of the practitioners, we could assume that they would be speaking from personal experience. This can be a crucial limitation; however, the goal of an MLR is also to show different ways of thinking, i.e., empirical versus practical, since it can have two types of audience: academic and industrial. Moreover, the inclusion of grey literature can help to overcome the scarcity of studies retrieved by automatic searches in the digital databases of scientific literature.# Conclusion Validity\n\nThreats in this category concern the degree to which the conclusion we reach is credible or believable [157]. The first two authors of the paper reviewed each step of the process to ensure the reliability of the study and mitigate bias in data selection and extraction. Each severe disagreement between authors was resolved by consensus among all the authors. Furthermore, in order to strengthen our results, we followed both the guidelines from Kitchenham et al. [130]\u2014related to the formal literature selection\u2014and from Garousi et al. [120]\u2014associated with the grey literature inclusion. Moreover, we followed a rigid quality criteria assessment\u2014both for white and grey literature\u2014and we extracted our sources from the most used and known databases.# External Validity\n\nThreats in this category concern the extent to which our study\u2019s results can be generalized [157]. First of all, we conducted our MLR on the most known and suggested databases of literature [120, 130] to increase the quality of extracted sources and generalizability of our findings. Furthermore, we extracted only primary sources written in English, so the rest in other languages were excluded, thus possibly threatening our generalizability. Nonetheless, (i) the top venues in software engineering accept only papers written in English, and (ii) most of the literature reviews already present in the literature include exclusion criteria based on the English language.# 7 CONCLUSION\n\nIn this work, we reported on a multivocal literature review to shed light on adopting bots and conversational agents for software engineering purposes. The reason behind the choice of a multivocal review is related to the fact that bots and CAs are extremely linked to the practitioners\u2019 field, so we wanted to contribute both to researchers and practitioners. Specifically, we aimed to provide a representation and categorization of the (1) motivation for adopting bots, (2) challenges arising from their integration into software development processes, (3) best practices applicable contrasting the before-mentioned challenges, and (4) benefits derived from the adoption of bots. Moreover, we also provide a list of discussion points and opportunities for researchers to encourage new investigations on the matter.\n\nManuscript submitted to ACM",
        "context_id": 36,
        "question": "What type of agents are known as chatbots according to the introduction?",
        "answer": [
            "conversational agents"
        ],
        "context_length": 98124
    },
    {
        "context": "# 1. Introduction\n\nThe question of which player is the greatest of all time (i.e., the GOAT) is one that brings about strong opinions from sports fanatics. Within individual sports in particular, a wide variety of individual performance criteria can be used to argue in favor or against a given player being the GOAT. These criteria are often quantitative at face value, taking the form of individual performance statistics. However, these statistics need not be consistent with one another, and thus they ultimately prompt subjective value judgments on their relative importance. To add to this, other qualitative aspects about how a specific sport develops over time might alter their significance across generations.\n\nFor instance, consider Steffi Graf and Serena Williams; two widely successful players in women\u2019s professional tennis. Between the two, Williams leads in the number of major titles, holding 23 compared to the 22 held by Graf. However, Graf leads in the number of titles, holding 107 compared to the 73 held by Williams. Which of these statistics\u2014the number of titles or the number of major titles\u2014is more important? How does 1 additional major title fare against 34 additional titles? Note that these players had little overlap in competition: Graf played professionally from 1982 to 1999, whereas Williams turned professional in 1995 and only retired recently in 2022. How do the numerous differences in professional tennis from the 1980s to the 2020s (e.g., changes in equipment, training, court surfaces, and point systems; consolidation of prestige around certain tournaments; increased competition/access to the sport; a global pandemic that suspended multiple tournaments in 2020) relate to the significance of these and other performance statistics?# 1.1. Contributions\n\nThe Graf-Williams example highlights an inherent difficulty with the problem of identifying the GOAT: individual performance statistics can only offer snapshots of history, and it is challenging to objectively contextualize them over time. Fortunately, most individual sports maintain regularly-updated rankings that reflect the moment in time.\n\nDate: September 19, 2024.\n\n2020 Mathematics Subject Classification: 60J20.\n\nKey words and phrases: sports analytics, rank aggregation, Markov chains.# Notes\n\n- All better than Connors and Lendl\n- Federer and Nadal better than Sampras\n- Djokovic and Sampras incomparable\n- All better than Connors, Lendl, and Sampras\n- Federer and Nadal better than Becker\n- Djokovic and Becker incomparable\n- Federer better than Djokovic\n- Nadal and Djokovic incomparable# Table 1. Summary of GOATs of the ATP\n\nThe surnames mentioned correspond to Boris Becker, Jimmy Connors, Novak Djokovic, Roger Federer, Ivan Lendl, Rafael Nadal, and Pete Sampras (in alphabetical order). Refer to Section 4.2 for more details.# Top 20\n\nGraf, Navratilova, Seles, S. Williams, V. Williams, and Wozniacki# Notes\n\n- Graf better than Navratilova\n- S. Williams and Navratilova incomparable\n- Graf better than Navratilova\n- Graf and S. Williams better than Seles\n- S. Williams and Navratilova incomparable\n- Navratilova better than Seles\n- Graf and Seles incomparable\n- S. Williams and Seles incomparable\n- Numerous players close to maximal# Table 2.\n\nSummary of GOATs of the WTA. The surnames mentioned correspond to Steffi Graf, Martina Navratilova, Monica Seles, Serena Williams, Venus Williams, and Caroline Wozniacki (in alphabetical order). Refer to Section 4.3 for more details.\n\nPlayers such as Carlos Alcaraz in the ATP and Aryna Sabalenka in the WTA perform well as top 3 players, but that they are at a disadvantage as top 5, 10, or 20 players due to their short career lengths at the time of writing.\n\nLastly, we analyze the obtained posets and note qualitative differences between the rankings of the two associations. We find that, in the ATP, being a \u201cgood top 20 player\u201d is correlated with being a good top 10, 5, and 3 player (with decreasing correlation in that order). However, in the WTA, being a \u201cgood top 20 player\u201d is only mildly correlated with being a good top 10 player, and it is nearly uncorrelated with being a good top 5 or top 3 player. This highlights a measurable difference in terms of career and dominance longevity for top players across the ATP and the WTA.# 1.2. Related Work.\n\nRadicchi [14] addressed the question of who is the GOAT in men\u2019s professional tennis using historical match-level data and a network diffusion algorithm similar to PageRank [7]. Baker and Hale [4] addressed the same question using time-varying paired comparison models and the results of matches in major tournaments. Temesi, Sz\u00e1doczki, and Boz\u00f3ki [20] addressed the question of who is the GOAT in women\u2019s professional tennis using pairwise comparison matrices and the results of matches between players who were ever ranked first. Still, from a philosophical point of view, sports rankings have been criticized for their inability to capture comparisons that are \u201ctoo close to call\u201d [6].\n\nIn contrast to existing methods, we propose the aggregation of historical rankings into a single poset as an alternative to the stringent requirements of a linear order. That is, some pairs of players can be conclusively compared, others cannot, and there might be various, pairwise incomparable GOATs. Thus, our conceptualization of the problem is most closely related to that of rank aggregation [10, 18] (refer also to Lin [13] for a survey).\n\nHowever, our methods are actually most closely related to those from the theory of card shuffling\u2014the design and analysis of methods to sample uniform random permutations. The set [n] := {1, 2, . . . , n} corresponds to a deck of n \u2192 N := {1, 2, . . .} cards, and different card shuffling procedures (e.g., riffle shuffling, shuffling by transposing two random cards, shuffling by transposing two random adjacent cards) correspond to different sampling methods.# The central questions in this area relate to convergence rates\n\nhow many times does a given shu#ing procedure need to be repeated so that a given deck is su!ciently close to uniformly distributed? Bayer and Diaconis [5] showed that (3/2) log n + \u03c9 ri#e shu#es are necessary and su!cient for a deck of n cards to start becoming well-mixed, exhibiting a cuto\u201d phenomenon [1] (so that a standard deck of 52 cars requires about 6 repetitions). Diaconis and Shahshahani [9] showed that the random transposition shu#e has a cuto\u201d at (1/2)n log n, whereas Lacoin [11] showed that the random adjacent transposition shu#e has a cuto\u201d at (1/\u03b52)n3 log n. Refer to Diaconis and Fulman [8] for more on card shu#ing.\n\nOur method in Section 3 is most similar to shu#ing by random (not necessarily adjacent) transpositions. However, our goal in this work is di\u201derent from card shu#ing in that we do not seek to sample uniform random permutations. Rather, we seek to sample permutations from a data-driven distribution that is somehow in agreement with historical ranking data. Given the large amount of existing work for the case of uniform random permutations, we anticipate analytical questions around the data-driven sampling of permutations to be challenging and of independent mathematical interest.# 1.3. Organization\n\nThe remainder of this work is organized as follows. For the purpose of self-containment, in Section 2 we summarize some necessary technical background. In Section 3 we design our methods. In Section 3 we implement our methods using data from the ATP and the WTA, and analyze the results of our numerical implementation. Lastly, in Section 5 we make concluding remarks.# 2. Background\n\nReaders familiar with the symmetric group may choose to skip Section 2.1, whereas readers familiar with Markov chains may choose to skip Section 2.2. In Section 2.3 we formalize our problem input along with other necessary technical concepts and notation.# 2.1. Symmetric Group\n\nLet n \u2192 N := {1, 2, 3, . . .}. A permutation of [n] := {1, 2, . . . , n} is a bijection \u03b5 : [n] \u2191 [n]. The symmetric group of degree n, denoted Sn, is the set of all permutations of [n] forming a group under function composition.\n\nFact 2.1. For n \u2193 1, |Sn| = n!# Example 2.2\n\nLet \u03b5 : [5] \u2191 [5] be the permutation expressed explicitly as \u03b5(1) = 5, \u03b5(2) = 4, \u03b5(3) = 1, \u03b5(4) = 2, and \u03b5(5) = 3. Equivalently, we express \u03b5 as follows:\n\n- In one-line notation, and write \u03b5 = 54123. Here, the ith entry of the word is \u03b5(i).\n- In cycle notation, and write \u03b5 = (153)(24). Here, the number following number i in a cycle is \u03b5(i). For example, 5 follows 1 in (153) so that \u03b5(1) = 5. Similarly, 1 follows 3 in (153) so that \u03b5(3) = 1. Note that function composition is performed from right to left, so that (153) = (13)(15) and equivalently \u03b5 = (13)(15)(24).\n\nCycles of length 2 are known as transpositions, since the e\u201dect of a cycle (i, j) for distinct i, j \u2192 [n] is to transpose the ith and jth entries. By convention, a transposition (i, j) is written as (i, j) if i < j and as (j, i) if j < i.\n\nFact 2.3. For n > 1, every \u03b5 \u2192 Sn is a product of at most n \u2194 1 transpositions.Moreover, every transposition can be expressed as a product of adjacent transpositions.\n\nFor ease of notation, let *sij = (i, j) denote a transposition and si = (i, i + 1) for i \u2192 [n \u2194 1]* denote an adjacent transposition. Consider the following example.# Example 2.4.\n\nNote that\n\n- *s13 = (13) = (12)(23)(12) = s1s2s1*,\n- *s15 = (15) = (12)(23)(34)(45)(34)(23)(12) = s1s2s3s4s3s2s1*, and\n- *s24 = (24) = (23)(34)(23) = s2s3s2*.\n\nTherefore,\n\n*\u03b5 = (153)(24) = (13)(15)(24) = (12)(23)(12)(12)(23)(34)(45)(34)(23)(12)(23)(34)(23) = (12)(34)(45)(34)(23)(12)(23)(34)(23) = s1s3s4s3s2s1s2s3s2.*\n\nThe last example illustrates the following.# Fact 2.5.\n\nFor *n > 1, every \u03b5 \u2192 Sn is a product of at most n(n\u21941)/2* adjacent transpositions.\n\nIn this work, we associate with *Sn two bi-directed graphs over n!* nodes (the number of nodes follows from Fact 2.1):\n\n1. The first one, depicted in Figure 1a for *n = 4, contains arcs (\u03b5, \u03d1) and (\u03d1, \u03b5) if and only if \u03b5 = \u03d1 \u00b7 sij for some 1 \u2197 i < j \u2197 n. For example, if \u03b5 = 4213 \u2192 S4 and \u03d1 = 4312 \u2192 S4, then the graph has the arc (\u03b5, \u03d1) since 4213 \u00b7 s24 = 4312 and s24* is a transposition. The undirected version of this bi-directed graph is known as the Bruhat graph.\n2. The second one, depicted in Figure 1b for *n = 4, contains arcs (\u03b5, \u03d1) and (\u03d1, \u03b5) if and only if \u03b5 = \u03d1 \u00b7 si for some i \u2192 [n \u2194 1]. For example, if \u03b5 = 4213 \u2192 S4 and \u03d1 = 4231 \u2192 S4, then the graph has the edge (\u03b5, \u03d1) since 4213 \u00b7 s3 = 4231 and s3* is an adjacent transposition. The undirected version of this bi-directed graph is realized as the 1-skeleton graph of the permutahedron.\n\nThe bi-directed graph in Figure 1b illustrates the commonly-used fact that adjacent transpositions suffice to generate *Sn*. However, as we explain in Section 3, the more edge-dense class of graphs illustrated in Figure 1a is better suited for the purposes of this work. The main properties of this class of graphs that we leverage in this work follow from Fact 2.3 and are summarized below.# Remark 2.6.\n\nFor *n > 1, the bi-directed version of the Bruhat graph on Sn is strongly-connected and has diameter n \u2194 1*.\n\nRefer to any abstract algebra textbook for more on the symmetric group.# 2.2. Markov Chains.\n\nThis section is based on Levin and Peres [12]. A discrete-time Markov chain is a sequence *X1, X2, X3, . . .* of random variables with the Markov property. That is,\n\n*Pr[Xt+1 = x|X1 = x1, X2 = x2, . . . , Xt = xt] = Pr[Xt+1 = x|Xt = x]*# Figure 1\n\nBi-directed graphs on S4, with nodes as permutations in one-line notation. (Bi-directed arcs are depicted as undirected edges.)\n\nfor all t \u2192 N whenever the conditional probabilities are well-defined. If the random variables share a finite co-domain, then the chain has a finite state space\u2014say, indexed by [m] for some m \u2192 N. The chain is time-homogeneous if\n\nPr[Xt+1 = x|Xt = y] = Pr[Xt = x|Xt\u21921 = y]\n\nfor all t \u2192 N, so that the transition probabilities are independent of t.\n\nIn the finite and time-homogeneous case, the transition probabilities can be represented by a row-stochastic matrix P\n\nPij = Pr[Xt+1 = j|Xt = i]\n\nfor all i, j \u2192 [m]. In effect, P encodes the transition probabilities of a random walk on a directed graph Gm = ([m], A), where (i, j) \u2192 A if and only if Pij > 0. Specifically, Xt \u2192 [m] corresponds to the node visited during the tth step and, conditioned on Xt = i, the ith row of P governs the random transition to the (t + 1)th step. \u03c9 > 0.\n\nThe chain P is irreducible if, for any i, j \u2192 [m], there exists some \u03d6 \u2192 N such that Pij > 0.\n\nFor any i \u2192 [m], its period is defined as the greatest common divisor of {t \u2192 N : Piit > 0}. The chain is aperiodic if gcd ({t \u2192 N : Piit > 0}) = 1 for all i \u2192 [m]. Note that Pii > 0 for all i \u2192 [m] is a sufficient condition for aperiodicity.\n\nThe following are classical results in the theory of Markov chains.# Theorem 2.7.\n\nIf P is an irreducible finite Markov chain, then there exists a unique probability distribution \u03bc \u2192 Rm over [m] with \u03bc = \u03bcP. The distribution \u03bc is referred to as the stationary distribution of P.# Theorem 2.8 (Convergence Theorem).\n\nIf P is an irreducible and aperiodic finite Markov chain with stationary distribution \u03bc, then there exist constants \u03f1 \u2192 (0, 1) and C > 0 such that\n\nmaxi \u2208 [m](Pt)i \u2194 \u03bcTV \u2197 C\u03f1t,\n\nwhere (Pt)i is the ith row of Pt and \u2198\u00b7\u2198TV is the total variation distance.\n\nTheorem 2.8 gives analytical grounds for the repeated application of an irreducible and aperiodic finite Markov chain to sample from its stationary distribution.# 2.3. Sports Rankings and Partially Ordered Sets.\n\nA partially ordered set (poset) is a pair \u03b5 = ([n], \u2243\u03b5) consisting of a ground set [n] and a partial order \u2243\u03b5 satisfying:\n\n1. Reflexivity: For any i \u2192 [n] we have that i \u2243\u03b5 i.\n2. Antisymmetry: For any i, j \u2192 [n] we have that if i \u2243\u03b5 j and j \u2243\u03b5 i, then i = j.\n3. Transitivity: For any i, j, k \u2192 [n] we have that if i \u2243\u03b5 j and j \u2243\u03b5 k, then i \u2243\u03b5 k.\n\nFor i, j \u2192 [n], let i \u21d0\u03b5 j if i \u2243\u03b5 j and i \u21d2= j. An element i \u2192 [n] is maximal if there is no j \u21d2= i \u2192 [n] with i \u21d0\u03b5 j. The definition of minimal is analogous.\n\nA poset may be represented pictorially by a Hasse diagram, which is a drawing of its transitive reduction. For example, if the edges in the graphs in Figure 1 were all oriented upwards, we would obtain Hasse diagrams of two different posets on the symmetric group of order 4, both with unique minimal element 1234 and unique maximal element 4321.\n\nA totally ordered set is a poset in which every two elements are comparable, and a chain is a subset of a poset that is totally ordered for the induced order (note that the use of the word \u201cchain\u201d in the context of a poset differs from its use in Section 2.2). For example, {1234, 1243, 2143, 2413, 2431, 4231, 4321} forms a chain in Figure 1b. An anti-chain is a subset of elements such that no pair of elements are comparable. For example, {1324, 2134, 1243} forms an anti-chain in Figure 1b.\n\nA poset \u03b5 \u2194 = ([n], \u2243\u03b5 \u2192) is an extension of \u03b5 = ([n], \u2243\u03b5) if, for any i, j \u2192 [n] with i \u2243\u03b5 j, we have that i \u2243\u03b5 \u2192 j as well. An extension is a linear extension if it is totally ordered. Refer to Stanley [19, Chapter 3] for more on posets.\n\nLet [n] represent a set of players. A ranking is a totally ordered set \u03b5 = ([n], \u2243\u03b5), which we treat as a permutation \u03b5 \u2192 Sn (in explicit notation) given the natural ordering \u2197 of N. In particular, we have that\n\nj \u2243\u03b5 i \u21d1\u21d3 \u03b5(i) \u2197 \u03b5(j) for all i, j \u2192 [n]. We more generally extend the notion of ranking to proper subsets of players, so that a ranking is a totally ordered set r = (S, \u2243r) for some Sr \u21d4 [n], which we again treat as a bijection r : Sr \u2191 [|S|] given the natural ordering r \u2197 of N. In this way, our problem input is a collection R = {r1, r2, ..., r\u03d1} of historical sports rankings on subsets of players. In subsequent sections we will consider partial orders on the full set of players [n] together with their corresponding linear extensions as part of our problem output.\n\nLastly, note that we adopt the following notation. For i \u2192 [n], let Ri = {r \u2192 R : i \u2192 Sr} denote the sub-collection of rankings containing player i. Similarly, for distinct i, j \u2192 [n], let# 3. Data-Driven Random Walks on the Symmetric Group\n\nIn Section 3.1 we discuss the meaning of GOAT in relation to assumptions (in particular, ranking cutoffs) on the problem input. In Section 3.2 we design a data-driven transition rule for random walks on Sn. In Section 3.3 we adopt a notion of stochastic dominance to build a poset from the stationary distribution of this random walk. In Section 3.4 we discuss sampling procedures for this random walk.# 3.1. Cutoffs and the meaning of GOAT.\n\nRecall that [n] represents the set of players and that the problem input is a collection R = {r1, r2, . . . , r\u03d1} of historical sports rankings on subsets of the players, so that \u2199 r\u2193R Sr = [n]. Now, consider any player i \u2192 [n] and any ranking r \u2192 R. Certainly, player i being \u201ctop ranked\u201d in r is consistent with them being the GOAT, but how \u201ctop\u201d is top ranked? A full sports ranking might involve thousands of players. However, player i being among the top 10 in r is much more indicative than them being among the top 100, which is itself even much more indicative than them being among the top 1000. In some sense, we must adopt a definition for the concept of being at the top.\n\nIn the experimental part of this work, we restrict the rankings in R to involve the top 3, 5, 10, and 20 players as cutoffs from the full ranking data that might be available. We do this for two reasons:\n\n1. The first reason is conceptual. As argued previously, the difference between a player being ranked 100 or 101 (let alone 1000 or 1001) is not particularly relevant in relation to them being the GOAT, whereas the difference between them being ranked 1 or 2 very much is. By experimenting with different cutoffs we gain insights into the consequences of this subjective choice.\n2. The second reason is computational. Indeed, if each ranking r \u2192 R involved thousands of players, the set [n] would be very large and n! would be astronomically large. In our experiments, the different cutoffs always keep the size of the set under 300 players. Note that 300! is still an astronomically large number, with 615 decimal digits.\n\nIn short, we assume that being the GOAT is consistent with being a \u201cgood top \u03c2\u201d player, and we experiment with the choice of \u03c2 \u2192 N. In this way, having ever made it to the top \u03c2 is a necessary condition for being the GOAT, which restricts the set of contenders.# 3.2. Transition Rule.\n\nLet W \u2192 R \u21970 n\u2191n be a matrix with entries\n\nWij = 1 + prevalence ratio\u00b7 |{r \u2192 Rij : i beats j}| / |R| / |Rj| (1)\n\nfor all distinct i, j \u2192 [n]. The second term in the product, |{r \u2192 Rij : j \u21d0 ri}|, encodes the number of (cut off) historical rankings in which player i is ranked better than player j. We scale this quantity by the first term in the product, |R| / |Rj|, which is the players\u2019 relative prevalence above the cutoff. The motivation for this is as follows.# Remark 3.1.\n\nSuppose players i and j have short and long prevalence in the (cut off) historical rankings, respectively. In particular, it might be that player i made it to the top of the ranking for a brief period of time, consequently beating player j during this period. However, since the prevalence of player i is short, there might be significant periods of time during which player i fell below the cutoff while player j did not\u2014by definition, player j beat player i during these periods. This artifact of cutoffs is not captured by the term |{r \u2192 Rij : j \u21d0 ri}|, which by itself favors player i over player j. We correct for this in the model using their prevalence ratio |R| / |Rj|.\n\nIntuitively, the term |R| / |Rj| ensures that if the players have similar prevalence above the cutoff, the data points in which player i beats player j are taken at face value. Conversely, if the prevalence of player i is much smaller than that of player j, the data points in which player i beats player j are proportionally discounted. Finally, the additive unit is included to later on obtain an irreducible random walk. Note that (1) can be pre-computed efficiently.\n\nNext, for each \u03b5 \u2192 Sn and each 1 \u2197 i < j \u2197 n, let W(sij |\u03b5) \u221d {Wji, if j \u21d0 \u03b5; Wij, otherwise} (2) form a probability distribution over (not necessarily adjacent) transpositions. The interpretation of W(sij |\u03b5) is as follows:\n\n- j \u21d0 \u03b5i =\u21d3 i \u21d0 sij \u00b7 \u03b5j, whereas\n- i \u21d0 \u03b5j =\u21d3 j \u21d0 sij \u00b7 \u03b5i.\n\nIn other words, the probability of selecting transposition sij given \u03b5 is proportional to the agreement between the resulting permutation sij \u00b7 \u03b5 and the historical data, particularly with respect to the relative order between players i and j.# Remark 3.2.\n\nNote that (1)-(2) highlight the need to consider the set of all transpositions, not just those that are adjacent (recall the difference in Figure 1). Indeed, depending on the structure of R and the indexing of the players, it is possible that |Rij| = 0 for a large fraction of the adjacent choices of 1 \u2197 i < j (= i + 1) \u2197 n, in turn rendering (1) uninformative. For example, this could occur if the players are indexed in such a way that adjacent players generally cannot be compared pairwise because of their prevalence during non-overlapping time periods.\n\nNext, we use (2) to compactly define a row-stochastic matrix P \u2192 Rn!\u2191n! \u21970 such that, for any given \u03b5 \u2192 Sn, we have the transition probabilities:\n\n|Condition|Probability|\n|---|---|\n|if \u03d1 = sij \u00b7 \u03b5 for some 1 \u2197 i < j \u2197 n|1/2 \u00b7 W(sij |\u03b5)|\n|if \u03d1 = \u03b5|1/2|\n|otherwise|0|\n\nThe matrix P governs a data-driven random walk on Sn. Note that its support along any given row is of size O(n2) and can be evaluated efficiently on the fly.# Lemma 3.3.\n\nGiven (3), P is irreducible and aperiodic.# Proof\n\nThe additive 1 term in (1) implies the support graph of P is the bi-directed Bruhat graph, with the addition of self-loops. Fact 2.3 implies this graph is strongly connected, so that P is irreducible. Moreover, the non-zero probability of a lazy step in the second case of (2) implies P \u03b5,\u03b5 > 0 for all \u03b5 \u2192 Sn, so that P is aperiodic.# Corollary 3.4\n\nP has a unique stationary distribution \u03bc over Sn.# 3.3. Partial Ordering\n\nIn light of Corollary 3.4, consider the stationary distribution \u03bc of P. We combine this distribution with a suitable notion of stochastic dominance to obtain a partial order ([n], \u2243 \u03bc) over the players. In particular, for each i, j \u2192 [n], let\n\nj \u2243 \u03bc i \u21d1\u21d3 X\u2198\u03bc Pr[X(i) \u2197 k] \u2193 X\u2198\u03bc Pr[X(j) \u2197 k], \u20321 \u2197 k \u2197 n.\n\nRecall that \u03bc is a distribution over Sn, so that X is a random permutation (in explicit notation) and X(i) \u2192 [n] denotes the ranking of player i \u2192 [n]. Therefore, at an intuitive level, (4) states that player i is conclusively \u201cbetter\u201d than player j in \u2243 \u03bc if and only if, given that X \u221e \u03bc, player i is across the board more likely to rank better than player j.\n\nLet L(\u2243 \u03bc) denote the set of linear extensions of \u2243 \u03bc (refer to Section 2.3). In the experimental part of this work, we will consider the average rank of a player with respect to a random linear extension as a summary statistic of the player\u2019s career performance. In particular, for each i \u2192 [n] we will consider\n\n\u00af(i) = N\u2211X (i),T\n\nX 1 , X 2 , . . . , X T are sampled uniformly at random from L(\u2243 \u03bc).# 3.4. Sampling\n\nOur next goal is to obtain independent samples from the stationary distribution \u03bc induced by (3) to form an empirical poset according to (4). Based on Theorem 2.8, this can be achieved through the repeated execution of (3). At this point, the question of speed of convergence comes to the forefront. If we knew the mixing time \u03d6 \u2192 N of this Markov chain for a desired tolerance to error, we could execute the random walk collecting every \u03d6 th sample. However, based on the discussion in Section 1.2, we anticipate this is a very challenging theoretical question. Neither is it easy to obtain numerical upper bounds based on the eigenvalues of the transition matrix P due to its n! \u2208 n! size.\n\nGiven the sports analytics scope of this work, we do not attempt to answer these questions and simply acknowledge that analytical questions around data-driven random walks on the symmetric group are of independent mathematical interest. In the experimental part of this work, we execute this random walk for a large number of steps collecting every 2(n \u2194 1)th sample. Given the n \u2194 1 diameter noted in Remark 2.6 and the 1/2 probability of taking a lazy step in (3), this is the expected number of steps needed for the entire state space to be reachable from any given state.# 4. Numerical Implementation\n\nIn this section, we implement our methods using singles rankings from the \u201cOpen Era\u201d of the ATP and the WTA. As discussed in Section 3.1, we experiment with taking the top 3, 5, 10, and 20 players as different cutoffs from the full ranking data that is available.For each experiment, we execute a random walk as described in Section 3.4 until we collect 100,000 samples. We then use these samples to obtain data-driven posets over the players, as described in Section 3.3. We illustrate these posets as horizontal Hasse diagrams with maximal elements on the right-hand side (the full diagrams can be found in the Appendix). To estimate the average rank of the players in a given poset, we first sample 100,000 linear extensions uniformly at random using SageMath [17] and then follow (5).\n\nThe remainder of this section is organized as follows. In Section 4.1 we further describe the datasets we use for our experiments. In Section 4.2 we present our results using rankings from the ATP, whereas in Section 4.3 we present our results using rankings from the WTA. Refer to Tables 1 and 2 for a summary of some of the results. Lastly, in Section 4.4 we compare the results obtained for the ATP and the WTA to note qualitative differences between the two associations. All mentioned statistics that are external to our numerical implementation can be verified online [3, 22].# 4.1. Ranking Data.\n\nBoth the ATP and the WTA singles rankings are based on rolling 52-week period systems that award points to individual players according to their placement in different tournament categories. Rankings are typically published on Mondays, and points are dropped 52 weeks after being awarded. Note that the point systems have changed over the years: details on the current systems are available online in the official rulebooks [2, 21].\n\nWe retrieved historical rankings from the repositories compiled by Sackmann [15, 16] for both the ATP and the WTA. In the case of the ATP, this corresponds to 2230 weekly rankings from 1973 to 2023 (Sackmann notes that this data is complete from 1985, but that it is intermittent from 1973 to 1984 and that it is missing 1982). In the case of the WTA, this corresponds to 2052 weekly rankings from 1984 to 2023.\n\nNaturally, the limitations of the data we use induce limitations on our experimental results. Aside from missing data points, we in particular do not consider the \u201cAmateur Era\u201d and only partially capture players that were active during the onset of the \u201cOpen Era\u201d rankings. As a concrete example, we do not consider Margaret Court; a widely successful player in women\u2019s professional tennis that was active from 1960 to 1977. This data availability issue speaks to the difficulty of capturing the Amateur Era due to the lack of a centralized ranking system.# 4.2. Rankings from the ATP.\n\nIn Figure 2 we plot the top portion of the Hasse diagram of the poset over \u201ctop 3 players,\u201d where Djokovic, Federer, Nadal, and Sampras are incomparable maximal elements\u2014no one of these players is conclusively better than another. Note also that they all have a similar average rank. Conversely, we conclude that each of Djokovic, Federer, Nadal, and Sampras is better than Connors and Lendl as a top 3 player. Similarly, as top 3 players, Connors is better than Andre Agassi, who is better than Bj\u00f6rn Borg, who is better than Andy Murray, who is better than Carlos Alcaraz. In other words, these players form a chain in the poset. Formally, this is:\n\nDjokovic \u220b \u03bc Connors \u220b \u03bc Agassi \u220b \u03bc Borg \u220b \u03bc Murray \u220b \u03bc Alcaraz.\n\nThis is also reflected by significant jumps in the average rank from one player to the next. Interestingly, Alcaraz is near the top of the Hasse diagram despite turning professional most recently, in 2018.# Average Rank\n\n|Gustavo Kuerten|Roger Federer|Lleyton Hewitt|Andy Roddick|John McEnroe|Jimmy Connors|Rafael Nadal|\n|---|---|---|---|---|---|---|\n|Stefan Edberg|Bjorn Borg|Jim Courier|Andre Agassi|Ivan Lendl|Novak Djokovic|Andy Murray|\n|Carlos Alcaraz|Pete Sampras|Pete Sampras|Pete Sampras|Pete Sampras|Pete Sampras|Pete Sampras|# Figure 2.\n\nCuto\u201d at top 3 of full ATP rankings.\n\n| | | |Stefan Edberg|Pete Sampras|Roger Federer|Lleyton Hewitt|Bjorn Borg|Andre Agassi|Boris Becker|Jimmy Connors|Rafael Nadal|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | |Jim Courier|Andy Murray|John McEnroe|Ivan Lendl|Novak Djokovic|Mats Wilander|20| |\n|15|10|5|0| | | | | | | | |# Figure 3.\n\nCuto\u201d at top 5 of full ATP rankings.\n\nIn Figure 3 we plot the top part of the Hasse diagram of the poset over \u201ctop 5 players,\u201d where Djokovic, Federer and Nadal remain incomparable maximal elements, although Federer fares slightly better than the other two in terms of average rank. However, Sampras is# Average Rank\n\nNo longer a maximal element: as a top 5 player, he is worse than both Federer and Nadal, and he is incomparable to Djokovic. Lendl (both of whom he was better than as a top 3 player). Note that Djokovic and Connors are incomparable as top 5 players: their difference in average rank favors Djokovic, but it is not sufficiently conclusive. Moreover, note that Alcaraz no longer appears near the top of the Hasse diagram, having been displaced by other players who are better top 5 players than they are top 3 players, such as Guillermo Vilas (this is reflected in the average rank plot).\n\nFirst, note that Alcaraz and Vilas did not overlap in competition: Vilas played professionally from 1968 to 1992. Second, note that Alcaraz transitioned very quickly from below the top 20 to within the top 3 from February to September of 2022, and has consistently prevailed within the top 3 until the time of writing. His prevalence within the top 3 is significant independently of his short career at present. However, when the cutoff is relaxed to top 5, his short career-to-date puts him at a disadvantage compared to retired players with significant career-long prevalence in the top 5 (but perhaps not in the top 3), such as Vilas.# Figure 4\n\nWe plot the top part of the Hasse diagram of the poset over \u201ctop 10 players,\u201d where Djokovic, Federer, and Nadal once again remain incomparable maximal elements. However, unlike the previous cutoff, we conclude that Djokovic is better than Connors as a top 10 player. Qualitatively, we observe the inclusion of more players near the top of this Hasse diagram compared to the previous cases, indicating that there are more \u201cgood top 10 players\u201d than there are good top 3 or top 5 players. For example, Becker (who was far from maximal in Figures 2 and 3) is now close to maximal: only Federer and Nadal are better than him, and he is incomparable to Djokovic.# Players\n\n|David Ferrer| | | |Jimmy Connors| |\n|---|---|---|---|---|---|\n| |Guillermo Vilas| |John McEnroe| |Rafael Nadal|\n|Jim Courier| |Andy Murray| |Ivan Lendl| |\n| |Alexander Zverev| |Stefan Edberg| |Novak Djokovic|\n|Michael Chang| |Bjorn Borg| |Boris Becker| |\n| |Mats Wilander| |Andre Agassi| |Roger Federer|\n|Lleyton Hewitt| | | |Pete Sampras| |\n| |Yevgeny Kafelnikov| | | | |\n\nFigure 4. Cutoff at top 10 of full ATP rankings.# Figure 5\n\nLastly, in Figure 5 we plot the top part of the Hasse diagram of the poset over \u201ctop 20 players.\u201d Interestingly, only Federer and Nadal remain incomparable maximal elements.# Average Rank\n\nDespite this, only Federer is conclusively better than Djokovic as a top 20 player\u2014Djokovic and Nadal are incomparable as top 20 players. Qualitatively, we again observe the inclusion of even more players near the top of this Hasse diagram compared to the previous cases. For instance, the Djokovic \u220b \u03bc Connors \u220b \u03bc Agassi chain from Figure 2 is broken in Figure 5, where all three of these players are equally close to the top of the Hasse diagram and form an anti-chain.# Players\n\nMichael Stich\n\nAlexander Zverev\n\nMichael Chang\n\nRichard Krajicek\n\nYevgeny Kafelnikov\n\nGoran Ivanisevic\n\nJimmy Connors\n\nGuillermo Vilas\n\nBjorn Borg\n\nPete Sampras\n\nDavid Ferrer\n\nIvan Lendl\n\nRoger Federer\n\nTim Henman\n\nJuan Carlos Ferrero\n\nJohn McEnroe\n\nStefan Edberg\n\nNovak Djokovic\n\nRafael Nadal\n\nVitas Gerulaitis\n\nMats Wilander\n\nBoris Becker\n\nAndy Murray\n\nAndre Agassi\n\nLleyton Hewitt\n\nAndy Roddick\n\nTomas Berdych\n\nStan Wawrinka\n\nThomas Muster# 4.3. Rankings from the WTA.\n\nIn Figure 6 we plot the top portion of the Hasse diagram of the poset over \u201ctop 3 players,\u201d where Graf and S. Williams are incomparable maximal elements\u2014no one of these players is conclusively better than the other. Note also that they both have a similar average rank, albeit with a slight advantage for Graf. Note that S. Williams and Martina Navratilova are incomparable. On the other hand, we conclude that, as top 3 players, Graf is better than Navratilova, who is better than Justine Henin, who is better than Aryna Sabalenka. In other words, these players form a chain in the poset. Formally, this is Graf \u220b\u03bc Navratilova \u220b\u03bc Henin \u220b\u03bc Sabalenka.\n\nWe also conclude that, as a top 3 player, Sabalenka, Kim Clijsters, and Maria Sharapova are pairwise incomparable (i.e., they form an anti-chain).\n\nIn Figure 7 we plot the top portion of the Hasse diagram of the poset over \u201ctop 5 players,\u201d where Graf and S. Williams are again incomparable maximal elements. Note that while Graf is better than Navratilova, S. Williams and Navratilova are incomparable. Moreover, note that unlike the case in Figure 6, as top 5 players, both Clijsters and Sharapova are better than Sabalenka. This is a similar effect to that seen with Alcaraz in Section 4.2: Sabalenka turned professional in 2015 and had little overlap in competition with Sharapova.# Average Rank\n\n|Caroline Wozniacki|Monica Seles|Chris Evert|\n|---|---|---|\n|Victoria Azarenka|Martina Navratilova|Steffi Graf|\n|Simona Halep|Ashleigh Barty|Lindsay Davenport|\n|Amelie Mauresmo|Martina Hingis|Serena Williams|\n|Aryna Sabalenka|Justine Henin|Iga Swiatek|\n|Kim Clijsters| | |# Figure 6.\n\nCuto\u201d at top 3 of full WTA rankings.\n\n|Steffi Graf|Serena Williams|Martina Navratilova|Martina Hingis|Iga Swiatek|Lindsay Davenport|Ashleigh Barty|Chris Evert|\n|---|---|---|---|---|---|---|---|\n|Monica Seles|Justine Henin|Caroline Wozniacki|Simona Halep|Maria Sharapova|Aryna Sabalenka|Victoria Azarenka|Venus Williams|\n|Kim Clijsters|Amelie Mauresmo|Arantxa Sanchez Vicario|Dinara Safina| | | | |# Figure 7.\n\nCuto\u201d at top 5 of full WTA rankings.\n\n|Steffi Graf|Serena Williams|Lindsay Davenport|Martina Navratilova|Martina Hingis|Monica Seles|Maria Sharapova|Chris Evert|Simona Halep|\n|---|---|---|---|---|---|---|---|---|\n|Ashleigh Barty|Kim Clijsters|Justine Henin|Venus Williams|Iga Swiatek|Arantxa Sanchez Vicario|Caroline Wozniacki|Aryna Sabalenka|Amelie Mauresmo|\n|Gabriela Sabatini|Karolina Pliskova| | | | | | | |\n\nwho retired in 2020, and Clijsters, who retired in 2012 except for a brief comeback between 2020 and 2022. In particular, her prevalence within the top 3 is significant independently of her short career at the time of writing. However, when the cuto\u201d is relaxed to top 5, her# Average Rank\n\nShort career-to-date puts her at a disadvantage compared to long-established players with significant career prevalence in the top 5, such as Clijsters and Sharapova. Qualitatively, we observe the inclusion of more players near the top of this Hasse diagram compared to the previous cases, and that chains of players that were comparable as top 3 players become anti-chains and close to maximal as top 5 players (e.g., Lindsay Davenport and Seles).# Figure 8\n\nWe plot the top portion of the Hasse diagram of the poset over \u201ctop 10 players,\u201d where now the three of Graf, Navratilova, and S. Williams are incomparable maximal elements. In other words, while Navratilova is close to but not quite the GOAT as a top 3 or top 5 player, she is conclusively one of the GOATs as a top 10 player. As a top 10 player, Navratilova is better than Seles. However, Graf and S. Williams cannot be conclusively compared to Seles (even though they fare slightly better than her in terms of average rank).\n\n|Pam Shriver|Chris Evert|\n|---|---|\n|Gabriela Sabatini| |\n|Arantxa Sanchez Vicario|Martina Hingis|\n|Martina Navratilova|Conchita Martinez|\n|Amelie Mauresmo|Monica Seles|\n|Justine Henin|Venus Williams|\n|Steffi Graf|Karolina Pliskova|\n|Caroline Wozniacki|Lindsay Davenport|\n|Kim Clijsters|Maria Sharapova|\n|Serena Williams|Petra Kvitova|\n|Ashleigh Barty|Simona Halep|\n\nFigure 8. Cuto\u201d at top 10 of full WTA rankings.# Figure 9\n\nLastly, in Figure 9 we plot the top portion of the Hasse diagram of the poset over \u201ctop 20 players,\u201d where now all six of Graf, Navratilova, Seles, S. Williams, V. Williams, and Wozniacki are incomparable maximal elements. Qualitatively, we observe a significant and across the board increase in the number of players near the top of the Hasse diagram. This indicates that, in the WTA, there are many more good top 20 players than there are good top 3, 5, or 10 players.# 4.4. Comparing the Associations.\n\nFigures 5 and 9 are qualitatively different in that, as top 20 players, the number of players close to maximal is much larger for the WTA than it is for the ATP. Comparing the results from Sections 4.2 and 4.3, this difference is not nearly as apparent for the posets over top 3, 5, or 10 players. Here, we further investigate this difference from a more quantitative point of view. In particular, we ask questions of# Average Rank of Players\n\n|Chris Evert|Justine Henin|Hana Mandlikova|Jelena Jankovic|Pam Shriver|Vera Zvonareva|Victoria Azarenka|Helena Sukova|Kim Clijsters|Martina Navratilova|\n|---|---|---|---|---|---|---|---|---|---|\n|Na Li|Claudia Kohde Kilsch|Angelique Kerber|Maria Sharapova|Zina Garrison|Steffi Graf|Amanda Coetzer|Ana Ivanovic|Mary Joe Fernandez|Petra Kvitova|\n|Svetlana Kuznetsova|Caroline Wozniacki|Anke Huber|Nathalie Tauziat|Karolina Pliskova|Gabriela Sabatini|Manuela Maleeva Fragniere|Monica Seles|Naomi Osaka|Garbine Muguruza|\n|Mary Pierce|Arantxa Sanchez Vicario|Agnieszka Radwanska|Venus Williams|Iga Swiatek|Elina Svitolina|Jennifer Capriati|Conchita Martinez|Simona Halep|Serena Williams|\n|Ashleigh Barty|Aryna Sabalenka|Lindsay Davenport|Jana Novotna|Nadia Petrova|Elena Dementieva|Martina Hingis|Amelie Mauresmo| | |# Analysis of Player Rankings\n\nThe form: to what extent is being \u201ca good top player\u201d a predictor of being \u201ca good top player,\u201d where the stronger we expect the correlation to be. Intuitively, the smaller the difference between the two rankings, the stronger the correlation.\n\nIn Figure 10, we consider the set of players that appear among the first 25 by average rank for all cutoffs \u03c2 = 3, 5, 10, 20. This amounts to 15 players in both the ATP and in the WTA. Then, for each of these sets of 15 players, we plot a matrix of scatter plots of average rank and varying cutoffs. In this way, each point represents a player, and its coordinates correspond to the player\u2019s average rank for a combination of cutoffs. For each scatter plot, we also fit a linear model and report its coefficient of determination r\u00b2.\n\nFrom Figure 10 we note that, in both associations, being a good top 5 player is a reasonable predictor of being a good top 3 player, as is being a good top 10 player a reasonable predictor for being a good top 5 player. In both cases, being a good top 10 player is still correlated with being a good top 3 player, although much less so than with being a good top 5 player.\n\nHowever, when we consider being a good top 20 player as a predictor, we observe a major difference between the two associations. In the ATP, being a good top 20 player as a predictor is correlated with being a good top 10, 5, and 3 player (with decreasing correlation in that order). Conversely, in the WTA, being a good top 20 player is only mildly correlated with being a good top 10 player, and it is nearly uncorrelated with being a good top 5 or top 3 player.\n\nThis quantitative finding highlights a major difference in terms of career and dominance longevity for top players across the two associations. While we do not delve into causality in this work, we believe it would be practically important to further study the reasons and broader implications of this phenomenon.# Cutoff 3Cutoff 5Cutoff 10Cutoff 20ATP Top 25 by Average Rank (from Linear Extensions)  WTA Top 25 by Average Rank (from Linear Extensions)\n\n|Cutoff 3|Cutoff 5|Cutoff 10|Cutoff 20| | |\n|---|---|---|---|---|---|\n| |20|20|10|10| |\n| |20|20|10|10| |\n| |20|20|10|10| |\n| |20|20|10|10| |\n| |r\u00b2 = 0.71| |r\u00b2 = 0.71| | |\n| |20|20|10|10| |\n| |r\u00b2 = 0.43|r\u00b2 = 0.74| |r\u00b2 = 0.39|r\u00b2 = 0.82|\n| |20|20|10|10| |\n|r\u00b2 = 0.37|r\u00b2 = 0.64|r\u00b2 = 0.88|r\u00b2 = 0.05|r\u00b2 = 0.23|r\u00b2 = 0.41|# Figure 10. Average ranks (from linear extensions) with varying cutoffs.# 5. Conclusion\n\nIn this work, we proposed a mathematical framework to compare players, and in particular players spanning across different time periods in sports history. We aggregated cutoffs of historical ranking data into a single poset over top players by i) sampling from a data-driven random walk over the symmetric group and ii) adopting a notion of stochastic dominance. Ultimately, some pairs of players can be conclusively compared while others cannot. This approach departs from existing methods in that it relies on ranking data (rather than match-level data, which may or may not exist/be available) and formally comes to terms with the possibility that some comparisons are \u201ctoo close to call.\u201d The two steps in our framework leave ample room for further modeling choices, namely by adapting the construction of (1) and (4) as needed for the particular application in mind. These applications also motivate the analytical study of data-driven sampling of permutations.\n\nWe implemented our methods using data from both the ATP and the WTA to find the GOATs in the respective categories. Our experimental results suggest that Nadal, Federer, and Djokovic are the ones that tend to come ahead as GOATs of the ATP under most circumstances, whereas Graf and S. Williams are the ones that do so in the case of the WTA. Through our experiments, we also noted major differences in terms of career and dominance longevity for top players across the two associations.",
        "context_id": 37,
        "question": "How many major titles does Serena Williams hold according to the introduction?",
        "answer": [
            "23"
        ],
        "context_length": 42733
    },
    {
        "context": "# 1 Introduction\n\nSince their introduction in 2020, Vision Transformers (ViTs) [14] have been utilized for a wide variety of computer vision tasks such as image classification, synthesis, segmentation and more with great success [10, 32, 53]. Unlike Convolutional Neural Networks (CNNs), which require a structured representation throughout the network, ViTs can process variable length input sequences, even allowing for the sequences to be modified throughout the network. This gives ViTs stronger expressive powers than CNNs [49], but comes at a computational cost due to the quadratic scaling of the self-attention computation. Therefore, there has been increasing research interest in making ViTs more efficient while retaining their task performance. Token reduction has shown to be a promising subfield which directly decreases model complexity by reducing the input sequence through pruning or merging [20], thereby decreasing the computation cost of the self-attention operation. Haurum et al. [20] conducted an in-depth study of 13 different methods, and found that the baseline Top-K pruning-based method, and its extension EViT [34], outperformed the vast majority of more complex methods on four image classification tasks. However, token pruning has the disadvantage that it typically requires the backbone to be fine-tuned in order to maintain good performance even at high keep rates, which by design leads to information loss as tokens are removed, and performs poorly on image synthesis.# step k + 1\n\n| |0.0|0.25|1.0|0.8| |0.0|1.2|0.8|\n|---|---|---|---|---|---|---|---|---|\n|0.25|0.0|1.2|0.7|Update edges with D|1.2|0.0|0.3| |\n|1.0|1.2|0.0|0.3| |0.8|0.3|0.0| |\n|0.8|0.7|0.3|0.0| | | | | |\n\nFig. 1: Illustration of the Agglomerative Clustering Method. Prior hard merging-based methodologies have focused on using either partition-based approaches (e.g. DPC-KNN [73] or K-Medoids [41]) or graph-based (i.e. ToMe). All of these methods globally cluster the input tokens through the use of cluster centers. In contrast, our Agglomerative Token Clustering (ATC) method builds clusters locally, i.e. by iteratively combining the most similar tokens, until the desired amount of tokens remain. A step of this process is shown here, where a graph of nodes (in this case, tokens) are connected with edges based on their similarity. The most similar pair of nodes are combined, and the edges are updated using linkage function D, in this case Dcomplete (Eq. 2).\n\ntasks [2, 3]. Motivated by these observations, we focus on merging-based token reduction methods as they show the most versatility.\n\nWe present a novel merging-based token reduction method, Agglomerative Token Clustering (ATC), which outperforms all prior merging-based and pruning-based token reduction methods on both classification tasks and dense computer vision tasks such as image synthesis and object detection & segmentation, and achieve comparable performance without any fine-tuning, i.e. off-the-shelf. This is the most diverse set of experiments considered within token reduction, where previous methods have been evaluated just on classification [2, 20, 34], image synthesis [3], or detection and segmentation [38]. Through this suite of diverse tasks we demonstrate the efficacy of the proposed ATC method.\n\nATC is motivated by the observation that prior merging-based methods such as K-Medoids [41], DPC-KNN [73], and ToMe [2] all perform merging globally, which may lead to redundant clusters. Our hypothesis is that hierarchically merging similar, and thus redundant, observations results in more informative groupings. A natural and robust methodology for including this notion into token reduction is via agglomerative clustering [16], where tokens are iteratively clustered in a bottom-up hierarchical way, see Figure 1.# 1. Our contributions are the following:\n\n- We propose Agglomerative Token Clustering (ATC), a novel parameter-free hierarchical merging-based token reduction method.\n- Using ATC we achieve state-of-the-art performance on image classification, image synthesis, and object detection & segmentation tasks, outperforming all other token reduction methods, including both merging-based and pruning-based token reduction methods.\n- We show ATC can reach comparable performance to the prior fine-tuned state-of-the-art in image classification and object detection & segmentation when applied off-the-shelf, i.e. without any fine-tuning.# Efficient Transformers\n\nAs ViTs have become widely adapted by the computer vision community, there have been several attempts at making ViTs more efficient. These attempts range from model pruning [5, 8, 9, 42, 56, 71], quantization [33, 37], structured downsampling [19, 25, 39, 46], sparsification of the attention module [4, 65], part selection modules [22, 27, 28, 61, 62], and dynamically adjusting the size of input patches [1, 7, 12, 35, 63, 64, 72, 74]. Lastly, the field of token reduction has emerged, which is the topic of this paper.# Token Reduction\n\nThe goal of token reduction is to sparsify the sequence of patches, also referred to as tokens, processed by the ViT. This has been achieved through either token pruning or token merging [20]. Token pruning focuses on removing tokens either through keeping the tokens with the highest attention from the class (CLS) token [17, 20, 34, 40, 66, 67, 69], introducing a gating mechanism based on the Gumbel-Softmax trick [29, 30, 38, 50, 66], sampling based approaches [17, 70] modifying the training loop [8, 31] or reinforcement learning [45].\n\nToken merging, on the other hand, combines tokens instead of explicitly pruning them. This can be done either through hard or soft merging of tokens. Hard merging includes techniques such as the partition-based K-Medoids [20, 41] and DPC-KNN [73], as well as the bipartite graph-based approach Token Merging (ToMe) [2, 3]. Hard merging-based approaches are characterized by having the tokens assigned to clusters in a mutually exclusive manner. In contrast, soft merging techniques let tokens be assigned to multiple clusters resulting in cluster centers being a convex combination of tokens. This combination is either based on similarity between the spatial tokens [41], or the similarity between the spatial tokens and a set of explicit queries which are optimized [21, 52, 68, 75].\n\nThe token reduction field has been moving at an immense speed, resulting in little to no comparisons between methods. This was rectified by Haurum et al. [20], who compared 13 different token reduction methods over four image classification datasets. The study provided insights into the token reduction process through extensive experiments, showing that the Top-K and EViT pruning-based methods consistently outperformed all other token reduction methods on the considered classification datasets. However, Bolya and Hoffmann found that# 4 J. B. Haurum et al.\n\nmerging-based methods outperform pruning-based methods for image synthesis [3], while Bolya et al. showed that the merging-based ToMe [2] can perform well on several classification tasks without any fine-tuning.\n\nDespite its impressive performance, a core component of ToMe is the bipartite matching algorithm, where tokens are split into two exclusive sets between which a bipartite graph is created. This inherently limits which tokens can be merged as there is no within-set comparison and limits the keep rate, r, such that it must be 50% or higher. Therefore, we make a single, but important, modification to the ToMe method, replacing the bipartite matching algorithm with the classical agglomerative clustering method [16].# 3 Agglomerative Token Clustering\n\nSimilar to previous token merging methods, the objective of ATC is to merge redundant tokens, while preserving or enhancing the performance of the ViT model. We insert the token merging operation between the self-attention and Multi Layer Perceptron (MLP) modules in a ViT block. This is consistent with prior merging-based methods, such as ToMe [2].\n\nWe believe the agglomerative clustering algorithm is a more appropriate choice as it builds the clusters in a bottom-up manner, such that redundant features are clustered early on, while more diverse features are kept unmodified for as long as possible. Prior partition-based (e.g. DPC-KNN and K-Medoids) and graph-based (i.e. ToMe) merging methods are limited by having to create clusters globally, necessitating the selection of cluster centers which may be redundant. In contrast, ATC creates clusters in a sequential manner, resulting in a local merging approach which leads to the most redundant feature to be clustered at any step in the process. We revisit the ToMe method in Section 3.1 and agglomerative clustering in Section 3.2.# 3.1 Token Merging\n\nToMe was designed for seamless integration into ViTs, allowing for minimal performance loss without necessitating fine-tuning. Its key feature is a fast bipartite merging operation, placed between the self-attention and MLP modules in the ViT block. A bipartite graph is then constructed by setting the edge between nodes in token subsets A and B equal to their similarity, where the t highest valued edges are kept while allowing only a single edge for each node in subset A. Bolya et al. investigated how to construct A and B, and found the best performance was achieved by assigning tokens in an alternating manner. This inherently limits which tokens can be merged, which can lead to redundant clusters as spatially co-located patches contain semantically similar information.# 3.2 Agglomerative Clustering\n\nAgglomerative Clustering is a classical method for bottom-up hierarchical clustering, where each element is initially its own cluster [16]. The elements are com-# Agglomerative Token Clustering\n\nbined by iteratively comparing the clusters according to some linkage function with distance metric D(\u00b7), combining the two closest clusters in each iteration. This is repeated until a certain stopping criteria is met, such as the number of desired clusters, leading to a static reduction method, or a minimum distance between clusters, leading to a dynamic reduction method. This is illustrated in Figure 1. In this paper we consider the static reduction scenario. Similar to Bolya et al., we use the cosine distance as our distance metric D(\u00b7) and the keys from the self-attention module as token features. The choice of linkage function can have a large impact on how the elements are clustered. We consider the three most common ones: single (Eq. 1), complete (Eq. 2), and average (Eq. 3) [43].\n\n|Linkage Function|Equation|\n|---|---|\n|Single|D(I, J)single = mini\u2208I, j\u2208J D(i, j) (1)|\n|Complete|D(I, J)complete = maxi\u2208I, j\u2208J D(i, j) (2)|\n|Average|D(I, J)average = (1 / |I||J|) \u03a3i\u2208I \u03a3j\u2208J D(i, j) (3)|\n\nwhere I and J are clusters with elements i \u2208 I and j \u2208 J. After the stopping criteria has been reached we average the tokens in each cluster to get an updated cluster representation. However, as tokens are merged they represent more than one input patch. In order to advantage tokens that capture a larger spatial extent, we use the weighted average for the cluster representation and proportional attention in the self-attention module as proposed by Bolya et al.# 4 Experiments\n\nTo evaluate the versatility and applicability of ATC, we conduct assessments across a diverse set of tasks (image classification, image synthesis, and object detection & segmentation) and datasets. For the image classification task we follow the experimental protocol of Haurum et al. [20], evaluating multi-class and multi-label classification performance across four classification datasets using three DeiT models and token reduction performed at three discrete stages. We also follow the MAE experiments of Bolya et al. [2], evaluating on the ImageNet-1K dataset with token reduction at all stages following a constant and linearly decreasing schedule. For the image synthesis task, we follow the proposed protocol of Bolya & Hoffman [3], incorporating the token reduction method into the Stable Diffusion image generation model [53]. Lastly, for the object detection and segmentation task we follow the experimental protocol of Liu et al. [38], evaluating on the COCO-2017 dataset [36]. Two versions of the ViT-Adapter model are used with the token reduction method incorporated at three discrete stages.# 2.5\n\n| |ATCsingle|ATCcomplete|average|\n|---|---|---|---|\n|2.0|ATCcomplete| | |\n|1.5|ATCsingle| | |\n|1.0|ToMe| | |\n|0.5|DPC-KNN| | |\n|0.0|K-Medoids| | |\n\n0 2 4 6 8 10 25% 50% 70% 90%\n\nAverage Rank Keep Rate\n\nFig. 2: Average Token Reduction Rank (Lower is better). We compare our proposed ATC method with the hard-merging based token reduction methods investigated by Haurum rates, three model capacities, and four datasets, and plot with \u00b11 standard deviation similar to Haurum et al. We test three versions of ATC, varying the linkage function, and find that the three variants all outperform the prior merging-based methods.\n\nFig. 3: Percentage Point Difference per Keep Rate. We compute the average difference between our proposed ATC method and the best prior merging-based methods investigated by Haurum et al. [20]. We average across four keep rates for each keep rate, measured in percentage points. We average across the three model capacities and four datasets. We find that for high keep rates r = {70, 90}% ATC is comparable to the prior best merging method, while for r = {25, 50}% our proposed ATC method leads to significant performance gains.\n\nAll experiments are conducted using ATC with the single, complete, and average linkage functions, respectively. The different linkage functions are indicated using a superscript, such as ATCsingle for the single linkage function. For the image classification and object detection & segmentation tasks we report both off-the-shelf results, where ATC is inserted into the pre-trained model and evaluated without any fine-tuning, and results after fine-tuning. For the image synthesis task we report off-the-shelf results, similarly to Bolya & Hoffman [3].# 4.1 Cross-Dataset Classification Performance\n\nFollowing the experimental protocol proposed by Haurum et al. [20], we evaluate the performance of ATC across four classification datasets covering multi-class and multi-label classification: ImageNet-1K [13], NABirds, [59] COCO 2014 [36], and NUS-WIDE [11] datasets. ImageNet-1K and NABirds are evaluated with the accuracy metric, while COCO and NUS-WIDE are evaluated with the mean Average Precision (mAP) metric. Token reduction is applied at the 4th, 7th, and 10th ViT block, where at each stage only r \u2208 {25, 50, 70, 90}% of the available tokens are kept. The backbone model is a DeiT [58] model trained without distillation, across three model capacities: DeiT-Tiny, DeiT-Small, and DeiT-Base. The models are denoted DeiT-T, DeiT-S, and DeiT-B, respectively. We consider# 7\n\n|Model|ImageNet|NABirds|COCO|NUS-WIDE| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|(Top-1 Acc %)|(Top-1 Acc %)| |(mAP %)|(mAP %)| | | | | | | | | |\n|DeiT-T| |70|70|60|60|50|50|80|80|60|70|70|60|\n|DeiT-S| |70|70|60|70|60|60|80|80|60|70|70|60|\n|DeiT-B| |70|70|60|70|60|60|80|80|60|70|70|60|\n|ToMe|Limited to r \u2265 50%|-|-|-| | | | | | | | | |\n\nKeep Rate\n\n| |ImageNet|ImageNet|ImageNet|ImageNet|NABirds|NABirds|NABirds|NABirds|COCO|COCO|COCO|COCO|NUS-WIDE|NUS-WIDE|NUS-WIDE|NUS-WIDE|\n|---|---|---|---|---|\n| |25%|50%|70%|90%|25%|50%|70%|90%|25%|50%|70%|90%|25%|50%|70%|90%|\n|Keep Rate|70|70|60|60|60|50|50|60|80|80|60|70|70|70|60|60|\n\nFig. 4: Hard Token Merging Method Comparison with the DeiT Backbone.\n\nWe compare the hard-merging token reduction methods considered by Haurum et al. [20] with the proposed ATC method. All methods have been fine-tuned. Model performance is measured across keep rates, r, denoted in percentage of tokens kept at each reduction stage, and with the DeiT-{Tiny, Small, Base} models. Comparison with all 13 token reduction methods considered by Haurum et al. can be found in the supplementary materials. ImageNet and NABirds performance is measured with top-1 accuracy, whereas COCO and NUS-WIDE is measured with mAP. The baseline DeiT performance is noted with a dashed black line. Note that ToMe is limited to r \u2265 50%, and that ATCaverage and ATCcomplete often overlap.\n\nWe find that across all three backbone capacities, the fine-tuned ATC methods consistently outperform or match the performance of the previously proposed merging-based methods using all three proposed linkage functions, see Figure 2 and Figure 4 for details. We also investigate the average improvement in performance between ATC and the prior best merging-based methods, see Figure 3.# 8 J. B. Haurum et al.\n\n|DPC-KNN|K-Medoids|ATCaverage|DPC-KNN|K-Medoids|ATCaverage|\n|---|---|---|---|---|---|\n| | | | | | |\n\nFig. 5: Token Merging Visualization with r = 25%. We visualize the three token merging steps for DPC-KNN [73], K-Medoids [41] and our ATCaverage on two examples from NABirds [59] with a DeiT-B backbone. The first row is the input image, and each subsequent row is the constructed clusters after the first, second, and third reduction stage. In subfigure (a) we find that there is a major difference in the final clustering of the data, where our ATC method creates separate clusters for the bird, wood pole, and background. In contrast, DPC-KNN and K-Medoids create mostly arbitrary clusters. Similarly in subfigure (b) we see that the DPC-KNN method creates very arbitrary clusters, while K-Medoids and ATC create more meaningful clusters. However, the ATC clusters still better contain the bird in the image, while the K-Medoids clusters have background patches in all clusters. We find this to be a repeating occurrence and believe this is the reason for the large improvement by ATC on NABirds at r = 25%.\n\nWe find that at keep rates of 70% and 90%, all three linkage functions achieve comparable results, while at lower keep rates the single linkage function drops in performance. We believe this is due to the chaining phenomenon where two distinct clusters are merged due to outliers within the clusters [16]. However, at keep rates of 25% and 50% we find that both the average and complete lead to large performance improvements compared to the prior best merging methods (up to 2.5 percentage points as per Figure 3). In some cases we even find that at keep rates of 25% we can improve performance significantly, such as with the DeiT-S and DeiT-B backbones on NABirds, where ATC with the average linkage function results in a 5.7 and 9.6 percentage point increase in accuracy over the prior state-of-the-art, respectively. Through qualitative eval-# Agglomerative Token Clustering\n\nuation, as seen in Figure 5, we can provide intuition for why ATC outperforms prior merging-based methods. We find that DPC-KNN and K-Medoids create arbitrary clusters whereas ATC creates more meaningful clusters even at the third reduction stage with r = 25%.\n\nLastly, we find that applying ATC off-the-shelf on the DeiT backbone leads to good performance at high keep rates, matching or even outperforming the prior best performing merging methods. However, when the keep rate is lowered the performance drops dramatically, which can be rectified through fine-tuning. For full details on the off-the-shelf results, we refer to the supplementary materials.# 4.2 Classification with Self-Supervised Pretraining\n\nWe follow the protocol of Bolya et al. [2] and compare the performance on ImageNet-1K when applying token reduction on pretrained ViT models, specifically the MAE models trained initially with masked image modelling [23] and fine-tuned on ImageNet [13]. We consider the off-the-shelf performance using the publicly available checkpoints, as well as fine-tuning using the original fine-tuning protocol by He et al. [23]. We consider the ViT-Base, ViT-Large, and ViT-Huge models, where token reduction is applied at every block, following the constant and linear decreasing schedules proposed by Bolya et al.:\n\ntl = t (4)\n\ntl = 2t - 1, L2tl (5)\n\nwhere L is the total number of ViT blocks, tl is the number of tokens to be removed at ViT block l = {0, 1, . . . , L\u22121}, and t is a hyperparameter controlling the aggressiveness of the reduction. Both schedules result in tL tokens being removed in total, with the linear schedule removing more tokens at early layers.\n\nWe fine-tune the ViT block models using both schedules in the most aggressive settings considered by Bolya et al.: ViT-B with t = 16, ViT-L with t = 8, and ViT-H with t = 7. Note that we also fine-tune ToMe, leading to a second set of values that are a small improvement compared to the original paper. We also replicate the larger sweep over t values on off-the-shelf ViT models with weights from different training protocols [23, 55, 57], originally performed by Bolya et al. These are found in the supplementary materials. We find that ATC consistently outperforms ToMe across both token reduction schedules and when using off-the-shelf and fine-tuned models, see Table 1. We find that ATC drastically outperforms ToMe when using the linear token scheduler, and when using models with lower backbone capacity. This is especially observable on the ViT-Base backbone where using a linear schedule off-the-shelf leads to a 10.5 percentage point improvement when using ATC instead of ToMe. When fine-tuning the backbone using the same token scheduler this gap is reduced to 1.6 percentage points when fine-tuning, with ATC still outperforming ToMe. This illustrates the clear general benefit of using ATC for adapting already trained.# Table 1: MAE Pretrained Backbone Comparison\n\nWe compare ToMe and ATC with a self-supervised MAE backbone on ImageNet-1K. We consider three backbones: ViT-Base, ViT-Large, and ViT-Huge. Tokens are removed at each ViT block, following the constant (Eq. 4) or linear (Eq. 5) token schedules. The best performing method per column is denoted in bold. A blue background indicates that the token reduction method was applied off-the-shelf on the ViT backbone, whereas all other models have been fine-tuned. For each ATC method we write the performance improvement over ToMe in parenthesis after the model accuracy.\n\n|Schedule|ViT-B (t = 16)|ViT-L (t = 8)|ViT-H (t = 7)| | | | | |\n|---|---|---|---|---|---|---|---|---|\n| |Constant|Linear|Constant|Linear|Constant|Linear| | |\n|No Reduction|83.6|85.9|86.9| | | | | |\n|ToMesingle (ours)| | | | | | | | |\n|ATCaverage (ours)|80.1 (+1.6)|67.1 (+10.5)|78.5|79.7 (+1.2)|65.8 (+9.2)|56.6| | |\n| |84.2| |84.8 (+0.6)|82.5 (+2.4)|80.1|86.0| | |\n| | | |86.4 (+0.4)|85.6 (+0.6)|85.0| | | |\n|ATCcomplete (ours)|80.2 (+1.7)|67.1 (+10.5)| | | | | | |\n|ATC| |84.8 (+0.6)|82.6 (+2.5)| | | | | |\n| | |84.9 (+0.7)|82.6 (+2.5)|86.4 (+0.4)|85.6 (+0.6)|86.4 (+0.4)|85.8 (+0.8)| |\n|ToMesingle (ours)| | |82.3 (+0.4)|80.2 (+1.6)|81.9| | | |\n| |82.2 (+0.3)|80.0 (+1.4)|78.6|85.1| | | | |\n| | |85.3 (+0.2)|84.5 (+0.7)|83.8|86.4| | | |\n| | | |86.6 (+0.2)|86.3 (+0.2)|86.1| | | |\n|ATCcomplete (ours)|82.5 (+0.6)|80.1 (+1.5)| | | | | | |\n|ATC| |85.3 (+0.2)|84.5 (+0.7)| | | | | |\n| | |85.4 (+0.3)|84.3 (+0.5)| |86.7 (+0.3)|86.3 (+0.2)| | |\n| | | |86.7 (+0.3)|86.4 (+0.3)| | | | |# Table 2: Stable Diffusion FID Comparison\n\nWe apply ToMe and ATC over the self-attention blocks to an off-the-shelf (i.e. frozen) Stable Diffusion model, denoted with a blue background. We compare the FID score across different keep rates (Note that a lower FID score is better). The best FID score per column is denoted in bold, and the best per row is underlined.\n\n|r (%)|100|90|80|70|60|50|40|\n|---|---|---|---|---|---|---|---|\n|ToMesingle (ours)|33.80|33.51|33.45|33.33|33.40|33.36|33.22|\n| |33.80|33.77|33.61|33.59|33.57|33.63|33.67|\n| |33.80|33.63|33.48|33.53|33.43|34.01|36.95|\n|ATCcomplete (ours)|33.80|33.67|33.71|33.70|33.74|33.56|33.65|\n|ATC| | | | | | | |# 4.3 Image Synthesis with Stable Diffusion\n\nBolya & Hoffman [3] demonstrated how a modified ToMe algorithm can be incorporated into an image generation model, specifically Stable Diffusion [53], without any fine-tuning, resulting in improved quality of the generated images as well as generation speed. We follow the experimental setup of Bolya & Hoffman, inserting the token reduction method only over the self-attention block. We generate two images with a resolution of 512 \u00d7 512 pixels for each class in the ImageNet-1K dataset, following the exact setting from Bolya & Hoffman [3].# Table 3: Object Detection & Segmentation Results\n\nWe compare the performance of ATC and ToMe on the dense object detection & segmentation task using the COCO 2017 dataset, following Liu et al. [38]. The ViT-Adapter method is used, with DeiT-T and DeiT-S backbones. The best performing method per column is denoted in bold. A blue background indicates that ATC and ToMe were applied off-the-shelf on the frozen ViT-Adapter backbone, whereas all other models have been fine-tuned.# (a) ViT-Adapter-T Backbone\n\n|mAPbox|mAPmask|r (%)|r (%)| | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| |25|50|70|90|25|50|70|90| | | |\n|ViT-Adapter-T|45.8|40.9|ToMesingle (ours)|33.8|43.7|45.5|45.8|31.5|39.2|40.7|40.8|\n|ATCaverage (ours)|13.9|38.5|44.8|45.7|12.6|34.6|39.9|40.8| | | |\n|ATCcomplete (ours)|34.9|43.9|45.6|45.8|32.3|39.3|40.6|40.8| | | |# (b) ViT-Adapter-S Backbone\n\n|mAPbox|mAPmask|r (%)|r (%)| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |25|50|70|90|25|50|70|90| | | | |\n|ViT-Adapter-S|48.5|42.8|ToMesingle (ours)| |16.9|41.9|47.4|48.4|14.4|36.8|41.8|42.7|\n|ATCaverage (ours)|-|42.4|47.8|48.4|-|38.2|42.3|42.7| | | | |\n|ATCcomplete (ours)|38.3|46.6|48.0|48.4|34.7|41.3|42.4|42.7| | | | |\n\nWe implement the setup using the HuggingFace Diffusers framework [48], use the stable-diffusion-v1-5 model, and use the exact same seed for each model. We investigate the effect when using keep rates of r \u2208 {40, 50, 60, 70, 80, 90}%. In order to evaluate the quality of the generated images, we measure the Fr\u00e9chet Inception Distance (FID) score [26] between the generated images and a reference dataset consisting of 5000 images, created by taking the first five validation images per ImageNet-1K class.\n\nWe compare the FID scores of the ToMe and ATC models in Table 2. The ToMe results are computed using the same setup as the ATC models, and therefore differ from the original paper. Even though our generated images lead to a generally higher (thus worse) FID for ToMe, we find that the general trends observed in the original paper still hold. We find that across all linkage functions the ATC model outperforms or matches the ToMe model. While ToMe achieves a minimum FID of 33.57, this is outperformed by both the single and average linkage functions with FID scores of 33.43 and 33.22, respectively. The complete linkage function in general performs worse than both the single and average linkage functions, but we also find that the results with single linkage diverges when r \u2264 50%. We observe that the average linkage function leads to better results as the keep rate is reduced, achieving the best performance when r = 40%, whereas the other methods peak at r = 50% and r = 60%. By looking at the generated images, see Figure 6, we find that the average linkage function manages to keep a lot more of the distinctive patterns and contextual background. In comparison, the single linkage function loses the head pattern, and all methods except ATCaverage convert the tree branch to a tree pole resulting in a change in pose of the generated magpie.# J. B. Haurum et al.\n\nToMe\nATCsingle\nATCaverage\nATCcomplete\n\nFig. 6: Image Synthesis Visualization. We visualize image synthesis results for the \u201cmagpie\u201d ImageNet class. The first row is the standard Stable Diffusion result, with each subsequent row having token merging applied with r \u2208 {80, 60, 40}%. Examples for all considered keep rates and more classes can be found in the supplementary materials. We find that as the keep rate is lowered the ATCsingle method drastically changes the image, specifically the head and patterns of the magpie, as well as having a more monochrome background. We see that even at r = 40% ATCaverage manages to keep most of the details such as the branch the magpie is sitting on, whereas ToMe and ATCcomplete keep the general patterns but switches from a branch to a wooden pole (highlighted with a red arrow), leading to a change in pose of the generated magpie.# 4.4 Object Detection and Segmentation\n\nLiu et al. [38] conducted a systematic comparison of several token pruning methods for object detection and segmentation on the COCO 2017 dataset [36]. The Mask-RCNN model [24] is used for predicting bounding boxes and segmentation masks with a ViT-Adapter backbone model [10], building upon an ImageNet pretrained DeiT model [58]. Instead of the typical windowed self-attention used in ViT-Adapter, Liu et al. use global self-attention and train using the original ViT-Adapter settings for 36 epochs. Tokens are reduced at the 4th, 7th, and 10th ViT block, and a DeiT-Tiny and DeiT-Small backbone are used. When applying the token reduction method, the ViT-Adapter-Tiny and ViT-Adapter-Small models are fine-tuned for 6 and 4 epochs, respectively, using the MMDetection framework [6] and following the training protocol of Liu et al. Unlike the original protocol by Liu et al. which only considered a keep rate of r = 70%, we extend the considered keep rates to r \u2208 {25, 50, 70, 90}%, similar to the protocol used by Haurum et al. [20]. We evaluate both ToMe and ATC in the off-the-shelf and fine-tuned scenarios. As the Injector and Extractor modules in the ViT-Adapter expect the original number of tokens, we back-project through the clustering when relevant, leading to the original number of patches.\n\nAs is apparent in Table 3, we find that both ToMe and ATC are very strong token reduction methods for detection and segmentation. When applying ToMe and ATC off-the-shelf (i.e. without fine-tuning the ViT-Adapter backbone) with keep rates r \u2208 {70, 90}%, both methods can match the detection and segmentation performance of the baseline ViT-Adapter backbones. When the keep rate is lowered to r = 50% we find that our ATC method with the average and complete linkage function outperforms ToMe by 4 percentage points in detection mAP and 3 percentage points in segmentation mAP.\n\nWhen fine-tuning the Tiny and Small backbones we see major improvements at keep rates of 25% and 50%, such as a 26 and 22 percentage points improvement in bounding box mAP for ATCsingle with the Tiny and Small backbones and r = 25%, respectively. In comparison, fine-tuning has less of an effect on ATCaverage and ATCcomplete as the off-the-shelf performance is already high. We also observe that after fine-tuning, our ATC method still outperforms ToMe, though with a smaller margin, and in several cases outperforms the baseline ViT-Adapter performance. Lastly, we see that by fine-tuning, we can get comparable performance to the baseline method at keep rates of 50%, whereas ToMe is several percentage points worse than the baseline ViT-Adapter.# 5 Discussion\n\nThrough our experiments we have demonstrated how our proposed ATC method consistently outperforms prior merging-based token reduction methods across a diverse set of tasks. This includes the prior best merging-based method ToMe, which we confidently outperform across all considered tasks. While our analysis have been focused on comparing ATC with prior merging-based approaches,# 14 J. B. Haurum et al.\n\nWe also find that ATC outperforms pruning-based methods on the image classification task (Following the setup from Sec. 4.1) and object detection and segmentation, where ATC outperforms the prior state-of-the-art pruning-based SViT method from Liu et al. [38]. Detailed experimental results including the pruning-based methods are available in the supplementary materials, and establishes that our ATC method is the current best token reduction method.# 6 Limitations\n\nWe show that ATC is a very adaptable method, achieving state-of-the-art performance across the different classification, image synthesis, and object detection & segmentation tasks considered. However, ATC is not without its limitations. Firstly, ATC requires the selection of a linkage function, adding an extra hyperparameter. While there is not a specific linkage function that is the best at all tasks, we find that the average or complete linkage functions are good general choices, whereas the single linkage function often underperforms when working with more aggressive keep rates, matching prior linkage function recommendations [16]. Secondly, we find that the inference throughput of the current implementation of ATC is hampered by the available implementations of the agglomerative clustering functions, which are all non-batched and often CPU-bounded. This is discussed and analyzed at lengths in the supplementary materials. However, there are also clear indications that these limitations can be lifted if the current frameworks are slightly modified.# 7 Conclusion\n\nIn this work, we introduce Agglomerative Token Clustering (ATC), a novel hard-merging token reduction approach grounded in the principles of classical bottom-up hierarchical clustering. ATC distinguishes itself by efficiently merging redundant observations early on, thus preserving the semantic richness of more diverse observations. We evaluate our method across the most diverse sets of tasks considered in the literature, covering both classification, synthesis, detection, and segmentation tasks. In image classification, image synthesis, and object detection & segmentation, ATC sets a new state-of-the-art, highlighting its significant contribution to the field. We also demonstrate that ATC can achieve comparable performance to the prior state-of-the-art without any fine-tuning, i.e. when applied off-the-shelf. We are optimistic that ATC will inspire subsequent advancements, leveraging classical clustering methods to enhance modern neural architectures.",
        "context_id": 38,
        "question": "What linkage function is used in the Agglomerative Token Clustering (ATC) method as shown in Figure 1 of the Introduction?",
        "answer": [
            "Dcomplete"
        ],
        "context_length": 34125
    },
    {
        "context": "# 1 INTRODUCTION\n\nMachine learning methods [1] have made significant progress under the closed-world assumption, where test data is drawn from the same distribution as the training set, known as in-distribution (ID). However, in the real world, models inevitably encounter test samples that do not belong to any training set category, commonly referred to as out-of-distribution (OOD) data. OOD detection [2] aims to identify and reject OOD samples rather than make overconfident predictions arbitrarily [3] while maintaining accurate classification for ID data. Models with superior OOD detection.\n\n\u2217 The first two authors contributed equally to this research.# Authors\u2019 addresses:\n\nShuo Lu, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Yingsheng Wang, Anhui University, Hefei, China; Lijun Sheng, University of Science and Technology of China, Hefei, China; Aihua Zheng, Anhui University, Hefei, China; Lingxiao He, Meituan, Beijing, China; Jian Liang, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China, liangjian92@gmail.com.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\n\u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n\nManuscript submitted to ACM# Lu et al.\n\nCapabilities are more reliable and have important applications in numerous security-critical scenarios. For instance, in medical diagnosis systems, a model that cannot detect OOD samples will misjudge unknown diseases and cause serious misdiagnosis [4]. Similarly, autonomous driving algorithms [5] should detect unknown scenarios and resort to human control to avoid accidents caused by arbitrary judgment.\n\nNotably, several previous efforts have been dedicated to surveying and summarizing OOD detection in recent years. Yang et al. [6] discuss OOD detection with several similar topics and categorize existing work into classification-based, density-based, distance-based, and reconstruction-based methods. Cui and Wang [7] conduct a survey on OOD detection from a methodological perspective but with an alternative classification criterion, including supervised, semi-supervised, and unsupervised methods. Additionally, Lang et al. [8] offer a review of OOD detection methods in natural language processing. However, previous works focus too much on the discussion from the perspective of methods and lack an in-depth exploration from the viewpoint of task scenarios. Establishing a clear taxonomy of task scenarios can enhance a comprehensive understanding of the field and assist practitioners in selecting the appropriate method. Moreover, given the recent introduction of new paradigms (e.g., test-time learning paradigm [9]) and methods based on large pre-trained models [10], there is an urgent need for a survey that incorporates the latest technologies.\n\nIn this survey, we for the first time review the recent advances in OOD detection with a problem-oriented taxonomy, as illustrated in Fig. 1. Based on whether the method needs to control the pre-training process, we categorize OOD detection algorithms into training-driven and training-agnostic methods. Considering the rapid development of large pre-trained models nowadays, we also regard large pre-trained model-based OOD detection as a separate section. In detail, training-driven methods achieve high detection capability by designing the optimization process of the training stage. They are further classified and discussed according to whether OOD data is used in training. Training-agnostic methods distinguish OOD data from ID ones based on a well-trained model, skipping the time-consuming and expensive pre-training process in practice. According to whether utilizing test samples to further improve OOD detection performance, we categorize them into post-hoc and test-time methods. Large pre-trained model-based OOD detection methods focus on models such as vision language models or large language models, which are pre-trained on vast datasets and excel in a wide array of tasks. We discuss them in terms of whether they have access to a few examples, including zero-shot, few-shot and full-shot scenarios.\n\nThe remainder of this survey is organized as follows. We recap the related work of OOD detection in Sec. 2. Next, we summarize training-driven OOD detection approaches in Sec. 3, and introduce the training-agnostic OOD detection methods in Sec. 4. Then, in Sec. 5, we introduce large pre-trained model-based OOD detection. An overview of the evaluation metrics, experimental protocols, and applications is presented in Sec. 6. Following that, we discuss promising trends and open challenges in Sec. 7 to shed light on underexplored and potentially critical avenues. Finally, we conclude this survey in Sec. 8.# 2 RELATED WORK\n\nAnomaly Detection. Anomaly detection (AD) involves identifying data points, events, or observations that deviate significantly from the dataset\u2019s normal behavior [119]. These anomalies can indicate critical incidents, such as fraud [120], network intrusions [121], or system failures [122]. The process involves using statistical, machine learning, or deep learning methods to model normal behavior and detect deviations. Anomaly detection is crucial in domains like cybersecurity, finance, healthcare, and manufacturing, where recognizing unusual patterns quickly can prevent significant losses or improve operational efficiency. While both AD and OOD detection aim to identify unusual or\n\nManuscript submitted to ACM# Recent Advances in OOD Detection: Problems and Approaches# Novelty Detection\n\nNovelty detection (ND) focuses on identifying new or unknown data points that a model has not seen during training [123]. This process is essential in situations where the system needs to adapt to evolving conditions or when it\u2019s crucial to flag new, unseen scenarios. Unlike AD, which seeks to find patterns that deviate from the norm, novelty detection aims to discover entirely new patterns. Applications include identifying new trends in social media [124], discovering new species in biological data [123], or detecting new topics in document streams [125]. Essentially, ND deals with surprises in familiar contexts, while OOD detection deals with data from unfamiliar contexts.# Lu et al.\n\nimage recognition systems [128], where encountering unknown objects or scenarios is common, and the system must be able to handle these gracefully. OSR is concerned with identifying new classes that are not present during training, which is generally achieved by dividing the categories of the same dataset into base classes and new classes, which means that new classes and base classes usually come from the same domain. In contrast, OOD detection focuses on identifying any data that is different from the training distribution, regardless of whether it belongs to a new class or a different domain altogether. Essentially, OSR is a subset of OOD detection, specifically aimed at classifying new class types within the same domain.# Outlier Detection\n\nOutlier detection (OD) is the process of identifying data points that are significantly different from the majority of the data [129]. It is similar to AD but focuses more on the identification of individual data points rather than patterns. Outliers can arise due to variability in measurement, experimental errors, or genuinely novel variations. Applications include fraud detection, fault detection, and removing anomalous data from datasets to improve model accuracy [130]. Compared to OOD detection, OD is more of a transductive scenario, as it inherently has access to outliers. However, classical OOD detection only encounters outliers at the time of deployment.# Zero-shot learning\n\nZero-shot learning is a paradigm in machine learning where the goal is to recognize objects without having seen examples of these objects during the training phase [131\u2013133]. The fundamental challenge in zero-shot learning is to effectively transfer knowledge from seen to unseen classes [131]. It has been primarily tackled by learning semantic relationships between classes through attributes or by embedding both seen and unseen classes into a shared semantic space. While both deal with unknowns during inference, zero-shot learning tries to classify new categories [134], whereas OOD detection flags data that is anomalous or unfamiliar to the training data distribution [64].# Selective Classification\n\nSelective classification, also known as reject option classification, provides a mechanism for models to abstain from making a decision when they are not sufficiently confident in their predictions [135, 136]. Selective classification involves a model deciding when to make a prediction based on its confidence level, effectively choosing not to predict when uncertain [137]. On the other hand, OOD detection identifies data points that differ from the training distribution, aiming to flag them as unfamiliar to the model. While both methods manage uncertainty, selective classification deals with confidence in making predictions on ID data [136, 138], and OOD detection focuses on recognizing and handling data that is not represented in the training set.# 3 PROBLEM: TRAINING-DRIVEN OOD DETECTION\n\nIn the training-driven OOD detection problem, researchers design the pre-training process to obtain models with superior OOD detection capabilities. Based on whether OOD data is accessible during training, we further divide methods under this scenario into two folds: training with only ID data and training with both ID and OOD data, as shown in Fig. 2.# 3.1 OOD Detection Approaches with only ID Data\n\nOverview. Given the ID data, approaches in this section train a model on them and aim to utilize the model for detecting OOD test samples while ensuring accurate classification of the ID data. Approaches with only ID data focus specifically on mining information from ID data, without explicitly relying on other information from real-world OOD data. We further differentiate these methods into the following four categories: Reconstruction-based, Probability-based, Logits-based, OOD Synthesis, and Prototype-based. Considering real-world requirements, we also delve into a specific scenario: Long-tail ID data.# Reconstruction-based\n\nThe reconstruction-based methodology offers a new research avenue by scrutinizing the discrepancy between sample representations before and after reconstruction, which relies heavily on the reconstruction.# Recent Advances in OOD Detection: Problems and Approaches# Test\n\nFig. 2. Illustration of training-driven OOD detection approaches. Dashed borders indicate that they are not used in the specific phase. OOD images are excluded in the \u201cApproaches with only ID data\u201d on the left but are included in the \u201cApproaches with both ID and OOD data\u201d on the right. In both cases, OOD labels are not utilized.\n\nPerformance of models. Fundamentally, the objective of the reconstruction task is to recuperate the inherent semantic content of the dataset, guided by a designated supervisory signal. This premise rests on the notion that OOD data intrinsically possess semantic characteristics that are incongruent with those of ID label information. By quantitatively assessing the extent of this semantic deviation, the model becomes equipped to discern OOD data with a high degree of precision. The approximate process of such methods is shown in Fig. 3.\n\nMoodCat [11] masks random portions of input images and utilizes a generative model to synthesize new images based on the classification results, implementing strong constraints during the synthesis process. Similarly, Zhou [12] introduces an auxiliary module to extract activations of feature vectors, aiding the model in constraining the latent reconstruction space to filter potential OOD data. Following this, MOOD [13] and MOODv2 [14] leverage pretraining tasks based on masked image modeling, exhibiting significant advantages in learning the internal distribution of data. PRE [15] introduces the concept of normalized flow, combined with a penalty based on typicality to constrain reconstruction errors, which discerns differences between OOD and ID data well.\n\nRecently, diffusion models have made remarkable strides in both training stability and the quality of generated images. Leveraging diffusion models to detect OOD data has become a new research direction. Graham et al. [18] address this issue by introducing DDPM, utilizing it for the reconstruction of noise-disturbed images. The model employs multidimensional reconstruction error for the identification of OOD data. The information bottleneck of this model can be externally regulated. Similarly, LMD [16] utilizes diffusion models for OOD validation. LMD disrupts data and then employs diffusion models to reconstruct images separated from the original manifold, distinguishing OOD data by comparing differences with the original manifold. DiGuard [17] directly employs a pre-trained diffusion model for semantic mismatch guidance, aiming to leverage the diffusion model to amplify the disparity between reconstructed OOD images and the original images.# F ' (x)\n\nFig. 3. Illustration of reconstruction-based OOD detection approaches. After extracting the feature representation \ud835\udc3f (\ud835\udc40 ) of the image through a neural network, it is then fed into a reconstruction network (such as VAE or DDPM) to obtain the reconstructed image representation \ud835\udc3f \u2191 (\ud835\udc40 ). By comparing the dissimilarity between the original and reconstructed images, we can identify OOD data, as OOD samples typically exhibit greater dissimilarity before and after reconstruction.\n\nIn this domain, Li et al. [19] model each ID category during training using multiple Gaussian mixture models, and during the prediction phase, it combines Mahalanobis distance metrics to assess the likelihood of anomalous classes. Huang et al. [20] address this issue by introducing two regularization constraints. The density consistency regularization aligns the analytical density with low-dimensional class labels, and the contrastive distribution regularization helps separate the density between ID and OOD samples. Furthermore, LID [21] introduces a new detection criterion for the OOD detection paradox in the context of data generation by deep generative models. It measures whether data should be classified as in-distribution by estimating the local intrinsic dimension (LID) of the learned manifold of the generative model, when the data is assigned a high probability and the probability mass is non-negligible.# Logits-based\n\nThese algorithms primarily focus on the predictions of neural networks, particularly on Logits, which are the results of the neural network\u2019s output layer. Logits typically represent the model\u2019s confidence or probability for each category. LogitNorm [22] proposes a method using logit normalization to enforce a constant vector norm on logits during training to mitigate issues of model overconfidence. Similarly, UE-NL [23], derived from Bayesian networks, normalizes logits while simultaneously learning embeddings and uncertainty scores. It adjusts the learning intensity between samples during training, rendering the learning process robust. DML [24] challenges the potential hindrance to OOD detection performance. Drawing on empirical insights, DML enhances OOD detection performance by decoupling logits and balancing individual components, thereby mitigating the impact of each attribute on the results. It enhances OOD detection performance by decoupling logits and scenario MaxNorm as a parameter, thereby balancing the influence of each attribute on the outcome.\n\nManuscript submitted to ACM# Recent Advances in OOD Detection: Problems and Approaches# Lu et al.\n\nhigh-quality OOD samples. CMG [27] then generates pseudo OOD data by providing class embeddings mixed as conditions to a Conditional Variational Autoencoder (CVAE), and subsequently utilizes this data to fine-tune the classifier for OOD detection. SEM [33] introduces a problem setting for full-spectrum OOD detection, where negative samples are generated using Mixup operations during training. It then leverages both high-level semantic information and low-level non-semantic features, such as style, to identify the source of the data. SHIFT [30] proposes a direct synthesis of OOD image samples based on training examples. It achieves this by employing CLIP to eliminate ID object regions within the training samples. The latent diffusion model is then utilized to substitute these regions with authentic features, all the while taking into account the contextual background. This methodology thereby establishes the model\u2019s capability for rejection. However, ensuring the quality of generated data is often challenging. Furthermore, the quality of the generated data itself may have inherent flaws. ATOL [31] introduces an auxiliary task, wherein both auxiliary ID and auxiliary OOD data coexist. In a low-dimensional latent space, distinct regions are manually sought for both auxiliary ID and auxiliary OOD data, ensuring non-overlapping characteristics in the input space. Subsequently, data generation by the generator guarantees the non-overlapping properties of the input space. Finally, reliability is upheld by aligning genuine ID data with auxiliary ID data, effectively alleviating issues related to erroneously generated OOD instances.# Prototype-based\n\nDuring the model training process, prototype-based OOD detection methods aim to model the ID data using prototypes to learn the common distribution characteristics of the ID data. In the testing phase, the model measures the differences between the sample and class-level prototypes to determine the category of the sample. The general procedure of these methods is roughly illustrated in Fig. 5.# Image\n\nEncoder\n\nProjector\n\nh\n\nPrototype\n\nPrototype\n\nData Sample\n\nData Sample\n\nSimilarity\n\nSimilarity\n\nDecision\n\nBoundary# Lu et al.\n\nDivergence from the output of a pre-trained model. This approach signifies the potential to augment the robustness of OOD detection beyond the achievements of previously optimized methodologies, further fortifying the overall performance. Additionally, Open-Sampling [43] leverages noisy labels from the OOD dataset to rebalance the class priors of the ID training dataset. These labels are sampled from a predefined distribution that complements the original class prior distribution. COOD [44] uses the supervised model to combine individual OOD measures into a single ensemble, similar to the concept of random forests. This approach addresses the limitations of individual OOD methods and can also overcome issues related to data imbalance.# 3.2 OOD Detection Approaches with Both ID and OOD Data# Overview\n\nIn some known deployment scenarios, real OOD data can be easily collected at a low cost. Some methods based on this assumption focus on how to use OOD data for better detection performance. Differing from methods involving OOD Synthesis, in these research directions, models have access to real-world OOD data during the training phase. The primary focus in such problems is on optimizing the model\u2019s decision boundary, rather than the OOD data itself. Due to the introduction of real OOD information, the boundary of these two categories will be accurately calculated.# Boundary Regularization\n\nThe Boundary Regularization class of methods belongs to the traditional Outlier Exposure (OE) approaches. The central idea of Hendrycks et al. [45] and Hein et al. [47] are to fully leverage OOD data to optimize the model\u2019s decision boundary, thus achieving OOD detection. Proponents of this concept can utilize auxiliary anomaly datasets to enhance the OOD detector, enabling it to generalize and detect anomalous information not encountered during training. The central idea of this method can be grasped from Fig. 6.# Recent Advances in OOD Detection: Problems and Approaches\n\nover the parameters of \ud835\udc4b , where \ud835\udc44 \u2191 represents auxiliary anomaly data and \ud835\udc4d denotes the uniform distribution. LOE represents the cross-entropy loss with respect to \ud835\udc4d. The fundamental purpose is to compel the model to optimize the OOD data distribution to a uniform distribution, a principle that is universal in OE-type approaches. The specific design of LOE can depend on other task requirements and the chosen OOD score. This design can utilize a maximum softmax probability baseline [2] detector to detect anomalous data. Compared to traditional softmax scores, EnergyOE [49] builds upon OE by leveraging energy scores for better discrimination between ID and OOD samples, and it is less prone to issues of overconfidence. Specifically, its calculation formula:\n\n\ud835\udc4e (\ud835\udc44; \ud835\udc4b ) = \u2198\ud835\udc4f \u00b7 \ud835\udc50\ud835\udc51\ud835\udc52$\ud835\udc43 \ud835\udc4c\ud835\udc3f (\ud835\udc40 )/\ud835\udc44 ,\ud835\udc4a\n\nwhere the temperature coefficient \ud835\udc4f is used and \ud835\udc4b (\ud835\udc44) denotes the discriminative neural classifier \ud835\udc4b (\ud835\udc44) : R\ud835\udc42 \u2192 R\ud835\udc4a, which maps an input \ud835\udc44 \u2194 R\ud835\udc42 to \ud835\udc53 real-valued logits.\n\nMohseni et al. [48] train a model using a self-supervised approach, optimizing the objective function for unlabeled OOD samples using pseudo-labeling to generalize OOD detection capabilities. Vyas et al. [46] similarly employ self-supervised training of the classifier. Unlike the OE approach, its aim is to find a gap between the average entropy of OOD and ID samples. MixOE [50] takes into account the beneficial effect of subtle OOD samples on enhancing the generalization ability of OOD detection. Its main idea is to mix ID and OOD data samples to broaden the generalization of OOD data. Training the model with these outliers can linearly decrease the prediction confidence with inputs from ID to OOD samples, explicitly optimizing the generalization ability of the decision maker.# Outlier Mining\n\nThe traditional OE concept assumes the existence of ID input Din and OOD input Dout, both independently and heterogeneously distributed, originating from different sources. However, this premise cannot be fully guaranteed in the current training process due to potential noise in the training OOD data. Outlier Mining differs slightly from the traditional OE approach in that, although it also utilizes real-world OOD samples to address the issue, it focuses on identifying the optimal selection within the existing OOD data. The main process is depicted in Fig. 7.# Lu et al.\n\nPOEM [52] raises methodology leans towards excavating anomalies that carry more representative characteristics. Li and Vasconcelos [51] propose a method of data resampling to acquire representative outlier data for training, assigning higher priority score reweighting to hard negative instances. Iterative optimization, leveraging adversarial principles, selects target data. POEM employs posterior sampling to unearth anomalies with elevated boundary scores from an extensive auxiliary dataset, facilitating a nuanced comprehension of the intricacies within OOD samples. DAOL [53] takes the disparity between authentic data and auxiliary OOD data as a starting point to formulate an OOD dataset. Utilizing the Wasserstein ball, it models the distribution of all OOD data, selecting the most challenging OOD data within the ball for training.\n\nBeyond solely relying on raw data, another direction in addressing this issue involves synthesizing representative outlier data by utilizing authentic OOD data through information extrapolation. DivOE [55] introduces a novel learning objective to alleviate challenges associated with limited auxiliary OOD datasets. It achieves this by adaptively inferring and learning information from surrogate OOD data through the maximization of differences between generated OOD data and original data, given the specified anomalies. This adaptive inference extends to a broader spectrum, addressing the limitations imposed by a finite auxiliary OOD dataset. Moreover, DOE [54] introduces a Min-Max learning strategy to identify the most challenging OOD data for a synthetic model. Through model perturbation, the data is implicitly transformed, and the model continues learning from this perturbed data to improve its robustness.\n\nImbalance. Given practical requirements, research is progressively increasing regarding scenarios where ID data is imbalanced during training yet still provides OOD data information. The concept behind PASCAL [56] is to segregate tail-end data and OOD data solely through contrastive loss during training phase, aiding the model in better distinguishing between the two. To address the issue of model confusion between OOD samples and tail-end class data, COCL [57] incorporates a learnable tail class prototype during the training process. This prototype serves to bring tail-end samples closer while distancing them from OOD data, thereby mitigating the model\u2019s bias towards OOD samples. Choi et al. [58] take a different approach, suggesting that factors affecting the performance of OOD detection may be related to the imbalance in the cross-class distribution of auxiliary OOD data. Consequently, it proposes an energy regularization loss, specifically regularizing auxiliary samples from the majority classes to address the issue of class imbalance in OOD data. EAT [59] utilizes dynamically assigned virtual labels during the model training process to train OOD data, thereby expanding the classification space.\n\nDespite the success and considerable attention received by methods like Outlier Exposure in the research community, there are voices questioning the essence of allowing access to OOD data during training. Nevertheless, concerns are raised that the superior classification performance observed in certain datasets may not necessarily translate to competitiveness in real-world deployment, challenging the original intention of OOD detection.# Recent Advances in OOD Detection: Problems and Approaches# Overview\n\nGiven a well-trained model, this problem scenario involves utilizing only the intermediate results computed by the trained model during testing, without modifying any parameters of the model, to accomplish the OOD detection task. Post-hoc methods are favored for their lightweight nature, low computational costs, and the fact that they require minimal modifications to the model and objectives. Its main objective is to construct an effective scoring function that can accurately reflect the behavior of ID data. These characteristics make them highly desirable for convenient and straightforward deployment in practical scenarios.\n\nPost-hoc approaches are categorized into five types: Output-based, Distance-based, Gradient-based, Feature-based and Density-based. Recent work on this type of problem has some recent progress. A summary of the key factors involved in such methods is given in Table 1.# Output-based\n\nAlgorithms based on output primarily aim to explore the latent representations of the output from the intermediate layers of neural networks, which include logits and class distributions, among others. MSP [2] was the first to employ the maximum softmax value to validate OOD detection effectiveness. For OOD samples, their output probability distribution tends to be closer to a uniform distribution, demonstrating the model\u2019s inability to correctly classify the category. In contrast to the MSP method, MaxLogits [60] detects anomalies by comparing the maximum logit value in the logits vector output by the neural network. Logits, representing the model\u2019s confidence in each category, are the neural network\u2019s output before the softmax layer, without undergoing softmax transformation. Meanwhile, Energy [49] introduces the Helmholtz Free Energy, which theoretically aligns with the input probability density and is less susceptible to overconfidence issues. GEN [61] introduces the concept of generalized entropy and directly utilizes Bregman divergence to compute the statistical distance between the model\u2019s probability output and uniform distribution, aiming to identify OOD data. Additionally, leveraging sufficient prior knowledge might be a viable solution. ZODE [62] perform predictions on samples across multiple pre-trained models simultaneously to determine.# Recent Advances in OOD Detection: Problems and Approaches\n\nand training set embeddings, a threshold is designed to determine whether the data belongs to the ID data. NNGuide [65] takes a step further in the direction of granularity by combining the idea of KNN. It assigns weights before the traditional OOD Score, depending on the nearest neighbor distance between the sample and the embeddings in the training set.# Gradient-based\n\nGrad [68] research indicates that gradient-based methods also contribute to OOD detection by quantifying model uncertainty through the gradients propagated in reverse during backpropagation. If the input sample is an ID sample, the gradients of the model with respect to these samples tend to be relatively small and stable. Conversely, the gradients are typically larger or more irregular for OOD samples. GradNorm [69] posits that the gradient magnitude for ID data surpasses that of OOD data. Leveraging this observation, it employs gradient vector norms, computing KL divergence through softmax output and uniform distribution backpropagation to detect OOD data. In contrast, GradOrth [70] adopts an alternative perspective, recognizing that crucial features of OOD data reside in a low-rank subspace. Consequently, it shifts focus to calculating gradient projection norms in this subspace to identify OOD data. GAIA [71] employs a combination of gradient anomaly checks and aggregation. It enables the model to interpret indeterminacy attributively, introducing channel-wise average abnormality and zero-de$ation abnormality without prior knowledge to gauge the extent of data distribution variations. The OPNP [72] method discovers that the OOD detection capability of the model is highly sensitive to parameters and activated neurons that are close to zero. Therefore, this method utilizes a pruning behavior for parameters and neurons to remove those leading to overfitting, thereby enhancing the model\u2019s generalization ability.# Overview\n\nTest-time adaptive methods, which employ a classifier trained on the training set, strive to utilize test data, either the complete test set or a series of unlabeled mini-batches, to enhance OOD detection performance through model adaptation. Test-time adaptive approaches are based on the theoretical insight [140] that detecting OOD samples using only ID samples without any additional knowledge is impossible. These methods can be divided into two categories.# Density-based\n\nRecent advancements in density-based OOD detection models have demonstrated substantial performance gains, predicated on the models\u2019 ability to accurately capture and understand the intrinsic characteristics of the true data distribution. For example, GEM [86] models the feature space of ID data as a class-conditional multivariate Gaussian distribution. Under this assumption, it designs new statistical metrics to validate the model\u2019s performance. Using a modeling approach based on Gaussian mixture models, GEM Score is aligned with the true log-likelihood for capturing OOD uncertainty. However, while GEM strictly relies on Gaussian assumptions, recent works ConjNorm [87] introduce a novel framework based on Bregman divergence, extending the consideration of data distributions to encompass an exponential family of distributions. By redefining density functions for data, this model has a broader range of applications.# Roles of Features, Logits, and Probabilities\n\nBy decomposing the features of the penultimate layer in a neural network, it identifies the null space that is irrelevant to classification but exhibits exceptional performance in OOD detection. Its computation formula can be expressed in the following form:\n\n\u2198\ud835\udc5a \u21d0\ud835\udc40 \ud835\udc4d\u21d2\u21d0 2 + LogSumExp\ud835\udc4b (\ud835\udc40), (5)\n\nwhere \ud835\udc5a is a scaling constant, computed by the model. Here \ud835\udc40 = \ud835\udc40 \ud835\udc4d + \ud835\udc40 \ud835\udc4d\u21d2 and \ud835\udc40 \ud835\udc4d\u21d2 is the projection of \ud835\udc40 to \ud835\udc48 \u21d2. And it have \ud835\udc5b \ud835\udc40 \ud835\udc4d\u21d2 = 0. Additionally, \ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5d\ud835\udc5e\ud835\udc58\ud835\udc4e\ud835\udc44\ud835\udc3f represents the computation process of the energy function [49], and \ud835\udc4b (\ud835\udc40) represents the logit output of the model. The first term here represents virtual logits, while the second term represents the score of the energy function.\n\nNeco [78] subsequently reveals the prevalent phenomenon of neural collapse in contemporary neural networks, impacting OOD detection performance. The observation of orthogonal trends between ID data and OOD data features is leveraged to differentiate OOD data. ASH [79] is a straightforward method for dynamic activation modification where a substantial portion of activations in samples during later stages of model training is either removed or slightly adjusted. NAC [80] introduces a measure of neuron activation coverage, based on the premise that if a neuron in a neural network is rarely activated, this state may indicate a higher likelihood of OOD data. By quantifying this statistical property, NAC aims to distinguish between ID and OOD data. Based on the existing literature on feature shaping, Zhao et al. [81] utilize a piecewise constant shaping function to partition the feature domain into disjoint intervals, estimating a scalar within each interval as an approximation. As the interval width approaches zero, an approximation of the maximum logits can be obtained. The goal of BLOOD [82] is to smooth the representations of intermediate layers in neural networks for predicting OOD data. This study found that, compared to OOD data, ID data exhibits smoother variations in the intermediate layer representations of neural networks. Leveraging this characteristic, new statistical measures can be designed to discriminate against anomalous data. In addition, another line of work focused on neuron activation pruning, which is a new activation shaping scheme proposed based on the research foundation of ASH. SCALE [83] emphasizes scaling as a crucial metric for evaluating samples, and it similarly finds significantly lower pruning rates for OOD data. Therefore, the proposed reshaping of intermediate layer tensors can effectively enhance detection performance.# Recent Advances in OOD Detection: Problems and Approaches# Model-optimization-based\n\nModel optimization-based methods enhance the trained model by leveraging unlabeled data during the post-training phase.\n\nA line of these methods [88, 91] advocate for harnessing a combination of unlabeled ID and OOD data, termed \"wild data\", which is plentiful and readily available in the real-world scenario, to OOD detection. The motivation of WOODS [88] is to clean the wild data to get reliable OOD candidates, and then the model regularization can be performed with the knowledge of them. Afterwards, to understand the role of wild data in OOD detection, SAL [91] explains how they help OOD detection from the lens of separability and learnability. Notably, wild data is not entirely equivalent to test OOD data if wild OOD data is not from the test datasets, inevitably leading to incorporating unintended information into the model. In addition, the extra training requirement in WOODS, thought necessary, incurs significant costs and is generally undesirable.\n\nAnother line of model-optimization-based approaches draws inspiration from the semi-supervised-learning (SSL) techniques [141], aiming for a more efficient, lightweight training process during the post-training phase. Pseudo-labeling [142] is a simple but effective way to label the test data, which enhances learning from test unlabeled data. The method proposed by Yang et al. [9], termed AUTO, employs only pseudo-OOD data to refine the model. The role of pseudo-ID data in AUTO is to mitigate catastrophic forgetting with a semantically-consistent objective, thereby maintaining the accuracy of ID classification. In contrast, ATTA [90] and SODA [89] harness both pseudo-ID and pseudo-OOD data to refine the trained model. SODA employs a dual-loss approach to tackle pseudo-ID and pseudo-OOD data simultaneously, while ATTA distinguishes them with different weighting techniques.# Model-optimization-free\n\nModifying the original trained model is infeasible in certain security-sensitive scenarios. Therefore, methods enabling test-time adaptation without requiring model updates, termed \"model-optimization-free\" (MOF) techniques, are increasingly garnering interest. These approaches enhance the utilization of test data by either memorizing it or incorporating additional modules on top of the original model.\n\nBoth ETLT [92] and GOODAT [94] retain the integrity of the original model by training an add-on module to adjust the OOD score, rather than altering the original model itself. ETLT observes a linear correlation between the feature representation and the OOD score of a test input. In other words, for a given image, the pair consisting of its feature and OOD score (feature, OOD score) exhibits a linear correlation. Furthermore, the aforementioned pairs of ID and OOD data is linearly separable. Based on the observation, ETLT proposes to learn a linear regression module trained from the (feature, OOD score) pair. The availability of complete test datasets may not always be feasible, therefore, Fan et al. [92] also provide an online variant to ensure safer deployment. Similarly, GOODAT develops an add-on named graph masker, designed specifically for graph data. It integrates GIB-boosted losses and employs it as the metric for OOD scoring. In contrast, AdaOOD [93] avoids any additional training burden through a non-parametric k-nearest neighbours approach. The core principle behind AdaOOD is the maintenance of a memory bank, similar to the approach taken by AUTO.# Lu et al.\n\nrealistic problem scenario, namely Continuous Adaptive Out-of-Distribution (CAOOD) detection, aimed at addressing the challenge of constantly changing ID and OOD distributions in the real world. The meta-learning approach is employed to swiftly adapt models in response to the complexities encountered in various scenarios in MOL.# Fig. 9. Overview and trends in LPM-based OOD Detection Methods.# 5 PROBLEM: LARGE PRE-TRAINED MODEL-BASED OOD DETECTION\n\nLarge pre-trained models have showcased remarkable performance in numerous downstream ID classification tasks, but their potential in OOD detection tasks remains a less-explored area. Recent research [145] highlights a correlation between higher ID classification accuracy and better OOD detection performance. Consequently, large pre-trained model-based OOD detection problem comes naturally. In recent years, large pre-trained models of various types, including single-modal (ViT [146], BERT [147], Diffusion [148]), visual language models (VLMs) (CLIP [149], multi-modal Diffusion [150] ALIGN [151]), and large language models (LLMs) (GPT3 [152]), have been increasingly utilized for OOD detection tasks, as shown in Fig. 9. Leveraging the powerful representational capabilities of large pre-trained models has further relaxed the constraints of OOD detection tasks, leading to a focus on more challenging and realistic scenarios, which has emerged as a new hotspot. Given the number of ID shots exposed to the large pre-trained model, large pre-trained model-based OOD detection can be classified into Zero-shot, Few-shot, and Full-shot OOD detection, as shown in Fig. 10. The performance evaluations of several relevant competitive methods are summarized in Table 2 to provide an understanding of the performance level of OOD detection in this area.# Recent Advances in OOD Detection: Problems and Approaches# Fig. 10. Illustration of large pre-trained model-based OOD detection approaches.\n\nIn the training phase, zero-shot approaches require only category labels of ID classes. Few-shot approaches need a subset of images of each ID class along with the category labels (indicated by dashed lines). Full-shot approaches utilize both the category labels and all images of each ID class. None of these approaches use labels or images of OOD categories.\n\nhave access to ID images and rely solely on textual category knowledge. It\u2019s important to clarify that \u201cID\u201d here refers to the ID of the specific downstream task, not the dataset ID data during pre-training.# VLMs-based\n\nExisting research on zero-shot OOD detection using VLMs can be categorized into two main approaches based on which VLMs are utilized as a basic backbone.# Di\"usion-based\n\nTraditional generative approaches, such as di\"usion models [148], capture the ID distribution effectively. Subsequently, these methods are used to identify OOD samples by assessing the likelihood that a given test input is derived from an OOD source. In a recent study in RONIN [97], the di\"usion model has been applied to achieve OOD object detection, which utilizes the di\"usion model to generate ID inpainting, while the CLIP model is employed to calculate the similarity score.# CLIP-based\n\nGiven the remarkable proficiency of CLIP in correlating texts and images, a substantial number of researchers have ventured into zero-shot OOD detection leveraging CLIP. Typically, in the zero-shot OOD detection setup, add-ons are added to the pre-trained model to better adapt it to the OOD detection task. ZOC [10] achieves zero-shot OOD detection by training an image description generator on a large image captioning dataset [153], enabling the model to generate candidate unseen labels. It\u2019s essential to note that ZOC treats CLIP merely as a feature extractor and doesn\u2019t impart OOD detection capabilities to CLIP itself.\n\nThe subsequent work CLIPN [99] empowers CLIP with the ability to say \u201cno\u201d by a \u201cno-prompt\u201d encoder. Other approaches alleviate the need for additional training and focus on enhancing the performance of post-hoc zero-shot OOD detection. A simple baseline of clip-based zero-shot OOD detection is to use the normalized text-image similarity as an OOD score. Ming et al. [98] further substitute the score with the maximum concept matching (MCM) score and offer a comprehensive theoretical explanation of this modification.\n\nMoreover, NegLabel [154], LAPT [102] and CLIPScope [96] enrich the text information through a predetermined corpus. Concretely, NegLabel selects negative words (labels) from the corpus, which are dissimilar to ID labels, to enhance OOD detection performance. Furthermore, CLIPScope uses those labels to revise the original score by applying Bayesian rules. In contrast, LAPT leverages text-to-image generation models or image retrieval models to obtain images corresponding to the given ID and Negative ID labels, followed by prompt tuning.\n\nIt is noteworthy that ZOC, CLIPN, NegLabel and CLIPScope each devise unique OOD scores for integrating into their respective models. Conversely, the MCM score stands out for its generalizability across different models. The calculation of MCM is as follows:\n\nManuscript submitted to ACM# Recent Advances in OOD Detection: Problems and Approaches# Overview\n\nGiven the large pre-trained model and a few ID data, we can adapt the model using the ID data and subsequently detect OOD test data. Zero-shot OOD detection does not necessitate any training images, making it suitable for scenarios with high-security requirements. However, it may face challenges related to domain gaps with ID downstream data, which can limit the performance of zero-shot methods. Therefore, there are many few-shot methods employed in OOD detection, and their effectiveness is often superior to that of zero-shot OOD detection.# Studies\n\nThe direct way of adapting a large pre-trained model with several ID samples is fine-tuning. Ming and Li [159] and Dong et al. [110] conduct studies on the impact of fine-tuning on OOD detection within the context of VLMs, with a special focus on CLIP. Ming and Li [159] pay more attention to the role of parameter-efficient fine-tuning (PEFT) methods and the OOD score. The MCM score, introduced by Ming et al. [98] as an innovative measure, alongside methods based on prompt learning for detecting OOD instances, is recognized for its effectiveness. Similarly, Dong et al. [110] conduct an extensive comparison between various fine-tuning approaches, such as PEFT and traditional methods, and discover that PEFT exhibits superior performance in detecting OOD instances, which echoes with the conclusion of ? ]. Following this, Kim et al. [160] believe that Finetune Like You Pretrain (FLYP), a fine-tuning method, deserves attention due to its good performance on classification tasks. Specifically, FLYP is to mimic contrastive language-image pre-training as CLIP. After comparing the performance of FLYP and PEFT methods on zero-shot OOD detection, Kim et al. [160] find FLYP yields better OOD detection performance than PEFT. It should be noted that while Fort et al. [157] discuss \u201cfew-shot\u201d OOD detection, their approach relies on a small number of outlier examples rather than ID samples, making it unsuitable for inclusion in this section.# Fine-tuning based\n\nA prevalent method for fine-tuning CLIP using limited ID data involves prompt learning [161, 162], where the fine-tuning focuses on the prompt\u2019s contextual words, maintaining the pre-trained parameters unchanged. To distinguish between ID and OOD data, both LoCoOp [111] and ID-like Prompting Learning [109] are designed to enhance the learning of context vectors from the near-ID feature perspective. LoCoOp employs entropy maximization to distance ID-irrelevant local features (such as the backgrounds in ID images) from the textual embeddings of ID classes. Similarly, ID-like Prompting Learning generates ID-like data\u2014outliers within the vicinity of ID samples\u2014to refine the context vector. Additionally, it incorporates a diversity loss to enhance the variance among the sampled OOD data. However, recent work EOK-Prompt [107] and GalLop [108] argues that optimizing only a prompt using numerous features from an image wastes valuable information. Instead, EOK proposes an orthogonal method to optimize the novel local prompt, making better use of the limited image data, while GalLop optimizes global prompts and local prompts by leveraging global and local features, respectively. Furthermore, based on the mentioned studies, DSFG [110] posits that traditional fine-tuning approaches applied to CLIP may inadvertently lead to a loss of critical OOD knowledge. To address this issue, DSFG adopts a strategy to retain broad OOD knowledge by merging the original features with those modified through fine-tuning, followed by the training of a classifier. DSFG\u2019s plug-and-play capability makes it seamlessly compatible with all fine-tuning-based approaches, enhancing its practicality and value.\n\nThe aforementioned few-shot tuning methods inevitably suffer from overfitting on limited-shot instances. Chen et al. [163] proposed a training-free few-shot OOD detection method, Dual-Adapter [163], which constructs two types of adapters by extracting positive and negative features to aid in OOD detection. The currently mentioned few-shot methods lack OOD supervision, so CLIP-OS [106] tries to find such supervision and also achieves stunning results.# Meta-learning based\n\nMeta-learning aims to devise a learning approach that enables rapid adaptation to new challenges [164, 165]. OOD-MAML [115] adapt model-agnostic meta-learning (MAML) for few-shot OOD detection.# Overview\n\nThis setup is generally less realistic than the first two (zero-shot and few-shot). However, we list them separately to ensure a comprehensive review of existing methods across the spectrum. Given the full set of ID data and corresponding labels, VLMs can enhance OOD detection significantly by fine-tuning. Moreover, a novel task called \u201cPT-OOD\u201d detection is introduced.# Fine-tuning based\n\nWith access to the complete dataset, then more data can be used to fine-tune the large pre-trained model or the data can be used to better simulate the ID distribution, facilitating the differentiation of OOD data. NPOS [29] proposes a non-parametric outlier synthesis technique to distinguish ID and OOD data by fine-tuning CLIP with complete ID data. In contrast, TOE [118], while also using CE loss to constrain the model during fine-tuning, builds on the ideas of OE by focusing on textual outliers within the CLIP framework to control the model\u2019s recognition capabilities, which differs significantly from directly using OOD images.# PT-OOD Detection\n\n\u201cPT-OO\u201d samples are OOD samples with overlap in pretraining data. After investigating and elucidating the effects of various pre-training methodologies (supervised, self-supervised) on PT-OOD detection, Miyai et al. [117] observe the low linear separability in feature space significantly degrades the PT-OOD detection performance. They suggest using distinctive features for each instance to distinguish between ID and OOD samples.# 6.1 Evaluation metrics\n\nIn the vast majority of OOD detection tasks in the visual domain, the following evaluation metrics are commonly used:\n\n- AUROC (Area Under the Receiver Operating Characteristic curve). This metric quantifies the likelihood that a classifier will assign higher scores to ID samples compared to OOD samples. An elevated AUROC value is indicative of superior model performance, signifying an enhanced ability to distinguish between ID and OOD instances. Consequently, a higher value is desirable.\n- AUPR (Area under the Precision-Recall curve). This metric is pertinent when the ID class is considered the positive class and is particularly valuable in the context of imbalanced class distributions. It assesses the balance between precision and recall, with a higher AUPR value indicating superior model performance. Therefore, a greater AUPR value is desirable.\n- FPR@95 (False Positive Rate at 95% True Positive Rate). This metric delineates the false positive rate (FPR) at the juncture where the true positive rate (TPR) reaches 95%. It essentially gauges the proportion of OOD samples erroneously identified as ID, thus providing insight into the model\u2019s propensity for false alarms at a high sensitivity threshold. A reduced FPR@95% TPR is indicative of a model\u2019s enhanced specificity in correctly flagging OOD samples while maintaining high sensitivity towards ID samples. Therefore, a lower value is desirable.\n\nManuscript submitted to ACM# Recent Advances in OOD Detection: Problems and Approaches# 6.2 Experimental Protocols\n\nThese metrics provide comprehensive insights into the OOD detection performance, considering different aspects such as the ability to distinguish between ID and OOD samples, handling imbalanced class sensitivity towards ID samples, and controlling the false positive rate at a specific true positive rate threshold.\n\nIn the traditional experimental protocol for OOD detection, test data is exclusively classified as either ID or OOD. However, as the field has advanced, there is now a more nuanced distinction between OOD and ID data, which has led to variations in the evaluation process.\n\nSubsequently, OOD data is categorized into near-OOD and far-OOD based on the degree of covariate shift from ID data. This categorization corresponds to dividing OOD detection tasks into near-OOD and far-OOD detection. It is evident the near-OOD detection task is more challenging, however, numerous methods [98, 109, 157] have demonstrated excellent performance in this area.\n\nRecently, Yang et al. [33], Bai et al. [166] propose that we should consider cases where covariate shift occurs in ID data, which is not taken into account previously. This is crucial to prevent the loss of model generalization. The samples mentioned earlier are termed as cs-ID data, an abbreviation for \u201ccovariate shift ID\u201d data. Consequently, a new experimental protocol has been explored, called full-spectrum OOD detection. During the testing phase, the model is expected to identify near-OOD and far-OOD instances. Additionally, it should refuse to provide predictions for the OOD data and accurately predict ID and cs-ID data.# 6.3.1 Computer Vision\n\nMost of the efforts in OOD detection have been devoted to the field of computer vision, we list extensive vision-related tasks as follows:\n\n- Image Classification. The majority of tasks discussed in this paper focus on OOD detection within the realm of image classification. In such task scenarios, commonly utilized ID datasets include MNIST [172], CIFAR-10 [173], CIFAR-100 [173], and ImageNet-1K [174]. Various OOD datasets are constructed accordingly for evaluating different methods [175], with the most frequently employed datasets being iNaturalist [176], SUN [177], Places [178] and Textures [179]. Additionally, full-spectrum OOD is typically evaluated on three benchmarks: DIGITS, OBJECTS, and COVID, as proposed by Yang et al. [33].\n- Semantic Segmentation. Recent works [90] have started delving into the dense OOD detection task, also known as anomaly segmentation. The datasets used for evaluation include the Cityscapes dataset [180], the Road Anomaly dataset [181], and the recently developed SOOD-ImageNet [182].\n- Object Detection. The application of methods related to OOD detection in the field of object detection is relatively nascent, with only a few studies exploring this area [34]. Evaluations are commonly performed using datasets like PASCAL-VOC [183] and Berkeley DeepDrive-100K [184].\n- Autonomous Driving. Autonomous driving has long been a crucial practical application of OOD Detection. Recently, Mao et al. [167] utilized the CARLA [185] system to simulate and evaluate the performance of OOD Detection in autonomous driving scenarios.\n- Medical Image Analysis. In the field of medical image analysis, OOD detection is crucial. Depending on the specific category of medical image, OOD detection employs various datasets including CIFAR-10 and Kvasir-Capsul [186].\n\nManuscript submitted to ACM# Table 3. Summary of Datasets\n\nThe CARLA System is a simulation platform designed for evaluating OOD Detection in the field of autonomous driving, hence its entire row is filled with dashes. Other \u201c-\u201d symbols represent numbers that vary depending on usage. More detailed descriptions can be found in the code repo.\n\n|Task|Dataset Name|Data Type|# Classes|# Samples|Papers|\n|---|---|---|---|---|---|\n|Image Classification|CIFAR-10|Images|10|60,000|[10, 99]|\n| |CIFAR-100|Images|100|60,000|[10, 99]|\n| |MNIST|Images|10|70,000|[2, 81]|\n| |ImageNet-1K|Images|1,000|1,431,167|[98, 99]|\n| |iNaturalist|Images|5,089|675,170|[98, 99]|\n| |SUN|Images|397|108,754|[98, 99]|\n| |Places|Images|&gt;205|&gt;2,500,000|[98, 99]|\n| |Textures|Images|47|5,640|[98, 99]|\n|Semantic Segmentation|Cityscapes|Images|-|25,000|[90]|\n| |Road Anomaly Dataset|Images|-|100|[90]|\n|Object Detection|PASCAL VOC|Images|20|2,913|[34, 97]|\n| |BBD100K|Video-&gt;Image|Variable|100,000|[34, 97]|\n|Autonomous Driving|CARLA System|-|-|-|[167]|\n|Medical Image Analysis|Kvasir-Capsul|Images|-|4,741,621|[168]|\n|Text Category|News Category|Text|-|210,000|[169]|\n| |SST-2|Text|-|215,154|[169]|\n| |CLINC150|Text|150|22,500|[170]|\n|Intent Detection|Banking|Text|77|13,083|[170]|\n|StackOverflow| |Text|20|20,000|[170]|\n|Audio|MSCW|Audio|-|&gt;23400000|[171]|\n| |Vocalsound|Audio|-|21,024|[171]|\n|Graph data|TU|Graph data|-|Variable|[94]|\n| |OGB|Graph data|-|Variable|[94]|\n\nOOD detection has significant applications across various fields such as human action recognition [187] and solar image analysis [188, 189]. For further details, please refer to the accompanying code repository due to space limitations. The same applies to the subsequent paragraphs.\n\nManuscript submitted to ACM# Recent Advances in OOD Detection: Problems and Approaches# 6.3.2 Natural Language Processing\n\nOOD detection is also explored in various tasks across numerous Natural Language Processing applications. The two most common applications are as follows:\n\n- Intent Detection. Intent Detection is a significant application of OOD detection in NLP. The datasets used for evaluation include CLINC150 [190], Banking [191], StackOverflow [192], among others.\n- Text Category. In text category OOD detection applications, datasets like News Category [193] and SST-2 [194] are commonly utilized to form ID/OOD pairs and assess the models\u2019 detection capabilities.# 6.3.3 Beyond Computer Vision and Natural Language Processing\n\nIn addition to the two data modalities mentioned above, OOD detection still has many important applications across various types of data.\n\n- Audio data. In audio OOD detection, MSCW (Micro-EN) [195] and Vocalsound [196] are usually used as ID datasets, and they also act as OOD for each other.\n- Graph data. Recent studies have proposed various approaches for OOD detection in graph data. The existing graph-level OOD detection benchmark comprises a total of 10 pairs of datasets from TU [197] and OGB datasets [198], which has been widely utilized.\n- Reinforcement learning. Currently, there is a rising trend of integrating OOD detection with reinforcement learning [199] to bolster model robustness. Mohammed and Valdenegro-Toro [200] provide a benchmark and explore methods for crafting tailored reinforcement learning environments that can generate OOD data.# 7 EMERGING TRENDS AND OPEN CHALLENGES\n\nDespite rapid advancements in OOD detection, numerous emerging trends and less-explored challenges remain. In this section, we explore emerging trends and open challenges from three distinct perspectives: methodologies, scenarios, and applications.# 7.1 Better methodologies of OOD detection\n\nMeta-Learning Adaptation. Faced with the quick adaptation challenge in test-time OOD detection, meta-learning algorithms, which provide a learning-to-learn paradigm to efficiently adapt the model to new test data, may be the solution. Additionally, in addressing the dauntingly vast sample space of potential outliers inherent in training-driven OOD detection, improved sampling methods could offer a pathway to the efficient utilization of outliers [52].\n\nTheoretically-Driven Score Designing. In the traditional single-modal regime, numerous post-hoc scores have been carefully designed for OOD detection, effectively reducing training costs. However, as the field has progressed into the multi-modal domain, there is an increasing demand for theoretically-driven score designs. MCM [98] is a notable example, but it is merely an extension of Softmax and does not deeply explore the relationships between text and image. Therefore, more advanced scores are needed.# 7.2 More practical scenarios of OOD detection\n\nUnder the current trend, there is a growing need for the emergence of more practical scenarios, driven by the limitations of existing impractical restrictions.\n\nQuick Test-Time Adaptation. The advent of test-time scenarios like CAOOD [144] significantly advances the application of OOD detection in real-world contexts, promising improved reliability and adaptability.\n\nMulti-Modal Detection. Exploring multi-modal OOD detection enhances our grasp of data dynamics and boosts model efficacy across varied sensory inputs. Additionally, the forthcoming integration of LLMs into multi-modal OOD.# Additional Modalities\n\nWhile OOD detection has made strides across diverse sectors, its potential in speech and physiological signal analysis remains largely untapped. In particular, OOD issues are prevalent in emotion-related physiological signals due to the variability in subjects\u2019 emotional states. Therefore, this represents a promising area for further exploration. Some of the current methods [202] also provide benchmarks for reference.# Human-in-the-loop Application\n\nA pivotal future direction, proposed by Vishwakarma et al. [203], is the integration of human insight into the detection process. This human-in-the-loop approach, particularly in high-stakes decision-making, will be vital for enhancing the accuracy and responsiveness of OOD detection systems, merging human intuition with algorithmic precision.# Web Image Scraping\n\nTo automate the process of scraping images from the Internet, Miyai et al. [155] propose a zero-shot ID detection task, a concept derived from zero-shot OOD detection. An image will be classified as an ID image if it contains ID objects; it will only be considered an OOD image if it lacks any ID objects. It presents a novel perspective on the application of OOD detection and merits further exploration.# 8 CONCLUSION\n\nOOD detection is critical for trustworthy machine learning. In this paper, we provide a comprehensive review of recent advances in OOD detection, focusing for the first time on the problem scenario perspective: training-driven, training-agnostic, and large pre-trained model-based OOD detection. We also summarize extensively used evaluation metrics, experimental protocols, and diverse applications. We believe that our novel taxonomy of existing papers and extensive discussion of emerging trends will contribute to a better understanding of the current state of research, assist practitioners in selecting suitable approaches, and inspire new research hotspots.\n\nManuscript submitted to ACM",
        "context_id": 39,
        "question": "What type of data is the OOD detection aiming to identify and reject?",
        "answer": [
            "OOD samples"
        ],
        "context_length": 61566
    },
    {
        "context": "# 1. INTRODUCTION\n\nIn human brains, sodium ions (Na+), when exposed to an electric field gradient of negatively charged macromolecules and proteins, experience nuclear quadrupolar interaction that results in bi-exponential decay in transverse (T2) relaxation of nuclear spins when the ions are not in fast motion, a situation in which correlation time between sodium ions and electric field gradient is much shorter than the inverse of Larmor frequency, \u03c4c &lt;&lt; 1/\u03c901,2. Sodium ions in fast motion cancel out the effect of quadrupolar interactions, resulting in mono-exponential T2 decay1-4. Sodium ions in bi-exponential T2 decay were historically, but incorrectly, considered as invisible \u201cbound\u201d (chemically) sodium5,6 because their short-T2 components were not detectable by then-NMR (nuclear magnetic resonance) techniques1-4. The terminology however remains in use in today\u2019s sodium MRI, although concerns have been raised recently by some researchers, with no better substitutes yet7. For convenience, this article refers \u201cbound\u201d (not chemically) sodium to those showing bi-exponential T2 decay and \u201cfree\u201d sodium to those showing mono-exponential T2 decay.\n\nThe free and bound sodium ions can appear in both intra- and extra-cellular spaces8-10 depending on their relative correlation time with electric field gradient1,2.\n\nSodium (23Na) MRI currently acquires signals from both free and bound sodium ions, and quantifies total sodium concentration (TSC) at voxels of an image. TSC is a unique measure for non-invasive assessment of disruption in ionic homeostasis of cells in, or recovery from, pathological conditions such as stroke, tumor, multiple scleroses, epilepsy, bipolar disorder, and mild traumatic brain injury8-12. However, TSC is dominated by free sodium from cerebrospinal fluid (CSF) which has a high sodium concentration (~145 mM) and overshadows alteration in intracellular sodium which has a much lower concentration (~15 mM). Separation of mono- and bi-T2 sodium signals can remove CSF impact and highlight intracellular alterations, especially at early stage of a disease happening at cellular level or in early (cellular) response to a treatment.\n\nThe difference in T2 relaxation was extensively explored in sodium MRI as a means to separate the two populations of sodium in the brain. Triple quantum filtering (TQF) was considered a standard for human studies, in which MR signals were generated solely from triple-quantum (TQ) transitions13. TQF techniques, however, require multiple radiofrequency (RF) pulses for excitation and multi-step phase cycling to eliminate single-quantum (SQ) signals13-17, leading to long scan time (20\u201340 min) and high specific absorption rate (SAR, causing a safety concern).More problematic is that TQF has much low signal-to-noise ratio (SNR) about 10 times lower than SQ.15-17 These difficulties hamper TQF to be widely used on humans.\n\nAlternative approaches were proposed. Inversion recovery (IR), adopted from proton (1H) MRI, exploits a difference in T1 relaxation between the mono- and bi-T2 sodium ions, and suppresses signals from the mono-T2 sodium of longer T1 time.18-20 The IR approach needs an extra RF pulse for the suppression and worsens SAR issue, not favorable to human studies. It also suffers from incomplete suppression of the mono-T2 sodium signals which are ~10 times higher than the bi-T2 sodium signals, due to spatial inhomogeneity of B1+ field although adiabatic pulses are usually used, and complicates quantification of the bi-T2 sodium owing to unknown residual mono-T2 sodium signals.9,11,18 To overcome these drawbacks, another alternative approach, called short-T2 imaging, was proposed in which SQ images were acquired at multiple echo times (TEs) and then subtracted from each other to produce an image of the short-T2 component of bi-T2 sodium.21-24 In such a way, SAR was reduced to, and SNR was increased to, the level of SQ images, favorable to human studies in clinic. Unfortunately, the subtraction could not completely eliminate mono-T2 sodium signal (~20% in residual), degrading accuracy of bi-T2 sodium quantification.24\n\nIn this study, the short-T2 imaging is generalized to multi-TE single-quantum (MSQ) imaging to improve accuracy of the separation between mono- and bi-T2 sodium signals, by replacing the subtraction with a matrix inversion. To develop MSQ technique, we optimized the TEs for data acquisition, investigated impact of T2 values on accuracy of the separation, and acquired the free induction decay (FID) signals to generate T2* spectrum for the matrix equation. To test MSQ technique, we implemented numerical simulations, phantom studies, and human studies. The results were supportive of the proposed MSQ technique. We also itemized limitations of MSQ technique and potential pitfalls in interpretation of separated sodium signals.# 2.1. Model of sodium signals\n\nA two-population model is used to describe single-quantum sodium signal m(t) evolving with time t at an imaging voxel \u2206V.\n\nm(t) = m1 'S1 (t) + m2 'S2 (t), &nbsp; t \u2265 0 &nbsp; &nbsp; &nbsp; Eq. [1a]\n\nm1 \u2265 0, m2 \u2265 0, and m1 + m2 = m(0)# 2.2. xeyavation of tze mono- and wi-T2 sodium signals\n\nGiven SQ sodium images at {TE1, TE2, \u2026, TEN}, Eq. 1a becomes a matrix equation Eq. 2.\n\n|6|7|\n|---|---|\n|6 \u2261 (!), !%, \u2026, !*)+|! = !(0;), < = 1,2, \u2026 , ?|\n|8 \u2261 (!!\", !#$)+| |\n|'!\"(0;))|'#$ (0;))|\n|7 \u2261 \u239b'!\" (0;%)|'$(0;%)\u239e|\n|\u239c \u22ee|\u22ee \u239f|\n|\u239d'!\"(0;*)|'#$ (0;*)\u23a0|\n\nSuperscript T is operator for matrix transpose. A solution to Eq. 2 is given by Eq. 3 via an established algorithm called non-negative least-squares (NNLS)25 in which non-negative condition on X is incorporated into the solution.\n\n8 = ??=H(78 \u2212 6)                                                            Eq. [3]# 2.3. Measuvement of T2 salues fov tze mono- and wi-T2 sodium yoyulations\n\nThe T2 values (T2,fr, T2,bs, T2,bl) in Eqs. 1b and 1c are required to perform the separation. They can be measured by acquiring FID signal s(t) on whole imaging volume and using effective T2 (i.e., T*) decay model of multi-exponential components to fit the FID signal in magnitude, i.e.,\n\n|J(#)| = \u2211 -L- exp (\u2212#/0%\u2217 ,-)                                                 Eq. [4]\n\nHereinafter, T2* replaces T2 as spin echo is not favorable to sodium MRI. The curve-fitting is accomplished also through the NNLS algorithm 25 when T* values are pre-distributed in a range.# 2.4. Impact of T2* values on the mono- and bi-T2 sodium evaluation\n\nIdeally, T2* values are measured at individual voxels. Practically, the measurement is time consuming and not favorable in clinical studies. A fast estimate is necessary. Intuitively, solutions to Eq. 1 are not very sensitive to T2* values due to exponential decays in Eqs. 1b and 1c. This observation can be verified theoretically and numerically. Theoretically, small changes in T2* values, {P0%2*, 0, u=fv, ws, wl}, lead to small changes {P!1, y=fv, wd} in !!\" and !#$ under the same m(t), that is,\n\n0 = P(!!\"'!\") + P(!#$'#$)\nEq. [5a]\n\n\u2212!2 = '!\"P!!\" + '#$P!#$\nEq. [5b]\n\n!2 \u2261 !!\"P'!\" + !#$P'#$\nEq. [5c]# Amplitudes\n\n{L-}, called T2* spectrum, determine relative incidence of T2* components in the imaging volume which counts all T* components from both mono- and bi-T2 sodium populations. To pair up the short- and long-T* components of the bi-T2 sodium, the 60-40 split in intensity is a helpful guidance.\n\nAlternatively, empirical estimates of the brain tissues may be applicable to T2* values, because solutions to Eq. 2 are not so sensitive to T2* values due to exponential decay (see Sections 2.4 and 4.2 for details).\n\nFID signals, when acquired with an array coil, may have unique initial phases {O/,(, l=1, 2, \u2026, Nc} at individual elements, and need to be aligned to produce a resultant FID signal. Alignment (via phase correction) can be towards a reference phase such as zero phase, one of the initial phases, or mean phase across elements. In addition, signal intensity at individual elements needs to be scaled via \u201cFFT factor\u201d which is stored in the header of a raw FID data file.\n\nFID signals at the first few samples are distorted by hardware filtering during analog-to-digital conversion (ADC). The number of affected samples are in a range of 3\u201310 points, depending on sampling bandwidth, with the first sample having the largest distortion. This distortion alters measurement of T* components, especially the short T2* components which are critically important to the bi-T2 sodium. Correction for the distortion can be performed using an established exponential extrapolation (see Appendix A for details).# 2.5. Oytimalization of tze numwev of TEs\n\nIn principle, the more TEs the better differentiation between the T* relaxations of the mono- and bi-T2 sodium populations, and the better solutions to Eq. 2. In practice, the number of TEs is restricted by total scan time (TA), SNR, signal decay, and risk of motion artifacts across TEs. Therefore, a trade-off must be made for the number of TEs. To determine an optimal number of TEs, it is necessary to understand noise propagation in Eq. 2. Let singular value decomposition (SVD) of the matrix Y in Eq. 2a be\n\n7 = W X Y+\nEq. [6a]\n\nX = diag(^), ^%)\nEq. [6b]\n\n8 = _ X56 W7 6\nEq. [6c]\n\nSingular values (^), ^%) determine noise transfer (amplification or suppression) in Eq. 6c from the measured TE-images M to the separated mono- and bi-T2 sodium images X. However, Eq. 6c allows negative values in X when random noise contaminates M. This violates the \u201cnon-negative\u201d condition on X. Therefore, SVD analysis is applicable only to X elements with SNR \u2265 2 where the elements, with Gaussian noise, have 95.4% of chance in the territory of non-negative value.# 3. METHODS\n\nThe proposed MSQ technique is graphically illustrated in Fig. 1. The inputs are multiple TE images and an FID signal. The outputs are the mono-T2 (free), bi-T2 (bound), and total sodium images, as well as the map of field inhomogeneity \u2206B0 and the map of single-T*. In between are the data.# Processing Functionalities for the T* Spectrum\n\nMono- and bi-T2 sodium separation, and mapping of \u2206B0 and single-T*.\n\nMotion correction (MoCo) across multi-TE images is optional. Also optional is the low-pass (LP) filtering, which is a 3D averaging over a size of 3\u00d73\u00d73 voxels for instance, to reduce random noise on the bi-T2 sodium. The \u2206B0 and single-T* maps present spatial distributions of the B0 field inhomogeneity and the short- and long-T2* components, and provide indications for uncertain short-T* decays possibly caused by the B0 inhomogeneity. These maps are critical and complimentary to quantification and explanation of the separated mono- and bi-T2 sodium signals.# 3.1. Measurement of T* Values\n\nFID signals were acquired on whole brain of the study subjects right before sodium imaging. A product pulse sequence, either AdjXFve embedded in the manual shimming task or independent fid_23Na, was employed with parameters: TE=0.35\u20131.0ms, TR=100\u2013300ms, and averages =1\u2013128, and TA=0.2\u201339s. When a dual-tuned (1H-23Na) 8-channel Tx/Rx head array coil was used, there was a difference in initial phase and in FFT scale factors across channels. The initial phases were measured on channel image at central slice, and removed by aligning to zero phase. Channel signals were weighted with FFT scale factors and complex-value combined into a resultant FID. An additional step was performed to correct for distortion at the first few points of the resultant FID signal. Finally, a spectrum of T* values was calculated according to Eq. 4, at a resolution of 0.5ms for T* in 0.5\u2013100ms. Assignment of T* peaks to {T2*fr, T2*bs, T*bl} was based on their relative positions and intensities, i.e., T*fr \u2265 T2*bl >> T2*bs and intensity ratio 6:4 for the bi-T2 sodium. Repeatability of the T2* measurement is addressed in the Supporting Information.# 3.3. Optimization of TEs\n\nThe simulations were implemented via Eq. 6 for three cases: an ideal case serving as reference, practical case 1 having a large number of TEs, and practical case 2 having a small number of TEs. The ideal case had 80 TEs, i.e., TE = (0, 1, 2, \u2026, 79) ms, to cover entire range of T2* decays. The two practical cases, suggested by existing human studies24,29-31, had total scan time (TA) limited to 22 min for 8 TE-images, and the TEs were chosen to be most sensitive to T2* decays29. Thus, the case 1 had 8 TEs = (0.5, 1, 2, 3, 4, 5, 7, 10) ms, and the case 2 only had two TEs = (0.5, 5.0) ms but 4 averages at each TE. The SVD singular values (\u03bb1 \u2265 \u03bb2 \u2265 0) were calculated for each set of TEs via Eq. 6. The optimal set of TEs would be the one that had \u03bb2 producing the minimum amplification of random noise in the separated mono- and bi-T2 sodium images.# 3.4. Tomyutev Simulations\n\nThe mono- and bi-T2 sodium separation was carried out via Eq. 3 at a typical set of T2* values, (TE0%, TE1, TE2%, TE3) = (50.0, 3.5, 15.0) ms, and an optimal two-TE scheme, TE = (0.5, 5.0) ms. Sodium signals were calculated via Eq. 1, with an additive Gaussian noise, N(0, \u03c32), at each of noise trials (independent from each other), S(t) + N(t). The mono- and bi-T2 signal amplitudes (A1, A2) were simulated to vary in a normalized range of 0.0\u20131.0 at a step size = 0.1. The separation was implemented using the function lsunonneg() in MATLAB, and repeated Nnoise times at each of the specific amplitudes. Mean and standard deviation (SD) were reported as separated sodium signal. Nnoise = 1054 was chosen to detect a 10% of SD, or 0.1 effect size d = \u0394\u03bc/SD, in difference between the mean and true value at 90% power and 5% significant level under the two-sided Student\u2019s t-test27.# 3.5. Phantom studies\n\nFour phantoms, custom-built and described in previous work, were studied. They were 50-mL centrifuge tubes filled with a mixture of distilled water, 10% w/w agar powder, and sodium chloride (NaCl) at three concentrations (90, 120, and 150mM) and at 150mM without agar, mimicking bi- and mono-T2 sodium signals in the brain tissues. Sodium MRI was performed on a clinical scanner at 3T (MAGNETOM Trio Tim, Siemens Medical Solutions, Erlangen, Germany) with a dual-tuned (1H-23Na) volume head coil (Advanced Imaging Research, Cleveland, OH). The data acquisition was implemented using an SNR-efficient, three-dimensional (3D) pulse sequence called the twisted projection imaging (TPI), with parameters: rectangular RF pulse duration = 0.8ms, flip angle=80\u00ba (limited by SAR and TR), field of view (FOV)=220mm, matrix size=64, nominal resolution=3.44mm (3D isotropic), TPI readout time=36.32ms, total TPI projections =1596, TPI y-factor=0.4, TR=100ms, TE1/TE2=0.5/5ms, averages=4, and TA=10.64min per TE-image. The image reconstruction was offline implemented on a desktop computer (OptiPlex 7050, 8GB memory, Windows 10, DELL, Round Rock, TX) using a custom-developed programs in C++ (MS Visual Studio 2012, Microsoft, Redmond, WA). Separation of the mono- and bi-T2 sodium signals was implemented using a custom-developed program as described above.# 3.6. Human studies\n\nThe human studies were conducted with the approval of local Institutional Review Board (IRB) at NYU Grossman School of Medicine, New York, NY, USA. The study subjects included nine healthy adults (age 39.6\u00b121.4 years between 21\u201374 years; 3 males and 6 females) and six patients with diverse neurological disorders (1 bipolar disorder, 3 epilepsy, 1 multiple sclerosis, and 1 mild traumatic brain injury; age 30.5\u00b115.1 years between 18\u201359 years; 3 males and 3 females), after the exclusion of one subject and one patient due to motion between the two TE-images. The study was performed on a clinical 3T MRI scanner (Prisma, Siemens Healthineers, Erlangen, Germany) with a custom-built 8-channel dual-tuned (1H-23Na) head array coil. The same TPI pulse sequence as in the phantom studies was used for data acquisition. Images were reconstructed using the gridding algorithm, off-line and channel-by-channel, and combined into a resultant image via the sum-of-squares (SOS) algorithm. To decouple random noise across channels, an orthogonal linear transform (detailed in Ref. 30) was performed in which physical channel data.# 3.7. Mayying of \u2206B0 and single-T2*\n\nTo map \u2206B0 (or \u2206f0 = a\u2206B/2b), Hermitian product method was performed via Eq. 7 at individual imaging voxels to calculate phase differences {(O,, < = 1, 2, \ud835\udc53\ud835\udc53 \ud835\udc53\ud835\udc53 \ud835\udc53\ud835\udc53 , ? \u2212 1} between TEs {0;, < =, 1, 2, \ud835\udc53\ud835\udc53 \ud835\udc53\ud835\udc53 \ud835\udc53\ud835\udc53 , ?}. Image amplitude at individual channels were corrected with the FFT factors {e, f = 1, 2, \ud835\udc53\ud835\udc53 \ud835\udc53\ud835\udc53 \ud835\udc53\ud835\udc53 , ?8}. Phase unwrapping was not performed due to small intervals in the TEs and, in general, small inhomogeneity in the B0 field in sodium MRI. Computation for \u2206B0 map is fast (0.078s) on a Mac laptop computer for images of size 64\u00d764\u00d764 at two TEs.\n\n(g/ = %9(*5)) \u2211 ,<)) *5)\u0394\", \u2044\u0394#$, Eq. [7a]\n\n(O, = i\u210e1JT{\u2211*8) e(% \u2219 !(\u2217(0;) \u2219 !((0;,=))} (< , Eq. [7b]\n\n(0;, = 0;,=) \u2212 0;, Eq. [7c]\n\nTo map single-T*, a MATLAB curve-fitting function fit(x, y, \u2018exy1\u2019) was used to calculate single-T* values at each voxel via Eq. 8. A restriction (T*max<100ms) was enforced to exclude unreasonable values caused by noise. The computation time is acceptable (10min17s).\n\n|!(0;)| = L/exp (\u22120;/0%\u2217) , , 0 \u2264 0%\u2217 \u2264 0%,>?@\u2217 Eq. [8]# 3.8. xignal-to-noise vatio\n\nIn a region of interest (ROI), SNR was calculated via Eq. 9 in a simplified way for both volume and array coils by taking the ratio of mean intensity x to noise standard deviation (SD) in noise-only background regions. A factor of 0.655 was applied to noise SD to account for Rician distribution in magnitude images. For SNR mapping, pixel signal is used in the calculation.\n\nH?l = 0\ud835\udc53\ud835\udc53\u210e55 H/Hn Eq. [9]# 3.9. Estimation of extva- and intvacellulav solume fvactions\n\nThe estimates give the up-band of volume fractions when all the mono-T2 sodium are assigned to extracellular space while the bi-T2 sodium to intracellular space, in the case that mono- and bi-T2# 3.10. Statistical Significance\n\nA regular statistical significance (P=0.05) was applied to the comparisons, via Student\u2019s t-test, between the two sets of data in this work. Minimum sample size for the t-test is 16, with 80% power, 5% significance level, two-sided test, and 1.0 effect size.27# 4.1. Measurement of T* Values\n\nFig. 2 presents a representative of T* values on a healthy subject (52 years old, male), with and without correction for the distortion at the first five ADC samples (Figs. 2a1,b1). The correction removed distortion and reduced overall residual fitting error from 2.33% to 1.49% (Figs. 2a2,b2). The correction also improved resolution of short-T* components: from singlet at 2.5ms to doublet at 0.5ms and 2.5ms (Figs. 2a3,b3). A high resolution in T* was achieved at 0.5ms, with residual fitting error less than 1.5%. Notably, a few of sparse peaks appear in the T2* spectrum, indicating that T* values are well clustered in the human brain and that a single set of T2* values is applicable to the separation of mono- and bi-T2 sodium.# 4.2. Sensitivity to T* Values\n\nFig. 3 shows the computed impact of T2* values on the accuracy of separation of the mono- and bi-T2 sodium signals (mfr, mbd). The calculation was performed at a typical set of (T2*fr, T*bs, T*bl) = (50.0, 3.5, 15.0) ms in two extreme cases: the mono-T2 sodium dominating, mfr = 0.9, and the bi-T2 sodium dominating, mbd = 0.9. The impact of individual T* components are shown in columns. In column A, an error in T*fr caused an error in mfr or mbd much smaller for the dominant one (e.g., \u2206mbd < 2.2% when \u2206T2*fr < 20%, and \u2206mfr < 2.9% when \u2206T*fr < 5.0%). In column B,# 4.3. Optimization of TEs\n\nThe ideal scheme of 80 TEs is presented in Fig. 4a on Yfr(TE) of the free sodium at a typical Tfr=50ms and on Ybd(TE) of the bound sodium at a typical {T2*bs, T2*bl} = {3.5, 15.0}ms. The intuitively-favorable scheme of 8 TEs was presented in Fig. 4b, while the optimal candidate of two TEs was presented in Fig. 4c. Singular values (^1, ^2) of the three TE schemes were compared against each other in Fig. 4d. In Fig. 4e is singular values of the 2-TE scheme, slowly changing with the TE2 increasing. The ^2 is less than 1.0 for the 8-TE and 2-TE schemes, leading to an amplification of noise. Therefore, a better choice for less noise amplification is the 2-TE scheme, in which TE2 at 5ms produced a value near maximum of ^2 while preserving higher signal than the larger TE. Thus, the 2-TE scheme is selected for the human studies.# 4.4. Phantom Simulations\n\nFig. 5 demonstrates the simulated separation of mono- and bi-T2 sodium signals (mfr, mbd) at a typical set of (Tfr, T2*bs, T2*bl) = (50.0, 3.5, 15.0) ms and two-TE scheme TEs=(0.5, 5.0), at three SNRs: extra-high (100), high (50), and regular (25). The mean and standard deviation (SD) of the separated mfr and mbd were presented. The SD (error bar) consistently decreased with SNR increasing. There was an underestimate for mfr or mbd near the maximum value 1.0, but an overestimate near the minimum value 0.0, with an amount decreasing with SNR increasing.# 4.5. Phantom Studies\n\nFig. 6 summarizes the outcomes of phantom studies. Fig. 6a shows phantom arrangement of four tubes and sodium images at TE1/TE2=0.5/5ms. Fig. 6b shows FID signal from the four tubes at# 4.6.1. T* salues in wzole wvain acvoss suwjects\n\nFig. 7. presents a scattering plot of individual T2* components from the T* spectra across all the subjects studied. Typical T2* spectra and FID signals are shown in Fig. 7a for a healthy young subject (21 years old, male) and in Fig. 7c for an epilepsy patient (31 years old, male). The peaks in the T* spectra were sparse and just 2\u20134 peaks, suggesting that a global set of T2* values (T*fr, T*bs, T2*bl) be a plausible estimate for the whole brain (Figs. 7a, c). However, these T* values are slightly different from subject to subject (Fig. 7b). The short-T* component is clearly crowded in a range of 1-5ms, while the long-T2* is widely scattered in three bands centered at 10, 20, and 30ms, respectively. Interestingly, the long-T* component is shifted to lower values in the patient group, compared with the healthy group. There seems no difference between males and females in the healthy group. Therefore, the T2* values are heterogeneous across the subjects.# 4.6.2. Mono- and wi-T2 sodium seyavation\n\nFigs. 8 and 9 present two typical cases of the human studies in full implementation of the mono- and bi-T2 sodium separation. Case 1 (Fig. 8) is from a 26-year-old healthy female, and includes 3D sodium images of the brain at TE1/TE2=0.3/5ms (Fig. 8a), FID signal of whole brain and associated fitting error and T* spectrum (Fig. 8b), the separated sodium images from the two-TE2 images using (T2*fr, T2*bs, T2*bl) = (50.0, 6.0, 19.0) ms (Figs. 8c1-c3), and inverse-contrast displays (Figs. 8c4-c5). In Fig. 8d are SNR, \u2206B, and single-T* maps calculated from the two-TE.# 4.6.3. Estimates of extva- and intvacellulav solume fvactions\n\nIn the healthy group (Fig. 10), the difference in volume fraction between the gray and white matters is significant (P=0.023): 89.6\u00b14.5 % vs. 94.0\u00b12.6 % for the intracellular space (in line with the literature, 75% vs. 92%8), and 10.4\u00b14.5 % vs. 6.0\u00b12.6 % for the extracellular space. No significant\n\nimages in Fig. 8a. Case 2 (Fig. 9) is from a 59-year-old male patient with bipolar disorder. The separated sodium images were attained at (T*fr, T2*bs, T2*bl)2 = (50.0, 2.5, 7.0) ms according to the peaks in Fig. 9b3.\n\nFig. 8 indicates that signals from CSF in the brain were effectively separated into the mono-T2 sodium image (Fig. 8c2 or c4), while signals from brain tissues such as gray and white matters were separated into the bi-T2 sodium image (Fig. 8c3 or c5). Notably and surprisingly, signal intensity across brain tissues looks more uniform in the bi-T2 sodium images than in the mono-T2 sodium images (Fig. 8c2), total sodium images (Fig. 8c1), and TE1-images (Fig. 8a1). SNR in Fig. 8d1 are 25+ in most regions of the brain, assuring a robust separation as suggested in the simulations in Fig. 5. The field inhomogeneity \u2206B0 in Fig. 8d2 varied between \u00b120Hz across the brain, with the largest off-resonance in the prefrontal and occipital lobes, leading to visible blurring of the tissues in the bi-T2 sodium images (Fig. 8c3 or c5, sagittal). The single-T* map in Fig. 8d3 provides a spatial distribution of short and long T* components across the brain, complementary2 to T* spectrum in Fig. 8b3. It also indicates that majority of long T2* components are located in the prefrontal lobe in this particular case (Fig. 8d3, sagittal).\n\nFig. 9 demonstrates potential benefits from the bi-T2 sodium images of patients with neurological disorders such as bipolar disorder which is known to cause abnormally-high intracellular sodium concentration in the brain but locations are unknown.38,39 The bi-T2 sodium images (Fig. 9c3 or c5) clearly highlighted brain regions of an elevated bi-T2 sodium signal against surrounding tissues, with a ratio of 1.78 vs. 1.40 (or 27.1% increase) before the separation (Fig. 9c1). These regions have no visible contrast in the total or TE-images (Fig. 9a1 or c1). SNR in these regions is 40+ (Fig. 9d1), supporting a robust separation. The field inhomogeneity \u2206B0 in these regions is low (<5Hz, Fig. 9d2), excluding field-induced artifacts. The single-T* map in Fig. 9d3 shows abnormally low T2* values in these regions, confirming an increase in short-T2* components.# 5. DISCUSSION\n\nThe proposed MSQ technique has been demonstrated, using the computer simulations, physical phantoms, and human subjects, to be able to separate mono- and bi-T2 sodium signals voxel-wise. The physics behind the technique is the intrinsic difference in T2 relaxation between sodium nuclear spins: mono- vs. bi-exponential decay. In the restriction of total scan time, the two-TE scheme, instead of the eight-TEs, was selected for smaller transfer of random noise during the separation (Fig. 4). The measurement of T* spectrum from FID signals of entire brain and the application of a global set of T* values (T2*fr, T2*bs, T2*bl) were tested feasible to humans. In case of not plausible for a global set of T2* values (i.e., T* spatially varying substantially 40-47), multi-regional sets, or a linear combination of them, may be used.\n\nThe quantification of sodium concentration after the separation is not addressed in this study because it is involved in a complicated procedure of 1) calibration that transforms sodium signal into concentration and 2) correction for inhomogeneous sensitivity of coil elements. These processes deserve a separate study to deal with.\n\nThe limitations of MSQ technique are obvious, and understanding them is crucial to practice of the technique. The first limitation is the two-component model (mono- and bi-T2 populations) which is of risk to produce a false-positive error for bi-T2 sodium. If there are mono- and bi-T2 sodium populations in a voxel, then they are separatable. If not, such as two mono-T2 sodium decays at different T2* values in a voxel, they would be falsely separated into bi-T2 sodium because they can be combined mathematically (not physically) into a bi-exponential decay mimicking a true bi-T2 sodium decay. This kind of false positive error stems from the fact that the separation is based on mathematical model, instead of physical model as does the TQF separation. Understanding this kind of false positive errors is critical to proper interpretation of the separated bi-T2 sodium signals.\n\nThe second limitation is the misguided separation of single mono-T2 sodium component of short T* 2 value in a voxel, such as regions in nose and sinuses (Figs. 8c3 and 9c3).situation, the MSQ separates it into bi-T2 sodium of underestimated intensity. These mis-separated regions can be identified by means of the maps of \u2206B0 and single-T* (Figs. 8d and 9d).2\n\nThe third limitation is the underestimate of bi-T2 sodium signal caused by the TE1 image, as illustrated in the phantom study (Fig. 6c3). The separation (Eq. 2) assumes TE-image intensity1 exactly at TE1 (i.e., a very short readout time). Actual TE1-image intensity is an average over readout time during which short-T2* components decay significantly when readout is relatively long, such as readout T=36.32ms about ten times long of a short-T2* at 3ms in this study. Therefore, the underestimation alters with readout time or pulse sequence. To mitigate the problem, two strategies may apply. One is to replace TE1 value in Eq. 2 with an effective (larger) value that accounts for short-T* decay during the readout. The other is to shift T2*bs to a larger2 value, as did in previous work on the phantoms.48 Alternatively, correction for the underestimates is integrated into calibration of image intensity for sodium concentration (Fig. 6c4).# 6. CONCLUSION\n\nThe data presented in this study have demonstrated the feasibility of proposed multi-TE single-quantum sodium MRI technique to separate between mono- and bi-T2 sodium signals in a fashion of voxel by voxel. The MSQ technique is based on a solid physics related to the intrinsic difference in T2 relaxation between the two populations of sodium nuclear spins. The two-TE sampling scheme stands out for smaller noise transfer during the separation. A global set of T2* values (T*fr, T*bs, T2*bl) measured on T* spectrum of whole brain was tested applicable to humans. However, the MSQ technique has limitations and requires cautions in practice.",
        "context_id": 40,
        "question": "What type of imaging technique is generalized to multi-TE single-quantum (MSQ) imaging in the study?",
        "answer": [
            "short-T2 imaging"
        ],
        "context_length": 29465
    },
    {
        "context": "# BRDF-NeRF: Neural Radiance Fields with Optical Satellite Images and BRDF Modelling# 1. Introduction\n\nOver the past two decades, significant progress has been made in image processing algorithms. In particular, 3D surface reconstruction has bene-\n\nPreprint submitted to ISPRS Journal of Photogrammetry and Remote Sensing September 19, 2024# Modeling Surface BRDF with NeRF\n\nFited from high-resolution spatial data and algorithms such as semi-global stereo matching (SGM), which can generate detailed surface maps of urban and natural environments (Hirschm\u00fcller, 2008; Rosu et al., 2015; Pierrot-Deseilligny and Paparoditis, 2006). However, state-of-the-art approaches still face challenges, particularly with radiometrically heterogeneous surfaces, complex reflectance functions, or diachronic acquisitions. Recent research has attempted to address these challenges by leveraging learning algorithms capable of modelling complex resemblance functions, given appropriate architectures and sufficient training datasets (Chang and Chen, 2018; Chebbi et al., 2023; Wu et al., 2024).\n\nAt the same time, a new approach to surface reconstruction has emerged with Neural Radiance Fields (NeRF), which differs from other learning-based methods by operating in a self-supervised manner. It works on single pixels rather than patches, treats non-Lambertian surfaces and generates new synthetic views (Mildenhall et al., 2020). Besides, NeRF is capable of estimating the Bidirectional Reflectance Distribution Function (BRDF) of the surface at the same time. Understanding the BRDF of continental surfaces is crucial for a variety of applications, including land cover mapping, assessment of Earth\u2019s radiation budget, climate change studies, vegetation density analysis, and intercalibration of spaceborne sensors (Dumont et al., 2010).\n\nHowever, due to the anisotropic nature of the Earth\u2019s reflectance, estimation of the BRDF generally requires numerous angular measurements, which is challenging with a few satellite images.\n\nThe aim of this paper is to model the surface\u2019s BRDF explicitly from sparse satellite views, and improve surface reconstruction, particularly over landscapes with anisotropic reflectance characteristics (e.g., bare soil, vegetation). We select NeRF as the algorithmic framework for its capacity to model angle dependent surface reflectance. Our BRDF-NeRF workflow (Figure 1) is designed for satellite acquisitions with only three synchronous views and incorporates the semi-empirical Rahman-Pinty-Verstraete (RPV) BRDF model, widely used in the remote sensing community to represent the BRDF of natural surfaces (Rahman et al., 1993).\n\nTo the best of our knowledge, this work is the first to integrate a BRDF model into neural radiance fields for land surfaces. It extends our previous work on neural radiance fields with sparse optical satellite images (Zhang and Rupnik, 2023). The open-source code is available at github.com/LulinZhang/BRDF-NeRF. The terms DSM and surface, as well as SGM and stereo matching are used interchangeably throughout this article.# Figure 1: BRDF-NeRF Workflow\n\nA few satellite RGB images, and the corresponding low-resolution depth maps calculated using a classical stereo matching algorithm are fed into BRDF-NeRF to predict the normals n, and the RPV parameters \u03c10, k, \u0398 and \u03c1c, describing the surface reflectance. \u03c10 represents the amplitude component, k controls the overall shape of the anisotropic behaviour, \u0398 establishes the degree of forward or backward scattering, and \u03c1c allows to model the hotspot effect. The five parameters are integrated into a RPV renderer to generate the synthetic image. In the meantime, high resolution depths are obtained by accumulating the weights in the volume estimated by BRDF-NeRF.# 2.1. Neural Radiance Fields\n\nThe vanilla NeRF (Mildenhall et al., 2020) leverages a large number of images captured with a pinhole camera to represent small-size scenes as particles that emit light, instead of reflecting light. Subsequent NeRF variants proposed to relax some of those defining constraints, without compromising the quality of the outputs, i.e., the synthesised images and the 3D model. In the following paragraphs we briefly discuss the state-of-the-art approaches relevant to our work.# NeRF from few views\n\nSince NeRF relies solely on pixel values for network training, a large number of input images is essential for generating photo-realistic novel views. Attempts to train NeRF with sparse input images often result in overfitting and inaccurate estimation of scene depth, leading to artifacts in the rendered novel views. This limitation restricts the applicability of NeRF and prolongs training time. To address this challenge, efforts have been made to adapt NeRF to sparse input images by introducing various regularization priors. A common approach is to incorporate depth supervision, including sparse depth (Deng et al., 2022; Wang et al., 2023; Somraj and Soundararajan, 2023; Guo et al., 2024) and dense depth (Wei et al., 2021; Roessle et al., 2022; Zhang and Rupnik, 2023). In addition, methods such as image features (Yu et al., 2021) or semantic regularization (Xu et al., 2022) have also been explored.# NeRF and BRDF\n\nWhile vanilla NeRF excels in view synthesis, it cannot relight or edit materials, due to its inability to decompose outgoing radiance into incoming radiance and surface material reflectance. Some researchers proposed to extend NeRF to incorporate information on the Bidirectional Reflectance Distribution Function (BRDF), which characterises how materials reflect light under different viewing and lighting conditions. The majority of BRDF-compatible NeRF variants, such as those proposed by (Bi et al., 2020; Srinivasan et al., 2021; Boss et al., 2021; Yang et al., 2022a; Verbin et al., 2022; Mai et al., 2023), adopt some version of the microfacet BRDF model (Walter et al., 2007). This model represents reflectance as the superposition of diffuse and specular components and typically includes a surface roughness parameter, influencing the appearance of the surface through a distribution of microfacet orientations. However, while microfacet BRDF models offer effective parameterization, they poorly model BRDF of natural surfaces (e.g., soil, vegetation) which exhibit a more or less marked backscattering behavior (hotspot effect). Additionally, data-driven BRDFs pre-trained on BRDF databases have been explored in NeRF (Zhang et al., 2021). However, the databases consist of artificial materials and have been built in controlled environments. The spectral and directional optical properties of natural materials are often very different.# Figure 2\n\nshows the scattering patterns of the Lambertian, Microfacet and RPV models. The latter is an anisotropic BRDF model widely used in remote sensing and which we will use in this work.# Figure 2: Scattering Patterns\n\n|(a) Lambertian|(b) RPV: back scattering|(c) RPV: forward scattering|\n|---|---|---|\n|(d) Microfacet|(e) RPV: bell shape scattering|(f) RPV: bowl shape scattering|\n\nLambertian (a), Microfacet (d) and RPV (b, c, e, f) BRDF models. RPV can be used to simulate complex scattering indices.# NeRF in Earth Observations\n\nThe Earth observation community has mainly focused on adapting the initial NeRF\u2019s design to meet the specificities of space imagery: changing shadows, dynamic scene due to asynchronous acquisitions, as well as sparse views. Shadow-NeRF (Derksen and Izzo, 2021) pioneered the application of NeRFs to satellite images, where the authors have explicitly modelled the shadows of the scene by leveraging the Sun\u2019s direction. Sat-NeRF (Mar\u00ed et al., 2022) extends Shadow-NeRF by replacing the pinhole camera model with an empirical push broom model (i.e., Rational Polynomial Coefficients) and modelling transient objects in the scene such as moving cars. EO-NeRF (Mar\u00ed et al., 2023) employs a novel geometry-based shadow rendering, resulting in more accurate digital surface models (DSMs). SpS-NeRF (Zhang and Rupnik, 2023) further adapted NeRF for scenarios with few satellite views by introducing spatial guidance within NeRF sampling, conditioned on low-resolution input depths. Sat-Mesh (Qu and Deng, 2023) used a latent vector to deal with inconsistent appearances in satellite imagery, while SUNDIAL (Behari et al., 2024) proposed a secondary shadow ray casting technique to jointly learn satellite scene geometry, illumination components and Sun direction. SatensoRF (Zhang et al., 2024) decomposed# Existing BRDF models\n\nNumerous BRDF models have been developed to describe the spectral and directional reflectance of natural and artificial surfaces. They can be classified into physical, empirical and semi-empirical models.\n\nPhysical models (Pinty and Verstraete, 1991) are based on rigorously defined physical parameters and offer the most accurate descriptions of observed scenes. However, a large number of multiangular observations are required to retrieve these parameters by model inversion, making them impractical for optically complex surfaces.\n\nEmpirical models (Walthall et al., 1985; Minnaert, 1941; Shibayama and Wiegand, 1985) are derived as simple statistical fits to observed data, and provide no additional insights into the surface type or structure.\n\nSemi-empirical models (Rahman et al., 1993; Wanner et al., 1995; Hapke, 1981; Roujean et al., 1992; Lucht et al., 2000) employ specific mathematical functions to best represent the physical interactions between the radiation field and the surface. They accept a reduced number of parameters, which facilitate their inversion.\n\nThe semi-empirical RPV model (Rahman et al., 1993) is among the most commonly used. It is capable of representing the reflectance of various natural surfaces with just four parameters (Figure 2), and has been used to address atmospheric radiation transfer problems (Martonchik et al., 1998a,b); classify forest types (Koukal et al., 2014); simulate plant leaves reflectance (Biliouris et al., 2009); estimate BRF values under unmeasured illumination and viewing angles (Lattanzio et al., 2007); estimate surface albedo (Martonchik et al., 1998a,b; Privette and Roy, 2002); and identify surface properties (Widlowski et al., 2001; Gao et al., 2003).# Deriving BRDF\n\nMost of the parameters controlling BRDF cannot be measured in the field, but are obtained by inversion of surface reflectance models on observations. To guarantee reliable estimates, the surface must be observed over a wide range of illumination/viewing angles. Laboratory and field measurements using goniophotometers have traditionally been used to measure reflectance (Lv and Sun, 2016; Sandmeier and Strahler, 2000; Combes et al., 2007). Over the last few decades, several spaceborne instruments have been designed to carry out multiangular observations, such as MISR, POLDER, MODIS, CHRIS/Proba, and VIIRS. These instruments have limited spatial resolutions, ranging from a few tens to a few hundreds of meters. (Labarre et al., 2019) have inverted the Hapke model on a set of 21 multiangular Pleiades images, acquired at a spatial resolution of 2m. However, such acquisitions are rarely available and inversion with three or four images is ill-posed.\n\nIn this paper, we explore the potential for estimating the BRDF of natural surfaces using as few as three high-resolution multispectral optical images. Our approach offers new possibilities for studying solar radiation reflected from the Earth\u2019s surface, taking advantage of the multiplicity, temporal coverage, and high spatial resolution of optical satellite imagery.# 3. Radiance Fields with RPV Reflectance\n\nWe briefly introduce the vanilla NeRF architecture and discuss two key ingredients of our BRDF-NeRF: geometry modelling (depths and normals) as well as the radiometric rendering (RPV BRDF model). The BRDF-NeRF workflow is described in Figure 1.# Preliminaries\n\nNeRF (Mildenhall et al., 2020) represents a continuous volumetric field of a static scene that emits light, optimized with a fully connected deep network. Given a 3D point x = (x, y, z) accompanied with a viewing angle wr = (dx, dy, dz), NeRF predicts a volume density \u03c3 and a colour c = (r, g, b). NeRF renders images by sampling N query points along each camera ray and accumulating the colours with weights defined by density, and imposes the rendered images to be close to the training images. Each camera ray r is defined by an origin point o and a viewing direction vector wr such that r(t) = o + t * wr. Each query point xi in r is defined as xi = o + t * wr,i where ti lies between the near and far bounds of the scene, tn and tf.rendered pixel value C(r) of ray r is calculated as follows:\n\nC(r) = &Sigma; i=1N \u03b1i ci (1)\n\nwhith \u03b1i = 1 \u2212 e\u2212\u03c3\u03b4i, Ti = Qi\u22121(1 \u2212 \u03b1j) and \u03b4i = ti+1 \u2212 t. \u03b1i represents the opacity of the current query point xi and Ti is the transmittance. The contribution of colour ci to the accumulated colour C(r) increases with opacity and transmittance.# 3.1. Geometric modelling\n\nWe incorporate geometric information to extend the applicability of BRDF-NeRF to sparse view acquisitions and to predict surface normals that are essential for accurate estimates of BRDF, as detailed in Section 3.2.# Depth supervision.\n\nInstead of querying ray points crossing the entire volume of the scene, as is the case in the vanilla NeRF, we narrow it down to a buffer space defined around the location of an approximately known surface. This tactic reduces ambiguity and enables reliable volume densities to be estimated with fewer images. We further encourage the depths predicted by NeRF to remain close to the input surface by using the following loss term introduced in (Zhang and Rupnik, 2023):\n\nLdepth(r) = &Sigma; r\u2208Rsub (corr(r)(D(r) \u2212 D(r))2 , (2)\n\nwhere D(r) are the predicted depths calculated as D(r) = &Sigma; i=1N T\u03b1t,i\n\nwhile the D(r) are the input depths obtained from stereo matching on low-resolution images. We have observed that the performance of stereo matching on low resolution images is marginally affected by a change in surface BRDF and can therefore provide sufficiently good depth initialisations for our radiance fields. The parameter corr(r), which corresponds to the similarity score obtained by stereo matching, acts as a weight or confidence. It adjusts the level of supervision, having a strong impact where confidence is high and a minimal impact where input depths are uncertain. Rsub is a subset of rays that satisfy at least one of the following two conditions: (1) S(r) > \u03a3(r); (2) (D(r) \u2212 D(r)) > \u03a3(r), where S(r)2 = &Sigma; i=1N T\u03b1(ti \u2212 D(r))2 represents the uncertainty of the predicted depth, and \u03a3(r) = 1 \u2212 corr(r) represents# 3.2. Radiometric rendering\n\nThe geometric approach presented above guarantees decent 3D reconstructions of Lambertian scenes. Next, we adapt this approach to handle non-Lambertian natural surfaces by estimating a BRDF and incorporating it into the rendering Equation (1).# RPV equation\n\nWe estimate the reflectance of natural surfaces using the Rahman-Pinty-Verstraete (RPV) model (Rahman et al., 1993), a semi-empirical model well suited to satellite images (see Equation (3)). We chose this model for its simplicity, its physics-based parameters and its ability to represent asymmetric BRDF, including the hotspot effect. The latter corresponds to a sharp increase in reflectance, which becomes maximum when the illumination and viewing directions are coincident.\n\nIn this model, the colour c of a surface point, defined by the normal vector n, the illumination direction wir and the viewing direction wr (Figure 3), is calculated as the product of the incoming light Lir, the cosine of the incident angle |wir \u00b7 n|, and the bidirectional reflectance factor simulated by RPV:\n\nc(n, wir, wr) = Lir \u00b7 |wir \u00b7 n| \u00b7 RPV(n, wir, wr),           (3)# Wir\n\nLir is set to a unit vector, and |wir\u00b7n| is approximated by |wir\u00b7[0, 0, 1]| because the analytical normal n is not sufficiently smooth. The RP V term can be broken down into an amplitude parameter \u03c10 and three angle-dependent functions: modified Minnaert function M, Henyey-Greenstein function FHG, and backscatter function H:\n\nRP V (n, wir, wr) = \u03c10 \u00b7 M (\u03b8ir, \u03b8r, k) \u00b7 FHG(g, \u0398) \u00b7 H(\u03c1c, G),\n\nwith M (\u03b8ir, \u03b8r, k) = (cos\u03b8ircos\u03b8r(cos\u03b8ir + cos\u03b8r))k\u22121, FHG(g, \u0398) = (1 \u2212 \u03982) \u00b7 (1 + 2\u0398cosg + \u03982\u03b8ir + tan2\u03b8r, G) = 1 + (1 \u2212 \u03c1c)/(1 + G), and the geometric factor G = (tan\u22122tan\u03b8irtan\u03b8rcos\u03a6)1/2. The illumination wir and viewing wr directions are decomposed into zenith angles \u03b8ir and \u03b8r, azimuth angles \u03a6ir and \u03a6r, relative azimuth angle \u03a6 and phase angle g, all defined in a spherical coordinate system determined by the surface normal n.# Detailed Description of RPV Model Input Parameters\n\nThe RPV parameter \u03c10 in Equation (4) plays the role of pseudo-albedo. The modified Minnaert function controls the anisotropic behaviour of the surface using the parameter k. If k \u2248 1 the surface is quasi-Lambertian; if k<1 a bowl-shaped pattern dominates (reflectance values increase with the viewing zenith angle); and if k>1 a bell-shaped pattern dominates (reflectance values decrease with the viewing zenith angle) (Widlowski et al., 2004). The parameter \u0398 of the Henyey-Greenstein function controls the amount of radiation scattered in theforward (0 \u2264 \u0398 \u2264 1) or backward (-1 \u2264 \u0398 \u2264 0) directions. The backscatter function H is written as a function of a geometric factor G and a parameter \u03c1c, which represents the sharp increase in reflectance in the hotspot direction. When \u03b8ir = \u03b8r and \u03a6ir = \u03a6r, the geometric factor disappears and H reaches its maximum value, contributing to increase total reflectance. When estimating the RPV model, the ranges of variation of the parameters \u03c10, k, \u0398 and \u03c1c are fixed to [0, 1], [0, 2], [-1, 1] and [0, 1] (Koukal et al., 2014).\n\nTo give the reader an intuition about how RPV parameters affect reflectance, we analyse the bidirectional reflectance function (BRF) of selected points in one of our datasets (Figure 4). The BRFs are plotted by varying the viewing directions (zenith and azimuth angles between [0\u25e6, 90\u25e6] and [0\u25e6, 360\u25e6]) and fixing the Sun\u2019s direction to \u03b8ir = 52.1\u25e6 and \u03a6ir = 142.5\u25e6. The pseudo-albedo of the selected surface point estimated by BRDF-NeRF is \u03c10 = [0.122, 0.105, 0.091], the normal n=[0, 0, 1]. Among the six combinations of (\u0398, k, \u03c1c) given in Table 1, backward scattering (Figure 4 (b)) was predicted by BRDF-NeRF. Note that this BRF is consistent with a result generated independently over the same area with 21 Pleiades views (Labarre et al., 2019).# 1. RPV Model Parameter Sets\n\n|Param.|Backward|Forward|Bowl shape|Bell shape| |Hotspot effect|\n|---|---|---|---|---|---|---|\n|k|0.996|0.996|0.500|1.500|0.996| |\n|\u0398|-0.174|0.174|-0.174|-0.174|-0.174| |\n|\u03c1c|0.979|0.979|0.979|0.979|0.500|0|# 3.3. Network architecture\n\nThe network architecture presented in Figure 5 consists of two progressively trained components: the geometric part and the RPV part. Initially, the geometric part is pre-trained on the assumption of a Lambertian surface. After this pre-training phase (i.e., \u223c20% of the total duration of the training time), the RPV part is introduced. Three MLPs predicting k, \u0398 and \u03c1c as well as the analytical normal n engage in training, while the albedo \u03c10 is finetuned to match the pseudo-albedo of our new rendering equation (see Equation (3)). The separation of the geometry part from the RPV part, which handles the case of non-Lambertian surfaces, ensures that the final training stage works on well-initialised normal vectors.\n\nThe input spatial locations x are transformed by positional encoding, the activations for \u03c10, k, \u0398 and \u03c1c are sigmoid function, and the k and \u0398 are scaled to [0, 2] and [-1, 1] to match their real value ranges (Koukal et al.,\n\n**Figure 4: RPV Model Parameters \u2013 BRF**\n|(a) Point location|(b) Backward|(c) Forward|\n|---|---|---|\n|(d) Bowl shape|(e) Bell shape|(f) Hotspot effect|\n\nReflectances displayed in polar coordinates correspond to point A shown in (a). They are generated by evaluating the RPV model over a range of viewing directions, where \u03c10 and n are predicted by our BRDF-NeRF and fixed, while the Sun position is fixed and displayed as a white symbol. (b) backward scattering corresponding to the RPV parameters (\u0398, k, \u03c1c) found by BRDF-NeRF; (c) forward scattering obtained by modifying \u0398; (d-e) transition between bowl shaped and bell shaped scattering obtained by changing the k parameter; (f) hotspot effect with modified \u03c1c. The combinations of the six parameters are provided in Table 1.# (256 channels)\n\n8 layers\n\nRenderer\n\nPretrained on Lambertian sin\n\nWnir Wr\n\nsigmoid\n\nsoftplus\n\nFigure 5: BRDF-NeRF Architecture.\nThe input 3D locations x are sampled along the camera rays and fed into BRDF-NeRF to query density and RPV parameters. BRDF-NeRF consists of shared 8-layer spatial MLPs, 1-layer feature MLP and four 1-layer RPV MLPs. The 8-layer MLPs are followed by a softplus function to predict the density \u03c3, which is further used to calculate the analytical normal n. The RPV MLPs are concatenated with a sigmoid function to predict \u03c10, k, \u0398 and \u03c1c. The elements within the dashed rectangle are pre-trained on the assumption of a Lambertian surface for the first 20% of the total training steps. The colour c is predicted with the RPV rendering equation in Equation (3), where Wir and Wr represent the Sun and camera directions, respectively.\n\n2014). Ultimately, all the parameters of the BRDF-NeRF are optimized to minimise the combination of (1) the colour loss between the ground truth pixel colour C(r) and the predicted pixel colours C(r), and (2) the depth loss Ldepth(r) (Equation (2)):\n\nL = X C(r) \u2212 C(r) 2 2 + \u03bbLdepth(r), (5)\n\nwhere \u03bb is a weight balancing the contribution of colour and depth. See our experiment on finding optimal \u03bb in Section 4.6.# 4. Numerical experiments\n\nWe conduct a series of experiments to evaluate our BRDF-NeRF on novel view synthesis (Section 4.4) and altitude estimation (Section 4.5) tasks. In addition, we examine the influence of atmospheric correction from TOA (Top Of Atmosphere) to TOC (Top Of Canopy) (Section 4.3) and carry out ablation studies to determine the best training strategy (Section 4.6), and the most optimal way of rendering (Section 4.6).\n\n13# Evaluation Metrics\n\nPrecision metrics are Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index measure (SSIM) (Wang et al., 2004) for view synthesis, and Mean Altitude Error (MAE) for altitude extraction. Ground truth (GT) images are true images not seen during training, while the GT surface is a DSM generated with high-resolution Pleiades panchromatic images, with a ground sampling distance (GSD) of 0.5 m, and stereo image matching (Pierrot-Deseilligny and Paparoditis, 2006). BRDF-NeRF is also compared to competitions Sat-NeRF (Mar\u00b4\u0131 et al., 2022), SpS-NeRF (Zhang and Rupnik, 2023) as well as DSMs generated with SGM using full-resolution images (i.e., SGMZ1 with GSD = 2m). The source code for EO-NeRF (Mar\u00b4\u0131 et al., 2023) was not available for comparison. The RPV parameters are indirectly validated by examining quantitative metrics for novel view synthesis. Additionally, one can compare the BRF in Figure 4 (b) with an independent result derived from 21 Pl\u00b4eiades images in (Labarre et al., 2019).# Training\n\nOur network is trained with the Adam Optimizer (lr=5e-4, decay=0.9, batch size=1024). We use SpS-NeRF\u2019s ray sampling strategy to sample 64 stratified points along each ray, accompanied with 64 guided points following a Gaussian distribution. We optimize BRDF-NeRF for 100k iterations, which takes \u223c10 hours on NVIDIA GPU with 40GB RAM. For fair comparison, the competitive methods (i.e., Sat-NeRF and SpS-NeRF) are also trained for 100k iterations with 64 + 64 points along each ray, which takes \u223c6 hours. BRDF-NeRF is less efficient in training than competitive methods, mainly due to the normal analytical computation. Computational efficiency was not the aim of this work and could be improved in the future with techniques such as tensor decomposition.# Light visibility\n\nIt is important to take into account the visibility of samples when scenes contain occlusions, for example when acquiring images in mountainous or urban areas. Visibility describes the transmission of a sample between the light source and the query point. Brute-force computation of the light visibility is computationally expensive, as it requires marching rays from all the query points along the camera ray to the light source. Previous work treats the light visibility with different strategies: (1) assuming that light visibility is the same everywhere (Mai et al., 2023; Boss et al., 2021); in this scenario, a shadow is embedded into the albedo colour; (2)# 4.2. Dataset\n\nEvaluations are carried out on two sites (Djibouti, Lanzhou). We extract two regions of interest (\u223c 1.5 km \u00d7 1.5 km) in each site, which we refer to as A and B (e.g., Dji-A and Dji-B). In each dataset, we distinguish three test scenarios ranging from easy (novel view interpolation) to very hard (novel view extrapolation). The input images are RGB with a GSD of 2m and undergo atmospheric correction prior to processing (see Section 4.3). Sadly, we could not perform experiments on the open DFC dataset (Bosch et al., 2019) representing urban scenes since RPV is designed for natural surfaces.# Djibouti Dataset\n\nIt is located in the Asal-Ghoubbet rift (Republic of Djibouti) (Labarre et al., 2019) and consists of 21 multiangular Pleiades 1B images collected in a single flyby on January 26, 2013. Three quasi-nadir images are chosen for training, and three other images for testing, including interpolation and extrapolation scenarios (Figure 6 (a)).# Lanzhou Dataset\n\nIt is located in Lanzhou (China) and consists of 3 Pleiades 1B images acquired on April 23, 2013 and 3 Pleiades 1A images acquired on June 29, 2013. This is a multi-date dataset where the position of the Sun changes between acquisitions (Figure 6 (b)).# (b) Lanzhou\n\nFigure 6: Testing Scenarios \u2013 Sun and Viewing Angles. The angles are displayed in polar coordinate systems, where the radius and angular coordinate represent zenith and azimuth, respectively. We design up to three test scenarios, which differ in the viewing angle of the test images. Easy corresponds to a case where the test images are interpolated from the training images, while hard and very hard correspond to extrapolation. Three training images are used in Djibouti, four in Lanzhou (two of each epoch).# 4.3. The effect of atmospheric correction\n\nAtmospheric correction is important for two reasons. Firstly, the atmosphere affects the signal received by the satellite sensor. Secondly, the RPV model, like all BRDF models, describes TOC reflectances whereas Pleiades images are typically supplied as calibrated at TOA. We apply the atmospheric correction using an Orfeo ToolBox (OTB) software library (national d\u2019\u00e9tudes spatiales, 2002) which is based on the 6S radiative transfer code (Vermote et al., 1997). The correction model requires four atmospheric parameters: ozone content UO3 (cm-atm), water vapor content UH2O (g/cm2), aerosol optical thickness \u03c4A (unitless) and atmospheric pressure Pa (hPa). The first three parameters are estimated from ancillary datasets corresponding to the day of image acquisition, available on NASA\u2019s Earth observation website (https://neo.gsfc.nasa.gov/). The atmospheric pressure is approximated using the formula Pa = 1013.25\u00b7(1\u22120.0065\u00b7Z/288.15)5.31, where Z is the surface altitude expressed in meter. The four parameters, along with the adjacency radius for both epochs in the Lanzhou dataset, are shown in Table 2. The Djibouti dataset had been corrected for atmosphere by the satellite image.\n\n16# Table 2: Atmospheric Correction Parameters\n\n|Epoch|UO3 (cm-atm)|UH2O2 (g/cm)|\u03c4A (unitless)|Pa (hPa)|adjacency radius (-)|\n|---|---|---|---|---|---|\n|23/04/2013|0.3220|1.7333|0.4665|783|1.0|\n|29/06/2013|0.2969|2.5625|0.0980|783|1.0|\n\nValues for ozone content UO3, water vapor content UH2O, aerosol optical thickness \u03c4A and atmospheric pressure Pa, as well as the adjacency radius used for atmospheric correction in the Lanzhou dataset.# Figure 7: Atmospheric Correction \u2013 Visualisations\n\nComparison between input images without and with atmospheric correction for the Lzh-A multi-date dataset. The similarity of images tones between different epochs is improved after atmospheric correction.\n\n(a) Without atmospheric correction\n\n(b) With atmospheric correction# Figure 8 and Table 3\n\nCompared results based on images with and without atmospheric correction on novel view synthesis and altitude estimation. Results without atmospheric correction gained generally worse metrics and displayed artifacts in synthetic images.# Table 3: Ablation of Atmospheric Correction \u2013 Quantitative Evaluation on Lzh-\n\n|Method|AC|MAE \u2193|PSNR \u2191|PSNR \u2191|SSIM \u2191|SSIM \u2191|\n|---|---|---|---|---|\n| | | |Easy|Hard|Easy|Hard|\n|SpS-NeRF|\u2718|3.697|28.282|26.131|0.880|0.858|\n|SpS-NeRF|\u2714|3.558|29.548|24.755|0.965|0.900|\n|BRDF-NeRF|\u2718|3.439|33.315|29.857|0.946|0.923|\n|BRDF-NeRF|\u2714|3.420|32.196|28.420|0.979|0.940|\n\nExperiments with \u2714and without \u2718atmospheric correction (AC). BRDF-NeRF \u2714performs best, with the smallest MAE and biggest SSIM. Although BRDF-NeRF \u2718shows slightly higher PSNR, qualitative visualisation in Figure 8(b) reveal stripe artefacts, which are not present in BRDF-NeRF \u2714in Figure 10(e).# 4.4. Novel view synthesis\n\nQuantitative metrics are presented in Table 4 while qualitative visualisations are provided in Figures 9 and 10. The visualisations here are limited to the most challenging scenario (very hard for Djibouti and hard for Lanzhou) due to space limitation. Our BRDF-NeRF outperforms Sat-NeRF and SpS-NeRF. Among competitive methods, SpS-NeRF produces less blurry renderings than Sat-NeRF (compare Figure 9 (a) and (c) as well as Figure 10 (b) and (d)). Nevertheless, both methods produce minimal hallucination effects (Figure 9 (a), Figure 10 (b,d)). The two competitive methods remain less sharp and far from the colour tone of the NeRF which includes our realistic RPV-based BRDF (compare Figure 9 (c) and (e)). PSNR and SSIM metrics are best for BRDF-NeRF, followed by SpS-NeRF in second place. The margins between BRDF-NeRF and competitive approaches increase from easy to very hard mode, indicating greater robustness of BRDF-NeRF. From single to multiple epoch datasets (i.e., from Djibouti to Lanzhou), the image quality rendered by Sat-NeRF and SpS-NeRF decreased significantly, while BRDF-NeRF recovered photorealistic images in both cases.# 4.5. Altitude estimation\n\nQuantitative metrics are presented in Table 4, whereas qualitative visualisations are provided in Figures 11 and 12. Sat-NeRF, which was not designed for scenarios with few images, estimates surface altitudes that are tens of meters away from the ground truth surface. SpS-NeRF performs better, thanks to the dense depth supervision, as opposed to supervision with sparse points in Sat-NeRF. Visual assessment reveals that altitudes predicted# Table 4: Quantitative Evaluation\n\n|Metric|Method|Dji-A|Dji-B|Lzh-A|Lzh-B|\n|---|---|---|---|---|---|\n|PSNR|Sat-NeRF|32.747|31.818|29.979|25.470|\n| |SpS-NeRF|38.832|38.174|29.548|30.904|\n| |BRDF-NeRF|41.844|40.823|32.196|32.165|\n|PSNR|Sat-NeRF|25.542|23.699|25.963|20.811|\n| |SpS-NeRF|28.348|27.468|24.755|24.148|\n| |BRDF-NeRF|36.232|35.448|28.420|27.814|\n|PSNR|Sat-NeRF|23.581|21.288|/|/|\n| |SpS-NeRF|23.144|22.31|/|/|\n| |BRDF-NeRF|33.35|32.376|/|/|\n|SSIM|Sat-NeRF|0.927|0.950|0.962|0.925|\n| |SpS-NeRF|0.975|0.979|0.965|0.970|\n| |BRDF-NeRF|0.985|0.988|0.979|0.975|\n|SSIM|Sat-NeRF|0.766|0.825|0.909|0.760|\n| |SpS-NeRF|0.840|0.887|0.900|0.928|\n| |BRDF-NeRF|0.957|0.965|0.94|0.953|\n|SSIM|Sat-NeRF|0.676|0.768|/|/|\n| |SpS-NeRF|0.614|0.72|/|/|\n| |BRDF-NeRF|0.918|0.942|/|/|\n|MAE|Sat-NeRF|12.85|18.059|61.299|27.489|\n| |SpS-NeRF|1.438|1.761|3.558|3.235|\n| |BRDF-NeRF|1.378|1.614|3.42|2.941|\n| |SGMZ1|1.061|1.052|1.409|1.220|\n\nThe best and second best performing metrics are in blue and magenta. For each dataset, we train a BRDF-NeRF to render three images in easy, hard (and very hard when existing) modes at the same time. BRDF-NeRF achieves better PSNR, SSIM and MAE than Sat-NeRF and SpS-NeRF. However, BRDF-NeRF has higher MAEs than SGMZ1, which we attribute to NeRF\u2019s design that handles pixels individually without taking context into account.# Figure 8: Ablation of Atmospheric Correction \u2013 View Synthesis and Altitude Estimation Visualisation\n\nResults on the Lzh-A site using images without atmospheric correction (\u2718AC). The view synthesis is displayed in the first line, and the altitude estimation in the second. BRDF-NeRF synthesises images with fewer artifacts than SpS-NeRF and estimates surface closer to GT. Performance is further improved on images with AC (Figure 10 (e) and Figure 12 (e)).\n\nby Sat-NeRF are either flat (Figure 11 (a-b)), or contain a made up pattern (Figure 12 (a)). SpS-NeRF altitudes are more faithful to ground truth but remain noisy. Our BRDF-NeRF outperforms both versions of NeRF, producing less noisy surfaces while retaining detail.\n\nCompared with surfaces obtained with the classical stereo matching (Pierrot-Deseilligny and Paparoditis, 2006), BRDF-NeRF appears smoother and less detailed (compare Figure 12 (f) and (h)) in areas with good texture. However, on poorly textured areas where stereo matching is challenging, our BRDF-NeRF predicts coherent altitudes (compare Figure 11 (e) and (g)). Note also that our ground truth surface was generated with stereo matching algorithms, thus the comparison is possibly slightly biasing the MAE metric in favour of the stereo matching surface.\n\n20# Figure 9: Novel View Synthesis \u2013 Djibouti Dataset\n\n|(a) Sat-NeRF Dji-A|(b) Sat-NeRF Dji-B|\n|---|---|\n|(c) SpS-NeRF Dji-A|(d) SpS-NeRF Dji-B|\n|(e) BRDF-NeRF Dji-A|(f) BRDF-NeRF Dji-B|\n|(g) GT Dji-A|(h) GT Dji-B|\n\nRenderings of sites A and B of the Djibouti dataset in the very hard scenario (Figure 6). Sat-NeRF and SpS-NeRF renderings are less sharp and detailed than BRDF-NeRF. Note that the colour tone of BRDF-NeRF is closest to that of ground truth (GT).\n\n21# Figure 10: Novel View Synthesis \u2013 Lanzhou Dataset\n\nRenderings of sites A and B of the multi-date Lanzhou dataset in the hard scenario. Hallucination artefacts are revealed in both Sat-NeRF and SpS-NeRF but are more pronounced in the latter. Despite this, SpS-NeRF renders sharper surface details than Sat-NeRF. BRDF-NeRF generates images with fine details and much less artifacts.\n\n|(a) Sat-NeRF Lzh-A|(b) Sat-NeRF Lzh-B|\n|---|---|\n|(c) SpS-NeRF Lzh-A|(d) SpS-NeRF Lzh-B|\n|(e) BRDF-NeRF Lzh-A|(f) BRDF-NeRF Lzh-B|\n|(g) GT Lzh-A|(h) GT Lzh-B|\n\n22# Figure 11: Altitude Estimation \u2013 Djibouti Dataset\n\n|(a) Sat-NeRF Dji-A|(b) Sat-NeRF Dji-B|\n|---|---|\n|(c) SpS-NeRF Dji-A|(d) SpS-NeRF Dji-B|\n|(e) BRDF-NeRF Dji-A|(f) BRDF-NeRF Dji-B|\n|(g) SGMZ1 Dji-A|(h) SGMZ1 Dji-B|\n|(i) GT Dji-A|(j) GT Dji-B|\n\nSat-NeRF fails to recover the altitudes. SpS-NeRF recovers noisy altitudes. The noise is suppressed to some extent in BRDF-NeRF. SGMZ1 generates generally more detailed surfaces, but underperforms in weakly textured areas (see green rectangle).# Figure 12: Altitude Estimation \u2013 Lanzhou Dataset\n\n|(a) Sat-NeRF Lzh-A|(b) Sat-NeRF Lzh-B| |\n|---|---|---|\n|(c) SpS-NeRF Lzh-A|(d) SpS-NeRF Lzh-B| |\n|(e) BRDF-NeRF Lzh-A|(f) BRDF-NeRF Lzh-B| |\n|(g) SGMZ1 Lzh-A|(h) SGMZ1 Lzh-B| |\n|(i) GT Lzh-A|24|(j) GT Lzh-B|\n\nSat-NeRF fails to recover the altitudes. Surfaces obtained by SpS-NeRF and BRDF-NeRF are similar, with SpS-NeRF manifesting slightly noisier altitude predictions and a hallucination artefact. SGMZ1 recovers surfaces with more detailed topography but more noise than SpS-NeRF and BRDF-NeRF.# Training strategy\n\nOur BRDF-NeRF model is trained progressively to ensure proper initialisation of the geometry (i.e., density weights in NeRF) before learning the RPV parameters. We perform an ablation with different pre-training strategies to determine the optimal point at which the transition from Lambertian to RPV model should occur. In the Preno approach, the entire network is trained without pre-training. In the Presho, Premed and Prelon approaches, we start from the Lambertian assumption and switch to the RPV model at different training steps, as shown in Figure 13. The initial learning rate is set to 5e4 and decreases to 3.65e5, 2.15e5 and 1.27e5, respectively.\n\nQualitative results are presented in Figures 14 and 15, while quantitative metrics are provided in Table 5. The absence of pre-training leads to blurry synthetic images and noisy altitude estimations. Performance differences between Presho, Premed, and Prelon are minor, with Premed emerging as the most optimal choice.# Depth Loss Weighting\n\nWe perform an ablation experiment to evaluate the contribution of the depth loss term. Removing the term entirely results in very poor altitude predictions, confirming that a simple NeRF architecture is unable to learn from just three views. By increasing the weight in the range [1, 3 50], altitude metrics improve consistently (see Figure 15). The PSNR and SSIM metrics corresponding to the synthetic image quality reach a maximum at \u03bb = 3 10, suggesting that assigning greater importance to depths compromises the rendering quality. We set the \u03bb = 3 10 in all our experiments.# Surface or volume rendering\n\nThe rendering equation in Equation (3) can be applied as surface rendering (Rensur) or volume rendering (Renvol). In surface rendering, the RPV parameters n, \u03c10, k, \u0398 and \u03c1c are estimated at the surface by accumulating N points along the ray and applying the rendering once for each ray. In volume rendering, rendering is applied to each sample individually, associating it with a color c, and accumulation is done on the sample colours instead of RPV parameters. Rensur is more rigorous than Renvol, as the latter assumes every point along the ray follows the RPV reflectance, while Rensur assumes the same only for points on the surface which is concordant with the RPV model definition. We demonstrate in Table 6 and Figure 16 that Rensur outperforms Renvol and adopt this rendering method in our experiments.# Figure 13: Training Strategies\n\nFours training strategies of BRDF-NeRF from no pre-training to long pre-training.\n\n|Pre|\u03bb|MAE \u2193| |PSNR \u2191| | |SSIM \u2191| |\n|---|---|---|---|---|---|---|---|---|\n| | | |Easy|Hard|VHard|Easy|Hard|VHard|\n|no| |1.632|38.057|33.446|31.186|0.966|0.93|0.884|\n|sho|10|1.449|41.166|35.36|32.999|0.983|0.951|0.913|\n|med|3|1.378|41.844|36.232|33.35|0.985|0.957|0.918|\n|lon| |1.39|41.415|35.645|31.663|0.984|0.947|0.88|\n| |\u03bb = 0|9.432|39.408|34.755|31.870|0.973|0.941|0.900|\n|med|\u03bb = 31|1.877|40.894|35.822|33.318|0.982|0.951|0.914|\n| |\u03bb = 350|1.353|41.109|34.915|32.107|0.983|0.949|0.908|\n\nTable 5: Training Strategies \u2013 Quantitative Evaluation. Pre refers to the various training settings shown in Figure 13, while \u03bb is the parameter that balances the contribution of colour and depth losses in Equation (5). The best and second best performing metrics are in blue and magenta. Premed achieved the best PSNR and SSIM and the second best MAE. \u03bb = 350 ranks the best for MAE, but has worse PSNR and SSIM than Premed. Tests correspond to Dji-A dataset.\n\n|MAE\u2193| |PSNR\u2191| | |SSIM\u2191| | |\n|---|---|---|---|---|---|---|---|\n| |Easy|Hard|VHard|Easy|Hard|VHard| |\n|Renvol|1.399|41.743|35.867|31.770|0.985|0.955|0.903|\n|Rensur|1.378|41.844|36.232|33.350|0.985|0.957|0.918|\n\nTable 6: Rendering \u2013 Quantitative Evaluation of Renvol and Rensur. Renvol produces new views and surfaces close to Rensur with slightly poorer metrics overall. Tests correspond to Dji-A dataset.# Figure 14: Training Strategies \u2013 View Synthesis Visualisation\n\nDifferent pre-training strategies are tested on the very hard scenario of Dji-A. Preno generates a blurry image; Presho, Premed and Prelon synthesised images that are close to GT, while Premed shows advantages in restoring better details, particularly in the zoom-in view labeled in the orange rectangle. \u03bb = 0 recovers blurry image, \u03bb = 31 and \u03bb = 350 synthesised novel views with unified image tone, but zoom-in view shows blurrier detail than \u03bb = 3.\n\n|(a) Preno, \u03bb = 310|(b) Presho, \u03bb = 310|\n|---|---|\n|(c) Premed, \u03bb = 310|(d) PreLon, \u03bb = 310|\n|(e) Premed, \u03bb = 0|(f) Premed, \u03bb = 31|\n|(g) Premed, \u03bb = 350|(h) GT|# Figure 15: Training Strategies \u2013 Altitude Estimation Visualisation on Dji-A sites\n\nUsing different training strategy for BRDF-NeRF. Preno\u2019s surface is very noisy; Presho, Premed and Prelon generate similar surfaces, among which Presho is the noisiest. \u03bb = 0 fails to recover topographic reliefs due to a lack of depth supervision. Performance is improved in \u03bb = 31, with noisy details. \u03bb = 31 estimates surface smoother than \u03bb = 350, thanks to the good quality of input depth.\n\n|(a) Preno, \u03bb = 310|(b) Presho, \u03bb = 310|\n|---|---|\n|(c) Premed, \u03bb = 310|(d) Prelon, \u03bb = 310|\n|(e) Premed, \u03bb = 0|(f) Premed, \u03bb = 31|\n|(g) Premed, \u03bb = 350|(h) GT|# Figure 16: Rendering \u2013 Visualisations\n\n|(a) Renvol novel view|(b) Renvol surface|\n|---|---|\n|(c) Rensur novel view|(d) Rensur surface|\n|(e) GT novel view|(f) GT surface|\n\nRenvol rendered novel view with similar quality to Rensur, while zoom-in shows blurrier details. The surface qualities of Renvol and Rensur are similar with no obvious differences.\n\n29# 5. Conclusion\n\nWe presented BRDF-NeRF, an extension of NeRF adapted to sparse satellite imagery, capable of estimating realistic BRDFs for natural surfaces. By incorporating the semi-empirical Rahman-Pinty-Verstraete (RPV) model, BRDF-NeRF enhances the rendering of anisotropic surface reflectance, leading to improved quality of both synthetic images and recovered surface altitudes. Although our experiments show promising results, certain limitations remain. At present, our method does not explicitly model shading effects, and although our surface reconstructions outperform other NeRF-based approaches, they still lack the regularity achieved by SGM. Future work will address these challenges.",
        "context_id": 41,
        "question": "What is the submission date of the preprint to the ISPRS Journal of Photogrammetry and Remote Sensing?",
        "answer": [
            "September 19, 2024"
        ],
        "context_length": 42055
    },
    {
        "context": "# 2 Data-Driven Material Characterization Framework\n\nIn the depicted framework below (see Figure 1), we illustrate the different stages of our data-driven characterization process. When fully developed, the proposed AI GRU model [15] will facilitate the materials characterization by estimating the cristal properties using the experimental dataset.\n\nWe notice that there are three different stages or modules interrelated in order to help experts in their final decisions:\n\n1. Experiment Stage: At this stage - see Figure 1, the engineers and researchers do real physical experiments, define relevant properties that may fit the physical or chemical situation and collect concrete samples to construct a data dictionary that summarises the experimental specifications. These collected data are then used in modeling, simulation, or even to design data-driven models.\n2. Information Stage: This stage - Figure 1 - mainly concerns the data-driven model development. The data dictionary is pre-processed and the data are prepared using normalization, regularization and features reduction to respond to a task or goal oriented request. Finally an AI model is designed and tested to get better of its performances.\n3. Decision Stage: In this stage we integrate the constructed AI driven model in the decision process. Here, the data-driven model will be tested to see its compliance by regards to the client and user requirements.# Smart Data-Driven GRU Predictor for SnO2 Thin films Characteristics# X-ray Diffraction for SnO2 Characterization\n\nIn this section, we will describe in details the process of structure materials characterization using the non destructive X-ray diffraction technique - see Figure 2. This latter shows a derivation of Bragg\u2019s law, where a sample of crystal material is placed at the center of the instrument, and an incident beam of X-ray is directly oriented towards this sample - see details in Figure 2. Then, the outgoing beam, who makes a 2\u03c9 with awards one, is obtained as a result of interaction with the atoms of the crystal. Every wavelength \u03b5 of the X-ray beam should be proportional to the spacing of the set of crystal planes (hkl), d and the reflection will occur if Bragg\u2019s law below is satisfied - See the equation 1, where, n is a positive integer, \u03b5 a wavelength and d is gain size.\n\n2dsin(\u03c9) = n\u03b5, Where \u03b5 = 1.540A\u2191 (1)\n\nNext, to compute lattice parameters a and c, we use the equation 2 obtained from lattice spacing formula for a tetragonal crystal system [16], where h,k and l are Miller indices defining an (hkl) plane in the lattice:\n\n1 = h2 + k2 + l2 (2)\n\nd2 = a2 + c2\n\nBy applying the Bragg law\u2019s and considering the constants A = \u03b52 /(4a2) and C = \u03b52 /(4c2), we obtain:\n\nsin2(\u03c9) = A(h2 + k2) + Cl2. (3)# Fig. 2: XRD Diffraction Scheme.\n\npatterns, we determine the C value in : sin2(\u03c9) \u2192 A(h2 + kk 2) = Cl2. Finally, the Grain size D, is calculated using Scherrer relation [17]:\n\nD = K \u2191 \u03b5\n\n\u03d1 \u2191 cos(\u03c9)\n\nWith K the Scherrer constant, on which depends the crystallites, and is typically in [0.9, 1.0]. \u03d1 is the full width at the half-maximum (FWHM).\n\nWe notice that the obtained patterns has interference maxima (peaks) corresponding to the diffracted X-ray by the regularly planes of the crystal lattice - see Figure 3. It\u2019s worth nothing to know that X-ray diffraction techniques involves two main methods resumed in (1) powder or (2) single crystal diffraction.\n\nIn our research work, we especially have a focus on the diffraction of X-ray for single crystal of thin oxide SnO2. In addition, samples of SnO2 thin films were elaborated with sol-gel dip coating technique. After getting a reliable thin films, we have optimized some experimental parameters such as solution, speed of dip, number of dip and others and finally we get these parameters that correspond to the final output sample with its proper characteristics.\n\nIn the Figure 3, we represent some results patterns from X-ray diffraction technique that were treated with HighScore software [13]. The first pattern represent theoretical sheet obtained from data based on the perfect SnO2 thin films, and the others are the different cases obtained with changing experimental parameters, as mentioned clearly in the Figure 3. These collected data are considered and examined during the development of the GRU model.# Smart Data-Driven GRU Predictor for SnO2 Thin films Characteristics# Fig. 3: X-Ray diffraction patterns of SnO2 thin films obtained by Sol-Gel (Dip Coating) technique.\n\na) ideal pattern of Sno2, b) pur SnO2, c) SnO2 with HCI solution, d) SnO2 with DEA solution, e) SnO2 with acetic acide.# 4 GRU-Based Predictor\n\nAs a streamlined version of complex AI models [18][19][20][21][22], the gated recurrent unit (GRU) [23] achieves good performance and do faster computing. A GRU model contains a reset gate and an update gate. These gates are composed by sigmoid activations functions \u03d6 to force their output values to lie in the interval [0, 1]. The reset gate controls at what extent the previous states should we remembered. In the same way, the update gate tracks how much of the new states are just a copy of the old ones - See Figure 4.\n\nHere are the analytical equations that map the GRU cell\u2019s computation states at each time step t. Let consider the notations, where W\u2192, W\u2019 \u2192, and b \u2192 weight matrices and biases for each gate referred to as index sigmoid function. The current input is X t, and R, Z are the reset and update gates respectively, H t\u21931 denotes the previous iteration\u2019s hidden state, whereas H t corresponds to the current hidden state.\n\nZ t = \u03d6(W Z X t + W R H t\u21931 + b Z)\n\nR t = \u03d6(W R X t + W R H t\u21931 + b R)\n\nG t = tanh(W G X t + W G(R t H t\u21931) + b G)\n\nH t = Z t H t\u21931 + (1 - Z t) G t# 5 GRU Model: Design and Experiments\n\nIn this section, we design a GRU-Based Predictor for structural properties of SnO2, and its possible adoption for the characterisation of various other materials. Clearly, we depicted in the following the different steps that are used to construct, tune and test the GRU model, starting by the essential data collection and preparation phase for each AI data-driven modeling.# Step 2: Collect Data samples D that describe the Context of the Problem# Step 4: Feature Selection on D for a given goal g# Step 5: Design and Tune Network AI model using &lt; P, D &gt; and get best performance that better fits the goal g\n\nFig. 5: GRU-Based Model Implementation# 1. Problem Identification:\n\nAs mentioned in section 1, the objective of this work consists to analyse the obtained X-Ray diffraction spectra using other programs such as High-Score [13], Origin [25] and to compare them to a specters in the program data base. The spectra properties is resumed in lattice characteristics a, b and grain size D. We notice# Faiza Bouamra, Mohamed Sayah, and al.\n\nthat the comparison with previous existing results in the literature, enables us to verify the nature of the structure and proves that it concerns tetragonal phase of Tin Oxide SnO2 thin films.# 2. Data Sample Collection:\n\nTo determine a structure for 1d, 2d and 3d materials we must determine structural parameters as well as: lattice parameters a, b and c to classify them among the seven systems and fourteen Bravais lattices as mentioned in the crystallography.# 3. Data Pre-processing:\n\nData pre-processing techniques are applied, including duplicate detection and removal, missing data imputation, outlier detection and treatment. Data transformation techniques such as normalization and standardization are also employed to ensure data consistency.# 4. Feature Selection:\n\nRelevant features are selected, with the assistance of domain experts possessing deep knowledge and expertise in the field. These experts provide valuable insights into the key characteristics that define part quality and prioritize the most important features.# 5. Design and Tuning of Network AI model:\n\nA GRU model is generated after the learning process and its performance is proven using testing.# \u2013 Data Description and Collection\n\nExperts in material physics specify precisely the parameters that should be considered for the characterization of structural materials. Also, all the pipeline tools that are used during the characterization process are identified and configured. After, a data dictionary is constructed as illustrated in the Table 1. Thus, physical symbols and their respective designations are specified and validated by material physics experts. We note that this step is crucial and should be done correctly to get adequate data-driven models that fit precisely the situation of characterization. After, the task of data collection has to be considered with care, knowing its sensitivity and determination in the learning process. It is important to notice here that inaccurate data with poor quality should lead to catastrophic decisions. Many AI models are ineffective because of the lack of data and/or the quality of data.# \u2013 Data Pre-processing\n\nOnce the data has been specified and then collected, we can analyze statistically their nature, either by visualization or by computing statistical characteristics, such as the mean \u03bc, the standard variation \u03d6, the Median and others. This will enable us to choose the best data preparation technique to use - See Table 2. We have noticed that the parameters Tsec(%s) and Time(%s) are constants because their standard deviations are null (See Table 1, \u03d6= 0.0\u2020) and there are multiple scalings in the data set (See Table 1, \u03bc(Vdip) = 06.09 \u2192  << \u03bc(Trec) = 454.55 \u2192 and \u03d6(\u03c9) = 03.68 \u2192 << \u03d6(Trec) = 52.22 \u2192) and this is not sane to use the data set without normalization and reduction. Thus, we have applied a normalization on the data to have a single scaling for our data and to avoid the multi-scaling bias.# Smart Data-Driven GRU Predictor for SnO2 Thin films Characteristics# Table 1: Parameters specification including input or system variables and decision variables, with their respective numerical types.\n\n|Symbol|Definition|Type|\n|---|---|---|\n|\u03c9|Angle between thin films planes and incident beam|float|\n|Ndip|Number of dip|Integer|\n|Vdip|Velocity of dip|Integer|\n|Tsec|Drying temperature|Integer|\n|Trec|Annealing time|Integer|\n|Time|Drying time|Date|\n|P sol|Pure SnO2|Boolean|\n|D sol|Diethanolamine \u2019DEA\u2019 Solution|Boolean|\n|H sol|Hydrochloric acid Solution|Boolean|\n|A sol|Acetic acid solution|Boolean|\n|a|Lattice parameter for tetragonal structure in tow direction x and y|float|\n|b|Lattice parameter for tetragonal structure in z direction|float|\n|D|Grain size|float|# Table 2: Description of Data Set distributions of the different features and statistical metrics \u03bc, \u03d6 and Median.\n\n|t|\u03c9|Ndip|Vdip|Tsec|Trec|Time|Psol|D sol|Hsol|Asol|a|b|D|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|t 0|34.2599|35|7|100|500|120|1|0|0|0|2.62|2.61|99.58|\n|t 1|34.36|7|7|100|500|120|1|0|0|0|2.61|2.60|212.94|\n|t 2|27.0841|35|7|100|500|120|0|1|0|0|4.65|2.61|112.11|\n|t 3|27.08|7|7|100|500|120|0|1|0|0|4.65|2.60|182.07|\n|t 4|26.96|35|7|100|500|120|0|0|1|0|2.60|2.60|602.63|\n|t 5|34.4126|35|7|100|500|120|0|0|0|1|2.60|2.60|602.63|\n|t 6|34.0546|6|7|100|400|120|1|0|0|0|2.63|2.63|1351.28|\n|t 7|26.827|7|7|100|400|120|1|0|0|0|4.69|0|630.41|\n|t 8|26.82|8|7|100|400|120|1|0|0|0|4.69|0|882.48|\n|t 9|27.0714|35|2|100|400|120|1|0|0|0|4.65|2.61|124.65|\n|t k|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\u00b7 \u00b7 \u00b7|\n|t n|27.02|35|2|100|400|120|0|1|0|0|4.66|2.60|171.09|\n|\u03bc|29.63|22.27|06.09\u2192|100|454.55\u2192|120|00.55|00.27|00.09|00.09|04.55|02.07|52.22|\n|\u03b5|03.68\u2192|14.63|02.02|0.0\u2020|52.22\u2192|0.0\u2020|00.52|00.47|00.30|00.30|00.23|00.69|51.46|\n|Median|27.08|35.00|07.00|100|500.00|120|01.00|00.00|00.00|00.00|04.65|02.28|31.50|\n\nSometimes and to overcome the lack of the real experimental data, Gaussian augmentation to the dataset are applied. Each data transaction t is used to generate new mutant.# Feature Selection and Reduction\n\nAt this stage of data preparation, features are splitted in input and output variables and some parameters may be just dropped. This techniques called features reduction should be done under the advice of experts in materials physics. In our context, the considered system variables are: \u03c9, N dip, V dip, T rec, P sol, Dsol, H sol, Asol, and the decision variables a, b, D.# 5.2 GRU Predictor Models: Learning and Testing\n\nWe have generated experimentally the best configuration of the GRU model and the optimal learning parameters are depicted below. In fact, the description of the parameters and their values used along the following experimentation section are introduced right after.\n\n- Tsteps (Time steps refers to the sequence length in the input data.): 24\n- IN:#Features + OUT:#Decisions (Indicates the number of distinct attributes measurements in input.): 8 (input parameters) + 1 (decision variable features #a, #b, #D).\n- Loss (This function measures the dissimilarity between the predicted values and the ground truth values during training.):\n- Metrics (These custom functions are used to evaluate the model\u2019s performance during training and testing.): MSE, R2, S.\n- #Layer (It denotes the number of layers in the neural network model.): 2\n- #Cells (It specifies the number of cells or units per layer of the GRU model.): 64\n- #Epochs (This parameter represents the number of times the model iterates over the entire training dataset.): limit <= 800 + StopEarling (SE)\n- #Batch (It refers to the number of samples or data points processed in each training iteration.):\n- Act func (It is a mathematical function that controls the outputs of each neuron in the GRU neural network.): Relu\n- %Dropout (Dropout is a regularization technique used during training to avoid overfitting and to boost the training.): 20%\n- #FCDense (Known as fully connected layers.): 1\n- Adamlr (They are used to configure the lr learning rate.): lr = 0.0001# Table 3: Tuning hyper-parameters of the learning process GRU64, GRUb64 and GRU64Models.a D\n\n|Model|Metrics|Tsteps|#Layers|#Cells|#Epochs (Early Stopping)|#Batch|Act func|Adamlr|\n|---|---|---|---|---|---|---|---|---|\n|GRU64a|MSE/S|50|3|(8, 64, 1)|450|25|Relu|0.001|\n|GRUb64|MSE/S|50|3|(8, 64, 1)|480|25|Relu|0.001|\n|GRUD 64|MSE/S|50|3|(8, 64, 1)|270|25|Relu|0.001|# Smart Data-Driven GRU Predictor for SnO2 Thin films Characteristics\n\nAfter the tuning of the GRU model training parameters, we will now define an experimental protocol that consists first, in the adoption of the latter fixed learning parameters in Table 4. Second, for each crystal materials characteristics a, b, and D, we have determined respectively the corresponding models as GRUa(8,64,1), GRU(8,64,1), and GRUD(8,64,1) - see Table 4.b. These are trained and then tested using the metrics MSE and S included in the section 4.# Table 4: Performance of the GRU models based on Metrics:\n\n(a) MSE: Mean Square Error, (b) \u03bcS: Mean scores S, and (c) ModeS: Mode Scores\n\n|Model|MSE|\u03bcS|Mode S|#Cells|Act func|\n|---|---|---|---|---|---|\n|GRU 64|7.2901e-14|0.0659|0.5970|(8, 64, 1)|Relu|\n|GRUb 64|7.7273e-05|0.0675|0.8797|(8, 64, 1)|Relu|\n|GRUD 64|1.8649e-09|0.0166|0.1749|(8, 64, 1)|Relu|\n\nAt this time, we have depicted the traceability of the training and the testing of our designed models GRUa(8,64,1), GRUb(8,64,1), and GRUD(8,64,1). Three figures are presented to define the dynamics of the GRU predictor models. In green curve, we have the learning loss function (loss), and the purple curve represents the generalization or the val-loss function. Finally, the red sensitivity function determines whether the learning is stable or not - see Figures 6, 7, 8.# Loss and Training Loss VS. Epochs\n\nTraining loss\n\nLoss\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n100 200 300 400\n\nEpochs# Fig. 6: GRU(8,64,1) Model - Learning loss, Generalization val-loss and Sensitivity scores# Smart Data-Driven GRU Predictor for SnO2 Thin films Characteristics\n\nepochs parameter is greater than 310, the dynamics of the training converges to optimal GRU network weight values.\n\nIn Figure 7, we can clearly see that the learning is very sensitive around the epoch, but converges around epoch = 300 and demonstrates better performances in terms of learning and generalization for the generated model GRUb(8,64,1).\n\nFor the model GRUD(8,64,1) generation - see Figure 8 -, we note that the learning converges earlier and gives steady results in epochs 80 without any sensitivity.# 5.3 GRU Model Predictions\n\nAfter the GRU model construction and the performance evaluations for each GRU model predictor of the different structural parameters #a, #b, and #D, we will now apply the predictor models to predict the crystal properties considering the truth experimental values in the dataset dictionary.\n\nFor each structural feature, we use the corresponding model named GRUX(8,64,1), where X is in the set {a, b, D}. Thus, the truth and the estimated properties a, b, and D are depicted in Table 5 using the above predictor models - see the subsection 5.2. We noticed that in Table 5 (a), the estimated lattice parameter a in the transactions t0, t2, and t6 are exactly the same as the truth experimental values. The remaining estimated values in {t1, t3, t4, t5} have an absolute difference at most equal to 3.00e-7 compared to the truth experimental values. This finding can be observed in the case of the lattices b and D. In fact, the largest absolute difference 2.00e-2 is shown in the transaction t5.\n\nThe range of [4.0e-6, 5.9e-5]. The Figures 9, 11, 13 below show in detail the lattice is in addition, the difference between the estimated and truth values for the D R2 metric which demonstrates the statement that test truth estimated values of a, b, and D lattices and the predicted computed values are almost equal. In addition, the truth/predicted discrepancies in the Figures 10, 12, 14, are very small and even zero 0.# Smart Data-Driven GRU Predictor for SnO2 Thin films Characteristics# Actual vs Predicted Values\n\n|1.0|R2 = 1.00|\n|---|---|\n|0.8| |\n|0.4| |\n|0.2| |\n|0.0| |\n\nActual Values\n\nFig. 11: GRU(8,64,1) Square R2 Metric.b# Truth/Prediction Discrepancies\n\n1.00.80.60.40.2\nTesting Space Size\n\nFig. 12: GRU(8,64,1) Truth-Prediction Discrepancies.b# Smart Data-Driven GRU Predictor for SnO2 Thin films Characteristics# Actual vs Predicted Values\n\nR2 = 1.00\n\n| |0.0|0.1|0.2|0.3|0.4|0.5|0.6|\n|---|---|---|---|---|---|---|---|\n|0.4| | | | | | | |\n|0.3| | | | | | | |\n|0.2| | | | | | | |\n|0.1| | | | | | | |\n\nFig. 13: GRU(8D,64,1) Square R2 Metric.# Truth/Prediction Discrepancies\n\nTesting Space Size\n\nFig. 14: GRU(8D,64,1) Truth-Prediction Discrepancies.# Conclusion\n\nThis research work is very beneficial in the sense that it simplifies the hard and complex task of real experiments in materials physics. Indeed, the crucial role of characterization techniques in material physics, particularly emphasizing the significance of gathering data on various physical properties, is not always easy, especially for some methods such as X-ray diffraction that are widely used but still be challenging due to cost, complexity and safety. In brief and to streamline this process, X-ray diffraction outcomes for tin oxide (SnO2) thin films are produced via the Dip-Coating methods deposited onto glass substrates. Then, Various experimental parameters were adjusted to enhance the thin film properties for specific applications. In addition, The structural characteristics, including lattice parameters a and c and the grain size D, are computed, confirming the tetragonal phase of thin films in most cases. Finally, due to the extensive array of experimental parameters to be optimized across multiple samples, the use of artificial intelligence can be highly desirable to be considered. We have adopted the use of artificial intelligence techniques, particularly a Gated Recurrent Unit (GRU) model to predict but also estimate structural characteristics and many physical properties of materials.\n\nThe relevance and benefits of the proposed data-driven GRU-Based model Predictor for material properties characterization highlight the importance of AI in material physics and leverage discussion about this new pivotal AI methods for understanding and extracting structural properties of crystalline materials. Again, this may show the emphasizing on AI and its role in characterizing material properties across various fields, when limitations in terms of computation capacity for ab-initio calculation and simulation methods. Our GRU models have gained prominence in enhancing X-ray diffraction\u2019s capabilities and strengthening predictions of materials\u2019 structural properties in an easy and safe thin films SnO2 characterization. Furthermore, we proposed a data-driven framework for material characterization, focusing on the proposed AI GRU model among others. In the soon future, we will continue exploit derive crystal properties through experimental data to design specific AI models that can facilitate and revolutionize the field of material science and characterization, offering solutions to the challenges faced by engineers and researchers when using traditional techniques and computational constraints.",
        "context_id": 42,
        "question": "What technique is used to elaborate thin film samples as mentioned in the introduction?",
        "answer": [
            "Sol-Gel Dip-Coating"
        ],
        "context_length": 21114
    },
    {
        "context": "# Task Learning Framework\n\nOur main contributions include:\n\n1. We propose a Transformer-based framework that integrates multi-physical information for BraTS, reducing uncertainty in model representation and thereby improving segmentation accuracy.\n2. We construct a multi-branch network architecture to extract modality-specific features separately, avoiding interference from irrelevant modalities in specific BraTS tasks.\n3. We design an Adaptive Feature Fusion (AFF) module to fuse information from different MRI modalities, forming multi-scale features shared across tasks.\n4. We develop a multi-source and multi-scale feature decoder, which respects the differences between segmentation tasks and fully utilizes both common and individual features.\n5. We conduct comprehensive evaluations using real-world datasets. Our experiments on the BraTS2019 and BraTS2020 datasets demonstrate multiPI-TransBTS's superior performance over existing methods in terms of Dice coefficient, Hausdorff distance, and Sensitivity. To facilitate further research, the source code for the multiPI-TransBTS framework is available at https://github.com/JoetheReindeer/multiPI_TransBTS.# 2 RELATED WORK\n\nMRI is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in clinical practice [15]. Manual segmentation of brain tumor extent from 3D MRI volumes is a very time-consuming task and the performance is highly reliant on the operator\u2019s experience [16]. In this context, a reliable fully automatic segmentation method for brain tumor segmentation is necessary for an efficient measurement of the tumor extent [16]. For this reason, many research works have been undertaken to apply deep learning techniques for the BraTS task. These techniques can be classified roughly into four categories: CNN-based methods, RNN-based methods, GAN-based methods, and Transformer-based methods.# 2.1 CNN-based methods\n\nConvolutional Neural Networks (CNNs) inherently incorporate the spatial hierarchy of features within an image. Utilizing local receptive fields and weight sharing, CNNs embed the prior knowledge that similar patterns, such as edges or textures, are likely to recur across various parts of a brain tumor. This underpinning principle has facilitated the expansion of CNN applications within the BraTS domain, leading to the development of numerous models tailored to address the intricacies of these tasks.# 2.1.1 2D CNN\n\nTraditional CNNs generally consist of several convolutional layers, followed by fully connected layers at the end to output a single label. To adapt this architecture for direct image-image mapping, Fully Convolutional Neural Networks (FCNNs) replace fully connected layers with additional convolutional layers, enhancing their utility in the BraTS challenges. For example,# Pereira et al. [15]\n\nproposed an automatic segmentation method using CNNs that leverage small 3 \u00d7 3 kernels, enabling deeper network architectures while mitigating overfitting due to the reduced number of weights. Kamnitsas et al. [17] introduced a dual pathway architecture that processes images at multiple scales to incorporate both local and contextual information more effectively. Zhao et al. [18] combined FCNNs with Conditional Random Fields (CRFs) to achieve segmentation results with appearance and spatial consistency.\n\nLi et al. [19] developed an FCNN based on the U-Net architecture augmented with inception modules, optimizing the model for asymmetrical tumor regions. To further enhance model accuracy, Chen et al. [20] incorporated a Left-Right Similarity Mask (LRSM) into their FCNNs, addressing the inherent asymmetry in tumor imaging. Zhou et al. [14] utilized multi-task networks to distribute common information effectively and address class imbalance within the data. More recent developments include Cinar et al.'s DenseNet-UNet hybrid model [21] and Ullah et al.'s Multiscale Residual Attention-UNet (MRA-UNet) [22], both designed to refine BraTS segmentation accuracy. In addition, Allah et al. [23] introduced the U-Net model for enhanced localization of tumors, and Rehman et al. [24] proposed the RAAGR2-Net with a Residual Spatial Pyramid Pooling (RASPP) module to preserve location information across network layers.# 2.1.2 3D CNN\n\n3D CNNs offer a solution to the slice-level inconsistencies resulting from 2D CNNs by harnessing the three-dimensional continuity of MRI data. Chen et al. [25] proposed the Multi-Level DeepMedic model that utilizes multi-level information to achieve more precise segmentation. Isensee et al. [26] developed nnU-Net, an adaptable and self-configuring system designed to automatically adjust to various medical imaging tasks without manual intervention. This model effectively addresses the diverse challenges presented by different medical imaging datasets.\n\nAdditionally, Li et al. [27] suggested the use of cascaded 3D U-Nets for enhanced performance in BraTS tasks, while Chang et al. [28] designed a residual dual-path attention-fusion 3D CNN to amalgamate global and local channel information. Raza et al. [29] introduced the dResU-Net, combining features of residual networks and U-Net for robust segmentation capabilities.\n\nDespite their potential, 3D CNNs are often constrained by their substantial computational demands and the network size required, which can become prohibitive, particularly with anisotropic datasets [26]. Therefore, 2D CNNs continue to be a popular choice due to their reduced computational requirements and robust performance across varying imaging conditions [18].\n\nIn summary, CNNs, through their architectural design, introduce general priors concerning spatial hierarchies, translation invariance, and local feature consistency. U-Net, in particular, brings additional specific priors about the importance of multi-scale features and the integration of detailed and contextual information within its unique architecture, proving essential for complex segmentation tasks like those found in BraTS.# 2.2 GAN-based methods\n\nGenerative Adversarial Networks (GANs) have significantly enhanced the BraTS performance by generating synthetic images that closely resemble authentic ones, thereby expanding the training.# 2.3 RNN-based methods\n\nRecurrent Neural Networks (RNNs) are initially designed to handle sequential data, which have the same properties as MRI slices. RNN can introduce the prior knowledge that the input data has a temporal or sequential relationship. This is particularly relevant for the BraTS task, where consecutive slices of the brain may show the gradual growth or movement of a tumor.\n\nTo harness this sequential data effectively, Deng et al. [37] integrated a Conditional Random Field with a Recurrent Neural Network (CRF-RNN). This combination leverages the sequential dependencies across slices to improve the consistency and accuracy of segmentation.\n\nLong Short-Term Memory (LSTM) networks, an advanced form of RNNs, are specifically designed to handle long-term dependencies within sequential data. This ability is crucial in brain tumor segmentation, where characteristics of distant slices may influence the interpretation and segmentation of subsequent slices.\n\nBuilding on these capabilities, Hu et al. [38] proposed the UNET-LSTM algorithm, which aims to address the challenge of sample imbalance in the dataset. By integrating LSTM with the U-Net architecture, this approach enhances the model's ability to predict more balanced and accurate segmentations across the dataset.# 2.4 Transformer-based methods\n\nUnlike CNNs, which primarily focus on local correlations through convolutions, Transformers excel in capturing global context due to their self-attention mechanism. This mechanism can model relationships between distant regions in an input image, providing a comprehensive understanding of spatial contexts. Unlike RNNs, which process data sequentially, Transformers can handle different parts of the image in parallel, effectively managing long-range dependencies.\n\nTo fully exploit the merits of both Transformers and CNNs, numerous Transformer-CNN hybrid models have been developed. These models combine the global contextual capabilities of Transformers with the robust local feature extraction of CNNs, particularly leveraging the U-shaped architecture.\n\nFor instance, TransUNet [39] integrates the self-attention mechanism of Transformers with the encoding-decoding structure of UNet. This was one of the first models to effectively blend local and global information for segmentation accuracy increase. UNETR [40] employs a stack of transformers as the encoder, connected to a decoder via skip connections. This design allows for an effective synthesis of multi-level feature information. CKD-TransBTS [5] features a dual-branch hybrid encoder and a feature calibration decoder within a U-Net-like structure, integrating features at various scales.\n\nIn addition, TranSiam [41] consists of two identical sub-networks where convolutions extract detailed information at lower levels, and Transformers handle global information processing at higher levels. SDV-TUNet [42] utilizes multi-head self-attention and sparse dynamic adaptive fusion to meticulously extract global spatial semantic features, crucial for precise BraTS.\n\nTraditional Transformers operate on fixed-size patches, which can somewhat restrict their ability to process multi-scale information. To overcome this limitation, Swin Transformer utilizes a hierarchical feature representation strategy to capture both local and global information adeptly [43]. Similarly, IMS2Trans [44] employs Swin Transformer technology to enable efficient information sharing and fusion among different modalities.\n\nIn summary, the application of Transformers in brain tumor segmentation leverages their ability to capture global context and handle multi-scale information while maintaining minimal inductive biases. The self-attention mechanism allows these models to dynamically focus on crucial regions of the image, making them exceptionally suitable for the complex BraTS task. However, current methods predominantly focus on exploiting the generic capabilities of models and overlook the integration of domain-specific knowledge related to brain tumor imaging. This oversight limits further enhancements in the BraTS performance, suggesting a need for more specialized approaches that incorporate specific clinical and imaging insights into the model architecture.# 3.1 Physical principles of MRI\n\nMRI exploits the principles of nuclear magnetic resonance to generate detailed images of the body. In MRI, atomic nuclei with odd numbers of protons or neutrons, such as hydrogen, possess nuclear spin with a quantum mechanical property. By applying a radiofrequency pulse at a specific frequency, which corresponds to the energy difference between two states, these nuclei can be excited from their lower energy state to a higher one. This phenomenon is known as nuclear magnetic resonance [45].\n\nUpon removal of the RF pulse, the nuclei return to their lower energy state through a process called relaxation, which occurs in two primary forms: spin-lattice relaxation and spin-spin relaxation. Spin-lattice relaxation releases absorbed energy to the surrounding molecular lattice, returning the nuclei to thermal equilibrium. The rate of this relaxation is measured by the T1 relaxation time. Spin-spin relaxation involves the dephasing of spins in the transverse plane due to interactions among the spins themselves, without energy transfer to the lattice. The rate of spin-spin relaxation is characterized by the T2 relaxation time [45].\n\nT1-weighted imaging and T2-weighted imaging are techniques used to highlight different tissue characteristics based on their T1 and T2 relaxation times. T1-weighted imaging employs short repetition time (TR) and short echo time (TE) to emphasize differences in T1 properties between different types of tissue. T2-weighted imaging utilizes long TR and long TE to accentuate variations in T2 relaxation time [45].\n\nTo enhance the diagnostic capabilities of MRI, other two techniques tend to be used at the same time. One is gadolinium contrast-enhanced T1-weighted imaging (T1Gd), in which Gadolinium enhances the contrast by shortening the T1 relaxation time of nearby water protons, making the affected areas appear brighter in the images [46]. The other one is T2 Fluid Attenuated Inversion Recovery (FLAIR). This technique involves an inversion recovery pulse that nulls the signal from fluids, particularly cerebrospinal fluid, to suppress the background fluid signal and enhance the detection of lesions [47].\n\nEach of these MRI techniques provides a unique perspective on tissue properties and pathological changes. Moreover, tumors\u2019 borders are often fuzzy and hard to distinguish from healthy tissue [48]. For these reasons, medical analysis and diagnosis are usually carried out in combination with multiple MRI modalities [9, 10].# 3.2 Structure of brain tumors\n\nBrain tumors refer to a diverse collection of intracranial neoplasms, comprising over 20 distinct types, each with its unique biology [1]. The MRI images of brain tumors, illustrated in Fig. 1, are composed of four primary sub-regions: necrotic core (NCR, green), edema (ED, yellow), non-enhancing tumor core (NET, red), and GD-enhancing tumor core (ET, blue) [4, 49]. Each of these sub-regions plays a crucial role in the clinical diagnosis and treatment of brain tumors [50]. It is important to note that these MRI sub-regions do not strictly represent biological entities, but arerather image-based constructs. Often, there is limited evidence in the imaging data for the presence of the non-enhancing solid core [13]. For this reason, the non-enhancing solid core has been excluded from the 2017-present BraTS dataset.# 4 METHODOLOGY\n\nBuilding on the observation that different modalities exhibit variable performance across tumor region segmentation tasks, we propose multiPI-TransBTS, a Transformer-based framework that integrates multi-physical information for the BraTS task. The framework leverages common image data attributes such as spatial and semantic information, alongside specific information relevant to# 4.1 Overall framework\n\nThe overall model design is based on several key considerations: (1) CNNs are effective at capturing spatial information and local patterns, so multiPI-TransBTS predominantly utilizes CNN architectures; (2) As image data are high-dimensional and often contain redundant information, such as similar pixel values in smooth regions, multiPI-TransBTS adopts an encoding-decoding structure; (3) Given the heterogeneity in tumor size and shape, multiPI-TransBTS incorporates skip connections from the U-Net architecture to handle the significant variation in tumor characteristics; (4) Since different brain tumor regions exhibit distinct characteristics across different MRI scan modalities, multiPI-TransBTS employs a multi-branch network architecture to extract modality-specific features separately; and (5) recognizing that different modalities exhibit varying performance across tumor region segmentation tasks, multiPI-TransBTS implements a Task-Specific Feature Introduction (TSFI) strategy.\n\nThe overall framework of multiPI-TransBTS, shown in Fig. 3, is divided into three main components: the encoder, the fusion module, and the decoder.# Framework of multiPI-TransBTS\n\n|WT|Conv|Dec3|Dec2|Dec1|\n|---|---|---|---|---|\n|TC|Conv|DecE3|DecE2|DecE1|\n|ET|Conv|F1|FE|FE|\n| |F2|F3|WT|Conv|\n| | |TCF1|Conv|ETF1|\n| |AFF1|AFF2|AFF3|F4|\n|FLAIR|1|FLAIR|2|FLAIR|\n|3|FLAIR|T2|Emb+Enc1|T2F1|\n|T2|Emb+Enc2|T2F1|Emb+Enc3|T2F1|\n| |4|T1Gd|Emb+Enc1|T1GdF1|\n| |Emb+Enc2|T1GdF1|Emb+Enc3|T1GdF1|\n| |Emb+Enc4|T1|Emb+Enc1|T1F1|\n| |Emb+Enc2|T1F1|Emb+Enc3|T1F1|\n| |Emb+Enc4|SRSA|F1|SRSA|\n| |F2|SRSA|F3|SRSA|\n\nFig. 3. Framework of multiPI-TransBTS. This framework consists of three main modules: an encoder, a fusion module, and a decoder. To simplify feature representation, features are divided into common features shared across tasks and individual features specific to each task, denoted by red and blue F-marked blocks, respectively. \"SRSA\" indicates the Spatial-Reduction Self-Attention module. \"AFF\" and \"FE\" stand for adaptive feature fusion and feature enhancement. \"Conv\" and \"Dec\" are abbreviations for convolution and decoder, respectively.# II. Adaptive feature fusion module\n\nThe fusion module in multiPI-TransBTS integrates complementary information from multiple sources or modalities into a unified representation. It performs multi-modality fusion across multi-scale features extracted by the backbone network, allowing fusion at different resolutions or layers. This strategy effectively combines fine-grained and coarse information, enabling the model to capture a broad range of contextual details.\n\nTo enhance feature representation, the fusion module uses Squeeze-and-Excitation (SE) mechanisms [55], allowing the network to recalibrate channel-wise importance and emphasize key features while suppressing less informative ones. Additionally, it incorporates an element-wise attention mechanism to amplify crucial spatial information, leading to a more accurate representation of important details in the input data.\n\nThis combination of channel-wise and element-wise attention mechanisms strengthens the model's ability to focus on critical features across both spatial and channel dimensions, thereby improving performance in segmentation tasks.# III. Multi-feature decoder\n\nThe decoder integrates both common and individual features from various tasks to reconstruct.# 4.2 Transformer-based encoder\n\nThe encoder in multiPI-TransBTS consists of an initial convolutional layer followed by a PVT-like architecture. This architecture is organized into four stages, each responsible for generating feature maps at different scales, as illustrated in Fig. 3. To effectively capture the features from each modality separately, the first three stages include four branches, while the final stage is an exception. Each branch follows a similar structure, represented by the \"SRSA\" module in Fig. 3, which includes an Embedding Layer and Transformer Encoder. The internal details of the \"SRSA\" module are depicted in Fig. 4.# Fig. 4. \"SRSA\" module in Pyramid Vision Transformer (PVT)\n\nThis module represents the Spatial-Reduction Self-Attention (SRSA) mechanism, which is responsible for reducing the spatial dimension of input sequences while capturing relevant attention.# 4.3 Adaptive Feature Fusion Module\n\nThe Adaptive Feature Fusion (AFF) module is designed to fuse information from different modalities of MRI scans, forming multi-scale features shared across multiple tasks. As shown in:# Fig. 5\n\nThe AFF module consists of three main submodules: channel-wise feature enhancement, feature fusion, and element-wise feature enhancement. The channel-wise feature enhancement uses Squeeze-and-Excitation (SE) Networks, the feature fusion is performed using convolutional networks, and the element-wise feature enhancement employs feature recalibration.# II. Feature Fusion\n\nFeature fusion is performed using a convolution operation to combine the multiple features. This can be mathematically expressed as:\n\n\ud835\udedf\ud835\udc56 = Conv3(\ud835\udedd\ud835\udc56) (\ud835\udc56 = 1,2,3). (13)# I. Decoders for WT and TC segmentation\n\nIn multiPI-TransBTS, a multi-level decoder is used to achieve multi-scale information fusion. The decoders for WT and TC segmentation, see the \"Dec\" module in Fig. 3, have similar internal structures, as depicted in Fig. 6(a). Based on the correlation analysis between region segmentation and the MRI modalities (see Fig. 2), the WT and TC segmentation decoders only use signals from the two most relevant modalities to prevent noise from overwhelming the signal.\n\nWithout loss of generality, consider the current module to be the decoder at level i (where i=1,2,3). The input data to the decoders for WT and TC segmentation includes four feature vectors:\n\n1. The common feature at level 4\u2212i, denoted as \ud835\udc054\u2212\ud835\udc56,\n2. The fused feature from the previous level, \ud835\udc1fY 1,\ud835\udc56\u2212\n3. The T2-channel feature at level 4\u2212i, \ud835\udc05T2 ,4\u2212\ud835\udc56\n4. The other-channel feature at level 4\u2212i, \ud835\udc05X \u2212\ud835\udc56.4\n\nLet the output of the decoder be \ud835\udc1fY. The \ud835\udc56 decoders for the WT and TC segmentation can be represented by the following series of operations:\n\n\ud835\uded3\ud835\udc561 = Conv1 (Concat ((\ud835\udc054X \u2212\ud835\udc56\u2a02\ud835\udc054\u2212\ud835\udc56), \ud835\udc054\u2212\ud835\udc56)) (\ud835\udc56 = 1,2,3). (15)\n\n\ud835\uded3\ud835\udc562 = Conv2 (Concat ((\ud835\udc054\u2212\ud835\udc56T2 \u2a02\ud835\udc054\u2212\ud835\udc56), \ud835\udc054\u2212\ud835\udc56)) (\ud835\udc56 = 1,2,3). (16)\n\n\ud835\uded3\ud835\udc563 = Concat (Conv3 (Concat(\ud835\uded3\ud835\udc561, \ud835\uded3\ud835\udc562)) , \ud835\udc054\u2212\ud835\udc56\u2a02(\ud835\udc054\u2212\ud835\udc56X T2 \u2a02\ud835\udc054\u2212\ud835\udc56)) (\ud835\udc56 = 1,2,3). (17)# 5.1 Datasets\n\nSimilar to many previous studies [41, 62, 63], the training and testing datasets used in our experiments were obtained from the BraTS2019 and BraTS2020 datasets, which are part of the BraTS Challenge. These datasets consist of multi-institutional pre-operative MRI scans, primarily focused on the segmentation of intrinsically heterogeneous brain tumors in terms of appearance, shape, and histology.\n\nAll BraTS multimodal scans are provided in NIfTI format (.nii.gz) and include a) native T1-weighted (T1), b) post-contrast T1-weighted (T1Gd), c) T2-weighted (T2), and d) T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes. Each dataset has been manually segmented by one to four raters following a standardized annotation protocol, and the annotations were subsequently validated by experienced neuro-radiologists. These annotations encompass the GD-enhancing tumor (ET \u2014 label 4), the peritumoral edema (ED \u2014 label 2), and the necrotic and non-enhancing tumor core (NCR/NET \u2014 label 1). The relationships between these labels and the final segmentation regions are outlined in Table 1.\n\nBraTS2019 and BraTS2020 include 335 and 369 annotated subject samples, respectively. Consistent with prior research [62, 63], we randomly divided the provided brain tumor segmentation dataset into training, validation, and test sets using an 8:1:1 split ratio.# 6 EXPERIMENTAL RESULTS\n\nThe experiments were conducted on a server equipped with two NVIDIA Quadro RTX8000 (48GB) graphics cards. The code was implemented using PyTorch. The initial learning rate was set to 0.01, and the SGD optimizer was employed for learning rate optimization. A batch size of 12 was used during model training, and the total number of training epochs was set to 100.# 6.1 Performance analysis\n\nThe performance comparison between multiPI-TransBTS and the baseline methods was conducted on two datasets. The baseline methods consist of 10 models, including both classical approaches, such as IVD-Net [65], UNet++ [66], and AttentionU [67], and recent representative models, like UCTransNet [68] and F2Net [69]. The results for the BraTS2019 and BraTS2020 datasets are presented in Table 2 and Table 3, respectively. In the tables, bold red numbers indicate the best-performing model, while bold black numbers denote the second-best model. Note that# 6.2 Visualization of Segmentation Results\n\nTo provide an intuitive comparison of the performance of multiPI-TransBTS with other methods, we visualize the segmentation results on the BraTS2019 and BraTS2020 datasets. Due to space constraints, we randomly selected three samples from each dataset. In the figures, the blue, brown, and yellow regions represent necrotic tumor cores, enhancing tumors, and edematous regions, respectively. Red arrows indicate areas where our results outperform the baselines, while white arrows highlight some false positive results from the baseline methods.\n\nThe visualization of segmentation results on the BraTS2019 dataset is presented in Fig. 8, with the randomly selected samples being TCIA09_255, TCIA03_375, and TCIA08_406. As shown in Fig. 8, the segmentation results from multiPI-TransBTS are notably closer to the ground truth.# 6.3 Ablation study\n\nTo evaluate the contribution of each component within the multiPI-TransBTS framework, we conducted an ablation study. For this purpose, we developed three derived versions of the multiPI-TransBTS model: mT-AFF, mT-PCI, and mT-FE. The specific modifications for each version are as follows:\n\n- (1) mT-AFF omits the Adaptive Feature Fusion (AFF) module;\n- (2) mT-PCI excludes the Task-Specific Feature Introduction (TSFI) strategy;\n- (3) mT-FE removes the Feature Enhancement (FE) module.\n\nThe results of these models on the BraTS2019 and BraTS2020 datasets are shown in Tables 4 and 5, respectively. Tables 4 and 5 provide a detailed performance comparison between the original multiPI-TransBTS model and its derived versions. It is clear that the original multiPI-TransBTS generally...# Fig. 8\n\nVisualization of segmentation results on the BraTS2019 dataset. 'GT' in the figure refers to the ground truth.# Fig. 9\n\nVisualization of segmentation results on the BraTS2020 dataset. 'GT' in the figure refers to the ground truth.# CONCLUSION\n\nIn this study, we introduced a novel framework for the BraTS challenge, multiPI-TransBTS: a Transformer-based architecture that integrates multi-physical information. This information includes general image attributes such as spatial and semantic information, along with specific information relevant to BraTS, particularly multimodal imaging knowledge.\n\nTo incorporate multi-physical information, multiPI-TransBTS leverages spatial-reduction attention modules, which enable the encoder to integrate multimodal information across different input sequences efficiently. Additionally, an adaptive feature fusion module uses both channel-wise and element-wise attention mechanisms to fuse complementary information from multiple modalities into a unified representation. The decoder employs a personality feature introduction strategy, which blends common features shared across tasks with individual features specific to each task. This process ensures that the abstracted features are optimally reconstructed into segmentation outputs.\n\nThrough rigorous experimental evaluations on two publicly available datasets, multiPI-TransBTS demonstrates superior performance compared to the state-of-the-art models. It achieves the best Dice, HD95, and sensitivity scores for WT and TC segmentation. For ET segmentation, it attains the best Dice and HD95 scores but does not achieve the highest sensitivity, indicating a trade-off between precision and recall. This trade-off presents a future research direction in balancing.# Segmentation Performance\n\nOverall, multiPI-TransBTS significantly outperforms existing models, validating the effectiveness of introducing multi-physical information into a Transformer-based framework for BraTS segmentation tasks. These advancements hold the potential to substantially enhance clinical outcomes for patients with brain tumors.",
        "context_id": 43,
        "question": "What type of module is designed in multiPI-TransBTS to fuse information from different MRI modalities?",
        "answer": [
            "Adaptive Feature Fusion (AFF) module"
        ],
        "context_length": 26809
    },
    {
        "context": "# 1 Introduction\n\nOptimization problems are prevalent in numerous modern scientific and engineering domains, necessitating increasingly intricate and compute intensive algorithms. They can also be of different nature depending on the application domain, the underlying computational complexity, the extent of information made available to the solvers, etc. This paper addresses blackbox continuous optimization problems requiring large-scale parallel architectures. More precisely, the goal is to find a real-valued solution x \u2208 Rn that minimizes (or maximizes) a continuous objective function f: Rn \u2192 R. The function f is given as blackbox, meaning that no predetermined mathematical knowledge is available about the function, such as its derivatives or any information about its structure (e.g. is the function smooth or convex?). A blackbox optimization algorithm then operates by probing f for input x, obtaining the corresponding fitness value f(x), and proceeding accordingly for the rest of the search process in an iterative manner. Blackbox optimization problems have received considerable attention for being essential.# Blackbox Optimization Algorithms\n\nIn many application domains, these include complex engineering models or numerical simulations, and generally application fields where problem specifics remain elusive. For instance, applications in aeronautics necessitate optimizing aerodynamics through simulations of airflow around vehicles, nuclear physics involves simulating heat or particle diffusion to optimize the dimensions of containment vessels, urban transportation systems require optimizing a traffic flow determined by the patterns of stoplights, etc. Traditional gradient-based approaches cannot be applied to such problems. This led to the development of various classes of blackbox optimization algorithms, also known as derivative-free algorithms [34, 16].# CMA-ES Algorithm\n\nWe are interested here in the so-called CMA-ES (Covariance Matrix Adaptation Evolution Strategy) algorithm [26], and more specifically in its IPOP-CMA-ES [10] (Increasing Population CMA-ES) variant. CMA-ES is a state-of-the-art blackbox optimization algorithm, which is, along with its variants, the best optimizer in the GECCO black-box optimization competition [62]. CMA-ES also has applications in many fields: neural networks [40], applied physics and engineering (e.g. for the design of thermal cloaks [19], optic cloaks [20], lenses [53], gas turbines [28]), autonomous sailing [43, 42], hydrology [63], sensor networks [5], wind energy [58], solar energy [33], to cite a few.\n\nCMA-ES is an iterative algorithm: at each iteration, it samples a set of points (called the population) using a probability distribution determined by the current mean point and a n\u00d7n covariance matrix, with n the dimension of the objective function f. The qualities of the points are evaluated with f and used to update the mean point and the matrix for the next iteration. These updates involve linear algebra operations and aim at directing the search towards interesting neighbouring areas. The IPOP-CMA-ES [10] (Increasing Population CMA-ES) restart strategy improves CMA-ES, especially for more complex problems [62]. After one CMA-ES execution (also referred to as a descent) ends, because it is for example trapped in a local optimum, a next CMA-ES descent is started with a greater population size, and so on. This enables IPOP-CMA-ES to employ increasingly thorough searches, at the cost of more and more function evaluations, to eventually find better optima.# Challenges and Approaches\n\nCMA-ES and IPOP-CMA-ES can thus be used to tackle challenging blackbox optimization problems. However, large optimization problems still require a lot of compute power, because the function evaluations can be individually time-consuming, and/or because many function evaluations (i.e. CMA-ES iterations) can be needed to solve complex problems. Two main approaches can be distinguished in the literature to accelerate CMA-ES and IPOP-CMA-ES for large optimization problems.\n\nThe first approach focuses on reducing the cost of the linear algebra operations while retaining as much quality as possible in the search for solutions. This is done by storing a reduced number of parameters instead of a full matrix of n\u00b2 parameters, leading to reduced costs for updating and using the matrix. These so-called large-scale CMA-ES variants can store k < n vectors of n parameters [36, 39, 6, 38, 37, 29], or only n parameters [59], or even only the sampled points [32]. Subsequent works compared the respective merits of some of those large-scale variants [67, 55, 66] taking into account their lower ability to retain information about the local function landscape, which leads to less effective convergence and to a lower quality for the final solution.\n\nThe second approach does not degrade the solution quality but relies on parallelism for the independent function evaluations [49, 11]. This was for example used for specific application domains [40, 17, 30]. Additionally, certain parameters can be adapted during the CMA-ES descent [11], in the hope of finding settings which best fit the parallel hardware. Another way of using parallelism is to run multiple CMA-ES descents concurrently, while trying to improve the convergence of any given descent using information from the other descents. In optimization, this method is known as the island model. Some island models were proposed for the original CMA-ES [50, 49, 60]. Such island models have also been used in specific domains [21, 47, 52], or for multi-objective optimization [13]. Some works also implement an island model for a large-scale CMA-ES variant [7, 14, 15]. However, to the# Best of our knowledge, there currently exists no work on the parallelization of IPOP-CMA-ES\n\nwhich implies running descents of increasing population sizes. In this paper, we thus focus on the use of parallelism and HPC to solve large optimization problems with IPOP-CMA-ES by speeding up both the linear algebra operations and the function evaluations. We present the following contributions.\n\n- We first show how the CMA-ES linear algebra operations can be accelerated thanks to BLAS and LAPACK routines. This requires the rewrite of some of these operations in order to introduce more efficient BLAS routines.\n- We present two parallel strategies for IPOP-CMA-ES to fully exploit a large number of CPU cores (up to several thousand). Such a number of CPU cores implies multiple compute nodes in distributed memory, each node being composed of multiple cores in shared memory. The goal here is to leverage large-scale parallelism (via multiple nodes) to benefit from the increasing number of (parallel) evaluations in IPOP-CMA-ES. The first strategy performs descents in the same order of population size as the original IPOP-CMA-ES, while the second strategy processes concurrently descents of different population sizes.\n- We thoroughly compare MPI[45]+OpenMP[56] implementations of our two strategies on 6144 cores (128 nodes) of the supercomputer Fugaku in order to determine which one is the most relevant on such a large-scale parallel architecture. These comparisons are performed with the reference BBOB[23] (Black-Box Optimization Benchmarking) benchmark for various dimensions and various function evaluation costs. We also present a fine analysis of those results, aggregated in different ways to investigate the impacts of the features of the parallel strategies on performance and on quality, depending on the targeted function, on the dimensions and on the evaluation costs.\n\nThe rest of this paper is organized as follows. In Section 2, we give some background on CMA-ES and on IPOP-CMA-ES. In Section 3, we show how we have introduced BLAS and LAPACK routines, and we describe the proposed parallel strategies. We report our detailed experimental study in Section 4, and we conclude the paper in Section 5.# CMA-ES with increasing population\n\nWe discuss here the main working principles of the Covariance Matrix Adaptation Evolution Strategy with Increasing Population[10] (IPOP-CMA-ES), starting with the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) it is based on.# The Covariance Matrix Adaptation Evolution Strategy\n\nIn order to find the optimum of an objective function n. At each iteration, CMA-ES samples a best current point (also called the mean or m) in Rf of dimension n, CMA-ES maintains points in Rn (is called the population size) around its mean m according to a normal law distorted along an ellipsoid [25]: see Figure 1. This normal law samples points in a circular fashion around the mean, using more points near the mean, and fewer points away from it. The widths and orientations of the ellipsoid are given by the so-called covariance matrix C. That is, sampling points involves a multivariate normal distribution N(0, C) with zero mean and covariance matrix C. This matrix is n \u00d7 n shaped: CMA-ES can thus adapt the space in which it samples points to the local shape of the function.# Figure 1\n\nConvergence example of CMA-ES on a function space. The white dot indicates the function optimum, the red ellipse the normal law, and the red crosses points sampled according to this law.# Algorithm 1\n\nPseudocode of CMA-ES with population size , for an objective function f\n\n1. initialize (mean: m, covariance matrix: C, variance: , evolution path of : p, evolution path of C: pc)\n2. while no stopping criterion met do\n3. . Perform one CMA-ES iteration\n4. for i = 1.. do\n5. xi sample point(m, C, )\n6. for i = 1.. do\n7. fi f (x)i\n8. update (m, C, , p, pc) using (x)i=1.. i and (fi)i=1..\n9. return the sampled point with the best quality\n\nThe mean m, as well as the matrix C, are updated depending on the qualities of the sampled points. These qualities correspond to the evaluations of the function f at all sampled points. For the covariance matrix, this update is called a matrix adaptation. This modification of the matrix aims at distorting the ellipsoid in ways that make it more likely to sample new points in the positions (relative to m) the best previous points were found in. This update requires O( \u21e5 n2) operations.\n\nThe scale at which CMA-ES searches for new points is also determined by the variance of the normal law. This variance is updated using a so-called evolution path p [57]. The evolution path is the sum of the last shifts of m. If m is generally shifting in one direction (the better points being farther along a certain axis), then the variance is increased. If m is not shifting in a specific direction (the ellipse remains in the same area, the better points being around m), then the variance is decreased. A similar technique is used for the matrix adaptation, with an evolution path pc.\n\nIn the end, the return value of CMA-ES is the sampled point with the best quality. A high-level algorithm of CMA-ES is given in Algorithm 1, and its convergence is illustrated in Figure 1.\n\nWe now provide a few more mathematical details. Sampling from N (0, C) requires the matrices B and D, where B is a matrix containing orthonormal eigenvectors of C, and D is a diagonal matrix containing the square roots of the eigenvalues of C. Eventually, we want to sample the points xk \u2208 Rn from the normal law N (m, 2C). The corresponding equation reads\n\nxk = m + BDzk, 81 \u2264 k \u2264 (1)\n\nwith zk \u2208 Rn sampled from N (0, I), I being the n \u00d7 n identity matrix. The equation for adapting the covariance matrix C according to the qualities of the newly sampled points then# Pseudocode of IPOP-CMA-ES with initial population size\n\nstart, multiplicative factor 2, maximum coefficient Kmax, for an objective function f\n\n1. K &nbsp; &nbsp; 1\n2. while K \u2264 Kmax and budget not exhausted do\n3. initialize (mean: m, covariance matrix: C, variance: , evolution path of : p, evolution path of C: pc)\n4. while no stopping criterion met do\n5. process one iteration of CMA-ES (see Algorithm 1, lines 4-8) using m, C, , p, pc and with population size K \u00d7 start\n6. K &nbsp; &nbsp; K \u00d7 2\n7. return the sampled point with the best quality (over all descents)\n\nreads\n\nC = C + c\u03bc \u2211i=1K wrk(i)(yiyiT C) + c1(pcpcT C)\n\nwhere c\u03bc \u2208 R and c1 \u2208 R are learning rate parameters for CMA-ES, wrk(i) \u2208 R is a weight depending on the rank of the point xi values having greater weights, and P when sorted by function value (points with better function i=1 wrk(i) = 1), and \u2200i \u2208 {1, K}, yi \u2208 Rn is such that xi = m + yi [24].\n\nwhich requires O(n3) operations with D from the matrix C In turn, computing B involves an eigendecomposition, the function dimension. Updating other variables like pc or p only requires at most O(n2) or O( n) operations and is thus less time-consuming.# 2.2 The increasing population restart strategy\n\nCMA-ES is a stochastic algorithm due to the random sampling of points within a distribution. Two executions of Algorithm 1 (also referred to as descents) on the same problem with the same parameters may thus sample different points and return a different best point. One execution may indeed find high quality points the other missed. As such, executing CMA-ES multiple times enables one to increase the quality of the final best point. The exact benefit of these multiple executions will of course depend on the shape of the objective function.\n\nIn order to identify if the search has settled on a local optimum and if the current execution must stop, multiple stopping conditions [9] have been proposed for CMA-ES. The stopping conditions can generally be understood as either the function quality not improving anymore, or the function being locally too flat, or even the sampling distribution being too small (i.e. the mean stops moving, or almost so). The best is then to restart the algorithm (i.e. start a new descent) in the hope of finding even better solution points.\n\nIt has then been shown that increasing the population size for each new restart enables faster convergence [10]. More precisely, this CMA-ES Increasing Population restart scheme (IPOP-CMA-ES) offers the same convergence rate as CMA-ES on simple functions, and a significantly better one on many complex functions. The corresponding high-level algorithm is presented in Algorithm 2. As usual with IPOP-CMA-ES [10, 41, 65, 8], we rely on a multiplicative factor of 2 at each restart for the population size. The initial population size start is hence multiplied by K = 2i for the i-th CMA-ES execution. Hence K ranges from 1 to a certain Kmax, with for example Kmax.\n\nAt each iteration multiple function evaluations are performed and the number of evaluations equals the population size. Since these evaluations are embarrassingly parallel, IPOP-CMA-ES offers thus an increasing degree of parallelism along its execution. When targeting a parallel version of IPOP-CMA-ES, this increasing degree of parallelism is an opportunity to reach important parallel speedups, but one also requires a# 3.1 High performance linear algebra\n\nBefore targeting large-scale parallel speedups via different parallel strategies, we first consider the performance of IPOP-CMA-ES and we study here how BLAS and LAPACK routines can be introduced in CMA-ES (hence in IPOP-CMA-ES). BLAS (Basic Linear Algebra Subprograms1) are high-performance implementations of standard linear algebra operations: vector operations (Level 1 BLAS), matrix-vector operations (Level 2 BLAS), and matrix-matrix operations (Level 3 BLAS). LAPACK (Linear Algebra PACKage2) then relies on BLAS routines to efficiently solve e.g. systems of linear equations, eigenvalue problems, singular value problems, etc. As presented in Section 2.1, multiple steps in CMA-ES actually involve linear algebra operations. We focus on Level 3 BLAS operations with a cubic time complexity for a quadratic input data size: these provide indeed a linear arithmetic intensity which enables their implementation to highly take advantage of current CPU architectures. We have hence been able to introduce BLAS and LAPACK routines in the following three steps, either straightforwardly or thanks to some rewriting of the linear algebra operations.\n\n- The eigendecomposition of the covariance matrix C can first benefit easily from LAPACK by using the dsyev routine.\n- Second, the original equation (see equation 2) for the covariance matrix adaptation does not involve any matrix-matrix multiplication. However, we can first rewrite equation 2 as\n(1 - c\u03bc - c1)C + c\u03bc(Xwrk(i)yiyiT) + c1ppc Xwrk(i) = 1.\n\nC\n\nWe denote by M the n \u00d7 n matrix equal to P=1 wrk(i)yiyiT. We have then\n\n8(r, c) \u2208 J1, nK2, Mr,c = \u03a3i=1n wrk(i)(yi)r(yiT)c = \u03a3i=1n(yi)r(wrk(i)(yiT)c) = \u03a3i=1nAr,iBi,c\n\nwhere A is a n \u00d7 matrix containing columns of (yi)i=1.. and B is a \u00d7 n matrix containing rows of (wrk(i)yT)i=1.., namely:\n\n|0|wrk(1)yT|1|\n|---|---|---|\n|0|1|B|\n|B|...|C|\n|C|A = @y1|A|\n|B|wrk(k)yT|C|\n|C|...|yk|\n|B|@|...|\n|wrk()yT| | |\n\nIn the end, we have rewritten the covariance matrix adaptation as\n\nC = (1 - c\u03bc - c1)C + c\u03bcA \u00b7 B + c1ppcT\n\n1See: https://www.netlib.org/blas/\n\n2See: https://www.netlib.org/lapack/# 3.2 The parallel strategies\n\nIn this section, we aim at deploying IPOP-CMA-ES on large-scale parallel architectures with thousands of CPU cores. Such architectures are based on multiple nodes (distributed-memory parallelism) composed of few multi-core processors each (shared-memory parallelism): we will thus rely on a hybrid MPI+OpenMP programming, using MPI for inter-process communications and OpenMP for multi-thread parallelism (with T threads in each MPI process). We consider an IPOP-CMA-ES execution with population sizes Kstart, K being a power of 2 ranging from 20 to Kmax (see Algorithm 2). For such an IPOP-CMA-ES execution, we propose here two generic strategies to obtain the best parallel speedups on a fixed (arbitrarily large) number of CPU cores. These strategies will then be specified and compared on a given supercomputer (Fugaku) in Section 4.# 3.2.1 Parallelism within a CMA-ES descent\n\nFirst of all, in our parallel strategies, each CMA-ES descent will be driven by a dedicated MPI process, hereafter referenced to as the main process. At each iteration of a descent, CMA-ES evaluates the objective function on points: these evaluations can be performed in parallel. In order to obtain the best parallel speedups, we aim at fully exploiting this parallelism level by processing each evaluation on a dedicated CPU core.\n\n- When \u2264 T, we can distribute the evaluations on the T threads of the main process.# Figure 2:\n\nIllustration of the core occupancy of a naive version of IPOP-CMA-ES with successive parallel descents.# Figure 3:\n\nIllustration of the core occupancy of the K-Replicated strategy.\n\nWhen &gt; T, we have to rely on multiple MPI processes. The main process will thus first generate the list of points where the objective function must be evaluated. These points are then \u201cscattered\u201d (using the corresponding MPI function) on a set of MPI processes. This sets an implicit synchronization among all processes involved in the same descent. Each of these MPI processes will evaluate the function on its points (using its T threads), and the objective function values are finally \u201cgathered\u201d (using again the corresponding MPI function) back to the main process.\n\nFinally, we will also consider multi-thread parallelism for the linear algebra operations performed in each descent (see Section 3.1): this will be detailed in Section 4.2.# 3.2.2 The K-Replicated strategy\n\nWhen targeting a parallel IPOP-CMA-ES execution, the first idea that may come to mind is running successively each descent of increasing K value, using the parallelism available within each descent (as described in Section 3.2.1). This is illustrated in Figure 2. The downside of this approach is an overall low CPU core occupancy: since descents with large K have a higher degree of parallelism, most of the CPU cores will be unused for the other descents.\n\nA first solution, hereafter referred to as \u2018K-Replicated\u2018, is to replicate the current K descent unto multiple, independent descents (with the same K value) until all the computing resources are used: see Figure 3. The number of those replicated descents is c/(K \u00d7 start), where c is the number of CPU cores available. We thus have more simultaneous descents at the start, when K is small, and fewer descents when K is larger; but the overall resource usage remains the same at any time. This way, all CPU cores are being used at all time, and since CMA-ES is a stochastic algorithm, the additional descents can help finding better solutions.\n\nRegarding the implementation, once two K descents are finished, their resources can be used for a subsequent 2K descent. This can be efficiently implemented as in the recursive Algorithm 3 using a hierarchy of MPI communicators, which represent sets of MPI processes.# 3 Pseudocode of the K-Replicated strategy\n\nThe global communicator MPI COMM WORLD (containing all MPI processes) and Kmax are used for the initial call.\n\nprocedure K-Replicated(communicator, K)\nif K > 1 then\nmy rank MPI Comm rank(communicator)\nsize MPI Comm size(communicator)\nmy half comm MPI Comm Split(communicator, my rank \u2264 size /2, my rank). splits\n\u2018communicator\u2018 in two halves of equal size\nK-Replicated(my half comm, K/2)\nif K \u2264 Kmax then\nCMA-ES descent(communicator, K \u00d7 start)\n\nthat can exchange messages [45]. New communicators can be created by specifying subsets out of a previously existing communicator: each process of the parent communicator indicate which child communicator it belongs to. The global communicator is thus split until the resulting communicators are small enough to be used for descents with K = 1. Then, each time a pair of K = 2i descents from a same parent communicator is finished, the control flows back up to this parent communicator for a K = 2i+1 descent. This is repeated until Kmax is reached and finished as well. Finally, in order to have a distinct random generator seed in each CMA-ES descent, we rely on the current time multiplied by the rank of the MPI process in the global communicator.# 3.2.3 The K-Distributed strategy\n\nThe K-replicated strategy exploits all the available CPU cores by replicating multiple descents with the same K value. These concurrent K descents increase the chances of finding better solutions for the stochastic IPOP-CMA-ES algorithm. We now consider a second strategy to leverage parallelism among multiple CMA-ES descents for large-scale parallel architectures, which stems from three observations.\n\nFirstly, a descent of population size can be sped-up up to a factor of (provided the communication and linear algebra times are short enough). Secondly, a descent of population K \u00d7 start usually takes, roughly, K times as long to reach a given quality as a start descent. This can be interpreted as CMA-ES taking in more information about a local landscape of the function before making the choice of where to move next, which requires more time. The benefit of using a larger population size is that, in many cases, the decision will be more \u2018informed\u2019, causing CMA-ES to avoid being trapped in a local optimum for longer and ultimately finding better solutions before the end of the descent. Finally, although the previous observation applies in general, there are also cases where, for some quality ranges on some objective functions and for some K values, a K \u00d7 start descent will be faster or slower than K times the duration of a start descent. We interpret these different behaviors as the impact of the properties of the function landscape on the optimal population size. Note that, especially for more complex functions, the landscape properties can change depending on the region of the search space or on the scale it is observed at.\n\nFrom the first and second observation, we can reasonably expect that several descents, each with a different population size and each running on CPU cores, will generally improve the quality of their current best solution for roughly the same amount of computation time. Then, from the third observation, we can expect that, thanks to their different population sizes, some of these parallel descents will be faster than others to reach certain qualities. Therefore, running# 4 Experimental results on Fugaku\n\nIn this section, we conduct a step-by-step performance analysis on the supercomputer Fugaku of the different parallel and high performance strategies described before. We first start describing our experimental set-up including the setting of the optimization benchmark functions, the considered parallel computing environment and how our parallel strategies are specified for the Fugaku hardware. Then, we report the benefits of using sequential and multi-threaded BLAS/LAPACK routines for CMA-ES. Our main results dealing with the different parallel strategies are then reported and analyzed in a comprehensive manner.# 4.1 Experimental setup\n\nFirstly, for the purpose of comparing the considered algorithms from a pure optimization perspective, and for benchmarking purposes, we consider the COmparing Continuous Optimizers (COCO) [27] framework providing an implementation of the Black-Box Optimization Benchmarking (BBOB)[3] test suite [23]. BBOB is a state-of-the-art blackbox test suite, used in particular in a reference workshop held yearly in the well-established Genetic and Evolutionary Computation Conference (GECCO). More than 200 algorithms were already benchmarked within this framework. BBOB provides a set of 24 continuous functions exposing different properties believed to represent a relatively broad range of blackbox optimization problems that one may encounter in practice. These functions are organized into five groups of increasing difficulty in terms of separability, multi-modality, illness, conditioning, etc. The first group of functions (f1 to f5) are separable. The second group contains functions of low or moderate conditioning (f6 to f9), while the third group contains unimodal functions with high conditioning (f10 to f14). The fourth and fifth group contain multi-modal functions with respectively adequate (f15 to f19) and weak (f20 to f24) global structure. All functions are available in multiple dimensions: in this paper we will study dimensions 10, 40, 200 and 1000. It is to notice that algorithms benchmarked using the BBOB functions, and the underlying COCO platform, usually fix as a budget the total amount of function evaluations that an algorithm is allowed to query. This is in fact a common practice in a blackbox optimization scenario, where the number of function evaluations is considered to be critical. In other words, this allows one to fairly evaluate the ability of an algorithm to search for a high-quality solution, when facing a range of optimization problems with different structural properties and dimensions, while using 3See also: https://numbbo.github.io/data-archive/bbob/# Evaluation of Blackbox Function Performance\n\nThe minimum number of blackbox function evaluations. As such, although the time it takes to query one blackbox function evaluation may vary across different BBOB functions of different dimensions, the BBOB test suite does not allow to explicitly control the evaluation times, which are in fact very short (less than 9ms in dimension 1000 on average across all functions). In practice however, function evaluation time directly impacts the overall CPU time required to run an algorithm using a specified budget in terms of the total number of function evaluations. Importantly, the time to evaluate a blackbox function is an important feature to account for when fairly assessing the performance of a parallel optimization algorithm, since it can directly impact the computation grain size (i.e. the amount of work performed by each \u201ctask\u201d in parallel). Therefore, in our work, and in addition to the broad range of functions provided by the BBOB test suite, we also consider to accommodate different blackbox evaluation times by adding artificial additional times to the BBOB function evaluations. For dimensions 10 and 40, for which BBOB functions have low evaluations costs, we will hence also study additional costs of 1ms, 10ms and 100ms. This will allow us for a more comprehensive and realistic performance assessment of the considered parallel algorithms.\n\nNote that having such greater evaluation times, even for small dimensions, is easily encountered in function optimization. For instance, Roussel et al.[35], use CMA-ES for parameter estimation with objective functions of dimension 8 and evaluation costs in the order of magnitude of the second. Evaluation costs may be even larger when scientific simulations or neural networks are involved: for example, using CMA-ES to find hyper-parameters for neural network training can necessitate evaluation times of 5 or 30 minutes in dimension 19[40].\n\nOther examples of evaluation times include:\n\n- Groundwater bioremediation (about 8 minutes)[48]\n- Aerodynamics of a shape (about 3 or 11 minutes)[31]\n- Molecular docking (4 hours)[44]\n- Automotive crash simulation (about 17 or 29 hours)[18]\n- Neural network trainings for computer vision (0 to 30 minutes)[12]\n- Document classification (average of 2.5 or 5.8 hours)[64]# Parallel Performance on Large-Scale Architectures\n\nSecondly, for the purpose of studying the parallel performance of our algorithms on large-scale parallel architectures with thousands of CPU cores, we consider running our algorithms on top of the supercomputer Fugaku. In June 2024, Fugaku was the fourth most powerful supercomputer in the TOP500 list[3], and the first in the world in the HPCG list[2] and in the GRAPH500 BFS list[1]. This massively parallel supercomputer contains 158,976 A64FX CPUs, which are ARM-based architectures developed by Fujitsu with 48 compute cores and 4 assistant cores each [54, 61]. The A64FX is divided in 4 CMGs (core memory groups) of 12 cores each. Each CMG is a NUMA (non-uniform memory access) node. Indeed, the memory space of the CPU consists of a set of HBM2 high-bandwidth memory and 2 levels of cache for each of the 4 CMGs. The CMGs communicate between each others and with the network using a ring bus. The A64FX CPUs are connected by the Tofu Interconnect D [4], a 6D torus topology. For our experiments, we consider using 128 A64FX CPUs of the Fugaku, hence representing a total of 512 CMGs and 6,144 compute cores.# Implementation Details\n\nRegarding our implementations, they are all based on the sequential C reference code of CMA-ES4, which we modified for the BLAS/LAPACK routines and the MPI+OpenMP parallelization. They are compiled with the Fujitsu C Compiler (version 4.11.1), the Fujitsu OpenMP and MPI libraries, and the Fujitsu thread-parallel implementation of BLAS/LAPACK. We let the C reference code set default values for all parameters, except for the initial mean m, the initial variance and start. In order to better adapt to the BBOB search space, we indeed set at the start of each CMA-ES descent the initial mean m to a point selected uniformly at random in the BBOB search space, and the initial variance to 1/4 of the search space width. Regarding start, the usual setting is of the order of magnitude of ten. In order to obtain the best parallel speedups and to compare our strategies on a large number of CPU cores within a single setting.\n\n4See: https://github.com/cma-es/c-cmaeswe target the processing of each evaluation on a dedicated CPU core (see Section 3.2.1). For our MPI+OpenMP performance tests on Fugaku, we thus choose to have start = 12. This way we can have T = 12 threads in each MPI process: the K \u21e5 start evaluations of a K descent are thus performed with K MPI processes with T threads each. Each A64FX runs 4 such MPI processes, i.e. one per CMG as usually done with NUMA architectures. For K-Replicated we set Kmax to 29 which leads to Kmax \u21e5 start = 29 \u21e5 12 = 6144 parallel evaluations executed on 6,144 cores (512 CMGs, 128 A64FX) for the final descent. For K-Distributed, we set Kmax to 28 which leads to (P8=0 2i) \u21e5 start = 511 \u21e5 12 = 6132 parallel evaluations on 511 CMGs. The K-i Distributed strategy thus uses 12 fewer cores than the K-Distributed one, but this is the fairest comparison we can make between these two strategies. We let the sequential IPOP-CMA-ES execute with Kmax = 29, and we ensure that this is the sole process running on the CMG of its core, so as to prevent cache interference with other processes. To keep our experiments manageable in a reasonable time, the execution limit of all experimented algorithms is 12 hours of wall-clock time. Except for Section 4.2 (BLAS/LAPACK tuning), we conducted 20 runs for each function and each strategy in dimensions 10 and 40. Due to time constraints, we performed at least 5 runs for each in dimensions 200 and 1000. The results presented in these sections are aggregated over these multiple runs, with the aggregation methods detailed later.# 4.2 Linear algebra performance results\n\nWe start our analysis by studying the performance impact of introducing BLAS/LAPACK routines in three linear algebra steps as described in Section 3.1. Figure 5 presents the performance gains with respect to each step, each step being executed sequentially for various dimensions and for various K values, with start = 12.\n\nMore precisely, the upper-left sub-figure reports the performance gain of using LAPACK specifically with respect to the eigendecomposition operation in CMA-ES. LAPACK enables us to accelerate the eigendecomposition step for problems of dimensions 40 and above, and significantly for problems of dimensions 200 et 1000 (up to 15.3x), where the C matrix is large enough to benefit from the LAPACK performance optimisations. Notice that for a relatively small dimension of 10, we found that using LAPACK leads to a performance loss, which is because the matrices maintained by CMA-ES are so small for such a dimension. However, since in dimension 10 the eigendecomposition accounts for only 9% of the overall linear algebra runtime (averaged over the 24 functions of BBOB), the overall loss in performance is negligible.\n\nThe upper-right part of Figure 5 presents BLAS performance gains with respect to the adaptation of the C covariance matrix. We distinguish here gains obtained when directly using Level 2 BLAS in equation 2 (see Section 2.1), and gains obtained with Level 3 BLAS thanks to our rewriting proposed in Section 3.1. While the use of Level 2 BLAS does not offer performance gains in dimensions 10, 40 and 200, our new computation scheme based on Level 3 BLAS offers very significant performance gains (up to 190x), especially for higher problem dimensions. The extra affectations (see Section 3.1) are thus offset by the BLAS gain, even for the lowest dimension.\n\nAs for the sampling operations, as reported in the lower-left sub-figure, we observe that using Level 2 routines directly in equation 1 (see Section 2.1) can only provide some gain for dimensions greater than 10. However, when the operations are rewritten using Level 3 routines (see Section 3.1), we are able to accelerate the reference C code for any dimension, with gains stronger than for Level 2 BLAS. Again the extra affectations (see Section 3.1) are offset by the BLAS gains.\n\nFinally, in the lower-right part of Figure 5, we report performance gains due to the sampling operations, but this time in a different context. The gain is computed with respect to all the linear algebra part (i.e. both sampling at lines 4-5 and update at line 8 in Algorithm 1), and not just to the sampling step like in the lower-left sub-figure. The eigendecomposition# Figure 5\n\n(upper-left) Performance gains for the eigendecomposition of the C matrix when using LAPACK over the reference C code (written without LAPACK). (upper-right, resp. lower-left) Performance gains for the adaptation of the C matrix (resp. for the sampling) when using Level 2 or Level 3 BLAS over the reference C code (without BLAS). (lower-right) Performance gains over the reference C code (without BLAS and LAPACK) for all the linear algebra part, with LAPACK for the eigendecomposition and Level 3 BLAS for the C matrix adaptation, when using Level 2 or Level 3 BLAS routines for the sampling. The IPOP columns correspond to a IPOP-CMA-ES execution with successive descents using K from 1 to 28.\n\nuses LAPACK and the matrix adaptation uses Level 3 BLAS, whereas Level 2 or Level 3 BLAS are used for the sampling. Although the gains obtained solely for the sampling may be deemed relatively small compared to the ones obtained solely for the covariance matrix adaptation, using Level 3 BLAS for the sampling operations still has a relatively substantial impact when LAPACK and BLAS routines are already used to optimize the other linear algebra steps. For instance, this enables us to increase the overall gain over the C reference code from 1.5 to 2.5 for all the linear algebra operations in dimension 1000. As a final remark, regarding the sampling and the adaptation steps, one can see stronger gains for K = 28 and for IPOP-CMA-ES than for K = 1: this is due to the larger population sizes, which lead to larger matrices. This shows that BLAS/LAPACK routines are even more relevant for IPOP-CMA-ES than for CMA-ES, the IPOP-CMA-ES increasing population sizes becoming eventually larger than the CMA-ES ones.\n\nTo further illustrate the impact of linear algebra operations, Table 1 presents the proportion of CPU time these operations consume relative to the total execution time of IPOP-CMA-ES, with and without Level 3 BLAS / LAPACK routines. As we can see, BLAS/LAPACK\n\n|Operation|CPU Time with Level 3 BLAS / LAPACK|CPU Time without Level 3 BLAS / LAPACK|\n|---|---|---|\n|Linear Algebra Operations| | |# Table 1: Proportions (averaged over all BBOB functions) of the linear algebra runtime within the overall runtime of a sequential execution of IPOP-CMA-ES (with start = 12 and Kmax = 28).\n\n|Dimension|Level 3 BLAS / LAPACK|10|40|200|1000|\n|---|---|---|---|---|---|\n|no|38%|36%|44%|69%| |\n| |33%|21%|18%|21%| |\n\nbecomes increasingly effective at reducing the proportion of linear algebra computations as the dimension increases. High dimensionality is a key factor that makes an optimization problem hard, making our linear algebra rewrites particularly valuable for tackling harder problems. Thanks to our BLAS/LAPACK rewrites, the linear algebra part is now a minority in the overall IPOP-CMA-ES runtime. Using additional costs for the evaluations (see Section 4.1) will make the linear algebra part even more minority.\n\nFinally, we tuned our BLAS/LAPACK implementations to determine the optimal number of threads (up to a maximum of 12) for each dimension. We found that dimensions 10 and 40 run best with 1 thread, dimension 200 with 4 threads and dimension 1000 with 12 threads. This aligns with the observation that larger dimensions entail larger matrices, which can be effectively managed by BLAS/LAPACK with a greater number of threads. However, the sizes of the involved matrices are not large enough, and the best speedup we obtain for running BLAS/LAPACK on multiple threads is 1.4\u00d7, for dimension 1000 with 12 threads. Due to this limited speedup, we believe that we would not benefit from distributing the linear algebra operations over multiple MPI processes (i.e. over more than 12 cores). That is why we chose to perform the linear algebra operations in parallel using only multi-threading within one MPI process (the main one for each descent, see Section 3.2.1), and up to 12 threads.\n\nNow that we have accelerated the linear algebra operations, with BLAS/LAPACK routines and as much as possible via parallelism, the time spent on function evaluations is the majority in the IPOP-CMA-ES execution time; hence, making the relevant parallelization of function evaluations of high importance. We will thus now focus on our two proposed parallel strategies.# 4.3 Parallel performance results\n\nIn this section, we delve into the performance behavior of the proposed K-Replicated and K-Distributed parallel algorithms. A thorough and fair performance assessment of our parallel variants requires to discuss two aspects. Firstly, although the function evaluation time can significantly influence the overall performance, it cannot be explicitly controlled in the COCO implementation of the BBOB functions. Hence, one has to adopt a more robust approach to assessing parallel performance relative to function evaluation time. Secondly, due to the stochastic nature of the considered algorithms, K-Replicated and K-Distributed are not expected to deliver exactly the same output as the sequential IPOP-CMA-ES. Consequently, it is essential to carefully define a metric that allows us to fairly evaluate the ability of the different algorithms to reach high-quality solutions within reduced time-frames. Therefore, in the next subsection, we begin by discussing the methodology we adopt to address these two aspects. Subsequently, we present our findings and state our main results.# 4.3.1 Performance assessment methodology\n\nFunction evaluation time. We first emphasize the relevance of the additional costs introduced in Section 4.1, by reporting in Figure 6 the share of MPI communications in the total runtime of a K = 28 descent involving 256 MPI processes. When considering zero additional# Figure 6\n\nMPI communication shares with respect to the total runtime, as measured by the Fugaku Instant Performance Profiler (FIPP) [68] for a K = 28 descent with 256 MPI processes, averaged over all BBOB functions of dimension 40. \u2018main\u2018 is the main MPI process (namely, the one with rank 0) driving the K = 28 descent (see Section 3.2.1) and processing the linear algebra operations (see end of Section 4.2), whereas \u2018evaluator\u2018 is one of the MPI processes performing only evaluations (here, the one with the highest MPI rank). Contrary to Table 1, we consider only K = 28 and all the evaluations are performed in parallel.# Figure 7\n\nFunction quality of the best solution found over runtime, averaged over 20 runs using the Expected Runtime (ERT) [22].\n\n15# Cost and Performance Analysis\n\nThe time spent in MPI communications (scatter and gather operations, see Section 3.2.1) is limited for the main process, but is the vast majority of the total time for another process involved in the parallel descent. This is due to the linear algebra part, only performed in the main process and leading to important waiting times for processes other than the main one in their MPI communications (namely at the scatter level). The linear algebra part can thus be a potential performance bottleneck when scaling on a large number of CPU cores.\n\nWhen adding extra costs in the function evaluation, one can see in Figure 6 that the MPI communication shares (i.e. the relative cost of the linear algebra part with respect to the total time, as well as the data transfers themselves) strongly decrease with an increasing additional cost, until becoming a minority. These extra costs enable us to also simulate real-life cases (see Section 4.1) where the linear algebra is not a performance bottleneck.# Solution Quality\n\nSince the optimal solutions of the BBOB functions are known, we can evaluate the quality of a solution relative to the optimal one. In our work, we measure quality by the difference \u03b5 between the function value of the best solution found so far by an algorithm and the function\u2019s optimal value. However, since the considered algorithms are stochastic, we get inspiration from the so-called Expected Runtime (ERT) [22] in order to aggregate the quality results obtained from multiple runs using different seeds for the same algorithm. More specifically, the ERT defines the empirical average time (over the different seeds) it takes for an algorithm to hit a solution of a given target quality \u03b5.\n\nNotice that two scenarios can happen. In the case all the runs of an algorithm were successful to hit the target quality \u03b5, the ERT is simply the average (over all runs) of the hitting times. In the case some runs were unsuccessful in hitting a target quality within the maximum affordable budget, we can assume that the algorithm could have been restarted for a new run until a success is observed (which is typically the case for stochastic algorithms such as ours). The time until a new run is successful can then be viewed as a random variable, which average value can be empirically estimated in a straightforward manner using the data available from the (other) successful runs at hand. Hence, the ERT value is simply defined as the sum of the execution times of all runs (including unsuccessful ones) divided by the number of successful runs. Notice that for the ERT to be correctly defined, at least one run must be successful. The reader is referred to [22] for more details.\n\nIn all our algorithms, it is easy to track the quality of the best solution found so far over time. Hence, we can easily compute the ERT of an algorithm when considering different targets. Consequently, we can report the expected convergence profile of an algorithm, as shown in Figure 7 on four illustrative BBOB functions. The reported convergence profile suggests a number of important issues to be carefully considered when assessing the relative performance of algorithms. Firstly, we can see that the relative behavior of the three algorithms depends on the tackled function, i.e., no algorithm is better than all others for all functions. Importantly, the relative performance of an algorithm depends on the considered target quality.\n\nWe can also see that not all algorithms can hit the same range of targets on all functions. Some algorithms are even not able to reach the same target that other algorithms can reach for the same function. Consequently, such observations raise a number of qualitative and quantitative questions, i.e., which algorithm is able to reach a given quality? Which algorithm reaches it faster than the others? What relative speedup can be obtained for a given solution quality? etc. In particular, parallel speedups cannot be defined in the conventional manner used in parallel computing. Instead, in our work, we consider nine fixed target quality values, namely \u03b5 \u2208 {10\u00b2, 10\u00b9.5, 10\u00b9, 10\u2070.5, 10\u2070, 10\u00b2, 10\u2074, 10\u2076, 10\u2078}. These values are actually the same as the ones used in the COCO framework [27]. Roughly speaking, this is intended to represent a range of target quality going from easy, to moderate and difficult to achieve. For each given pair of BBOB function and target quality \u03b5, we can then fairly analyze the relative ERTs achieved by the three algorithms. More specifically, in the following section, we define the speedup achieved.# Table 2: Speedups obtained by the two parallel strategies over the sequential IPOP-CMA-ES\n\nAggregated over the different pairs of BBOB functions and target qualities, for various function dimensions and function granularities. \u2019\u00a1\u2019 (resp. \u2019\u00bf\u2019) stands for the number of function-target couples for which K-Replicated reaches the target quality before (resp. after) K-Distributed. Function-target couples are accounted for when both parallel algorithms reach the target quality, so the sum of the counts may vary with the dimension and the granularity.\n\n|Dimension|10|10|10|10|40|40|40|40|200|1000|\n|---|---|---|---|---|---|---|---|---|---|---|\n|Additional cost|0|1ms|10ms|100ms|0|1ms|10ms|100ms|0|0|\n|K-Replicated avg. speedup|1.1|83|159|219|8.6|70|160|176|59|23|\n|std. dev.|4.0|234|432|639|24|171|520|618|183|55|\n|min. speedup|0.1|0.1|0.6|3.7|0.0|0.1|0.5|3.9|0.1|0.1|\n|max. speedup|30|1620|2995|5018|206|1182|3861|5121|1614|425|\n|/>|13/182|15/182|10/188|23/170|9/173|9/174|13/170|35/148|28/134|20/107|\n|K-Distributed avg. speedup|2.7|115|201|169|17|171|419|736|392|70|\n|std. dev.|3.2|259|378|255|39|302|799|2884|1580|257|\n|min. speedup|0.5|0.6|2.1|7.7|0.3|0.7|1.8|8.0|0.3|0.5|\n|max. speedup|20|1610|1901|2071|267|1645|3857|18080|13944|2397|# 4.3.2 Overall parallel speedup\n\nIn Table 2, we summarize some basic statistics concerning the speedups obtained by our two parallel strategies over the sequential IPOP-CMAE-ES. This sequential IPOP-CMA-ES leverages our Level 3 BLAS / LAPACK rewrites (with one thread) so as to focus here on the speedups obtained thanks to MPI+thread parallelism. More precisely, Table 2 reports the average, the standard deviation, the minimum and the maximum, speedups obtained respectively by K-Replicated and K-Distributed over the different pairs of BBOB functions and target qualities. The results are reported for each considered function dimension and function granularity. Additionally, the row \u201d\u00a1/\u00bf\u201d of Table 2 offers a direct comparison: each function-target pair is counted in the left-hand (resp. right-hand) number when K-Replicated reaches the target quality before (resp. after) K-Distributed. For example, the first cell with value 13/182 reads as K-replicated has better ERT than K-Distributed on 13 function-target pairs, whereas K-Distributed has better ERT than K-replicated on 182 function-target pairs.\n\nTwo main observations can be extracted from Table 2. Firstly, we can clearly see that K-Distributed (despite using a little less CPU cores) provides almost always a better average speedup than K-Replicated. The only exception is for dimension 10 with 100ms additional cost. Interestingly, even in such a case, as shown in the \u201d\u00a1/\u00bf\u201d row, the number of function-target pairs where K-Distributed is faster than K-Replicated is substantial. In fact, this holds true with no exception independently of the setting of the dimension and of the function additional cost. Moreover, we managed to obtain very high maximum speedups (over functions and targets) for both strategies, as soon as the computation grain or the dimension are large enough. It is also interesting to note that we even obtain \u2019super-linear\u2019 speedups for K-Distributed. This happens for dimension 40 with a function additional cost of 100ms, as well as for dimension 200, where the maximum observed speedup of K-Distributed is respectively 18080\u21e5 for function f7 with target 106, and 13944\u21e5 for f18 with 102, which is substantially larger than the 6144 cores used. One should indeed recall that the considered parallel strategies do not imply exactly the same search behavior as serial IPOP-CMAE-ES. Hence, these results.# Table 3: Speedups of K-Distributed over K-Replicated for all targets and functions in dimension 40 with an additional cost of 100ms.\n\nCells where K-Distributed is faster than K-Replicated (speedup > 1) are in bold font. \u2019X\u2019 indicates that K-Distributed did not reach the target. \u2019-\u2019 indicates that no parallel strategy reached the target.\n\n|Function|Targets| | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |102|101.5|101|100.5|100|104|106|108| | | | |\n|1|**0.6|0.9|0.9|1.0|1.0|1.2|1.3|1.4|1.4**| | | |\n| |2|**7.5|9.0|10|11|12|14|14|13|13**| | |\n|3|**1.2|5.7**|X|-|-|-|-|-|-| | | |\n| | | |4|**1.4|7.7**|-|-|-|-|-|-|-|\n| |5|**1.4|1.6|1.8|1.9|2.0|2.2|2.3|2.3|2.4**| | |\n| | | |6|**1.0|1.1|1.3|1.4|1.5|1.9|2.2|2.4|2.4**|\n|7|**1.0|1.1|1.3**|49|495|508|508|508|495| | | |\n| | | |8|**1.1|0.9|2.6|2.7|2.7|2.9|3.0|2.9|2.9**|\n| |9|**1.2|0.8|3.2|3.4|3.5|3.7|3.9|3.9|3.8**| | |\n|10|**8.7|9.5|11|12|13|15|15|15|14**| | | |\n| |11|**24|22|22|22|22|20|19|17|16**| | |\n| | |12|**1.3|1.3|1.2|1.2|1.1|0.9|1.4|2.0|2.2**| |\n| | | |13|**1.1|1.1|1.2|1.2|1.2|1.5|2.5|5.7|7.2**|\n| | |14|**2.0|0.6|0.8|0.9|1.0|1.3|4.2|9.7|16**| |\n| | |15|**1.2|5.9|11|14|14|12|11|11|10**| |\n| | | |16|**1.7|1.1|1.0|1.1|1.5|50|23|26|29**|\n| | | |17|**2.1|2.2|1.0|1.0|1.2|2.6|11|13|13**|\n|18|**1.7|0.8|1.1|1.3|1.5|12|32|40|38**| | | |\n|19|**1.5|2.2|1.0|2.6|3.1**|-|-|-|-| | | |\n|20|**0.9|0.9|0.9|0.8|8.1**|-|-|-|-| | | |\n|21|**1.4|0.8|0.8|0.7|0.1|0.1|0.2|0.2|0.2**| | | |\n|22|**1.8|0.7|0.7|0.8|0.0**|-|-|-|-| | | |\n|23|**2.0|2.0|1.9|0.9|0.4|1.2**|-|-|-| | | |\n|24|**1.0|2.1|3.7|4.5|13**|-|-|-|-| | | |\n\nSupport the fact that, for some specific functions/targets, the considered parallel strategies are able to show better search behavior than the original serial algorithm. To further illustrate the superiority of K-Distributed, we show in Table 3 a detailed view of the corresponding speedups obtained over K-Replicated for dimension 40 and an additional evaluation cost of 100ms. We clearly observe that K-Distributed is the faster solver for most targets. Notice that the relative speedup on function 7 is very high, K-Distributed being more than 500\u00d7 faster than K-Replicated, which is because K-Replicated is particularly inefficient for this function (as further detailed in Section 4.4). Besides, it is interesting to note that not all targets can be hit by the algorithms for any function which is specifically because some BBOB functions are more difficult to optimize than others. For the clarity of the presentation, a more thorough discussion of this important aspect is delayed to later.\n\nSecondly, we can clearly see in Table 2 that the function evaluation granularity as captured by the considered additional costs has a deep impact on the obtained speedups. Except for one case (when increasing the additional cost from 10ms to 100ms in dimension 10 for K-Distributed), the speedup of the parallel strategies over the serial algorithm increases indeed consistently with the function evaluation cost.\n\nAt this stage of the presentation, let us remark that although Table 2 provides a global view of the speedups the parallel strategies can achieve over the serial algorithm, the reported statistics are still to be very carefully interpreted. In particular, computing a speedup value can only be performed when both the sequential IPOP-CMAE-ES and the parallel strategy were successful in hitting a given target. This means that the average speedup values as shown in Table 2 discard the pairs of function/target where at least one algorithm was not able to hit the considered target. Generally speaking, we observed that the harder a target is, the more likely it is for serial IPOP-CMA-ES to be unsuccessful. This is exactly why the overall average speedup values reported in Table 2 decreases when going from dimension 200 to dimension 1000.# 4.3.3 Empirical cumulative distribution analysis\n\nIn this section, we analyze the relative performance of the considered algorithms using the so-called Empirical Cumulative Distribution Functions (ECDF) [46] as introduced in the COCO benchmarking framework6. Generally speaking, an empirical (cumulative) distribution function F : R ! [0, 1] is defined for a given real-valued data set, such that F (t) equals the fraction of elements in the data which are smaller than or equal to t. In an optimization setting, the ECDF is to be viewed as a measure of how many \u2019problems\u2019 a stochastic optimization algorithm can solve on average for a given time budget t. More specifically, for the purpose of our analysis, we consider the set containing the (function,target,run)-triplets labeled with the timestamp at which the algorithm was able to find a solution hitting a specified target value. The ECDF then counts for every timestamp t the proportion of (function,target,run)-triplets labeled with a timestamp smaller than or equal to t. In other words, the ECDF counts the proportion of targets that an optimization algorithm is able to hit as a function of the elapsed time t, i.e., the higher the proportion, the more powerful an algorithm.\n\nIn Figures 8 a) b) c) d), we present the ECDF curves for each algorithm across all dimensions (without additional cost). On an ECDF graph, a curve positioned to the left of another one for a given ECD value indicates a more powerful algorithm. Notably, K-Distributed\u2019s curve.\n\n|(a)|Dimension 10 (no additional cost)|\n|---|---|\n|(b)|Dimension 200 (no additional cost)|\n|(c)|Dimension 1000 (no additional cost)|\n|(d)|Dimension 40 (no additional cost)|\n|(e)|Dimension 40, additional cost of 1ms|\n|(f)|Dimension 40, additional cost of 100ms|\n\nHowever, such a decrease is not to be attributed to a parallel performance loss as the problem dimension decreases. It is instead to be attributed to the fact that many target values cannot be hit by serial IPOP-CMA-ES. Hence, a more fine grained assessment of the behavior of the different parallel strategies is needed to fully appreciate the benefits of the designed strategies, in particular as a function of problem dimension. This is to be studied in more detail in the next section introducing more advanced statistics.# Table 4: ECD value reached by each algorithm for the final timestamp of K-Distributed for various dimensions and granularities.\n\n|Dimension|10|10|10|10|40|40|40|40|200|1000|\n|---|---|---|---|---|---|---|---|---|---|---|\n|Additional cost|0|1ms|10ms|100ms|0|1ms|10ms|100ms|0|0|\n|Sequential IPOP|72%|31%|24%|21%|67%|34%|34%|33%|48%|39%|\n|K-Replicated|29%|82%|83%|83%|75%|74%|78%|78%|65%|57%|\n|K-Distributed|82%|82%|83%|82%|78%|79%|79%|80%|75%|64%|\n\nis almost always the leftmost which suggests that, without specific knowledge of a function landscape, K-Distributed is the superior choice. Similarly, K-Replicated\u2019s curve is to the left of the the sequential IPOP-CMA-ES one for most of the execution time. The ECDF analysis thus confirms the conclusions from Section 4.3.2: both parallel strategies outperform the sequential IPOP-CMA-ES, with K-Distributed being the most effective.\n\nNext, we analyze the dynamics of the algorithms with respect to function dimension. Firstly, higher dimensions show a larger gap between the parallel variants and the sequential IPOP-CMA-ES, leading to greater speedups for the parallel variants. Secondly, for each dimension, there is a cross-over ECD value where each parallel curve crosses and stays to the left of the sequential curve, meaning the parallel variant is generally better than sequential IPOP-CMA-ES for solving problems past this point. Sequential IPOP-CMA-ES is therefore faster only for the very easiest targets and, as the dimension increases, these cross-over values decrease, making the parallel variants the better choice for a broader range of problems. Thirdly, in Table 4, we report the ECD values for each algorithm at the final timestamp of the K-Distributed strategy. We observe that ECD values decrease with increasing dimension, especially at dimensions 200 and 1000. However, for these high dimensions, the parallel strategies show greater ECD values than the sequential IPOP-CMA-ES, K-Distributed having the greatest ones. Thus, as dimension increases (which makes optimization problems more challenging), the benefits of our parallel variants, particularly K-Distributed, become more pronounced.\n\nIn Figures 8 d) e) f), we report ECDF curves for different granularities at dimension 40 (dimension 10 leads to similar results). As with dimension, a higher granularity widens the gap between the parallel strategies and the sequential IPOP-CMA-ES. Additionally, a higher granularity increases the gap between K-Distributed and K-Replicated for ECD values greater than 0.4, making K-Distributed a better choice for more time-consuming problems.\n\nFinally, we observe specific effects in certain dimensions and granularities. In dimension 1000 (see Figure 8c), the sequential IPOP-CMA-ES stops at a lower ECD value due to the time limit, which prevents us from computing parallel speedups for many targets hit by our parallel strategies (see Section 4.3.2 and the lower average speedups for dimension 1000 in Table 3). However, past the last ECD value reached by sequential IPOP-CMA-ES, the slope of the curves for the two parallel strategies do not bend, showing that these parallel strategies are actually highly efficient in high dimension. This is why, along with the parallel speedups, ECDF profiles are necessary to appreciate the full extent of the performance of our strategies. To finish with, in dimensions 10 and 40 for all granularities, K-Replicated\u2019s final runtime reaches slightly more targets than K-Distributed, but only long after K-Distributed is over. This is because we have let K-Replicated run up to Kmax = 29 (as opposed to Kmax = 28 for K-Distributed) and because K-Replicated\u2019s design involves more descents. This enables K-Replicated to perform more exploration, at the cost of a much greater budget than K-Distributed.# Figure 9:\n\nFunction quality over the ERT for the different population sizes of K-Distributed.# Table 5:\n\nlog2K (averaged over 20 executions) of the first descent to reach a given quality for a K-Distributed run in dimension 40 (no additional cost). \u2019-\u2019 indicates that no descent reached the target.\n\n|function| | |targets| | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|\n| | |102|101.5|101|100.5|100|104|106|108| |\n| |1|0.1|0.1|0.1|0.1|0.1|0.1|0.1|0.1| |\n| |2|2.2|2.4|2.4|2.7|2.8|3.0|2.8|2.8| |\n| |3|0.7|3.0|-|-|-|-|-|-| |\n| |4|1.3|5.1|-|-|-|-|-|-| |\n| |5|0.1|0.1|0.1|0.0|0.1|0.1|0.1|0.1| |\n| |6|0.1|0.1|0.1|0.3|0.4|1.1|1.0|1.2|1.3|\n| |7|0.3|0.6|1.9|3.9|4.5|4.8|4.8|4.8| |\n| |8|0.3|0.7|1.4|1.6|1.6|1.7|1.8|1.8|1.9|\n| |9|0.3|0.8|1.7|1.9|1.9|2.0|2.0|1.9|1.9|\n| |10|1.8|1.6|1.8|1.7|1.9|2.0|2.0|2.0|2.1|\n| |11|3.0|2.9|2.9|3.0|2.8|2.6|2.6|2.5|2.6|\n| |12|0.2|0.3|0.7|0.9|1.1|1.2|1.2|1.6|1.6|\n| |13|0.0|0.1|0.1|0.7|1.2|2.4|2.6|3.1|3.1|\n| |14|0.1|0.0|0.0|0.0|0.0|0.1|1.5|2.4|2.6|\n| |15|0.6|2.1|4.2|5.6|6.4|6.5|6.5|6.5| |\n| |16|0.5|1.0|1.2|0.9|1.6|6.9|7.2|7.5|7.0|\n| |17|0.0|0.0|0.0|0.0|0.3|1.5|2.4|3.5|4.0|\n| |18|0.1|0.1|0.1|0.5|1.2|3.7|5.4|5.7|5.7|\n| |19|0.0|0.0|0.1|1.6|1.9|-|-|-|-|\n| |20|0.0|0.1|0.1|0.0|6.9|-|-|-|-|\n| |21|0.6|0.1|0.1|2.0|2.5|2.3|2.3|2.3|2.3|\n| |22|0.0|0.1|1.4|1.4|1.0|-|-|-|-|\n| |23|0.2|0.2|0.2|5.0|1.9|3.3|-|-|-|\n| |24|1.0|4.2|5.0|5.7|-|-|-|-|-|# 4.4 Impact of the population size\n\nIn this section, we analyze the effect of the population size on K-Distributed\u2019s performance in order to analyze its superior performance. We start by reporting in Figure 9 the convergence profiles for each distinct population size of K-Distributed on three illustrative BBOB functions. We observe that the fastest population size to reach a function quality varies depending on the targeted function and quality. For the first (i.e. the easiest) function qualities, or for functions with a very simple shape (such as a sphere for f1), the descents of K-Distributed are ordered by K value (hence by population size): the K = 20 descent is better than the K = 21 one, which is better than the K = 22 one, and so forth. However, for harder function qualities, and for more complex function (such as f17), some descents may stop being competitive after a given time. They typically reach this time in the order of K: first 20, then 21, etc. After a descent reaches such a time, the descent with the next larger population size becomes the most time-effective. We also see that the time before a descent stops being effective can vary greatly with the population size. Finally regarding f7, its shape consists in a step ellipsoidal function which includes many small regions with null gradients. A solver needs good global.# Figure 10:\n\nSpeedup of K-Distributed (over sequential IPOP-CMA-ES) against the best population size for function-target couples, averaged over 20 executions, for dimension 40, with (right) and without (left) additional cost.\n\nSearch abilities to find the optimum, which translates to a large population descent for CMA-ES. For this function, sequential IPOP-CMA-ES and K-Replicated waste CPU time with the first small population descents which last long (especially K = 20) and deliver limited qualities. This explains in particular the large performance gap between K-Distributed and K-Replicated on f7 in Table 3. To sum up, this overall dynamic shown in Figure 9 makes it difficult to predict which population size will be best for a given problem.\n\nTo confirm this analysis, we report in Table 5 the average log2K of the first descent to find a solution for different functions and targets. We see that lower population sizes are better suited for the first targets (i.e. for the columns with the highest power of 10). However, for other targets the best population size varies widely, with log2K ranging from 0.1 to 7.5. Note that these targets are more difficult to solve and correspond to the highest speedups of K-Distributed over sequential IPOP-CMA-ES. Besides, the best population size changes with each function for the final target (column with 108), and also with the target for each function. From this, we conclude that no population size is inherently better than another. Since we lack a reliable way to predict which population size will be more effective, the best strategy is to give an equal chance to each size and start them all at the beginning of the execution, which is precisely how K-Distributed operates.\n\nWe want to emphasize that K-Distributed\u2019s efficiency also relies on our parallel evaluations. Since in our K-Distributed design the number of cores used is proportional to the population size, the speedups are greater for larger population sizes, which makes the duration of the iterations closer among different population sizes. As a result, the convergence of each descent operates on a more similar time scale. Without such a parallel evaluation, the descent with larger population sizes would require much more time and would be less competitive compared to the ones with lower population sizes.\n\nLastly, we report in Figure 10 the speedups for K-Distributed over sequential IPOP-CMA-ES depending on the best population size for each target. One can first notice when comparing the two plots that K-Distributed\u2019s large populations lead to greater speedups when the function evaluation cost is longer. This is because longer evaluation times make the descents less sensitive to the costs of MPI communications and of linear algebra. Descents with large K values can\n\n22# then benefit at best from their higher parallel evaluation speedup. One can also see that K-Distributed\u2019s highest speedups are obtained for the larger population sizes. This is due to sequential IPOP-CMA-ES performing descents in an increasing order of population size, taking more time to start descents of larger populations (which also applies partly to K-Replicated). On the contrary, K-Distributed starts all descents concurrently which is beneficial when a large population is the most relevant for solving a given problem. In some instances, this can even produce super-linear speedups as presented in Section 4.3.2.# 5 Conclusion\n\nIn this paper, we investigated two parallel strategies for the IPOP-CMA-ES (Covariance Matrix Adaptation Evolution Strategy with Increasing Population) algorithm, designed for large black-box optimization problems on thousands of CPU cores. Both strategies leveraged BLAS and LAPACK routines to accelerate linear algebra operations, which required the rewriting of some operations to successfully benefit from the more efficient Level 3 BLAS. The first approach, K-Replicated, performs multiple descents with identical population sizes, increasing the population size as descents conclude and thus mirroring the progression of IPOP-CMA-ES. On the other hand the second strategy, K-Distributed, adapts IPOP-CMA-ES differently by initiating all descents simultaneously, each with a distinct population size.\n\nThanks to experiments on the supercomputer Fugaku, using MPI+OpenMP implementations on 128 A64FX CPUs of 48 cores each (6144 cores in total), with a reference black-box optimization benchmark extended with coarser computation grains, we determined that these parallel strategies greatly improved on the convergence speed of the original sequential IPOP-CMA-ES, reaching speedups up to several thousand. Notably, K-Distributed outperformed K-Replicated in the vast majority of cases. Moreover, due to its concurrent processing of multiple descents with distinct population sizes, K-Distributed occasionally exhibited super-linear speedups up to 18080\u00d7 on 6144 cores. We complemented these results with a detailed analysis of the superior performance of K-Distributed. According to our results, if one has a given allocated time on a large-scale parallel architecture, we recommend running K-Distributed and possibly restarting each descent once finished until the time is up.\n\nThis study opens up several interesting research avenues. The large-scale techniques used here could serve as a basis for parallelizing other variants of CMA-ES, such as the large-scale ones [36, 39, 38]. Additionally, the parallel IPOP-CMA-ES algorithm could be integrated with other optimization heuristics, such as global optimization [51] or hyper-parameter tuning [40]. Furthermore, investigating methods to predict the most effective CMA-ES population size for an objective function, either beforehand or at runtime, could strongly benefit both sequential and parallel black-box optimization.",
        "context_id": 44,
        "question": "What type of optimization problems does the paper address?",
        "answer": [
            "blackbox continuous optimization problems"
        ],
        "context_length": 68926
    },
    {
        "context": "# 1. INTRODUCTION\n\nStellar streams are a stunning example of the ongoing hierarchical assembly of the Milky Way via the gravitational interaction between our Galaxy and its satellites. They are the result of the disruptive tidal forces of the Galaxy on its companions such as globular clusters and dwarf galaxies. If the progenitor of a tidal stream is of low enough mass, the tidal debris that is liberated follows a path that is close to the orbit of its progenitor.\n\nLong tidal streams that extend over tens or even hundreds of degrees on the sky are of particular interest since they can be used to probe the global mass density profile, total mass and 3D shape of the Milky Way\u2019s dark matter halo (e.g., Helmi 2004; Johnston et al. 2005; Law et al. 2009; Law & Majewski 2010; Koposov et al. 2010; Malhan & Ibata 2019; Vasiliev et al. 2021). A stellar stream is the result of tidal fields removing stars from the outskirts of a progenitor at a velocity dispersion close to the cluster velocity dispersion, with the tide at the stream pericenter creating an outward ordered angular momentum distribution that leads to orbital shearing and a wider orbital spread at apocenter. Since globular clusters (GCs) have small internal velocity dispersions (\u21ad 2 km s\u21921), unperturbed tidal streams produced by GCs are both thin and dynamically cold. Therefore velocity dispersion variations, density perturbations and gaps along a GC stream might hold clues to past fly-by interactions and impact from dark matter subhalos which can produce gaps and heating which results in broadening the streams over time. If the streams do not pass through the Galactic disk they may be especially good antennae for detecting such dark subhalos. Consequently observations of both the spatial and velocity substructure in tidal streams have elicited great interest as potential probes of dark matter substructure (e.g., Johnston et al. 2002; Ibata et al. 2002; Siegal-Gaskins & Valluri 2008; Carlberg 2012, 2013; Erkal & Belokurov 2015; Bonaca et al. 2019).\n\nThe GD-1 stellar stream, first identified by Grillmair & Dionatos (2006), likely originated as a GC (Koposov et al. 2010). It is located in the Galactic halo, and being both thin and very long (spanning at least 100\u2191), is a good candidate for exploring both the global distribution of dark matter in the inner Milky Way (e.g., Koposov et al. 2010; Bowden et al. 2015; Malhan & Ibata 2019) and for detecting dark subhalos from the analysis of gaps and velocity dispersion variations. The progenitor remains undetected, possibly having been completely disrupted (de Boer et al. 2018; Malhan et al. 2018; Price-Whelan & Bonaca 2018). GD-1 is of great current interest since Gaia DR2 data led to the detection of density variations, gaps and o!-stream structures e.g., a \u2018spur\u2019 a \u2018blob\u2019 and \u2018wiggles\u2019 (Price-Whelan & Bonaca 2018; de Boer et al. 2020) which have been claimed as evidence of possible impacts by dark matter subhalos (Bonaca et al. 2019). Since the stream does not pass through the inner disk region (it has a perigalactic (apogalactic) distance of \u2192 14 (26) kpc, and inclination of 39 \u2191 to the disk plane, Koposov et al. (2010)) it is unlikely to have been heated by encounters with giant molecular clouds in the inner disk. It has also been shown that GD-1\u2019s orbit has a very low probability of impact with other (intact) GCs (Doke & Hattori 2022).\n\nAn alternative explanation for the gaps and density variations in the GD-1 stream has been proposed: regularly spaced gaps have been predicted to arise naturally due to epicyclic motions of stars along the stream (K\u00a8upper et al. 2012, 2015). From an analysis of the spectrum of stream gaps and after accounting for artifacts in the Gaia DR2 scanning law and projection e!ects, Ibata et al. (2020) conclude that the density profile of the stream shows periodic gaps separated by \u2192 2.64 \u00b1 0.18kpc and that such a regularly spaced gap distribution can be produced via epicyclic motions in a smooth Galactic potential. However since the regularly spaced gaps arising from internal dynamics are not predicted to be associated with increased velocity dispersions, obtaining kinematic data is useful for distinguishing this process from the e!ects of perturbations.\n\nMalhan et al. (2019b) used Gaia DR2 data to show that the GD-1 stellar stream possesses a secondary diffuse and extended stellar component (\u2192 100 pc wide) that surrounds the previously identified thinner component of the stream. They named this broader component the \u201ccocoon\u201d and showed that it was detected at > 5\u03c9 confidence level. Similar broad and extended morphological and kinematic features have also been found in several other streams (e.g., Jhelum, Phlegethon, Malhan et al. 2020; Malhan et al. 2022a; Ibata et al. 2023; Awad et al. 2024). Simulated GC streams that form in dwarf galaxies and are then accreted into Milky Way like halos show complex morphological features (like gaps, \u201cspurs\u201d, \u201cblobs\u201d, \u201ccocoons\u201d). Such accreted GC streams have complex morphologies in both cosmological simulations (Carlberg 2018; Carlberg & Agler 2023) and controlled simulations (Malhan et al. 2020). These complex morphologies are produced via the tidal interactions between the GCs and the central dark matter distributions in the GC\u2019s parent dwarf galaxy prior to their infall and subsequent accretion onto a massive host galaxy like the Milky Way. Furthermore it has been shown that the phase space structure of accreted GC streams could be used to obtain constraints on the dark matter distribution in the parent dwarf galaxy (Malhan).# 4 Valluri et al.\n\net al. 2020; Malhan et al. 2022a), making the phase space structure of the cocoon important to determine.\n\nRecently (Dillamore et al. 2022a) showed that the GD-1 stream is extremely sensitive to perturbations from the Sagittarius dwarf galaxy (Sgr) especially if it was quite massive (\u21ab 4 \u2191 1010 M \u2193) since the orbit of the GD-1 stream intersects with that of Sgr. They showed that a wide range of outcomes is possible with various types of substructures produced in and around the GD-1 stream (see their Fig 9). Interactions with the Sgr dwarf change the energy distribution of the GD-1 particles making it substantially broader; also the streams density profile is altered (affecting the surface brightness distribution along the stream) and its length is modified. While many of the induced perturbations are difficult to distinguish from the cocoon formation scenario above, the change in length is particularly interesting as this is a unique prediction of the heavy Sgr hypothesis.\n\nSince impacts from dark matter sub-halos which cross a stream also cause stream heating (in addition to producing short-lived gaps and off-stream features like \u2018spurs\u2019), it is particularly important to measure the velocity dispersions of robustly identified stream candidates. Thus the phase space structure of the GD-1 stream has the potential to improve our understanding of the nature of dark matter both within the Milky Way and in dwarf satellites that were accreted by our Galaxy.\n\nWhile Gaia photometry and proper-motions led to the detection of an extension to the stream as well as these various complex morphological features, there are fewer than 100 spectroscopically and chemically confirmed members from previous surveys with high radial velocity uncertainty of \u2192 1 \u2193 2 km s-1 (Bonaca et al. 2020; Gialluca et al. 2021; Ibata et al. 2020). An additional \u2192 200 spectroscopically confirmed members are available from the SDSS and LAMOST surveys but have lower radial velocity precision (\u2192 5 \u2193 20 km s-1). In this work we present 126 new spectroscopically confirmed members of the GD-1 stream obtained with the Dark Energy Spectroscopic Instrument (DESI) with \u2192 1 \u2193 10 km s-1 velocity precision (median precision 2.7 km s-1) and \u2192 0.2 dex metallicity precision. We also improve on 17 previous radial velocity measurements.\n\nStarting in December 2020 DESI carried out an approximately 6 month long \u201cSurvey Validation\u201d (SV) phase. The data from SV were used to validate scientific requirements, specifically the target selection algorithms (DESI Collaboration et al. 2022). During the SV period, while testing and streamlining targeting algorithms and analysis pipelines, there was also an opportunity to observe specific targets. In this period, DESI took several observations in the GD-1 stream region. Since these data were obtained primarily as part of the SV program, the target selection criteria and exposure times were slightly different from the main survey. After the completion of SV, there have continued to be observations in the GD-1 stream area.\n\nIn this paper we introduce the spectroscopically identified stars associated with the GD-1 tidal stream from the DESI SV observations, which was recently released to the public (DESI Collaboration et al. 2023a; Koposov et al. 2024). Additional data releases are planned at regular intervals. In section 2, we briefly describe the DESI Survey Validation data set in the region overlapping GD-1. In section 3 we discuss how we identify GD-1 stream members from the DESI targets using cuts in the stellar color-magnitude diagram, Gaia parallax, Gaia proper-motions as well as radial velocity and metallicity [Fe/H]. In section 4 we discuss the phase space distribution of stars in the stream, focusing on the identification and kinematics of the broader region referred to as the cocoon. Finally, in Section 5 we summarize our findings and discuss the implications of these results. We also give a preview of what to expect from future DESI data releases on the GD-1 stream region.# 2. DESI OBSERVATIONS OF THE GD-1 REGION\n\nThe Dark Energy Spectroscopic Instrument (DESI) is a robotic, fiber-fed, highly multiplexed spectrograph installed on the Mayall 4-meter telescope at Kitt Peak National Observatory (DESI Collaboration et al. 2022). A central feature of the instrument is the use of 5,000 robotic positioners that move individual optical fibers to pre-identified targets over a \u2192 3\u2191 field of view (DESI Collaboration et al. 2016; Silber et al. 2023; Miller et al. 2023). The DESI survey is a five-year survey of about 14,000 square degrees to obtain spectra for approximately 40 million galaxies and quasars and over 10 million Milky Way stars (DESI Collaboration et al. 2016).\n\nDESI was installed in 2019 and commissioning and survey validation extended through May 2021 (DESI Collaboration et al. 2023b). Since May 2021, the 5-year DESI survey has been underway and has obtained an unprecedented number of galaxy, quasar and stellar spectra. The primary scientific goal of the survey is to use spectroscopic redshifts of galaxies and quasars to obtain the most precise constraints on the expansion history of the universe ever obtained using the baryon acoustic oscillation and other methods (e.g., redshift space distortions) (DESI Collaboration et al. 2016; Levi et al. 2013).\n\nOver the first 5-years of the DESI Milky Way Survey (Allende Prieto et al. 2020; Cooper et al. 2023) it is expected that spectra of \u2194 7 million unique Milky Way# Figure 1\n\nGD-1 stream in great-circle coordinates (\u03c91, \u03c92) with the stream along \u03c91. Grey scale shows 2D histogram of surface density of Gaia DR3 stars with a broad color-magnitude, and a proper motion selection as indicated in the legend. The GD-1 stream is clearly visible as an over-density at roughly \u03c92 = 0. Magenta curves show the DESI tiles observed during commissioning and Survey Validation.\n\nStars at Galactic latitudes |b| > 20 \u2191 will be obtained during the main survey and millions more will be obtained during poor sky conditions (the \u201cback up program\u201d). The DESI-Milky Way Survey (MWS) uses a simple and inclusive target selection scheme focused on the thick disk and stellar halo. All targets are selected from the Gaia catalog (originally DR2, but subsequently DR3). Targets fall into three categories: MAIN-BLUE, MAIN-RED, and MAIN-BROAD, within the magnitude range 16 < r < 19. MAIN-BLUE prioritizes all Gaia point sources in this magnitude range with blue optical colors (g \u2193 r < 0.7) in the DESI Legacy Imaging Surveys (Dey et al. 2019). The MAIN-RED selection applies Gaia proper-motion and parallax criteria to sources with redder colors (g \u2193 r > 0.7) to increase the probability of observing distant halo giants. Sources with g \u2193 r > 0.7 that do not meet the astrometric criteria are targeted at lower priority in the MAIN-BROAD category (Cooper et al. 2023).\n\nDESI-MWS is expected to achieve an average spectroscopic completeness of \u2192 28% over the various MAIN target classes. The DESI-MWS is executed during bright-sky conditions (when high-redshift galaxy observations are inefficient) and shares the focal plane with a low-redshift Bright Galaxy Survey (BGS; Hahn et al. 2023), although BGS targets have higher priority for fiber assignment than MWS stars.\n\nThe DESI spectroscopic data presented in this paper were taken during the on-sky DESI SV from 2020/12/14 to 2021/5/14; the corresponding data (both spectra and the catalogs used in this paper) are now publicly available under DESI\u2019s Early Data Release (EDR) https://data.desi.lbl.gov/doc/releases/edr/vac/mws/.\n\nDuring the SV, the DESI Collaboration carried out observations to evaluate the targeting pipelines, instrument and observatory readiness in three separate subprograms SV1, SV2 and SV3. The MWS target selection for SV was previously described in Allende Prieto et al. (2020). Other DESI SV programs are discussed in DESI Collaboration et al. (2023b). Each fiber configuration in a given field is referred to as a \u201ctile\u201d. Some of these tiles had a larger percentage of fibers dedicated to Milky Way stars than expected for the main survey and in some cases went to fainter magnitudes (for the SV, stars in the range 19 < r < 20 were also included at lower priority.) As discussed in Section 7 of Cooper et al. (2023), several fields in the GD-1 region were observed during SV1 but without any special prioritization of potential GD-1 targets. In addition, 22 tiles were observed during SV2, also without any special prioritization for GD-1 stars. In total 45 tiles in the GD-1 region were observed during Survey Validation. For more details on the observing conditions and various programs for SV, readers are directed to the MWS EDR Value Added Catalogue paper (Koposov et al. 2024).\n\nFigure 1 shows the GD-1 stream region in a coordinate system in which \u03b51 is the angle on the sky along the stream and \u03b52 is the angle perpendicular to \u03b51. The transformation from Gaia DR3 sky positions (\u03d1, \u03d6) and proper motions (\u03bc\u03c9, \u03bc\u03b5) to this natural GD-1 frame was carried out using the coordinate transformation described in Koposov et al. (2010)1, which also yields proper motion in [\u03b51, \u03b52], hereafter referred to as \u03bc\u03d11 and \u03bc\u03d12 respectively. Most targets were also identified in the DESI Legacy Survey (DECaLs) (Dey et al. 2019), from which the g and r-band magnitudes for these stars were obtained.\n\n1 The pole of the great circle frame fitting the GD-1 stream has (RA, Dec)= (34.5987, 29.7331) in 2000 coordinates.\n\n2 Hereafter we drop the \u2192 and simply use \u03bc\u03c9 and \u03bc\u03b5 to denote \u03bc\u03c9 cos(\u03c9) and \u03bc\u03b5 cos(\u03b52).# 6 Valluri et al.\n\nwere measured. The Legacy Survey magnitudes were de-reddened using dust maps (Schlafly et al. 2010) updated for the Legacy Survey and available on the Legacy Survey website https://www.legacysurvey.org. These data were cross-matched with the Gaia DR3 catalog for parallax and proper motion measurements which is similar to the function estimated by Koposov (Gaia Collaboration 2022). The Gaia DR3 proper motions were corrected for solar reflex motion using Astropy. We used Astropy\u2019s default parameters for galactocentric frame defaults.set(\u2018v4.0\u2019) (Reid & Brunthaler 2004; GRAVITY Collaboration et al. 2018; Drimmel & Poggio 2018; Bennett & Bovy 2019).\n\nDistances to individual stars were estimated using the distance modulus in Equation 1 below (also see Appendix). Gaia parallaxes were zero-point corrected following the prescriptions in (Lindegren et al. 2021a).\n\nThe 2D histogram in Figure 1 shows the surface density of Gaia DR3 stars with DECaLS 0.2 \u2197 (g \u2193 r) < 0.4 and 18 \u2197 r < 20, \u219310mas yr \u21921 < \u03bc \u03d1 1 < \u21934mas yr \u21921 and |\u03bc \u03d1 2 | < \u21935mas yr \u21921. These selections are similar to and based on previous works (e.g., Price-Whelan & Bonaca 2018). The boundaries of the 45 DESI tiles in this region that were observed during SV are shown by magenta curves. The direction of motion of the stream is from positive \u03b5 1 to negative \u03b5 1.\n\nThe stellar spectra obtained by the DESI spectrograph are processed with the RVS pipeline, based on RVSpec (Koposov 2019), to measure each star\u2019s line-of-sight (radial) velocity and stellar atmospheric parameters. The spectra are further processed by the SP pipeline based on FERRE (Allende Prieto et al. 2006)3, which measures [Fe/H] and [\u03d1/Fe], among other properties. The RVS pipeline measures the radial velocity with a precision of \u2197 1 \u2193 10 km s \u21921 and [Fe/H] with a precision of \u21920.15-0.2 dex. For further details of the pipelines and their validation see Cooper et al. (2023) and Koposov et al. (2024).# 3.1. Initial selection\n\nOur search for GD-1 stream members begins by restricting to the region (galactic coordinates: 210 \u2191| < l < 90 \u2191 , 20 \u2191< |\u03b5 2 < 3\u2191 , \u219385\u2191 \u03b5 1 < 20\u2191 < b < 70 \u2191). Given the above initial cuts as an input, we show that the data are well identified in a color-magnitude diagram (CMD) as shown in Figure 3. The stars are well matched to an isochrone of 12 Gyr with [M/H] = \u21932.0 from the PARSEC isochrone database (Bressan et al. 2012; Tang et al. 2014; Marigo et al. 2017; Pastorelli et al. 2019, 2020)4. Other authors have used similar isochrones selections (e.g., Ibata et al. 2020).# 3.2. Color-Magnitude Diagram selection\n\nWe also apply the same broad cut in color-magnitude space and proper-motion selections as used in Figure 1. These selections expand slightly on the limits set by Price-Whelan & Bonaca (2018) and Malhan & Ibata (2019).\n\n3 https://github.com/callendeprieto/ferre and supporting python code piferre https://github.com/callendeprieto/piferre\n\n4 https://people.sissa.it/~sbressan/parsec.html# Figure 2\n\nGaia-DR3 stars in the GD-1 region defined in the frame of reference of the stream [|\u03c92| < 3 \u2191 , \u219285 \u2191 < \u03c91 < 20 \u2191 ] with a broad color-magnitude cut of 0.2 \u2191 g \u2192 r < 0.4 and 18 \u2191 r < 20.\n\n- (a) 2D histogram of \u03bc\u03b51 vs. \u03c91 with a spline (red curve) highlighting the GD-1 stream track.\n- (b) 2D histogram of \u03bc\u03b52 vs. \u03c91 with \u03bc\u03b51 within 2 mas/yr from red stream track in panel-(a). Cyan curve shows spline fit to stream.\n- (c) scatter plot of \u03bc\u03b52 vs. \u03bc\u03b51. Grey dots are all stars in panel-(b). Black points are stars with separation from red and cyan splines with \u03b5\u03bc\u03b51 < 2 mas/yr and \u03b5\u03bc\u03b52 < 2 mas/yr and parallax and broad color-magnitude cuts as described in the text.\n- (d) Selection in panel-(c) shown as a 2-D histogram in \u03c91 \u2192 \u03c92 (relative density in arbitrary units) with a spline representing stream track shown in red.# Table 1: Knots for four stream-track splines\n\n|\u03c91 [\u2191]|\u03bc\u03b51 [mas/yr]|\u03bc\u03b52 [mas/yr]|\u03c92 [\u2191]|VGSR [km/s]|\n|---|---|---|---|---|\n|90.0|-3.00|-5.26|-0.70|79.79|\n|-70.0|-1.50|-7.08|-0.87|59.15|\n|-60.0|-0.72|-7.44|-0.70|27.61|\n|-46.0|-0.10|-7.43|-0.38|-21.66|\n|-37.6|0.05|-7.18|-0.21|-46.38|\n|-29.2|0.22|-6.78|-0.09|-66.70|\n|-20.8|0.21|-6.27|0.00|-93.19|\n|-12.4|-0.18|-5.66|0.08|-113.29|\n|-4.0|-0.84|-4.98|0.16|-131.91|\n|-15.0|-0.02|-5.85|0.06|-107.81|\n|0.0|-1.20|-4.64|0.20|-144.31|\n|20.0|-2.50|-2.90|0.50|-257.76|\n\nWith the stream defined by the cuts discussed in the previous subsections, we now examine the radial velocities of the selected stars. Figure 4 (a) shows the solar reflex-motion corrected line-of-sight velocities in the Galactic Standard of Rest frame VGSR as a function of \u03b51 for stars in the GD-1 region selected by parallax, proper-motion (as shown in Figure 2), CMD cuts discussed in the text and include only stars with |\u03d6\u03b52| \u2197 3\u2191. Stars observed by DESI from the EDR catalog (DESI Collaboration et al. 2023a) are shown as black points. Stars observed by SDSS (Ahn et al. 2012) (blue) DR95 and LAMOST DR8 are shown by blue symbols. The 43 stars identified as GD-1 members using MMT-Hectochelle spectra (Bonaca et al. 2020) are shown by orange symbols. The selection of (Bonaca et al. 2020) only considered stars with velocities within 7 km s-1 from the stream track or the spur as GD-1 members, but we do not expand on this selection since none of their stars belong to the cocoon region we discuss later. All DESI-EDR measurement in Figure 4 (a) are given in a machine readable FITS file available at https://www.sdss3.org/dr9/ and http://www.lamost.org/dr8/.# Figure 3\n\nColor absolute magnitude diagram with the dereddened magnitudes from DECaLS. For this plot stars were selected with |\u03b5\u03c92| &lt; 0.2deg from the stream track and both \u03bc\u03b51 and \u03bc\u03b52 within 0.5mas/yr from the respective splines in Figure 2. This selection is only used to define the color-magnitude selection. We use the distance modulus vs \u03c91 relation in Eqn. 1 to correct for distance gradient along the stream. Overlaid is an isochrone from the PARSEC Isochrone database with a metallicity [M/H] = -2.0dex and age = 12Gyr (red curve). The light red curves are at |g \u2192 r| = 0.1 from the red curve.\n\nzenodo.org/records/11638330, the first 3 rows of the table are shown in Table 4 in Appendix B.\n\nWe note that 18 stars from the SDSS and LAMOST datasets were also observed by DESI. Where there were duplicates we selected the measurement with the smaller velocity uncertainty. Except for one star DESI velocity uncertainties were always smaller than uncertainties from SDSS and LAMOST. There were no overlaps with GD-1 members in the MMT sample of Bonaca et al. (2020) primarily because the DESI-EDR observations did not have much coverage of the spur area. Of the remaining 153 SDSS+LAMOST stars 14 did not have [Fe/H] measurements and were therefore excluded.\n\nIn summary DESI has added 126 new spectroscopically confirmed radial velocities and improved RV measurements for 17 previously observed members. A spline fit to the stream is shown in red (see below for details of how this fit is obtained) along with dashed lines showing \u03d6V GSR = \u00b130 km s-1 from the stream track. Figure 4 (b) shows kernel density estimates of \u03d6V GSR (from the stream track in the left panel) of stars observed by DESI (black), SDSS+LAMOST (blue) and MMT (orange). All stars within \u219385 \u2191 &lt; \u03b51 &lt; 20\u2191 are shown. A clear peak is visible identifying the GD-1 stream. In making the kernel density estimate we have used bandwidths 2.74 km s-1 for DESI (black curve), 7.25 km s-1 for SDSS+LAMOST (blue dot-dashed curve) and 1.5 km s-1 for MMT (orange dashed curve). The broader histogram corresponding to stars from SDSS+LAMOST reflects their larger measurement uncertainties, while the narrower MMT distribution reflects both the small uncertainties and the more restrictive selection of |\u03d6V GSR| &gt; 7 km s-1 imposed by Bonaca et al. (2020).\n\nIn order to measure the velocity dispersion of the stream we need to properly account for the uncertainties in the radial velocity measurements and account for background contamination. We use a mixture modeling approach to determine the velocity track and velocity dispersion of the stream. We use the approach previously adopted for other streams in Koposov et al. (2019, 2023). Focusing on the region along the stream of \u219355\u2191 &gt; \u03b51 &gt; 0\u2191, where the density of the stream and number of DESI RV measurements is the highest, we construct a model for the radial velocity distribution as a function of \u03b51 (only DESI measurements are used in determining the spline). We model the distribution as a Gaussian mixture where the center of the Gaussian representing the GD-1 stream is allowed to vary with \u03b51. The other Gaussian represents the background contamination. The variation of the radial velocity of stream stars is represented by a spline with 6 knots. The velocity dispersion of the stream is also allowed to vary and is represented by a spline with 4 knots. The mean and the velocity dispersion of the background contamination are fitted parameters. The specific likelihood function for the radial velocity conditional on \u03b51 is given in Equation 2 of Koposov et al. (2023). The model is implemented using the Stan probabilistic programming language (Carpenter et al. 2017) and is sampled using CMDstanpy package. The splines were implemented using the stan-splines package using code very similar to that provided in the supplementary material in Koposov et al. (2023). When computing the likelihoods of individual stars, we account for their radial velocity uncertainties by adding them in quadrature to the intrinsic velocity dispersions. The priors on model parameters are mostly non-informative. Velocity prior for individual spline knots is a normal distribution N (0, 300); the# Figure 4\n\n(a) Line-of-sight velocities (corrected for solar reflex motion and transformed to GSR frame) for stars from DESI (black points), SDSS (Ahn et al. 2012) or LAMOST (Yan et al. 2022) (blue diamonds), MMT-Hectochelle (Bonaca et al. 2020) (orange triangles). All stars were selected with the parallax, proper motion, and CMD cuts discussed in the text and include only stars with |\u03b5\u03c92| < . The stream is clearly visible and highlighted by the red curve which is a spline fit (see text). All DESI radial velocities and [Fe/H] measurements in this figure are given in Table 4 of Appendix B.\n\n(b) Kernel density distributions of stars with velocities within \u00b130 km s-1 (i.e. between dashed red lines) from the red stream track in left panel (\u219285 < \u03c91 < 20). The kernel band widths are 2.74 km s-1 for DESI (black curve), 7.25 km s-1 for SDSS+LAMOST (blue dot-dashed curve) and 1.5 km s-1 for MMT (orange dashed curve).# Table 2: Stream velocity dispersions\n\n|\u03c91|\u03d1V GSR|\u03d6|\u03d1VGSR|\n|---|---|---|---|\n|-46.0|6.195|2.38| |\n|-32.0|4.24|0.99| |\n|-18.0|3.27|0.60| |\n|-4.0|0.97|0.88| |\n\nFigure 5 (top) shows the measured radial velocity VGSR vs. \u03b51 for all DESI targets within \u00b13 from the stream track. The middle panel shows the same points in grey and location of spline knots in black. The uncertainties on the velocities are not visible as they are 15 km/s. Red lines show posterior samples from radial velocity curves. The lower panel of Figure 5 shows the constraints on the velocity dispersion along the stream in the region \u219355 \u2197 \u03b51 \u2197 0. The error-bars show the 1\u03c9 uncertainties determined from the posterior samples. The velocity dispersion measurements at different positions along the stream obtained with Stan are given in Table 2. The velocity dispersion in the \u21921 \u219340 \u2197 \u03b51 \u2197 \u219320 part of the stream is around 3-4 km s-1 measured with a small 0.6 km s-1 uncertainty. Higher velocity dispersion at \u03b5 = \u219346 is consistent with the same velocity dispersion within the errors. The measured dispersion selected using the proper motion, parallax, and CMD cuts using the selections described in Sections 3.1 and.# 3.4. Metallicity distribution of stream stars\n\nFigure 6 shows the distribution of DESI [Fe/H] measurements (from the RVS pipeline) for all stars (grey) selected using the proper motion, parallax, and CMD cuts using the selections described in Sections 3.1 and.# 3.2.\n\nStars with the additional constraint that they lie within 30 km s\u22121 from the stream track in Figure 4 (Section 3.3) are shown in black. It is clear from the black histogram in Figure 6 that only 2 stars have [Fe/H] > \u22121.5 and one more has [Fe/H] < \u22123.5. We exclude these stars bringing the sample from DESI to 143 (126 new, 17 improved). With these metallicity cuts 139 GD-1 stars from SDSS+LAMOST remain, along with 43 stars from MMT (Bonaca et al. 2020).\n\nThe median metallicity of the entire DESI GD-1 sample as measured by the RVS pipeline is -2.49\u00b10.16 (uncertainty determined by bootstrapping). Given the uncertainty of 0.1-0.2 dex in the [Fe/H] values recovered by the RVS pipeline, this is consistent with values from previous works: [Fe/H] = -2.24\u00b10.21 (Malhan & Ibata 2019), [Fe/H] = -2.3\u00b10.1 (Bonaca et al. 2020), and [Fe/H] = -2.47 recently reported by Ibata et al. (2023).\n\nThe DESI EDR dataset contains spectra for 507 stars that are also in the APOGEE DR17 catalog (Koposov et al. 2024). A detailed study of the random and systematic errors on DESI stellar atmospheric parameters compared with the corresponding APOGEE measurements is presented in section 5.1 of that paper. Their analysis shows that the median DESI metallicity has a median offset [Fe/H] DESI \u2212 [Fe/H] APOGEE = 0.06 \u2192 0.18 + 0.13 dex for the RVS pipeline and a median offset 0.03 \u2192 0.16 + 0.18 dex for the SP pipeline. The results of the RVS and SP pipeline for very metal poor stars have also been compared with follow up spectra with OSIRIS on Gran Telescopio Canarias presented by Allende Prieto et al. (2023) who show that the [Fe/H] values show excellent agreement with the OSIRIS values with a mean difference of 0.04 dex and a standard deviation of 0.14 dex. Since the DESI pipelines are still being improved and the systematic error likely depends on the stellar population, we do# Figure 5.\n\nTop: DESI-observations in the GD-1 region. Middle measurement of velocities at spline knots is shown by black points with error bars. The red curves show the samples from the posterior distribution. The grey points show the stars used to obtain the spline fit. Bottom: The velocity dispersion of the GD-1 stream as measured by the spline model. The error bars show the 1\u03c3 uncertainties in the velocity dispersion measurements derived from the posterior samples at four points along the stream.# Figure 6.\n\n[Fe/H] distribution for stars in the GD-1 stream using the proper motion cuts, parallax cut, CMD cut in the GD-1-region in previous figures (grey). Black histogram shows stars with the additional constraint that they have 30 km s\u22121 \u03b5V GSR < from the stream track in Figure 4(a). Only DESI measurements are used in this figure.# 4. IDENTIFICATION AND CHARACTERIZATION OF THE COCOON\n\nAs discussed in Section 1, an extended distribution called a \u201ccocoon\u201d has been identified surrounding the GD-1 stream and a few other tidal streams. While the origin of such cocoons is still uncertain (see Section 5), with the increased number of radial velocities provided by DESI it is now possible to identify and characterize the GD-1 cocoon in greater detail.\n\nOur final selection (after imposing the parallax, proper-motion, CMD, RV and metallicity cuts previously described) and with |\u03d6\u03b52| \u2197 3\u2191 can be seen in Figure 7. This plot only shows observations with full 6-D phase space and metallicity information. Malhan et al. (2019b) identified the GD-1 cocoon primarily in the region \u219310 \u2191 < \u03b51 < 0\u2191 based primarily on Gaia positional and proper motion data. DESI spectroscopic measurements confidently confirm the existence of the cocoon as a broad component in \u03b52 extending between \u219320 \u2191 < \u03b51 < \u21935\u2191.\n\nWe now analyze the phase space distribution of the stars in the region identified by the orange box. In this work we restrict our analysis to this range of \u03b51 since it is where the majority of the DESI spectroscopic measurements exist. We do not claim that this is the full extent of the cocoon. Spectroscopic observations at other positions along \u03b51 and extending perpendicular to the stream in \u03b52 are required to confidently assess its full extent of the cocoon and will be presented in future work. The cocoon might well extend further down to \u03b51 \u2192 \u219338\u2191 but we have few DESI-EDR observations in this region. This region also includes the \u2018spur\u2019 feature (seen as orange triangles above the main stream track) and we choose to ignore it here since a specific model has been postulated for the origin of this structure (Price-Whelan & Bonaca 2018; Bonaca et al. 2019).# 4.1. Multi-Gaussian Analysis\n\nWe wish to separate the \u201cthin stream\u201d and \u201ccocoon\u201d components in order to separately examine their phase space distributions. We use a Gaussian mixture model to fit observed quantities with two univariate Gaussians (representing the displacement of stars from the stream track in \u03d6\u03b52). One Gaussian represents the \u201cthin stream\u201d component N1(\u00af1, #1) while the other represents the cocoon N2(\u00af2, #2), with an unknown mixing fraction Q. We adapt the formalism of Equation 17 of Hogg et al. (2010) to write the likelihood function as follows, where xi refer to individual values of \u03d6\u03b52. We assume that positional uncertainty on all \u03d6\u03d12 values is zero since the Gaia positional uncertainty at G = 21 is much smaller (by a factor 107) than the spread in \u03d6\u03b52. Q is the fraction of stars in Gaussian component N1 (\u201cthin stream\u201d). The likelihood function, assuming each component is well described by a Gaussian, is given by:\n\nL = \u220fi=1N QN1(x |\u00af1, #1, \u03c9xi) + (1 - Q)N2(x |\u00af2, #2, \u03c9xi)\n\nL = \u220fi=1N \u221a(2\u03c0\u03c3(#1)) Q exp(-((xi - \u00af1)2)/(2 + \u03c9xi2)) + \u221a(2\u03c0\u03c3(#2)(1 - Q)) exp(-((xi - \u00af2)2)/(2 + \u03c9xi2))\n\nWe estimate the five parameters in the above model (\u00af1, #1, \u00af2, #2, Q) in two different ways. First, we use direct maximization of the likelihood in Equation 4 with the scipy.optimize package. We determine the distribution of the best-fit parameters by bootstrapping the observed sample 1000 times. Second, we generate the posterior distribution of the model parameters using the ensemble Markov Chain Monte Carlo sampling method emcee (Foreman-Mackey et al. 2013). We adopt uniform flat priors for all the parameters (\u00af1, #1, \u00af2, #2, Q). The posterior samples are generated with 64 walkers until the convergence of the mean parameter values to 1%. Since the maximum autocorrelation length of the chain Ncorr \u2243 80 steps, we...# Figure 7\n\nStars in the GD-1 region with |\u03b5\u03c92| < 3\u2191 from the stream track with parallax, CMD, proper motion, metallicity (\u21923.5 <[Fe/H]< \u21921.5) colored by velocity relative to the velocity spline in Figure 4(a). Points show stars observed by: DESI-EDR (circles), SDSS-DR9+LAMOST-DR8 (diamonds), and MMT-Hectochelle (triangles). The orange box shows the region we analyze for the cocoon (where most of the DESI observations lie). Boundaries of DESI tiles (fields) are shown by magenta curves.\n\nUncertainties on all parameters in this table are 16th and 84th percentile credible intervals inferred from the posterior probability distributions.\n\nWe also fit the \u03b52 distribution and the VGSR distribution jointly with two bivariate-Gaussian distributions described by the likelihood function:\n\nL = \u220fi=1QN1(x, y | \u00af1, \u00af1, #x1, #y1, \u03c9xi, \u03c9yi) + Ni N2(x, y | \u00af2, \u00af2, #x2, #y2, \u03c9xi, \u03c9yi),\n\nwhere both N1 (thin stream) and N2 (cocoon) are represented by 2D (bivariate) Gaussians N(\u03d6\u03b52, \u03d6VGSR). The results of the Bayesian MCMC analysis of the bivariate Gaussian model (9 parameters and their uncertainties) are also given in Table 3.# Figure 8\n\nDistribution of DESI stars in \u03b5VGSR and \u03b5\u03c92. The centrally concentrated component of the GD-1 stream and the broader cocoon component are clearly visible. The standard deviation of the Gaussian fit to the thin component #\u03b5\u03d12,thin = (0.15 \u00b1 0.014) \u2191 implies a FWHM of the \u201cthin stream\u201d component in \u03b52 to be 2.355\u2191\u03c9\u03d12,thin = 0.353 \u2191 \u2243 55 pc at the distance of 9 kpc) while the cocoon component is much broader with standard deviation #\u03b5\u03d1,cocoon2 = (1.245 \u00b1 0.117) \u2191 or FWHM \u2243 2.931 \u2191 \u2243 460 pc.\n\nFor the distribution in VGSR the bivariate analysis gives velocity dispersions #VGSR,thin = (2.60 \u00b1 0.65) km s\u22121 and #VGSR,cocoon = (8.06 \u00b1 0.98) km s\u22121 for the thin stream and cocoon respectively, with the fraction of stars in the thin stream Q = 0.60 \u00b1 0.05.\n\nThe standard deviations in \u03d6\u03b52 of the thin stream and cocoon obtained both by only modeling \u03d6\u03b52 versus by...# Table 3: Results of Univariate and Bivariate multi-Gaussian analysis\n\n|Parameter|Univariate|Univariate|Bivariate|Bivariate|\n|---|---|---|\n| |Thin|Cocoon|Thin|Cocoon|\n|\u03b5\u03c92 [deg]|\u21920.118 \u00b1 0.017|\u21920.149 \u00b1 0.161|\u21920.109 \u00b1 0.017|\u21920.165 \u00b1 0.154|\n|\u201d \u03d6\u03b52 [deg]|0.149 \u00b1 0.013|1.265 \u00b1 0.122|0.150 \u00b1 0.014|1.245 \u00b1 0.117|\n|\u03b5VGSR [ km s\u22121 ]|. . .|. . .|\u21921.37 \u00b1 0.49|1.23 \u00b1 1.24|\n|\u201d \u03d6VGSR [ km s\u22121 ]|. . .|. . .|2.60 \u00b1 0.65|8.06 \u00b1 0.98|\n|Q thin|0.63 \u00b1 0.05|. . .|0.60 \u00b1 0.05|. . .|# Figure 9\n\nStars from region in the orange box in Fig. 7 with the same selections. DESI observations are in black, SDSS+LAMOST in blue (no MMT stars lie in the cocoon region). In each scatter plot and histogram it is clear that there are both narrow and broad components in \u03b5\u03c92, \u03b5VGSR and \u03b5\u03bc\u03b51. The overall width in \u03b5\u03bc\u03b52 (perpendicular to the stream track) is smaller than in \u03b5\u03bc\u03b51 (along the stream track).# Figure 10\n\nTop: Relative radial velocity \u03b5VGSR vs. \u03b5\u03c92. Points show the thin and lower velocity dispersion stream stars (black) and thick and higher velocity dispersion cocoon stars (red) as determined by the bivariate double-Gaussian model. The vertical lines are at \u03b5\u03c92,thin \u00b1 \u201d \u03d6\u03b52,thin and include the majority of the thin stream stars identified as members of the thin component by both position and velocity. Bottom: Kernel density estimates of in \u03b5\u03c92 for all stars in the orange box in Figure 6 with Gaussians representing the thin stream (black curve) and cocoon (red curve) as estimated by the bivariate analysis.\n\nJointly modeling \u03d6\u03b52 and \u03d6VGSR yields nearly identical results for the angular widths of the thin stream and cocoon. Both methods also give Q \u2192 0.6 for the fraction of stars in the \u2018thin stream\u2019 component.# Valluri et al.\n\nFigure 11. Relative radial velocity and both components of proper motions of the stars from stream (left column) and cocoon (right column) using membership probabilities from the bivariate Gaussian model to define thin and cocoon components.\n\nFigure 10 (upper panel) shows the stars belonging to the \u201cthin stream\u201d with probability pthin \u21d0 0.5 as determined by the bivariate analysis considering both position and velocity \u03d6V GSR. The vertical lines are located at \u03d6\u03b5 2,thin \u00b1 # \u03b5\u03d1 2,thin from the bivariate analysis in Table 3 and include the majority of points determined to be members of the \u201cthin\u201d component by both univariate and bivariate analyses. The vertical lines are not used in subsequent analysis and only meant to guide the eye. Figure 10 (lower panel) shows the kernel density estimate of all the stars in the orange box in Figure 7. Overplotted as black and red curves are the Gaussians in \u03d6\u03b5 2 for the thin stream and cocoon respectively (as determined by the bivariate Gaussian analysis).\n\nFigure 11 shows various phase-space projections of the thin stream (left) and the broader cocoon component (right) with membership defined by the individual probabilities obtained from the bivariate analysis. It is clear that the cocoon has a much greater spread in \u03d6\u03bc \u03d1 1 and \u03d6V GSR than in \u03d6\u03bc \u03d1 2. Although the \u201cthin stream\u201d component defined in this way has a very narrow central component in \u03d6\u03bc \u03d1 1, some stars show significant velocity and proper motion deviations. The \u201cthin stream\u201d component also appears to have both narrow and broad components in \u03d6V GSR. Both the thin stream and cocoon are represented by a much broader Gaussian in velocity truncated at \u03d6V GSR = \u00b130 km s\u22121 (the velocity cut shown by the dashed lines in Fig. 4a). The mean and velocity dispersion of the background contamination are fitted parameters. The likelihood function is similar to that given in Equation 4.# 4.2. Accounting for contamination by background stars\n\nIn the above analysis we have not explicitly modeled the background population. Adding a third bivariate Gaussian component to represent contamination by a background population adds 5 additional parameters. Given the small number of stars in total in this region of the stream we did not attempt such a model. Instead we use the fact that both of the previous analyses yielded consistent values for the width of the thin stream. We use the probabilities of membership to the \u2018thin stream\u2019 and cocoon based on the univariate analysis in Section 4.1 to define each component. We then model each component\u2019s velocity distribution with a univariate Gaussian component for the velocity of the thin stream (cocoon).# 4.3. Metallicity of thin stream and cocoon components\n\nAs one would expect if some fraction of the higher velocity stars in both the \u2018thin stream\u2019 and cocoon are classified as belonging to the background, the velocity dispersions of these two components would decrease.\n\nThis model yields a velocity dispersion of 2.24 km s-1 with the fraction of stars assigned to the background found to be 0.15 \u00b1 0.04. This velocity dispersion for the thin stream component is consistent (within the uncertainties) to the dispersion obtained from the bivariate multi-Gaussian analysis in the previous section.\n\nWhen applied to the members of the cocoon component we find that the cocoon has a smaller velocity dispersion of 4.94 km s-1 and the background fraction in the cocoon is estimated to be 12%.\n\nTo assess whether this background fraction is reasonable we counted the number of stars (after applying the same cuts used to select stars in the orange box in Fig. 7) in three separate o!-stream regions with the same size as the orange box. The number of stars in these 3 regions compared to the number of cocoon stars varied from 4-8% suggesting that the number of background stars in the cocoon could be somewhat lower than in the above estimate of 12%.\n\nThe analyses in this subsection and the previous one both imply that the physically thin stream has a velocity dispersion of 2.2 km s-1, while the cocoon has a velocity dispersion between 5-8 km s-1.\n\nAdditional cocoon stars overlaps significantly with the distribution for thin stream stars. Figure 12 shows the [Fe/H] distribution functions for the thin stream component (grey/black) and the cocoon component (red), where the membership probability from the bivariate analysis was used to assign memberships.\n\nData from future DESI data releases will enable us to determine if the cocoon is an extended structure found along the entire stream or if it is more localized. It will also enable us to derive a more robust estimate of the velocity dispersion of the cocoon.\n\nOur measurement of the dispersion of the \u2018thin\u2019 stream component (\u03c9V GSR,thin \u2192 2 km s-1) is consistent with previous estimates of both other GC streams and the GD-1 stream itself (Gialluca et al. 2021).\n\nIn contrast, the larger velocity dispersion of the broader cocoon component of \u03c9V GSR,cocoon \u2192 5-8 km s-1 is comparable to the velocity dispersion of classical dwarf spheroidal galaxies like Carina, Draco, Fornax, Leo I etc.\n\nThis measurement is also comparable to the velocity dispersion of other dwarf galaxy streams (e.g., Li et al. 2022) which have velocity dispersions that range from 4-20 km s-1 with a mean value of around 11 km s-1 and are significantly larger than the dispersions of known GC streams (e.g., Li et al. 2022).\n\nA Kolmogorov-Smirnov 2-sided test yields p = 0.15, implying that we are unable to reject the null hypothesis that the two distributions are drawn from the same parent distribution.\n\nAnother way to assess if the cocoon velocity dispersion in Section 4.1 is inflated by background/foreground halo stars is to examine the metallicity distributions of the cocoon component.# Valluri et al.\n\nintrinsic metallicity dispersion of GD-1 can be com-\n\npletely explained by the current systematic floor in the\n\nuncertainty of [Fe/H] measurements resulting from the\n\nRV pipeline. Cooper et al. (2023) showed that [Fe/H]\n\nmeasurements with DESI for several GCs (M13, M92,\n\nNGC 5053) showed significant spreads of 0.3 dex or\n\ngreater and often had mean metallicities that were sys-\n\ntematically lower by 0.2 to 0.4 dex relative to literature\n\nvalues.\n\nWe caution that the DESI EDR metallicity values\n\nshould be treated as preliminary until ongoing improve-\n\nments to the spectroscopic pipelines are completed.\n\nSome of the improvements in progress including machine\n\nlearning based methods are expected to yield more ac-\n\ncurate metallicities and abundances from the low reso-\n\nlution, low S/N DESI spectra (J. Han, & DESI Collab-\n\noration, in prep, M. Haiger, & DESI Collaboration, in\n\nprep).# 5.2. Origin of the \u201ccocoon\u201d\n\nWhile its low metallicity, retrograde orbit and the\n\nthinness and coldness of the most visible part of the\n\nstream clearly imply that the progenitor of the GD-1\n\nstream was an accreted GC, the origin of the cocoon is\n\ncurrently uncertain. At least four possibilities exist.\n\n1. The cocoon could comprise GC stars that were\n2. tidally stripped from the GC while it was still in-\n3. side its parent dwarf galaxy (pre-accretion tidal\n4. stripping);\n5. it could be a tidal stream from a very metal poor\n6. dwarf galaxy, while the thin component could be\n7. the stream from an associated nuclear star cluster\n8. or GC;\n9. a thin tidal stream generated from the GC pro-\n10. genitor could have been heated by multiple inter-\n11. actions with dark subhalos in the MW;# 4.\n\nA thin tidal stream generated from the GC progenitor could have been perturbed and heated by a massive Sagittarius dwarf galaxy.\n\nWe now describe each of these possible origins in turn and discuss how one might distinguish between them with future spectroscopic data.# Cocoon stars originated from pre-accretion tidal stripping of the progenitor GC:\n\nSimulations show that star clusters can experience the strongest tidal force in the first few hundred Myr after formation (Meng & Gnedin 2022). Once the parent dwarf galaxy is accreted onto the Milky Way and is tidally disrupted, the pre-accretion tidal debris forms the cocoon while the intact GC is also liberated from the parent dwarf galaxy and continues to be tidally stripped forming the thinner GC stream (Carlberg 2018; Malhan et al. 2019b; Malhan et al. 2020; Carlberg & Agler 2023).\n\nThe structure and kinematics of cocoons so formed may be important probes of the central dark matter distribution of the parent dwarf galaxy. (Malhan et al. 2020) carried out a suite of over 100 controlled N-body simulations of dwarf galaxies and their GCs falling into a Milky Way-like potential. They showed that accreted GCs born in dwarf galaxies with cored dark matter halos exhibit fewer morphological peculiarities (e.g., gaps and off-track stream structures like the spur and blob) than their counterparts born in dwarf galaxies with cuspy dark matter halos. They argue that for a parent halo of a given mass, the physical width, radial and tangential velocity dispersions and spread in angular momentum averaged along an accreted GC stream can serve as diagnostics of whether the parent host galaxy had a cusp or a core.\n\nMalhan et al. (2020) showed that if considering dwarf galaxies of halo mass 109 M\u2609, the gentler tidal fields in a cored dark matter halo would result in tidal stream with a velocity dispersion of ~2-5 km s-1, while a dwarf galaxy with a cuspy halo would produce a stream velocity dispersion of order 8-12 km s-1.\n\nIn this pre-accretion tidal stripping scenario a cocoon velocity dispersion of ~8 km s-1 would imply that GD-1\u2019s parent dwarf galaxy had a cuspy dark matter halo while ~5 km s-1 would imply the parent dwarf had a cored halo. Additional observations along a larger stretch of the stream are required to draw definitive conclusions since this formation scenario predicts that cocoons are not restricted to short segments of the stream.# Cocoon stars originated from the parent dwarf galaxy:\n\nA second possibility is that the cocoon comprises the stars of GD-1\u2019s parent dwarf galaxy. The velocity dispersion of the cocoon as well as the metallicity are reasonably consistent with those of other Local Group dwarf galaxies. If the cocoon is the debris of the parent dwarf galaxy, the metallicity of the cocoon would imply that the parent had a stellar mass of ~104 to 106 M\u2609 (Kirby et al. 2013). Using the stellar mass - halo mass relation for low mass halos (Zaritsky & Behroozi 2023) this would imply a parent dwarf galaxy of halo mass ~2 x 109 M\u2609.\n\nDwarf galaxies tend to show greater metallicity dispersions than GCs but the metallicity distributions of the thin stream and cocoon components are currently statistically indistinguishable, although improved [Fe/H] measurements are needed to make a definitive statement. Nuclear star clusters do show large metallicity dispersions (e.g., Alfaro-Cuello et al. 2019; Nogueras-Lara 2022), but all known nuclear star clusters are more metal rich than GD-1.\n\nGD-1\u2019s low metallicity and retrograde orbit are strong indicators that progenitor was accreted from a dwarf galaxy. Previous work based on the location of GD-1 in action space relative to known GCs and halo stars suggests a possible association with debris from accreted dwarf galaxy(ies) such as \u2018Sequoia/Arjuna/I\u2019itoi\u2019 (Myeong et al. 2019; Bonaca et al. 2021; Malhan et al. 2022b). Accreted GCs which are about 60% of the GC population (Choksi et al. 2018; Choksi & Gnedin 2019; Creasey et al. 2019) tend to be on more radial orbits and tend to be found at larger radii (Chen & Gnedin 2022, 2023; Belokurov & Kravtsov 2024). In contrast, in situ GCs are more metal-rich and lie deeper in the Galactic potential (at more negative energy). GD-1 is peculiar because it is quite deep in the potential but rather than being on a radial orbit it is on a retrograde and nearly tangential (high angular momentum) orbit. A possible explanation is that it was brought in by a fairly heavy dwarf galaxy that experienced significant dynamical friction (Binney & Tremaine 2008).\n\nFollow-up high-resolution spectroscopy to obtain abundance data will be particularly useful in determining whether the cocoon has abundance signatures characteristic of GCs or dwarf galaxies. For example, a sodium-oxygen anti-correlation is seen in high-resolution spectra of most GCs (e.g., Marino et al. 2011) but not in dwarf galaxies.# Cocoon produced by dark matter subhalo impacts with a thin GC stream:\n\nThe third possible origin for the cocoon is heating by impacts from multiple dark subhalos with masses in the range 107 to 108 M\u2609 (e.g., Johnston et al. 2002; Ibata et al. 2002; Siegal-Gaskins & Valluri 2008; Carlberg 2012, 2013; Erkal & Belokurov 2015; Sanders et al. 2016; Banik et al. 2018; Bonaca et al. 2019; Banik et al. 2021; Carlberg & Agler 2023). Simulations provide insight into the origins of the broad cocoon-like structure.# 5.3. Other factors affecting the structure of the GD-1 stream\n\ncomponents produced by subhalo impacts. Encounters with small impact parameters between thin GC streams and dark subhalos cause the decrease of the angular momenta of stream stars ahead of the point of closest approach, since stars are pulled towards the perturber, and vice-versa. Stars with lower angular momenta than the progenitor spread to smaller galactocentric radii while stars with higher angular momenta spread to larger radii. The accumulation of sub-halo encounters with time is expected to lead to an increase in velocity dispersion (Ngan et al. 2015, 2016) along the stream and may also create gaps which orbital shearing will disperse over time. In addition, orbital precession in the non-spherical potential of a Milky Way-like system causes orbits to diverge on a time-scale of several Gyr (Ibata et al. 2002; Siegal-Gaskins & Valluri 2008). Since the effects of sub-halo encounters are initially fairly local, only old GC streams (from star clusters accreted long ago) will have had time to experience subhalo heating over significant stretches of the stream. Recent models suggest that the GD-1 stream is between 3-5.5 Gyr old (Gialluca et al. 2021), suggesting it might not be old enough to have been heated by subhalos along its entire extent. Ongoing and future observations of the GD-1 stream with DESI will enable us to determine whether the cocoon is a local or a much more extended feature.\n\nPerturbations from a massive Sagittarius dwarf: \u2014 de Boer et al. (2020) proposed that the spur in GD-1 could have been caused by interaction of the Sagittarius dwarf spheroidal galaxy (Sgr) with the GD-1 stream. Dillamore et al. (2022a) showed that if the Sgr dwarf was originally more massive than 4 \u00d7 1010 M\u2609, gravitational torques from it would affect tidal streams (like Pal-5 and GD-1) that lie within the orbital pericenter radius of Sgr. Such torques can cause asymmetries between the leading and trailing arms due to the folding back of one of the arms as well as the appearance of off-stream structures like the GD-1 \u201cspur\u201d and \u201cblob\u201d (Price-Whelan & Bonaca 2018) (the \u201cblob\u201d overlaps with the cocoon we characterized in previous subsections). Perturbations from a massive Sgr may even produce structures that appear like a separate stream on the sky but with kinematics similar to the main stream. Dillamore et al. (2022a) show that if Sgr has affected the GD-1 stream it could have also produced a thin offset stream similar to Kshir, a stream intersecting GD-1 discovered by (Malhan et al. 2019a).\n\nIn practice more than one of the above processes could have affected the GD-1 stream to varying degrees: the cocoon could have contributions from both pre-accretion tidal heating, stars from the parent dwarf galaxy and heating from sub-halo encounters. Additional observations, as well as modeling, will help to untangle the contributions from various processes.\n\nRecently Ibata et al. (2023) also identified a two-component structure for the GD-1 stream. They used the VLT/UVES spectrograph to follow up potential GD-1 candidates detected in the Gaia EDR3 and DR3 catalogs. The VLT/UVES data have velocity precision of 1 km s-1. They also obtained observations with the IDS long-slit spectrograph on the 2.5m Isaac Newton Telescope which have a lower velocity precision (\u2248 10 km s-1). Using 323 stars with line of sight velocity measurements they find a two component fit with velocity dispersions for the \u201cthin stream\u201d to be \u03c9 = 7.4 \u00b1 1.1 km s-1 while the cocoon has a velocity dispersion \u03c9 = 29.1 \u00b1 6.1 km s-1 with a fraction of 0.73 \u00b1 0.09 in the thin component.\n\nOur analysis in section 4 is restricted to a narrow range of \u03b51 values and implies a much smaller velocity dispersion of 5-8 km s-1 than found by Ibata et al. (2023). Their analysis is applied to the entire stream (100 < \u03b51 < 20) while our analysis above is restricted to (30 < \u03b51 < 0). When we use data from DESI, SDSS+LAMOST, MMT (all the data within the dashed red curves in Figure 4) to estimate the width of the thin stream and cocoon we find that the velocity dispersions of the thin component and cocoon are 1.89 \u00b1 0.38 km s-1 and 7.68 \u00b1 0.72 km s-1 respectively with a thin stream fraction Q = 0.58. Although these values are probably dominated by our measurements in the cocoon area, they are significantly smaller than the estimates from Ibata et al. (2023). While our estimated velocity dispersion for the thin stream component is more consistent with previous work on the GD-1 stream\u2019s thin component as well as other GC streams. Our estimated velocity dispersion for the cocoon is also more consistent with simulation predictions (Malhan et al. 2020; Carlberg & Agler 2023).\n\nAt the present time it is not possible to draw definitive conclusions as to the origin of the cocoon component of the GD-1 stream. The more important conclusion is that the GD-1 stream does have narrow and wide components approximately as expected in Milky Way simulations with an accreting sub-halo population. These initial data suggest that definitive conclusions will be possible as increased numbers of stars in individual streams become available and more streams are studied.# The DESI view of the GD-1 Stream and Cocoon\n\nIn addition to the mechanisms discussed in the previous section, there have been recent studies of at least a few other factors that can affect the structure and kinematics of tidal streams like GD-1.\n\nIt is thought that the massive merger in the Milky Way\u2019s history referred to as the Gaia-Enceladus-Sausage event (Belokurov et al. 2018; Helmi et al. 2018) could have resulted in the disk of the Milky Way becoming misaligned with the resulting triaxial dark matter halo (Dillamore et al. 2022b; Han et al. 2022, 2023) or otherwise cause torques on the stellar disk which cause it to tilt as it realigns with the halo. Simulations show that the tilting rate of disks following mergers can have angular speeds of \u2192 10 \u2193 20 km s \u21921 kpc\u21921 (e.g., Earp et al. 2017; Dodge et al. 2023). Recently Nibauer et al. (2023) showed that the gravitational torque from such a tilting disk can cause both narrowing and broadening (depending on the magnitude and direction of tilting) of tidal streams with pericenters within \u2192 20 kpc. Tilting can also cause changes in the line-of-sight width \u03d6\u03b5 2 as well as the width of the radial velocity distribution. They show that several of the features of the Pal-5 stream are consistent with a disk tilting rate of 15 km s\u21921 kpc\u21921. While these authors do not discuss the GD-1 stream it could also be affected by disk tilting since its pericenter radius is < 20 kpc.\n\nDespite the large number of possible origins for the cocoon described in this and the previous subsection, detailed models and additional spectroscopic data may enable us to determine its origin.# 5.4. GD-1 data expected from future DESI data releases\n\nFuture DESI data releases will include many more GD-1 members. The first year of the DESI main program (Year 1) has already obtained spectra for \u2192 350 GD-1 members from a total of \u2192 1300 targets observed in the stream area (2000 square degrees). Targets were selected by CMD and proper motion cuts and most were observed as part of secondary and tertiary observing programs designed for future survey planning purposes. The regions with confirmed members span \u219375\u2191 < \u03b5 1 < 15\u2191. The sample of confirmed members include stars down to r = 21 (the velocity uncertainty at the faint end is \u2192 15 km s \u21921). Current plans for the future include observing up to ninety 3\u2191 diameter fields in the GD-1 stream region. It is anticipated that by the end of the 5-year program DESI will obtain spectra for \u2192 3000 GD-1 stream members, making it one of the most thoroughly surveyed GC streams.\n\nMV gratefully acknowledges financial support from NASA-ATP award 80NSSC20K0509. SK acknowledges support from the Science & Technology Facilities Council (STFC) grant ST/Y001001/1. T.S.L. acknowledges financial support from Natural Sciences and Engineering Research Council of Canada (NSERC) through grant RGPIN-2022-04794. APC acknowledges support from a Taiwan Ministry of Education Yushan Fellowship and the Taiwan National Science and Technology Council (NSTC) grants 109-2112-M-007-011-MY3 and 112-2112-M-007-017-MY3. This material is based upon work supported by the U.S. Department of Energy (DOE), Office of Science, Office of High-Energy Physics, under Contract No. DE\u2013AC02\u201305CH11231, and by the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility under the same contract. Additional support for DESI was provided by the U.S. National Science Foundation (NSF), Division of Astronomical Sciences under Contract No. AST-0950945 to the NSF\u2019s National Optical-Infrared Astronomy Research Laboratory; the Science and Technology Facilities Council of the United Kingdom; the Gordon and Betty Moore Foundation; the Heising-Simons Foundation; the French Alternative Energies and Atomic Energy Commission (CEA); the National Council of Science and Technology of Mexico (CONACYT); the Ministry of Science and Innovation of Spain (MICINN), and by the DESI Member Institutions: https://www.desi.lbl.gov/collaborating-institutions. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the U. S. National Science Foundation, the U. S. Department of Energy, or any of the listed funding agencies.\n\nThe DESI Legacy Imaging Surveys consist of three individual and complementary projects: the Dark Energy Camera Legacy Survey (DECaLS), the Beijing-Arizona Sky Survey (BASS), and the Mayall z-band Legacy Survey (MzLS). DECaLS, BASS and MzLS together include data obtained, respectively, at the Blanco telescope, Cerro Tololo Inter-American Observatory, NSF NOIRLab; the Bok telescope, Steward Observatory, University of Arizona; and the Mayall telescope, Kitt Peak National Observatory, NOIRLab. NOIRLab is operated by the Association of Universities for Research in Astronomy (AURA) under a cooperative agreement with the National Science Foundation. Pipeline processing and analyses of the data were supported by NOIRLab and the Lawrence Berkeley National Laboratory (LBNL). The Legacy Surveys also use data products from the Near-Earth Object Wide-field Infrared Survey Explorer (NEOWISE), a project of the Jet Propulsion.# Valluri et al.\n\nLaboratory/California Institute of Technology, funded by the National Aeronautics and Space Administration. This paper made use of the Whole Sky Database (wsdb) created and maintained by Sergey Koposov at the Institute of Astronomy, Cambridge, with financial support from STFC and the European Research Council (ERC).\n\nThe authors are honored to be permitted to conduct scientific research on Iolkam Du\u2019ag (Kitt Peak), a mountain with particular significance to the Tohono O\u2019odham Nation.\n\nFor the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising from this submission.\n\nThis work has made use of data from the European Space Agency (ESA) mission Gaia7, the Gaia Data Processing and Analysis Consortium (DPAC8).\n\nThis work has made use of data from the Sloan Digital Sky Survey. Funding for SDSS-III has been provided by the Alfred P. Sloan Foundation, the Participating Institutions, the National Science Foundation, and the U.S. Department of Energy Office of Science. The SDSS-III web site is http://www.sdss3.org/.\n\nThis work also utilized data from the LAMOST survey. Data behind all figures in this paper are available as FITS files at: https://zenodo.org/records/11638330 DOI: 10.5281/zenodo.12535192.# A. MEASURING THE DISTANCE STREAM TRACK\n\nTo calibrate the distance gradient along the stream, we use the proper motion and parallax selection described in Section 3 and then focus on the subgiant branch stars as those form a narrow sequence on the CMD (see Figure 3). Specifically we select stars with 0.25 < g \u2193 r < 0.4 and assume that the subgiant branch has an approximately constant r + 4(g \u2193 r). The figure 13 shows the r + 4(g \u2193 r) \u2193 4.5 vs. \u03b51 for likely GD-1 subgiant stars. The red curve shows our fiducial distance modulus vs. \u03b51 relation.# B. TABLE OF DESI-EDR MEASUREMENTS IN THE GD-1 AREA\n\n|TARGET ID|RA|Dec|VGSR|[Fe/H]|V err|[Fe/H] err|\n|---|---|---|---|---|---|---|\n|Measurements from the RVS pipeline. The VGSR measurements have been corrected by 0.93 km s-1, the systematic error on RV measurements reported in (Koposov et al. 2024). No corrections have been applied to [Fe/H] measurements. RA and Dec values are from Gaia-DR3.|Measurements from the RVS pipeline. The VGSR measurements have been corrected by 0.93 km s-1, the systematic error on RV measurements reported in (Koposov et al. 2024). No corrections have been applied to [Fe/H] measurements. RA and Dec values are from Gaia-DR3.|Measurements from the RVS pipeline. The VGSR measurements have been corrected by 0.93 km s-1, the systematic error on RV measurements reported in (Koposov et al. 2024). No corrections have been applied to [Fe/H] measurements. RA and Dec values are from Gaia-DR3.|Measurements from the RVS pipeline. The VGSR measurements have been corrected by 0.93 km s-1, the systematic error on RV measurements reported in (Koposov et al. 2024). No corrections have been applied to [Fe/H] measurements. RA and Dec values are from Gaia-DR3.|Measurements from the RVS pipeline. The VGSR measurements have been corrected by 0.93 km s-1, the systematic error on RV measurements reported in (Koposov et al. 2024). No corrections have been applied to [Fe/H] measurements. RA and Dec values are from Gaia-DR3.|Measurements from the RVS pipeline. The VGSR measurements have been corrected by 0.93 km s-1, the systematic error on RV measurements reported in (Koposov et al. 2024). No corrections have been applied to [Fe/H] measurements. RA and Dec values are from Gaia-DR3.|Measurements from the RVS pipeline. The VGSR measurements have been corrected by 0.93 km s-1, the systematic error on RV measurements reported in (Koposov et al. 2024). No corrections have been applied to [Fe/H] measurements. RA and Dec values are from Gaia-DR3.|\n\n7 https://www.cosmos.esa.int/gaia\n\n8 https://www.cosmos.esa.int/web/gaia/dpac/consortium# Figure 13\n\nLeft: The stream distance gradient from the subgiant branch stars. The greyscale shows the distribution of the r + 4(g \u2192 r) \u2192 4.5 (a proxy for distance modulus) as a function of \u03c91 for stars with 0.25 < g \u2192 r < 0.4. The subgiant branch stars form a narrow sequence in the figure. The red curve shows the adopted distance gradient model (shifted down by 0.5 magnitudes for clarity). Right: Various relations for the distance to the GD-1 stream as a function of \u03c91 from Price-Whelan & Bonaca (2018) (blue dot-dashed), de Boer et al. (2018) (green dashed), Li et al. (2018) (black solid) and this work (red solid).# Table 4: DESI-EDR Observations of GD-1 region (data in Fig 4a)\n\n|TARGET-ID|RA [deg]|Dec [deg]|\u03c91 [deg]|\u03c92 [deg]|VGSR [km s-1]|V err [km s-1]|[Fe/H] [dex]|[Fe/H] err [dex]|\n|---|---|---|---|---|---|---|---|---|\n|39633374|192.76|58.91|-3.70|0.49|125.44|1.91|-1.92|0.05|\n|39633362|191.93|57.92|-4.44|-0.29|113.64|1.49|-1.50|0.036|\n|39633377|191.69|59.30|-4.10|1.03|130.81|1.74|-1.75|0.038|\n|. . .|. . .|. . .|. . .|. . .|. . .|. . .|. . .|. . .|\n|. . .|. . .|. . .|. . .|. . .|. . .|. . .|. . .|. . .|",
        "context_id": 45,
        "question": "How many new spectroscopically confirmed members of the GD-1 stream were obtained with the Dark Energy Spectroscopic Instrument (DESI)?",
        "answer": [
            "126"
        ],
        "context_length": 65865
    },
    {
        "context": "# 1. INTRODUCTION\n\nThe early-life environment is crucial in shaping individuals\u2019 outcomes, with potentially life-long, irreversible impacts in older age. The existing \u201cDevelopmental Origins\u201d literature has mostly focused on the long-term impacts of adverse nutritional, health and economic conditions during the prenatal and early childhood period (for reviews, see e.g., Almond and Currie, 2011; Almond et al., 2018). There is relatively little empirical evidence however, on the prolonged and cumulative consequences of early-life pollution exposure, despite the well-known adverse contemporaneous effects on infants\u2019 health (see e.g., Graff Zivin and Neidell, 2013, for a review). One of the main reasons for this is a lack of high-quality historical pollution data. Evidence on the long-term effects however, is important, since ignoring these substantially underestimates the total welfare effects caused by exposure to environmental toxins.\n\nThis paper addresses this directly, estimating the immediate and long-term impacts of a UK pollution reduction programme introduced in the mid 1950s. We deal with the general lack of historical pollution data by digitising local monthly measurements of black smoke and sulphur dioxide for a 20 year period, covering 1954 to 1973. We then exploit the staggered introduction of so-called \u201cSmoke Control Areas\u201d (SCAs) \u2013 i.e., zones introduced by local authorities that banned smoke emissions from residential as well as non-residential dwellings.\n\nWe study both (i) the effect of the introduction of SCAs on local pollution levels, and (ii) their (long-term) impact on individuals\u2019 human capital and health. The former relies on our digitization of historical monthly pollution measurements. The latter uses UK Biobank data: a large population-based cohort of approximately 500,000 individuals living in the United Kingdom for whom we observe their year-month of birth, as well as their eastings and northings of birth. The data also include rich information on individuals\u2019 later life health and economic outcomes, linked to administrative records. We focus on those born in England and merge in the exact location, boundary, and month of introduction of all SCAs introduced in English County Boroughs (CBs), obtained from Fukushima (2021). CBs are local authorities with administrative autonomy due to their population size or historical significance (i.e., relatively urban areas). Our identification# Immediate and Long-term Impacts\n\nWe then examine the immediate as well as long-term impacts of the introduction of Smoke Control Areas on individuals. Whilst there is a large literature on the contemporaneous effects of pollution on children\u2019s birth outcomes, we are aware of only a handful of papers that empirically examine the causal impacts of early-life pollution exposure on outcomes in older age.1 All of these exploit the 1952 London smog as an exogenous pollution event and show that those exposed to the smog are less likely to have a degree, work fewer hours, have lower fluid intelligence and are more likely to have developed respiratory disease (Bharadwaj et al., 2016; Ball, 2018; von Hinke and).\n\nThere is also an increasing literature studying the effects of pollution on outcomes in childhood or early adulthood, showing negative impacts on e.g., human capital formation, labour force participation and wages (Sanders, 2012; Isen et al., 2017; Bharadwaj et al., 2017; Persico, 2020; Persico and Venator, 2021; Heissel et al., 2022).\n\n1# Impact of Large-Scale Pollution Reduction Programme\n\nCompared to this literature that exploits a single extreme event, we investigate the impact of a large-scale pollution reduction programme, the aim of which was to improve air quality and with that, public health. This is similar to Isen et al. (2017), who exploit the introduction of the 1970 Clean Air Act Amendments that forced US counties with pollution levels that exceeded maximum concentrations to reduce their emissions but left others unaffected.# Footnote\n\n2 The vast majority (>98%) of particles emitted from coal combustion are smaller than 2.5 micrometers in diameter (also referred to as PM 2.5); sufficiently small to penetrate the lung system and reach the blood circulation and is therefore considered particularly harmful. Black Smoke consists of fine particulate matter and is emitted mainly from fuel combustion. Since pollution from road traffic was still minimal at the time, the vast majority of PM2.5 would have come from coal.are suggestive of some heterogeneity by SES, with these outcomes more likely to improve in high SES areas. These findings are robust to a host of sensitivity checks, but they are in contrast with the studies above that find long-term deteriorations of human capital in response to pollution exposure.\n\nWe highlight two possible reasons for this. First, rather than exploring a huge but transitory pollution spike (i.e., the London smog), we examine the long-term consequences of smaller but persistent changes in pollution exposure. It is possible that such different patterns of exposure differentially impact individuals\u2019 short as well as longer-term outcomes. Second, the different estimates may reflect differences in exposure to specific pollutants. Indeed, the 1952 London smog dispersed a range of toxins, including tar, carbon monoxide, carbon dioxide and sulphuric acid (Wilkins, 1954). In contrast, the introduction of SCAs targeted black smoke only, since a reduction in gaseous pollutants was not considered feasible at the time. The evidence suggests that the main change in moving from bituminous coal to smokeless fuel is a drop in particulate matter, with no changes in e.g., nitrogen oxides (including nitrogen dioxide), sulphur oxides (including sulphur dioxide) and carbon monoxide (Mitchell et al., 2016). Indeed, one of the strengths of this study is that we directly estimate this \u201cfirst stage\u201d, showing clear reductions in black smoke concentrations but not sulphur dioxide. Our findings therefore suggest that pollutants other than black smoke may be responsible for the adverse effects on human capital, whereas black smoke affects fetal and child growth.\n\nWe explore three sources of heterogeneity in the estimated impact of SCAs. In addition to heterogeneity by gender and SES, we examine whether the impact of SCAs varies by individuals\u2019 genetic \u201cendowments\u201d, as measured by one\u2019s polygenic score (also known as a polygenic index) that is specific to the outcome of interest. We consider genetic heterogeneity for three reasons. First, we are interested in whether the significant public health investment in smoke control affected genetic inequalities. Gene-by-environment analysis allows us to explore this empirically. Although we cannot change our genetic make-up, we can change the environment. Hence, understanding the extent to which (local) government policies affect population subgroups differently informs us about potential impacts on inequalities in relevant (health and economic) outcomes.ing gene-by-environment interplay helps us improve our understanding of the health and human capital production function. This literature emphasizes the role of complementarities between endowments and investments, suggesting that individuals with higher endowments benefit more from subsequent investments (Becker and Tomes, 1986; Cunha and Heckman, 2007). Muslimova et al. (2020) suggest the use of genetic information to capture such endowments, exploring its interaction with (a proxy for) exogenous parental investments.3 We follow this literature, but instead examine the interplay with public health investments. As such, positive gene-by-environment interactions are consistent with such complementarities. Third, finding evidence of gene-by-environment interplay provides evidence against arguments of genetic or environmental determinism. This in turn is crucial in the debate about whether one\u2019s success in life is due to efforts or circumstance (see e.g., Roemer, 1993; Roemer, 1996).\n\nOur genetic heterogeneity analyses provide suggestive evidence that the introduction of SCAs increased inequalities in population health but not in economic outcomes. Indeed, we find that the introduction of SCAs led to larger increases in birth weight and height among those with a high polygenic score for these outcomes. These results are consistent with the existence of complementarities in the health production function (see also van den Berg et al., 2023), but not the human capital production function (as in Muslimova et al., 2020).\n\nThe rest of the paper is structured as follows. Section 2 provides the background to roll-out of Smoke Control Areas in England and Section 3 describes the data sources used in our analyses. We set out the empirical strategy in Section 4, and discuss the results in Section 5. We explore the sensitivity of our findings in Section 6 and conclude in Section 7.# 2. BACKGROUND\n\nOne of the oldest accounts on the impact of smoke on health was addressed to King Charles II in 1661 in a treatise called \u201cFumifugium; or The Inconvenience of the Aer and Smoake of London.\n\nSuch complementarity analysis would ideally use parent-child trio or sibling data (as in Muslimova et al., 2020) to allow one to also isolate exogenous endowments. However, although there are around 40,000 siblings in the UK Biobank, our analysis sample only contains about 160 sibling pairs, making within-sibling analysis infeasible.# Impact of Smoke Pollution in London\n\nDissipated\u201d (Evelyn, 1661). This suggested that smoke pollution shortened the lives of Londoners. Despite this, the industrial revolution, the massive migration of workers to urban areas, and the overall increase in population meant that the smoke problem would continue to escalate for many centuries. Only after the Great London Smog in December 1952, which brought premature death to thousands of citizens, lawmakers and the public became fully aware of the potential damage of smoke, which led to the swift passing of the Clean Air Act in 1956 (Clean Air Act 1956).# Clean Air Act of 1956\n\nBefore the Clean Air Act, the UK\u2019s primary source of air pollution was emission from burning bituminous coal, with coal fires being the predominant form of heating in most dwellings far into the 1960s. The 1956 Clean Air Act consisted of two main parts. In the first part, the Act prohibits the emission of dark smoke from all buildings. The Act defines smoke as fly ash, grit, and gritty particles, and the shade of the smoke was determined by comparing the colour of the smoke against the Ringelmann Chart. The fine for breaching the law was a maximum of ten pounds for a private dwelling and 100 pounds for all other buildings. While the regulation contributed to improving the air quality in the nation, many exemptions to the law and generous lead times of up to seven years for industries hampered its full potential.# Smoke Control Areas (SCAs)\n\nIn contrast, the second part of the law gave local authorities across the UK the mandate to introduce so-called Smoke Control Areas (SCAs), which banned any smoke emissions inside the area. At the start, the fine for violating a smoke control order carried a penalty of up to ten pounds per offense which increased to 20 pounds in the revised 1968 Clean Air Act. It is important to note that, while the Act regulates the emission of smoke, it does not target gaseous pollutants that are present in coal such as sulfur dioxide (SO2). In fact, the reason for not targeting SO2 emissions was that its elimination was thought unattainable at the time since SO2 is equally present in bituminous and smokeless fuel.# Footnote\n\nSO2 is formed by the oxidation of sulfur in fuel combustion. It can cause direct harm to health by damaging the lung capacity and indirectly as a secondary particulate matter when reacting to other airborne particulate matters.operation date, and announce it to the public in a local newspaper as well as in the London Gazette. 5\n\nThe \u2018submission date\u2019 refers to the date on which the proposal was submitted for review to the Ministry of Housing and Local Government, usually leading to the Ministry confirming, perhaps with adjustments, the agreed date of operation and boundary for the proposed SCA. The SCA would then come into effect at the \u2018operation date\u2019, after which the monitoring and enforcement measures would start.\n\nTo comply with a smoke control order, the dwelling owner could substitute bituminous coal for (manufactured) smokeless fuel such as anthracite or gas (if available). For older dwellings, this typically required owners to adapt their appliances to new fuel types; 70% of the costs associated with such conversions were reimbursed by the local council, as long as the adjustment work was done before the \u2018operation date\u2019 (National Society for Clean Air, 1958; National Society for Clean Air, 1960). The local council would in turn receive a 40% (four-seventh of 70%) contribution from the exchequer to cover these additional costs; the other 30% was borne by the council. The total costs for these adaptations were not negligible, highlighting the willingness of local authorities and central government to control pollution levels despite significant expenses associated with them.6# 3.1. Smoke Control Areas\n\nWe use the data from Fukushima (2021) that record the exact location, boundaries and year-month of submission and operation for all 1,027 smoke control areas that were introduced in so-called County Boroughs (CBs) across England by 1973. These data come from multiple sources: local historical archives, notices in historical editions of the London Gazette, relevant Medical Officer of Health reports, as well as historical issues of \u201cSmokeless Air\u201d; a quarterly publication by the National Smoke Abatement Society (see, e.g., National Society for Clean Air, 1958). We restrict our attention to SCAs in CBs only, as these are predominantly residential areas. Indeed, although they only make up \u21923% of the total land area, they cover over a third of the English population (35% according to the 1951 Census).\n\nWe further collect data on the universe of SCAs (rather than those in CBs only) introduced from September 1958 to December 1973. These are reported by National Smoke Abatement Society, but they are only available at a more aggregate level (year-quarter rather than year-month). More importantly, they do not provide SCA boundaries, meaning we cannot pinpoint their exact location and shape within a CB. Our main empirical analysis therefore focuses on the exact boundaries and year-month of submission and operation of SCAs in County Boroughs only.7 The more aggregated data however, are useful for descriptive purposes, showing the extent to which local governments engaged with the new legislation and introduced areas of smoke control. Indeed, Figure 1 uses these data (Baker et al., 2024b) to present the number of submitted SCAs in England by year.8 This shows a steady increase in the number of SCAs during this time, reaching over 4,000 by 1973. Figure A.1 in Appendix A plots the spatial distribution of SCAs across England in 1973. Although any local authority could submit an application for a new SCA, the figure shows that\n\n7 We use the information on the universe of SCAs in our robustness analysis to identify geographical areas outside CBs that never introduced any SCA within our period of interest; see Section D.1.\n\n8 Although the first smoke control areas were introduced in 1957, shortly after the passing of the 1956 Clean Air Act, their exact \u2018milestone dates\u2019 were not recorded. We therefore only report the number of SCAs from 1958 onwards.they are concentrated in and around urban areas such as County Boroughs. While this may suggest that more densely populated areas were more likely to implement SCAs, and to do so earlier than other districts, we show in Appendix B that conditional on population density, the timing of SCA implementation across CBs is not strongly or systematically driven by predetermined district-level characteristics (i.e., socioeconomic composition), nor by pre-treatment pollution levels.# Figure 1: Smoke control areas in England by year and status.\n\n|Year|Submitted - England|Operating - England|Operating - England, CBs|\n|---|---|---|---|\n|1958|0|0|0|\n|1959|0|0|0|\n|1960|0|0|0|\n|1961|0|0|0|\n|1962|0|0|0|\n|1963|0|0|0|\n|1964|0|0|0|\n|1965|0|0|0|\n|1966|0|0|0|\n|1967|0|0|0|\n|1968|0|0|0|\n|1969|0|0|0|\n|1970|0|0|0|\n|1971|0|0|0|\n|1972|0|0|0|\n|1973|0|0|0|\n\nPlots the number of submitted and operating smoke control areas in England by year. Also shows the subset of smoke control areas operating in CBs in England, as these are the ones we use in our analysis. The totals for England have been digitised from reports by the National Smoke Abatement Society.\n\nWhen estimating the impact of the introduction of SCAs, we drop 15 out of the total 85 CBs because of significant boundary changes during our study period (e.g., total/partial splits and merges with neighbouring local authorities that make it impossible to consistently observe them over time). Of the remaining 70 CBs, 55 adopted at least one SCA before 1974, and 15 did not. We refer to these as \u201cadopting\u201d and \u201cnon-adopting\u201d CBs, respectively.9\n\n9 The following 15 CBs are dropped due to significant boundary changes: Croydon, Derby, Dudley, East Ham, Hartlepool, Middlesbrough, Smethwick, Teesside, Torbay, Walsall, Warley, West Bromwich, West Ham, West Hartlepool, Wolverhampton. Table A.2 lists the CBs remaining in our analyses by adoption status. The \u201cnon-adopting\u201d CBs include those that either never adopted a SCA or only adopted one in greenfield land areas within their jurisdictions (i.e., undeveloped land used for agriculture or landscape design).# 3.2. Weather\n\nGiven the importance of the weather for pollution dispersion as well as individuals\u2019 health (see e.g. Hanlon et al., 2021), we merge in daily data on wind speed, wind direction, precipitation, and temperature from the ERA5 reanalysis data (Hersbach et al., 2020). These have an approximate grid resolution of 25km and we assign weather measurements to SCAs, stations, and individuals by linking them to the nearest grid point. For SCAs, we link to its centroid, while we use the actual measurement and birth locations for stations and individuals, respectively.\n\nWe use these weather data in two ways. First, we directly control for weather conditions in our analyses, averaging individuals\u2019 exposure to the weather conditions during the prenatal period and the first two years of life. Second, we use the historical weather data to identify stations that are downwind from an SCA, and therefore potentially indirectly affected by its introduction. We discuss this in more detail below.# 3.3. Pollution\n\nTo construct our \u201cpollution panel\u201d, assessing the impact of the introduction of SCAs on local air pollution, we digitise six years of monthly pollution measurements and station locations from the Department of Scientific and Industrial Research (1954\u20131961).10 We combine these with measurements and station locations for 1961\u20131973 (DEFRA, 2022), allowing us to construct a panel of monthly black smoke and sulphur dioxide levels taken at measurement stations across England between 1954 and 1973. Note that this is an unbalanced panel, with fewer measurement stations at the start of the observation period and new ones being introduced over time. Indeed, we do not observe any pollution measurements in non-adopting CBs until 1962 when the first measurement.\n\n10 The pollution data from 1954-1960 comes from Fukushima (2021); the 1961 pollution data are from Baker et al. (2024a). The data were collected by the world\u2019s first coordinated national air pollution monitoring network: the UK Investigation of Atmospheric Pollution, run by the Warren Spring Laboratory. Historically, it monitored and compiled data on two main pollutants: black smoke and sulfur dioxide. Black smoke was measured using smoke samplers, drawing 50 cubic meters of air through a white filter paper over 24 hours. The density of the deposit was then assessed using a reflectometer. Sulphur dioxide was measured by drawing the same sample of air through a chemical solution that reacts with sulphur dioxide to form sulphuric acid. The measured acidity of the solution was then used to approximate the concentration of sulphur dioxide in the air sample. Loader (2002) gives a detailed description of these processes in the context of the monitoring network.# Pollution Stations and Their Classification\n\nWe assign pollution stations to SCAs as well as CBs by projecting the stations\u2019 locations onto our digitised SCA boundaries and the 1971 CB boundaries (University of Portsmouth, 2011; Southall, 2011), respectively. We define stations located inside a SCA as \u2018treated\u2019, and stations in non-adopting CBs as \u2018never-treated\u2019 (controls). When a CB is only partly covered by one (or more) SCA, we add stations that are inside the CB but outside the SCA boundaries to the group of \u2018never-treated\u2019 stations. We show the robustness of our results to dropping these altogether in Section 6 below.# Data Restrictions\n\nStarting from 160,833 station-year-month observations for black smoke and 156,395 for sulphur dioxide, we restrict our pollution panel as follows. First, since our SCA boundary data are restricted to stations located in CBs, we drop those located outside CBs, reducing our sample to 52,234 and 50,803 observations for black smoke and sulphur dioxide respectively. Second, we include only stations in adopting or non-adopting CBs, dropping those with border irregularities (see Section 3.1). Furthermore, since pollution is not measured in any of the non-adopting CBs until after 1961, the pollution panel covers 1962 onwards, leaving us with 35,145 station-year-month observations (401 stations) for black smoke and 34,951 station-year-month observations (399 stations) for sulphur dioxide.# Avg. concentration, mcg / m3\n\nsuggesting that the adoption of smoke control areas had a smaller impact on levels of SO2.# Figure 2: Historical measurements of pollution (black smoke and SO2) by station treatment status.\n\n|Pollutant|Control|Control|Treated|Treated| | |\n|---|---|---|---|---|\n| |Black smoke| | |Sulphur dioxide|\n|500| | | | |\n|400| | | | |\n|300| | | | |\n|200| | | | |\n|100| | | | |\n|0| | | | |\n\nHistorical measurements of pollution (black smoke and SO2) from 1962 to 1973 averaged over stations by treatment status and year-month. Average for treated stations in grey and average for control stations in black. Dashed lines show long term trends by group using locally weighted polynomial regression (LOESS).\n\nIn addition to defining treated pollution measurement stations as those located inside a SCA, we also construct an indicator for stations that are downwind of a SCA. Using the historical wind direction data from ERA5, we construct vectors of SCAs\u2019 prevailing wind directions in the two years prior to their submission, and use them to simulate the boundaries of pollution dispersion from the SCAs. We then classify as downwind any stations that are located within the dispersion boundary but outside the boundary of the originating SCA. We detail this procedure, and explore the robustness of our estimates to alternative approaches, in Appendix C.# 3.4. Individuals\n\nTo examine the long-term impact of the introduction of SCAs on individuals\u2019 human capital and health outcomes, we use the UK Biobank: a large prospective, population-based cohort living in the United Kingdom. Baseline information on approximately 500,000 individuals was collected between 2006 and 2010, when they were 40\u201369 years old. Participants are born between 1938.\n\n13and 1971, with the majority in the 1940s to mid 1960s. The UK Biobank is not representative, with individuals being on average healthier and wealthier than the general UK population (Fry et al., 2017). The data include detailed information on demographics, physical and mental health, cognition, and economic outcomes, obtained via interviews, questionnaires, and measurements taken by nurses. The data have also been linked to hospital and mortality records.\n\nWe use individuals\u2019 location of birth (i.e., eastings and northings) to assign them to SCAs and CBs by projecting their birth locations onto our digitised SCA boundaries and the 1971 CB boundaries, respectively. We classify individuals born in CBs that never introduced an SCA as \u2018never-treated\u2019 (controls), and those born inside SCA boundaries as treated, depending on their dates of birth. Analogous to the sample construction for the pollution panel, when a CB is only partially covered by one (or more) SCAs, we define individuals born inside the CB but outside the SCA boundaries as never treated and explore the robustness of our results to dropping the latter group altogether.\n\nTo study the consequences of smoke control on human capital and health production, we focus on four broad outcomes motivated by the existing literature on the early-life pollution environment. First, building on the literature on pollution exposure and individuals\u2019 health (see e.g., Currie and Walker, 2011), we rely on birth weight and adult height as two general indicators of early- and later-life health and development, allowing us to examine both the short and long-term impacts of pollution along these dimensions. Second, following the literature on the effects of pollution exposure and human capital production (see e.g., Isen et al., 2017; Ball, 2018; Persico, 2020; von Hinke and S\u00f8rensen, 2023), we explore individuals\u2019 years of education and fluid intelligence.\n\nOur measure of birth weight is self-reported and therefore likely to be measured with error, while adult height was measured by a nurse following a standardised protocol.11 We define years of education using individuals\u2019 qualifications, and measure fluid intelligence using a battery of questions designed to measure logic and reasoning ability, independent of acquired knowledge.\n\n11Despite birth weight being self-reported at a later age, it has been shown to correlate with a range of covariates in the expected direction (e.g., with non-singleton pregnancies, gender, maternal smoking) and it has good reliability (Tyrrell et al., 2013; Zhang et al., 2021).We standardise the latter with mean zero and standard deviation one.12\n\nStarting with 444,707 individuals with non-missing data on birth location and year-month of birth, we restrict our estimation sample as follows. First, we drop individuals who are born before September 1957, as this coincides with the UK education reform that raised the minimum school leaving age, affecting individuals\u2019 education, income and potentially health outcomes (Harmon and Walker, 1995; Clark and Royer, 2013; Davies et al., 2018).13 We also drop the last two birth cohorts born in 1970 and 1971 as they are more selected and very small compared to earlier cohorts (van den Berg et al., 2023). This leaves us with 113,704 participants. Second, since we observe exact Smoke Control Area boundaries in County Boroughs only, we restrict our sample to individuals born in adopting or non-adopting CBs, reducing our sample to 41,329 individuals. Third, we restrict our sample to individuals with a precisely measured birth location, leaving 16,573 individuals.14 Our final sample size then ranges between 5,749 and 16,535 depending on the number of missing values for our outcome of interest. In the robustness analysis, we also use an auxiliary sample where we take our data on the universe of SCAs to identify individuals born in districts that never introduced any SCA and include these participants in the never-treated group, increasing the sample size to between 9,868 and 25,410 individuals, depending on the frequency of missing data on the outcome.# 1. Descriptive Statistics\n\nPanel A of Table 1 presents the individual-level descriptive statistics from the UK Biobank, showing that approximately 45% of the sample is male, and individuals have just over 13 years of schooling, on average. The average birth weight is 3.3 kg, with an average height of 170 cm. We do not find strong differences between the treated and control groups, defined as individuals born.\n\n12 Table A.1 shows how we map qualifications to years of education using a definition similar to that of, e.g., Rietveld et al. (2013) and Okbay et al. (2022).\n\n13 Note that although the pollution analysis is restricted to the years 1962 onwards due to the absence of pollution data in never-treated CBs pre-1962, we do not apply this restriction to the individual-level analysis. Indeed, we obtain intention-to-treat estimates for the latter, examining the impact of the introduction of SCAs, as opposed to estimating the direct effect of a reduction in pollution. Hence, we use those born between September 1957 and the end of 1969 for the individual-level analysis.\n\n14 UK Biobank participants who could not report their birth location with any precision (e.g., areas within a larger town or city) were assigned a catch-all location in the approximate centre of the town or city. We drop these individuals since we rely on relatively precise birth location reports, e.g., distinguishing between two individuals born in the same CB but one inside and the other outside an SCA. However, our results are robust to including them in our estimation sample (see Section D.2). Note that birth locations are reported with 1km resolution.# Effects from Neighbouring SCAs\n\nWe also run our analysis dropping these units altogether and find very similar results.# Event of Interest\n\nOur \u2018event\u2019 of interest is the date on which SCAs are submitted to the Ministry. We focus on this date for two reasons. First, the proposed operation date was published at the time of submission, indicating when the area is most likely to become smokeless. Second, local councils reimbursed costs associated with stove conversions only if they were incurred before the operation date (see Section 2). Hence, households started requesting conversions immediately after the submission date.# Identification Assumptions\n\nOur identification assumes that, conditional on controls and fixed effects, the timing of the introduction of smoke control areas was random, and that the areas selected to be smokeless were not systematically different from those not selected. We explore both of these assumptions in detail in Appendix B.# Pre-Programme Characteristics\n\nFirst, we explore associations between pre-programme CB-level characteristics (i.e., population density, pollution levels, and socioeconomic composition) and the timing of SCA implementation, showing that apart from population density, these variables do not explain differences in the timing of SCA introduction, suggesting that the timing was largely random.# Systematic Differences\n\nSecond, we investigate whether stations and individuals inside SCAs were systematically different from those outside SCAs, but within the same CB. Although Figure 2 shows that treated stations on average report higher levels of pollution (i.e., between-CBs), we find that areas that are selected to become smokeless were slightly less polluted compared to those not selected within the same CB. Furthermore, our results suggest that residents in areas that were put under smoke control were of higher socio-economic status compared to those in control areas.\n\nTo alleviate concerns about these differences driving our findings, we include station/area fixed effects as well as station/area-specific trends in our analyses, accounting for systematic differences between areas that did and did not become smokeless within CBs.\n\n17# 4.1. Impact on pollution\n\nTo estimate the impact of the introduction of SCAs on local pollution levels, we estimate a two-way fixed effects model of the form:\n\nyst = \u03c9s + \u03b5t + \u03d1st + Dst + \u03d6st (1)\n\nwhere yst denotes the average pollution measurement (i.e., black smoke or sulphur dioxide) at station s in year-month t. The parameters \u03c9s and \u03b5t denote station and year-month fixed effects to account for systematic variation in pollution (or population density; see Appendix B) between stations, across time and for seasonality, and \u03d1t denotes station-specific linear trends. We consider three specifications of the term Dst. First, a dynamic event study specification, where Dst = \u2211\u03d1\u2191T \u03f1\u03d1 SCA\u03d1, and SCAst is an indicator denoting whether station s in year-month t is \u03d1 months away from being treated (i.e., submitted to the Ministry) and where the month before submission (\u03d1 = \u21931) is the reference month.15\n\nSecond, a static difference-in-difference approach, where Dst = \u03f1Adj Insides \u2194 Adjst + \u03f1Post Insides \u2194 Postst. We define Insides to be an indicator that is equal to one if the location of the measurement station is inside the exact SCA boundary, and zero otherwise.16 Adjst is a dummy that is equal to one when the SCA in which pollution station s is located has been submitted to the Ministry, but not yet entered operation (we refer to this as the \u2018adjustment period\u2019), and zero otherwise. Its coefficient \u03f1Adj captures any immediate drops in pollution levels following submission of the SCA as well as gradual reductions in pollution in the period between submission and operation when appliances were being upgraded to allow for smokeless fuel. Postst is a dummy equal to one when the SCA in which pollution station s is located started operating, and zero otherwise. Its coefficient \u03f1Post then captures the overall impact of a fully operating SCA on local pollution levels, relative to other stations that are not located within an SCA. This specification allows for differential impacts.\n\n15 We trim our sample to five years (60 months) before and after treatment for each station, though our results are robust to not trimming the sample or trimming more/fewer years (see Section D.3).\n\n16 Note that Insides only enters our specification interacted with other dummies, as the main term would be absorbed into the station fixed effects that we include in all specifications.of the adjustment period and that post-operation. Many CBs introduced multiple SCAs at different points in time. Although a station can only be within one SCA, they can be downwind of multiple SCAs given that pollution disperses along the wind direction vector. To investigate the importance of such spillovers, our third specification therefore allows for downwind stations to be differentially affected such that Dst = \u03f1Adj Inside s \u2194 Adjst + \u03f1Post Inside s \u2194 Postst + \u03f1Adj,DW Downwind s \u2194 Adjst DW + \u03f1Post,DW Downwind s \u2194 Postst DW. The terms Adjst and Postst are defined in the same way as in the previous specification, and Downwinds is a dummy for the station being downwind of any SCA. The binary indicator Adjst DW (PostDW) is equal to one when the upwind SCA enters the adjustment period (becomes operational) and zero otherwise.\n\nWe estimate Equation 1 using OLS, and \u2013 considering the recent literature on staggered treatments (Sun and Abraham, 2021; Callaway and Sant\u2019Anna, 2021; Borusyak et al., forthcoming) \u2013 we also report the group-time average effects in Appendix E, finding very similar results. We cluster our standard errors by station throughout.# 4.2. Individual-level analysis\n\nTo estimate the effect of the introduction of SCAs on individual human capital and health outcomes, we follow an approach analogous to the previous section but at the individual-level:\n\nyijt = \u03c2j + \u03b5t + \u03d1jt + Dijt + \u03c9xijt + \u03d6ijt (2)\n\nwhere yijt is the outcome for individual i, born in area j in year-month t. The area is defined as [CB \u2194 Inside], allowing for differences in areas that did and did not become SCAs within a CB. Hence, \u03c2j are [CB \u2194 Inside] fixed effects, controlling for spatial variation in the outcomes of interest (and accounting for systematic differences in socio-economic composition of treated and control areas within a CB; see Appendix B), \u03b5t are year-month fixed effects controlling for differences across cohorts and seasonality, and \u03d1jt are (CB \u2194 Inside)-specific linear (yearly) time trends. The latter account for differential dynamics in outcomes across CBs, as well as within CBs across areas thatdid and did not become smokeless.17\n\nThe vector *xijt* denotes additional covariates (sex, ethnicity and weather conditions in utero and in childhood) capturing further variation within CBs.# Figure 3: Event estimates \u2013 Impact on local pollution levels.\n\n|Estimate, mcg per cubic metre| | | | | | |Black smoke| | |Sulphur dioxide| | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | |Before| | | | |Adj.|After|Before|Adj.|After|\n|20|0|-20|-40|-605448423630241812\u22126\u2212\u2212|-|-| | | | | | |\n|0|6121824303642485460|-605448423630241812\u22126\u2212\u2212|-|-|-| | | | | | | |\n\nMonths relative to submission of SCA\n\nOLS event estimates showing the impact of smoke control on individuals. OLS specification includes year-by-month and station fixed effects, and includes a station-specific yearly linear time-trend to capture differences in linear dynamics between stations. Control group consists of never-treated stations from both adopting and non-adopting county boroughs.\n\nTrims the sample to 5 years before and after the SCA submission date, and restricts the sample to pollution data for years 1962 to 1973. The dashed red lines indicate the average of the pre- and post-adjustment period estimates, where the former are averaged to zero. The grey part refers to the adjustment period. Clusters standard errors by station.\n\nEquation 1 with a binary indicator for the pollution station being inside an SCA. Table 2 reports the OLS estimates of the introduction of SCAs on the local levels of black smoke (Columns 1\u20132) and sulphur dioxide (Columns 3\u20134) aggregated across all post-periods. Note here that the variable Post st is specific to the measurement station (rather than taking the average duration of the adjustment period of 16 months that is shown in grey in Figure 3). Columns 1 and 3 show that the submission of SCAs caused black smoke concentrations to drop by 8 mcg/m3 on average, with no significant changes in levels of sulfur dioxide. Given mean black smoke levels of 104 mcg/m3 prior to the introduction of SCAs, this is an \u21928% reduction. Black smoke concentrations drop by approximately 19 mcg/m3 (18%) once the SCA is in operation. Distinguishing between pollution measurements that are taken inside SCAs and those downwind, Column 2 shows that the treatment effect for black smoke is primarily driven by the former, with no significant reductions in pollution.# Table 2: Difference-in-difference estimates \u2013 Impact on pollution.\n\n|Depedent variable:|(1) Black smoke|(2) Black smoke|(3) Sulphur dioxide|(4) Sulphur dioxide|\n|---|---|---|---|---|\n|Inside \u2194 Adj.|\u21938.053 \u2197\u2197\u2197|\u21937.866 \u2197\u2197\u2197|\u21931.180|\u21931.148|\n| |(3.010)|(3.031)|(3.740)|(3.747)|\n|Inside \u2194 Post|\u219319.737 \u2197\u2197\u2197|\u219319.811 \u2197\u2197\u2197|\u21933.480|\u21933.677|\n| |(4.380)|(4.425)|(5.337)|(5.347)|\n|Downwind \u2194 Adj.| |\u21931.032| |\u21936.245|\n| | |(3.727)| |(4.743)|\n|Downwind \u2194 Post| |\u21938.225| |\u21937.317|\n| | |(5.631)| |(6.441)|\n|Observations|26,302|26,302|26,195|26,195|\n|Mean dep. var.|103.767|103.767|132.721|132.721|\n|R2|0.81|0.81|0.79|0.79|\n\nColumns: (1-2) level of black smoke, (3-4) level of sulphur dioxide.\n\nControl group consists of never-treated from both adopting and non-adopting county boroughs. Clusters standard errors by station. Includes year-by-month and station fixed effects, and includes a station-specific yearly linear time-trend to capture differences in linear dynamics between stations. Trims the sample to 5 years before and after the SCA submission date, and restricts the sample to pollution data for years 1962 to 1973. (*): p &lt; 0.1, (**): p &lt; 0.05, (***): p &lt; 0.01.\n\nfor downwind areas, though the sign of both estimates is negative.21\n\n21 Table A.3 in Appendix A explores the robustness of these findings by specifying alternative treatment variables: (1) the number of square kilometres (km2) of SCA surrounding a pollution station and the number of km2 of SCA upwind of the station, and (2) the sum of the two. These specifications are likely to be less precise, since the SCA km2 is not necessarily a good proxy for the number of dwellings affected (and therefore of actual pollution exposure).2 While the estimates have large standard errors and are not statistically significant, they suggest that an additional km of SCA is negatively related to black smoke concentrations, with more mixed results for sulphur dioxide.\n\n23# 5.2. Impact on individuals\n\nWe next examine how the introduction of smoke control areas and the subsequent reduction in local pollution translates into individual outcomes by plotting the dynamic event study estimates of \u03f1\u03d1 from Equation 2. Figure 4 shows the impact of smoke control areas on individuals\u2019 birth weight and adult height, while Figure 5 reports the estimates for years of schooling and (standardised) fluid intelligence. The red dashed line before and after the adjustment period denotes the average of the pre- and post-adjustment period estimates.# Figure 4: Event estimates \u2013 Impact on individuals.\n\n|Birth weight (kg)|Conceived before|Adj.|Conceived after| | | | |\n|---|---|---|---|---|---|---|---|\n|0.2|0.1|0.0| | | | | |\n|Height (cm)|Conceived before|Adj.|Conceived after| | | | |\n| |2|1|0| | | | |\n\nOLS event estimates showing the impact of smoke control on individuals. OLS specification includes year-by-month and (CB \u2194 Inside) fixed effects, and includes (CB \u2194 Inside)-specific linear time trends. Control group consists of never-treated individuals from both adopting and non-adopting county boroughs. Trims the sample to 5 years before and after the SCA submission date, and restricts the sample to birth cohorts in years 1958 to 1969. The dashed red lines indicate the average of the pre- and post-adjustment period estimates, where the former are averaged to zero. The grey part refers to the adjustment period. Clusters standard errors by CB.\n\nFigure 4 shows no evidence of trends in birth weight or adult height for individuals conceived prior to the SCA submission date. This is expected for birth weight, since these children were only exposed to the SCA in childhood and birth weight cannot be affected by changes in pollution after birth. Instead, we find that the birth weights of individuals conceived after the SCA operation.# Table 3: Difference-in-difference estimates \u2013 Impact on individuals.\n\n|Dependent variable:|(1) Birth weight|(2) Adult height|(3) Educ. attain.|(4) Fluid intelligence|\n|---|---|---|---|---|\n|Inside \u2194 Adj.|0.039|0.365|0.040|\u21930.152|\n| |(0.034)|(0.343)|(0.178)|(0.131)|\n|Inside \u2194 Post|0.058\u2197\u2197|0.942\u2197\u2197\u2197|\u21930.025|0.035|\n| |(0.027)|(0.180)|(0.135)|(0.065)|\n|Observations|8,510|11,922|11,689|4,106|\n|Mean dep. var.|3.317|169.917|13.236|0.000|\n|R\u00b2|0.066|0.543|0.107|0.149|\n\nColumns: (1) birth weight in kilograms, (2) adult height in centimeters, (3) years of education, (4) standardised fluid intelligence score. OLS specification includes year-by-month and (CB \u2194 Inside) fixed effects, and includes (CB \u2194 Inside)-specific linear time trends. Controls for sex, ethnicity, and weather in utero and during childhood. Control group consists of never-treated individuals from both adopting and non-adopting county boroughs. Trims the sample to five years before and after the SCA submission date, and restricts the sample to birth cohorts in years 1958 to 1969. Clusters standard errors by CB. (*): p < 0.1, (**): p < 0.05, (***): p < 0.01.\n\nIn Table A.4 we show the impact on the probability of being born with low birth weight (< 2, 500g). While these point estimates are negative, suggesting slight reductions in this risk of about 2 percentage points, they are not significant. This suggests that the impact we find on birth weight is not driven specifically by improvements among low birth weight individuals, but rather a general shift of the birth weight distribution.\n\nTable A.5 decomposes the total years of education into binary indicators for specific qualifications. This shows suggestive evidence that the introduction of SCAs decreased the probability of exiting the education system with lower secondary qualifications, while it increased the probability of obtaining an upper secondary degree, though the latter is not significant.men and women. On the one hand, if a reduction in pollution has a larger impact on male infant mortality due to them being weaker on average, it may increase the probability of survival of weaker boys, leading to a drop in average male birth weights. On the other hand, holding the survival rate constant, a reduction in pollution may help vulnerable populations more, increasing male birth weights.# 4. Gender Differences in Birth Weights\n\nTable 4 shows the estimates when we split the sample by gender. We find that the introduction of SCAs benefits males\u2019 birth weights more than females\u2019, with the former seeing an increase of 126 grams on average, while females see a modest rise of 21 grams.24 We find no strong gender differences for adult height with both genders experiencing an average increase of about 1 cm. Similarly, and consistent with the main analyses, we do not find strong evidence of impacts on educational attainment and fluid intelligence.\n\n24 This is consistent with the above example that holds survival rates constant. To explore this empirically, we examine the impact of smoke control on the probability of being female, shown in Table A.6. These results indicate that there are no gender-specific mortality impacts, and therefore provide suggestive evidence that the gender-specific estimates for our main outcomes are not driven by mortality selection. One plausible explanation for the bigger increase in male birth weights is therefore that the reduction in pollution disproportionally helps male (as opposed to female) growth.\n\n27# 5.2.2. Heterogeneity by individuals\u2019 genetic \u201cendowments\u201d\n\nNext, we examine heterogeneity by individuals\u2019 genetic \u201cendowments\u201d, as measured by one\u2019s polygenic score (or polygenic index) that is specific to the outcome of interest. As highlighted in the introduction, this analysis not only sheds light on the impact of smoke control on population inequalities, but is also informative about the existence of complementarities between endowments and public health investments and contributes to the debate about the importance of effort and\n\n28circumstance in shaping individuals\u2019 outcomes. We construct proxies for individuals\u2019 genetic \u201cendowment\u201d by running a Genome-Wide Association Study (GWAS) for each of our main outcomes in the UK Biobank, using only individuals who are not in (or related to individuals in) our main analysis sample. We then use these summary statistics to construct polygenic scores (PGS) for each outcome for the individuals in our analysis sample. We standardise all polygenic scores to have zero mean and unit variance in the analysis sample. We discuss the polygenic score construction in more detail in Appendix F, and report their predictive power in Table F.2.# Table 5: Difference-in-difference estimates \u2013 Impact on individuals.\n\n|Dependent variable:|(1) Birth weight|(2) Adult height|(3) Educ. attain.|(4) Fluid intelligence|\n|---|---|---|---|---|\n|Inside \u2194 Adj.|0.039|0.561 \u2197|0.050|\u21930.114|\n| |(0.032)|(0.294)|(0.152)|(0.108)|\n|Inside \u2194 Post|0.044\u2197|0.778\u2197\u2197\u2197|\u21930.011|0.071|\n| |(0.025)|(0.221)|(0.103)|(0.077)\u2197|\n|Inside \u2194 Adj. \u2194 PGS|\u21930.017|0.574\u2197\u2197|\u21930.158\u2197|0.106|\n| |(0.040)|(0.250)|(0.088)|(0.056)|\n|Inside \u2194 Post \u2194 PGS|0.043\u2197|\u21930.029|0.033|0.034|\n| |(0.025)|(0.168)|(0.055)|(0.060)\u2197\u2197\u2197|\n|PGS|0.102\u2197\u2197\u2197|3.706\u2197\u2197\u2197|0.562\u2197\u2197\u2197|0.236|\n| |(0.008)|(0.056)|(0.020)|(0.020)|\n|Observations|7,982|11,108|11,056|3,825|\n|Mean dep. var.|3.323|170.052|13.234|0.000|\n|R2|0.1|0.702|0.177|0.2|\n\nColumns: (1) birth weight in kilograms, (2) adult height in centimeters, (3) years of education, (4) standardised fluid intelligence score. OLS specification includes year-by-month and (CB \u2194 Inside) fixed effects, and includes (CB \u2194 Inside)-specific linear time trends. Controls for sex, genetic principal components, and weather in utero and during childhood. All covariates have been normalised to mean zero. Control group consists of never-treated individuals from both adopting and non-adopting county boroughs. Trims the sample to five years before and after the SCA submission date, and restricts the sample to birth cohorts in years 1958 to 1969. Clusters standard errors by CB. (*): p &lt; 0.1, (**): p &lt; 0.05, (***): p &lt; 0.01.\n\nFor each outcome, we add interactions between the two variables of interest and the PGS, capturing the extent to which the introduction of SCAs differentially affected individuals with different genetic \u201cendowments\u201d.25 Table 5 reports the results. We find that the impact of smoke control on\n\n25 Following Keller (2014), we also add interactions between the PGS and all covariates and principal components.# 5.2.3. Heterogeneity by socioeconomic status\n\nFinally, we examine whether the policy had differential impacts on individuals depending on the socio-economic composition of their local area. Since the UK Biobank does not have information on the socio-economic status of participants (or their parents), we merge district-level information on occupation from the UK Census to the UK Biobank to classify CBs as either high or low SES. To do this, we calculate the share of CB residents that are in professional, managerial, or technical occupations, and define CBs as high SES if they are above the median share of this distribution across all CBs, or low if they are below the median.\n\nWe estimate our main specification with additional interactions between the variables of interest and the dummy for being born in a low SES area. The results are presented in Table 6. We do not find strong evidence of differential effects by SES for birth weight and height. In contrast, the results suggest that high SES areas experienced an increase in education after SCA submission, with no impacts for lower SES areas, or even a reduction in intelligence relative to those born in higher SES neighbourhoods.\n\nbirth weight is larger for individuals with a high genetic endowment for birth weight, suggesting that this local government pollution reduction programme exacerbated genetic inequalities in birth weight. It is also consistent with the existence of complementarities between endowments and (public health) investments in producing child health: the returns to the investment are larger for those with higher endowments. For height, we see some evidence of complementarities during the adjustment period, but little evidence of heterogeneity in the effect size post SCA operation. Similar to the main analysis, we find no main effect on years of education and intelligence, though some evidence of genetic heterogeneity during the SCA adjustment period. Although these are of opposite sign for education and intelligence, they are only marginally significant, with no impacts post-SCA operation. We are therefore cautious not to overinterpret these.# Table 6: Difference-in-difference estimates \u2013 Impact on individuals.\n\n|Dependent variable:|(1) Birth weight|(2) Adult height|(3) Educ. attain.|(4) Fluid intelligence|\n|---|---|---|---|---|\n|Inside \u2194 Adj.|0.059|\u21930.054|0.304 \u2197\u2197\u2197|0.091|\n| |(0.036)|(0.314)|(0.077)|(0.066)|\n|Inside \u2194 Post|0.019|1.025 \u2197\u2197\u2197|0.157|0.045|\n| |(0.027)|(0.201)|(0.101)|(0.074)|\n|Inside \u2194 Adj. \u2194 Is low SES|\u21930.028|0.691|\u21930.477\u2197|\u21930.422 \u2197\u2197\u2197|\n| |(0.065)|(0.609)|(0.255)|(0.157)|\n|Inside \u2194 Post \u2194 Is low SES|0.070|\u21930.104|\u21930.359\u2197|\u21930.035|\n| |(0.048)|(0.333)|(0.196)|(0.139)|\n|Observations|8,510|11,922|11,689|4,106|\n|Mean dep. var.|3.317|169.917|13.236|0.000|\n|R2|0.068|0.544|0.109|0.152|\n\nColumns: (1) birth weight in kilograms, (2) adult height in centimeters, (3) years of education, (4) standardised fluid intelligence score. OLS specification includes year-by-month and (CB \u2194 Inside) fixed effects, and includes (CB \u2194 Inside)-specific linear time trends. Controls for sex, genetic principal components, and weather in utero and during childhood. All covariates have been normalised to mean zero. Control group consists of never-treated individuals from both adopting and non-adopting county boroughs. Trims the sample to five years before and after the SCA submission date, and restricts the sample to birth cohorts in years 1958 to 1969. Clusters standard errors by CB. (*): p &lt; 0.1, (**): p &lt; 0.05, (***): p &lt; 0.01.# 6. ROBUSTNESS\n\nOur main analysis shows that the introduction of SCAs substantially and persistently reduced black smoke emissions, but did not impact local sulphur dioxide concentrations. This in turn improved child health outcomes (in particular birth weight), with longer-term impacts on adult height. However, we find no consistent evidence of impacts on economic outcomes, including years of education and intelligence.\n\nThis section highlights that our findings are generally robust to a range of different specifications, samples and assumptions. We discuss these sensitivity checks in detail in Appendix D, but summarise them graphically in Figure 6. The figure shows six panels for our main outcomes of interest: the two pollution measures and four individual-level outcomes. The dots present the estimates of the impact of exposure during the \u2018adjustment period\u2019; the triangles present the estimates for the SCA being in operation. Both are shown with 95% confidence intervals, with opaque colours indicating that they are significantly different from zero. The first row in each panel repli-# Figure 6: Overview of robustness checks.\n\n|Coefficient|Inside x Adj.|Inside x Post|Black smoke|Sulphur dioxide|Birth weight (kg)|Height (cm)|Educational attain.|Fluid intelligence|\n|---|---|---|---|---|---|---|---|---|\n|Main specification| | | | | | | | |\n|Non\u2212adopting CBs control group, Sec. D.1| | | | | | | | |\n|Adopting CBs control group, Sec. D.1| | | | | | | | |\n|All non\u2212adopting districts control group, Sec. D.1| | | | | | | | |\n|Include catch\u2212all birth locations, Sec. D.2| | | | | | | | |\n|Trim to 2 years, Sec. D.3| | | | | | | | |\n|Trim to 4 years, Sec. D.3| | | | | | | | |\n|No trimming, Sec. D.3| | | | | | | | |\n|Restrict cohorts to 1958\u2212\u22121965, Sec. D.4| | | | | | | | |\n|Restrict cohorts to 1958\u2212\u22121966, Sec. D.4| | | | | | | | |\n|Restrict cohorts to 1958\u2212\u22121967, Sec. D.4| | | | | | | | |\n|Restrict cohorts to 1958\u2212\u22121968, Sec. D.4| | | | | | | | |\n|Restrict cohorts to 1959\u2212\u22121969, Sec. D.4| | | | | | | | |\n|Restrict cohorts to 1960\u2212\u22121969, Sec. D.4| | | | | | | | |\n|Restrict cohorts to 1961\u2212\u22121969, Sec. D.4| | | | | | | | |\n|Restrict cohorts to 1962\u2212\u22121969, Sec. D.4| | | | | | | | |\n|No time trend, Sec. D.5| | | | | | | | |\n|CB\u2212specific yearly trend, Sec. D.5| | | | | | | | |\n\nPlots the point estimates and their 0.95 confidence bands for our main coefficients and outcomes across our various robustness checks. First two columns show estimates from the pollution sample, remaining four columns show estimates from individual sample. For checks that are specific to the individual analysis, we do not report the estimates for the first two columns, but only in the final four (individual-level) columns.# 7. CONCLUSION\n\nThis paper examines the long-term effects of a national, large-scale pollution reduction programme that was rolled out in the UK in the late 1950s. We exploit temporal and spatial variation in the introduction of so-called \u201cSmoke Control Areas\u201d (SCAs); areas that banned all smoke emissions within its boundaries. Our identification compares pollution measurement stations (individuals) located (born) in or downwind of SCAs before and after their introduction, relative to those of a control group of never treated stations (individuals), controlling for weather variation, local area fixed effects, area-specific trends (and individual-level controls). Our main analysis specifies an event study approach, but we show that our results are robust to using group-time average effects that account for staggered treatment.\n\nUsing digitised historical pollution measurements, we provide one of the first empirical analyses of the dynamic and time-varying impact of the introduction of SCAs on local pollution levels (see also Fukushima, 2021). We show that they substantially and persistently reduced black smoke (but not sulphur dioxide) concentrations for at least five years post-introduction. This, in turn, affected individuals who were born in areas of smoke control, relative to those born elsewhere. We show that those exposed to SCAs had \u2013 on average \u2013 60g higher birth weights and are 1cm taller in adulthood. Relative to the mean, these represent increases of 2% and 0.6% respectively. In contrast to much of the existing literature, we find no evidence of impacts on years of education and intelligence, with estimates that are relatively close to zero throughout, though with some suggestive evidence of heterogeneity by the socio-economic composition of individuals\u2019 district of birth.\n\nWe highlight two potential reasons for our null result on education and intelligence. First, we examine the long-term impacts of small but persistent changes in pollution caused by the phased introduction of SCAs, whereas the existing literature that investigates the very long-term effects of pollution exposure have generally examined one large transitory pollution spike: the London smog. This may suggest that small, permanent reductions differentially impact long-term outcomes compared to extreme, but one-off events. Although this is possible, it is less likely given that the existing literature also shows that early life pollution exposure adversely affects human capital andLabour market outcomes already in early adulthood (e.g., Isen et al., 2017; Persico, 2020). Unless these individuals catch up later, or those unexposed drop down the distribution, this is perhaps unlikely to explain our findings.\n\nSecond, the fact that we do not find impacts on human capital outcomes may suggest that these are driven by exposure to pollutants other than those targeted by SCAs. Indeed, the quasi-experiments exploited in the existing economics literature (the London smog, but also e.g., toxic release inventory sites, and the US 1970 Clean Air Act Amendments) are likely to have affected a range of pollutants. In contrast, the introduction of SCAs targeted black smoke only, and the evidence suggests that moving from bituminous coal to smokeless fuel mainly affects black smoke/particulate matter, rather than other pollutants such as nitrogen oxides, sulphur oxides and carbon monoxide (Mitchell et al., 2016). This is also what we find, in that we show reductions in black smoke concentrations, but not in sulphur dioxide. Hence, this is consistent with the idea that pollutants other than black smoke are responsible for the adverse effects on human capital, whereas black smoke reduces fetal as well as child growth. Unfortunately, since black smoke and sulphur dioxide are the only two pollutants that are measured throughout our observation period, our data do not allow us to explore this possible explanation in more detail.\n\nWe also examine heterogeneity in our estimates by individuals\u2019 genetic \u201cendowments\u201d, obtained from the molecular genetic data available in the UK Biobank. In addition to this shedding light on whether the introduction of SCAs affected inequalities in our outcomes of interest, it is also informative about the existence of complementarities between endowments and public health investments. Our findings suggest that the introduction of SCAs increased inequalities in population health but not economic outcomes, with larger increases in birth weight and height among those with higher \u201cendowments\u201d.\n\nOur analyses estimate intention to treat (ITT) effects, identifying the impact of the introduction of SCAs on individuals\u2019 outcomes, rather than the impact of pollution. Indeed, the latter is endogenous, with lower social class individuals more likely to live in highly polluted areas. Given our newly digitised and rare historical pollution data, one option is to use an instrumental variable.\n\n35approach, instrumenting local pollution levels with the phased introduction of SCAs. We are re-\n\nluctant to do so however, since the policy itself may have led councils to change their spending\n\npatterns more generally. For example, by increasing their spending on pollution reduction poli-\n\ncies, they may have had to reduce local expenditures on health and education, with potential direct\n\nimpacts on our outcomes of interest.# 1. The 1956 Clean Air Act\n\nthat allowed for the introduction of SCAs marked a significant step in the government\u2019s aim to reduce air pollution and improve public health. With its requirement that coal be replaced with smokeless fuels, it was among the first that aimed to reduce pollution emitted from residential dwellings, with most previous policies aimed at industrial pollution. The move to smokeless fuels required adaptations of heating appliances, the cost of which was partially reimbursed by local authorities, which in turn would receive a contribution from the exchequer. By 1973, over 1,000 SCAs were introduced in English CBs, highlighting the widespread awareness of the health impacts of pollution as well as willingness to pay to reduce these not only by the government and local authorities, but also among the population.There are other reasons to believe that our coefficients underestimate the impacts of the introduction of SCAs. First, since pollution is linked to infant mortality and foetal loss, the higher levels of pollution in non-SCAs may have led to increased mortality (Fukushima, 2021). Assuming that these individuals were more vulnerable than those who survived, our estimates are likely to be a lower bound. Similarly, the fact that we only observe UK Biobank participants when they enter the data collection in 2006\u201310, we implicitly condition on survival until then. These survivors are likely to be stronger than those who did not make it, potentially attenuating our estimates of interest.\n\nNevertheless, our analysis highlights the importance of a healthy environment in early life, showing immediate as well as long term impacts on individuals exposed to pollution in the early childhood period. This has direct implications for policy, suggesting that interventions that aim to reduce pollution not only have contemporaneous health benefits, but also improve individuals\u2019 outcomes in older age. Ignoring such longer-term effects underestimates the total welfare effects caused by pollution reduction.\n\n37",
        "context_id": 46,
        "question": "What type of data does the paper utilize to investigate the long-term impact of Smoke Control Areas on individuals' health and human capital?",
        "answer": [
            "UK Biobank data"
        ],
        "context_length": 62691
    }
]