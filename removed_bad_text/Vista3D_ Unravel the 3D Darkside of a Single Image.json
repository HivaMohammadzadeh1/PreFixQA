[
    {
        "id_": "99271bef-c6df-4fed-a135-f1a1a04a3e73",
        "text": "# 1 Introduction\n\nSince the earliest times, our ancestors gazed upon the luminous moon, a symbol of mystery and wonder. Its bright facade, an elegant sphere in the cosmos, has always made us think about what remains hidden: the moon’s obscure and elusive dark side. This curiosity, as ancient as human history itself, represents our innate desire to uncover the concealed dimensions that exist beyond the visible.\n\nThis quest, once purely philosophical, has now ventured into the realm of practicality, propelled by the advancements in 3D generative model [29,34,42,45,48]. These technologies enable a broad range of applications, especially in gaming and virtual reality, allowing for the creation of rich, detailed environments and objects without extensive modeling.\n\nNevertheless, the development of robust large-scale 3D generative models remains a formidable challenge, predominantly due to the limited availability of 3D data. Numerous attempts [1,13,27] have been made to train 3D diffusion models on relatively small 3D datasets, condition on textual or visual prompts; Yet, ω Corresponding Author."
    },
    {
        "id_": "0c5b46f0-e2f6-443b-92b9-bde87b46a017",
        "text": "# Shen et al.\n\n|A grey Volvo XC90|A grey BMW X5|\n|---|---|\n|A white skin horse with flowing mane|A sleek silver horse with flowing mane|\n|A pale cream-color bear pillow|A pale cream-color panda pillow|\n|Reference|A muscle dog with capital S|\n| |A muscle dog wearing a superman cape|\n\nFig. 1: 3D Darkside of Single Image. By employing various text prompts, Vista3D is capable of unveiling the diversity of unseen views while retaining 3D consistency and detail. Two novel views and the normal map are visualized for each text prompt.\n\nThese endeavors often fall short in creating 3D objects with structural integrity and textural consistency. This challenge is further compounded in the context of reconstructing 3D objects from single images. In this context, two primary approaches emerge. The first considers the task as a problem of sparse-view reconstruction. However, this often leads to blurred 3D outputs due to the neglect of unseen elements, resulting in excessively blurred 3D objects [8, 52] as most views remain unseen.\n\nOn the other hand, the generative approach, which leverages large-scale 2D diffusion models [29, 42], introduces its own set of challenges. Efforts to develop 3D-aware 2D diffusion models [19, 21, 30, 32, 34, 39, 40, 51] involve fine-tuning 2D models with camera transformation modeling on 3D datasets [5,6]. Nevertheless, the prevalence of synthetic objects in these datasets can lead to a compromise in 2D diversity. This often results in the generation of oversimplified geometries and textures.\n\nIn this paper, we present Vista3D, a framework designed for reconstructing the unseen view (or \"darkside\") from a single image. Central to Vista3D is a dual-phase strategy: a coarse phase followed by a fine phase.\n\nIn the coarse phase, we leverage 3D Gaussian splatting [14] to swiftly create basic geometry and textures. To stabilize Gaussian Splatting optimization, we employ a gradient-based Top-K densification strategy, focusing on Gaussian points with the highest gradients. Additionally, we introduce two novel regularization terms targeting the Gaussian scale and transmittance values, significantly enhancing the convergence speed.\n\nThe fine phase then transforms this initial geometry into signed distance fields (SDF) for further optimization. Here, we employ FlexiCubes [38], an advanced differentiable isosurface technique, to refine the geometry. This refine-"
    },
    {
        "id_": "d1d693ec-f213-4ecf-b507-35de9d5ca246",
        "text": "# Vista3D\n\nment aids in learning the signed distance fields (SDFs), deformation, and interpolation weights. The parameters are optimized by ensuring fidelity to the original image and guided by a score function derived from diffusion priors.\n\nDespite these advancements, a unified representation and supervision across all views, both seen and unseen, prove insufficient for capturing the unique characteristics of different viewpoints and generating diverse, consistent 3D objects. To address this, we enhance the representation by implementing Disentangled Texture Representation, using two angularly disentangled networks for accurate texture prediction. Furthermore, our Angular-based Composition method amalgamates different diffusion priors, adjusting their gradients within specific angular bounds according to their gradient magnitudes. This strategic adjustment assures 3D consistency while promoting diversity in the unseen views.\n\nVista3D excels in efficiently generating diverse and consistent 3D objects from a single image within five minutes. Our extensive evaluations demonstrate its ability to maintain a flexible balance between the consistency and diversity of the generated 3D objects."
    },
    {
        "id_": "5190b352-c0a7-40a9-897b-1a1599c58c43",
        "text": "# We summarize our contribution as follows:\n\n- We present Vista3D, a framework for revealing the 3D darkside of single images, efficiently generating diverse 3D objects using 2D priors.\n- We develop a transition from Gaussian Splatting to isosurface 3D representations, refining coarse geometry with a differentiable isosurface method and disentangled texture for textured mesh creation.\n- We propose an angular composition approach for diffusion priors, constraining their gradient magnitudes to achieve diversity on the 3D darkside without sacrificing 3D consistency."
    },
    {
        "id_": "4cfd5bff-ca23-4af2-a2ed-ab844e9790e8",
        "text": "# 2 Related-works"
    },
    {
        "id_": "3b6b5dbf-3f6b-42d2-af82-cbb2530f0023",
        "text": "# 2.1 3D Generation Conditioned on a Single Image\n\nThe objective of image-to-3D generation is to create 3D objects from a single reference image. Initial methods [8, 52] approached this challenge as a variant of sparse view 3D reconstruction. However, these methods often resulted in blurred object outputs due to insufficient priors. Recently, drawing inspiration from text-to-3D initiatives that utilize Score Distillation Sampling (SDS) to elevate 2D diffusion priors into 3D generative models, image-to-3D works [24, 33, 34, 40, 42] have adopted a similar approach for 3D object generation based on a single image. However, 2D diffusion priors alone cannot ensure 3D consistency, as they are typically trained solely on image datasets. To address this, several studies [19–21, 39] have attempted to refine 2D diffusion priors with 3D data [5, 6], enhancing their ability to model 3D consistency. A notable example is Zero-1-to-3, which can generate novel views condition on single image and camera position. Integrating this refined model with SDS [30, 41] allows for the reconstruction of coherent 3D objects. Moreover, another stream of works [9, 17, 36, 46, 47, 50, 55] pretrained on large-scale 3D dataset [5] directly predicting the representation."
    },
    {
        "id_": "bbdc2490-68c8-4832-ab39-44d54bcc7c8a",
        "text": "# 4 Shen et al.\n\nof a 3D object from a single image. Diverging from previous works, our work does not solely view this as a 3D reconstruction issue. We redefine it as a 3D generation task aimed at uncovering the unseen 3D aspects behind a single image. Through a meticulously crafted framework, our method efficiently generates diverse and consistent 3D objects."
    },
    {
        "id_": "9c237455-88bf-4146-8815-7fa758cda33d",
        "text": "# 2.2 3D Representations for Generation\n\nPresently, most zero-shot text-to-3D and image-to-3D models utilize an optimization based pipeline, parameterizing the 3D object as a differentiable representation, which varies among different methods. The most prevalent representation in groundbreaking works like dreamfields [12], dreamfusion [29], and SJC [43] is Neural Radiance Fields (NeRF) [25]. However, training a NeRF is computationally intensive and takes long time to convergence. Magic3D [16] introduced a two-stage representation, initially learning a coarse NeRF, followed by refining the polygon mesh using a differentiable isosurface method, DMTet [37]. Fantasia3D [2] suggested directly optimizing DMTet [37] in separate phases for geometry and texture, but this often leads to mode collapse in the geometry phase and extends training time beyond NeRF. Gaussian Splatting [10, 14, 35, 44, 53] has gained attention for its efficiency in various 3D tasks, with several 3D generative models [3, 4, 41, 49] incorporating it for effective generation. However, as a point-based representation, it cannot yield high-fidelity meshes. In our approach, we employ Gaussian Splatting exclusively to create coarse geometry. This coarse geometry is then transformed into SDF, optimized with a hybrid isosurface representation, FlexiCubes [38], to produce high-fidelity meshes. Additionally, we propose an angular disentangled texture representation, tailored to the specifics of this task."
    },
    {
        "id_": "0942f967-b301-4ad7-8aab-74fe3d1237a8",
        "text": "# 3 Methodology\n\nIn this section, we outline our framework to generate detailed 3D object from single image with 2D diffusion priors. As depicted in Figure 2, our exploration of the 3D darkside of a single image commences with the efficient generation of basic geometry (Section 3.1), represented through 3D Gaussian Splatting. In refinement stage (Section 3.2), we devise a method for transforming the rudimentary 3D Gaussian geometry into signed distance fields, and thereafter, we introduce a differentiable isosurface representation to further enhance the geometry and textures. To enable diverse 3D darkside of given single image, we present a novel approach to constrain two diffusion priors (Section 3.3), enabling the creation of varied yet coherent darkside textures by bounding gradient magnitude. With these approaches, our method can efficiently generate diverse, high-fidelity meshes from a single image."
    },
    {
        "id_": "af8730e0-57ce-49b4-9c4b-3c5f014a7daa",
        "text": "# Vista3D"
    },
    {
        "id_": "68501af1-c45e-4b05-a6fd-610e5ec35c3d",
        "text": "# Rasterization"
    },
    {
        "id_": "9ef7ce1f-2ccd-46a0-b28a-79ee427349bf",
        "text": "# 3D Prior"
    },
    {
        "id_": "f4f119d3-7c5d-43a5-92aa-dabc22538a4c",
        "text": "# Stage I\n\nGaussian Splatting\n\nTop-K Densification\n\nRegularization\n\nInput Image"
    },
    {
        "id_": "11dc342c-08d3-4e7d-8a00-0af0208c026b",
        "text": "# Stage II\n\nSDF"
    },
    {
        "id_": "170cdd19-7e10-44db-a2b2-b4b8729aad63",
        "text": "# 2D Prior"
    },
    {
        "id_": "5f59feb3-a56c-499b-b87b-efc014a18d0f",
        "text": "# 3D Prior\n\nDisentangled Texture\n\nFlexiCubes\n\nRepresentation\n\nAngular-based Composition\n\nSDS"
    },
    {
        "id_": "b28b0728-393e-45d8-beda-550a2996f98c",
        "text": "# Fig. 2: Overview of Vista3D.\n\nWe generate high-fidelity mesh from single image input in a coarse-to-fine manner. In the coarse stage, we utilize Gaussian Splatting to learn a coarse geometry with a 3D-aware 2D diffusion prior. We further extract sign distance fields from Gaussian Splatting for refinement. Another 2D diffusion prior is enabled with an angular-based composition to explore diverse darkside while retain 3D consistency in refinement stage."
    },
    {
        "id_": "01acc3d3-501a-42b2-a695-9204641d06d9",
        "text": "# 3.1 Coarse geometry from Gaussian Splatting\n\nIn the coarse stage of our framework, we focus on constructing a basic object geometry using Gaussian Splatting. This technique, as described in [14], represents 3D scenes as set of anisotropic 3D Gaussians. Compared to other neural inverse rendering methods, such as NeRF [25, 26], Gaussian Splatting demonstrates a notably faster convergence speed in inverse rendering tasks.\n\nSome works [3, 41, 49] has attempted to introduce Gaussian Splatting into 3D generative models. In these methods, we found that directly using Gaussian splatting to generate detailed 3D objects requires optimizing a large number of 3D Gaussians, necessitating significant time for optimization and densification, which is still time-consuming. However, Gaussian Splatting can quickly create a coarse geometry from a single image using a limited number of 3D Gaussians within just one minute. Therefore, in our approach, we utilize Gaussian Splatting solely for the initial coarse geometry generation.\n\nSpecifically, each 3D Gaussians is parameterized by its central position, scaling, rotation quaternion, opacity, and spherical harmonics to represent color. To generate a coarse 3D object, we optimize a set of these Gaussian parameters ε = {ϑ}, where i ϑ i = {x, r, q, ω i, c i}. To render 3D Gaussians to 2D images, we utilized the highly-optimized tile based rasterization implementation [14].\n\nTo generate the coarse geometry of given single image I ref, we adopt Zero-1-to-3 XL [5, 19] as 2D diffusion priors ϖ ε with pretrained parameters ϱ. This prior enables denoising of novel views based on the given image I ref and relative camera pose ςφ. Accordingly, we optimize the 3D Gaussians ε with SDS [29]:\n\n↑ ϑ L SDS = E t,ϖ (ϖ ε (I R ↼ε)"
    },
    {
        "id_": "14c37c90-27c1-4c66-87ea-824d337f00fa",
        "text": "# 6 Shen et al.\n\nwhere φ denotes the camera pose sampled around the object with fixed camera radius and F oV, Iϱ R is the rendered image from 3D Gaussian set ε with camera pose φ, timestep t is annealed to weight the gaussian noise ϖ added to the rendered image. Beyond this basic approach, we introduce a Top-K Gradient-based Densification strategy to accelerate convergence and add two regularization terms to enhance the reconstructed geometry."
    },
    {
        "id_": "457590f7-eef1-47e3-85df-8df9795ffc15",
        "text": "# Top-K Gradient-based Densification.\n\nIn the optimization process, we find the periodical densification [14] with naive gradient threshold is hard to tune due to the nature randomness of SDS. So we instead use a more robust densification strategy. Only gaussians points with top-k gradients will be densified during each interval, this simple strategy can stablize training cross various given images."
    },
    {
        "id_": "9eb82809-e7bc-434d-954e-48b3b374b71c",
        "text": "# Scale & Transmittance Regularization.\n\nAdditionally, We add two regularization terms to encourage Gaussian Splatting to learn more detailed geometry in this phase. A scale regularization is introduced to avoid too large 3d gaussians, and another transmittance regularization is adopted to encourage the geometry learning from transparent to solid. The overall loss function in this stage can be written as:\n\n↑ ϑ L coarse = ↽ SDS ↑ ϑ L SDS + ↽ rgb ↑ ϑ L rgb + ↽ mask ↑ ϑ L mask + ↽ scale ↑ ϑ∑↔s ↔i ︸ ︷︷i ︸ Scale Regularization (2)\n\n↓ ↽ tr ↑ ϑ min(⇀, N f g1 ∑T k ); ︸ Transmittance Regularization︸︷︷ k\n\nwhere L rgb denotes the transmittance value for the k-th pixel in R, where N f g is the total number of foreground pixels. Additionally, ⇀ serves as a hyperparameter that is gradually annealed from 0.4 to 0.9, effectively regularizing transmittance over time."
    },
    {
        "id_": "e838e8a7-1a9e-438f-ac58-afdc8b167eb1",
        "text": "# 3.2 Mesh refinement and texture disentanglement\n\nIn the refinement stage, our focus shifts to transforming the coarse geometry, produced via Gaussian splatting, into signed distance fields (SDF) and refining its parameters using a hybrid representation. This stage is crucial for overcoming the challenges presented in the coarse stage, notably the surface artifacts frequently introduced by Gaussian splatting. Due to the inability of Gaussian splatting to provide direct estimates of surface normals, we cannot employ traditional smoothing methods to alleviate these artifacts. To counter this, our method incorporates a hybrid mesh representation, which entails modeling the 3D object’s geometry as a differentiable isosurface and learning the texture using two distinct, disentangled networks. This dual approach not only smooths out the surface irregularities but also significantly improves the fidelity and overall quality of the 3D model."
    },
    {
        "id_": "b81e378e-6dad-43ac-8390-5f42db190c46",
        "text": "# Vista3D"
    },
    {
        "id_": "e78c40f4-95bd-4bac-a151-32df12aa1239",
        "text": "# Geometry representation\n\nWe utilize FlexiCubes to represent the geometry in our approach. FlexiCubes is a differentiable isosurface representation which allow local flexible adjustments to the extracted mesh geometry and connectivity [38]. The geometry of an object is depicted as a deformable voxel grid with learnable weights. Deformation ⇁ i → R3 and sign distance field (SDF) si → R20 is learnt for every vertices vi in the voxel grid. And interpolation weights β → R and splitting weights γ → R are learnt for each grid cell to position dual vertices and control quadrilaterals splitting. Triangle meshes can be extracted from it differentiablely through Dual Marching Cubes [28]. To bridge the gap between the learned coarse geometry and the isosurface representation, we initially extract a density field from Gaussian splattings using local density queries [41], followed by the application of marching cubes [22] to extract a base mesh Mcoarse. Subsequently, we query this base mesh at grid vertices vi to obtain the initial Signed Distance Field (SDF) s(v). For stable optimization, the queried SDF is then scaled as follows:\n\ns(v) = max {|sj| : sj ▷ · s(vi) → S, sj < 0}, where S = {si} (3)\n\nwhere sj < 0 indicates the field within the object. The scale factor ▷ linearly increases from 1 to 3 during the optimization process."
    },
    {
        "id_": "1363ca7b-c090-44ca-a0ef-381bd29e03e6",
        "text": "# Disentangled Texture Representation\n\nFor texture learning, we employ hash encoding followed by a MLP to directly learn albedo. However, distinct from text-to-3D tasks, we recognize two primary supervision sources in this task: the provided reference image and the SDS gradient from 2D Diffusion priors. Typically, a substantial loss weight ↽rgb is assigned for the reference image. This dominant reference image supervision can decelerate the convergence of textures in unseen views, particularly when unseen views significantly differ from the reference view.\n\nTo address this, we separate the texture into two hash encoding, utilizing a ratio that combines with the relative azimuth angle ς◁ = ◁ ϱ↓ ◁ ref, where ◁ ϱ represents the azimuth of the sampled camera pose φ, and ◁ ref is the azimuth of the reference image. The hash encoding for a given query point 0 in the rasterized triangle mesh is expressed as:\n\nE = (1 ↓ 1)Hback(0) + 1Href(0) (4)\n\nwhere Href and Hback denote learnable hash encoding facing forward and back, 1 = (cos(ς◁) + 1)/2 is the balance factor that varies with the sampled azimuth angle. Then the encoded feature E is fed into a MLP to predict albedo values.\n\nWith these geometry and texture representation, we can render the 3D object to images by memory-efficient rasterization coupled with lambertian shading. Above learnable parameters 2 is refined with ↑ ς Lrefine:\n\n↑ ς Lrefine = ↽SDS ↑ ς LSDS + ↽SDF ↑ ς LSDF + ↽consistency ↑ ς Lconsistency (5)\n\n+ ↽rgb ↽SDS ↑ ς Lrgb + ↽mask ↑ ς Lmask;"
    },
    {
        "id_": "589a375d-b668-4f62-ab78-e161945f4d52",
        "text": "# 3.3 Darkside Diversity via Prior Composition\n\nIn implementing our pipeline, we encountered a key challenge related to the lack of diversity in unseen views. This issue largely stems from the reliance on the Zero-1-to-3 XL prior, a model trained on synthetic 3D objects from Objaverse-XL [5]. While this prior is adept at handling 3D-aware generation based on reference images and relative camera poses, it tends to produce oversimplified or overly smooth results in unseen views. This limitation becomes especially pronounced when dealing with objects captured in the real world.\n\nTo address this, we integrate an additional prior from Stable-Di!usion, known for its ability to synthesize diverse images."
    },
    {
        "id_": "5b9e4c7f-9ea9-4117-a317-d6f821c224dd",
        "text": "# Darkside diversification with 2D di!usion.\n\nWe introduce a second prior, ϖ φ with pretrained parameters 3, leading to two Score Distillation Sampling (SDS) loss terms ↑L SDS ε and ↑L SDS φ (Equation 1) for optimization. The optimal balance between these two priors remains relatively unexplored. While Magic123 [30] uses an empirical loss weight of 1/40 for the latter term, this approach may not fully harness the potential of the 2D prior. The key objective in introducing this 2D prior is to introduce greater diversity in unseen view. A small weight φ with ↑L SDS may largely limit its e!ect.\n\nTo enhance the diversity in the unseen aspects of the given image, we employ a gradient constrain method to merge these two priors. We reformulate the SDS loss as a score function [29], ↑ ς L SDS (ϱ, x) = ↓E t,z t |x ↑ ς logp ε (z t |y), where t is the timestep and z t ε is noise latent.\n\nHere φ ↑L SDS is a 3D-aware term conditioned on y = {ςφ, Iref}, while ↑L SDS is a diverse text-to-image term conditioned on text prompt y = P T. With different condition y, the score function of these two SDS term varies. To retain 3D consistency of unseen views, the magnitude of ↑ ς logp φ (z |y) need tot be constrained with respect to the 3D-aware term ↑ ς logp ε (z t |y). And to avoid the texture to be over-smoothed by the 3D-aware di!usion model, the magnitude of ↑ ς logp ε (z |y) t is indeed to be constrained with the ↑ ς logp φ (z t |y) term."
    },
    {
        "id_": "b5801abd-fc39-417b-b35d-c6397a7d62c5",
        "text": "# Angular-based Score Composition.\n\nSince the noise latents z t in both priors have di!erent encoding spaces, direct evaluation of their magnitudes using the predicted noise di!erence ϖ φ ↓ϖ is not feasible. Instead, we evaluate the magnitude of these terms by observing their gradient on the rendered image x, specifically ↑ x L SDS. Consequently, we establish upper and lower bounds for the gradient magnitude ratio of these two SDS terms, allowing for a more accurate and feasible evaluation method:\n\n||↑ x Lφ SDS ||2 Blower (1, 4) ↗ G = ||↑ x Lε SDS ||2 Bupper (1, 4) (6)\n\nWhen this ratio exceeds Bupper, we adjust the magnitude of ↑ x Lφ SDS using the factor Bupper/G. Conversely, if the ratio falls below Blower, we scale the"
    },
    {
        "id_": "a0860c57-d7fe-4ce9-84ce-3ff626d57fec",
        "text": "# Vista3D"
    },
    {
        "id_": "e6e6dc69-0c5a-409f-8373-384665c1aaf8",
        "text": "# 4 Experiments"
    },
    {
        "id_": "ebda7482-3bfd-4a52-8dc4-33559feb6053",
        "text": "# 4.1 Implementation Details\n\nCoarse geometry learning. In this phase, the input image undergoes pre-processing with SAM [15, 23, 34], where the object is extracted and recentered. We initialize all 3D Gaussians with an opacity of 0.1 and a grey color, confined within a sphere of radius 0.5. The rendering resolution is progressively increased from 64 to 512. This stage involves a total of 500 optimization steps, with the densification and pruning of 3D Gaussians occurring every 100 iterations. The top-K densification starts at a ratio of 0.5 and gradually anneals to 0.1, while the pruning opacity remains constant at 0.1. After the first densification, transmittance regularization is activated and selectively applied to the top-80% opacity values of 3D Gaussians to avoid affecting transparent Gaussians. Scale regularization is enforced using L1 norm. The weights of ↽ scale and ↽ tr are maintained at 0.01 and 1, respectively, throughout the optimization, whereas ↽ rgb and ↽ mask are gradually increased from 0 to 10000 and 1000, respectively. The timestep for SDS is linearly annealed from 980 to 20. For camera pose sampling, the azimuth is sampled in the range of [−180, 180] and elevation in [−45, 45], with a fixed radius of r = 2. This phase of optimizing the coarse geometry takes about 30 s.\n\nMesh refinement. In the refinement phase, we configure the grid size of FlexiCubes to 803 within the space [−1, 1]3. The coarse geometry obtained from the initial stage is recentered and rescaled to initialize the Signed Distance Field (SDF) for the vertices of this grid. Interpolation weights are set to 1, and all deformations start at 0. For texture, we use two hash encodings with a two-layer Multilayer Perceptron (MLP). The batch size is maintained at 4. The learning rate for deformation and interpolation weights is 0.005, while it’s 0.001 for SDF, and 0.01 for texture parameters. The rendering resolution is gradually increased from 64 to 512. In Equation 5, the loss weights are set as follows: ↽ rgb = 1500, ↽ mask = 5000, ↽ sdf = 1, and ↽ SDS = 1. We develop two versions for optimization: Vista3D-S and Vista3D-L. Vista3D-S performs 1000 steps of optimization solely with the 3D-aware prior, aiming to generate 3D mesh within 5 minutes. Vista3D-L undergoes 2000 steps of optimization with two diffusion priors to create more detailed 3D objects. The entire optimization process for Vista3D ranges from 15 to 20 minutes. In this stage, camera poses are sampled using a 3D-aware Gaussian unsampling strategy to expedite convergence (additional details are provided in the supplementary material). All experiments are conducted on an RTX3090 GPU.\n\nScore distillation sampling. In SDS optimization, the practice of linearly annealing the timestep t to adjust the noise level has been established as effective for producing higher-quality 3D objects [11]. However, in our experiments, we observed that linear annealing may not be the optimal strategy. Consequently, we..."
    },
    {
        "id_": "c2ff8328-1094-4ff7-8da8-bba6145a5334",
        "text": "# 10 Shen et al.\n\nhave implemented an interval annealing approach. In this approach, the timestep t is randomly sampled from an annealing interval rather than adhering to a fixed linear progression. This strategy has been found to effectively mitigate the artifacts commonly observed with linear annealing.\n\n|Reference|DreamGaussian|Magic123|Vista3D-S ( !\"× faster)|\n|---|---|---|---|\n| |2 minutes|2 hours|5 minutes|\n\nFig. 3: Qualitative Comparison on image-to-3D generation. We compare our Vista3D-S with DreamGaussian [41], and Magic123 [30]. Vista3D-S only takes 5 minutes to reconstruct a single 3D object, yielding competitive geometry and more consistent textures compared to Magic123 [30] with 20→ speedup."
    },
    {
        "id_": "0ea66185-cdb3-4344-b51d-483028541df0",
        "text": "# Angular diffusion prior composition\n\nIn our model, we utilize two diffusion models: Zero-1-to-3 XL [5,19] and the Stable-Diffusion model [31]. For the Stable-Diffusion model, the timestep t is scaled by the factor 1 to ensure consistency with the reference view. When editing with both diffusion priors, we start with a large initial upper bound Bupper = 100, which is linearly annealed to 10 across optimization iterations. For front-facing views, where 1 > 0.75, we adjust the upper bound using the factor (1↓1). The lower bound is specifically implemented for unseen views with 1 < 0.5, and its range is gradually reduced from 10 to 1 during the optimization process. For enhancements using the diffusion prior, we apply tighter constraints, with Bupper being reduced from 2 to 0.5. The text prompts utilized for the Stable-Diffusion model are derived from the image captions generated by GPT-4."
    },
    {
        "id_": "c3a46353-ded7-4e1f-8daa-02824de7336e",
        "text": "# Vista3D"
    },
    {
        "id_": "0b6c53de-b365-4f43-ab0f-e0cdde151287",
        "text": "# Qualitative Comparison\n\nIn Figure 3, we show our efficient Vista3D-S is capable of generating competitive 3D objects with a 20% speedup compared to existing coarse-to-fine methods. For Vista3D-L, as depicted in Figure 1 and Figure 4, we highlight our angular gradient constraint which distinguishes our framework from previous image-to-3D methods, as it can explore the diversity of the backside of single images without sacrificing 3D consistency. In Figure 3, we primarily compare our Vista3D-S with two baselines, Magic123 and DreamGaussian, for generating 3D objects from a single reference view. Regarding the quality of generated 3D objects, our method outperforms these two methods in terms of both geometry and texture. Regarding Vista3D-L, we compare it with two inference-only single view reconstruction models, specifically One-2-3-45 and Wonder3D. As shown in Fig. 4, One-2-3-45 tends to produce blurred texture and may result in incomplete geometry for more complex objects, while our Vista3D-L achieves more refined textures, particularly on the backside of 3D objects, using user-specified text prompts. And Wonder3D often resorts to simpler textures due to its primary training on synthetic datasets, which occasionally leads to out-of-"
    },
    {
        "id_": "f509ca66-c33f-4735-bf87-2fb734068141",
        "text": "# Figure 4: Qualitative Comparison with One-2-3-45 and Wonder3D\n\nIn this comparison, we render two views of each 3D object as generated by One-2-3-45 and Wonder3D. For Vista3D-L, we detail the text prompts utilized for the generation of each 3D object, showcasing three rendered views alongside a single normal map for a comprehensive comparison.\n\n|Reference|One-2-3-45|Wonder3D|Vista3D-L (ours)|\n|---|---|---|---|\n|A white standing panda, cartoon style| | | |\n|An anthropomorphic cat wearing a tweed outfit, complete with a matching cap| | | |\n|An anthropomorphic bull in a casual grey sweater and blue jeans| | | |\n|Two cute pandas stacked on top of each other| | | |\n|A dark orange and green dinosaur, lifelike style| | | |"
    },
    {
        "id_": "276d3d08-0bf4-4796-8854-f4fcc50d56de",
        "text": "# 12 Shen et al.\n\ndistribution issues for certain objects. In contrast, Vista3D-L offers zero-shot 3D object reconstruction by controlling two diffusion priors, enabling more detailed and consistent textural. Moreover, given that only a single reference view of the object is provided, we posit that the object should be amenable to editing during optimization with user-specified prompts. To illustrate this, we display several results in Figure 1 that emphasize the potential for editing.\n\n|Type|CLIP-Similarity ↑|Time Cost ↓|\n|---|---|---|\n|One-2-3-45 [18]|0.594|45 s|\n|Point-E [27]|0.587|78 s|\n|Shape-E [13]|0.591|27 s|\n|Zero-1-to-3 [19]|0.778|30 min|\n|DreamGaussian [41]|0.738|2 min|\n|Magic123 [30]|0.802|2 h|\n|DreamCraft3D [40]|0.842|3.5 h|\n|Vista3D-S|0.831|5 min|\n|Vista3D-L|0.868|15 min|\n\nTable 1: Quantitative Comparisons on generation quality in terms of CLIP-Similarity for image-to-3D task. Average generation time is reported."
    },
    {
        "id_": "a6ecc357-a43a-470b-8c0e-7d30dcd2e73d",
        "text": "# 4.3 Quantitative Comparison\n\nIn our evaluation, we employ the CLIP-similarity metric [19, 24, 30] to assess the performance of our method in 3D reconstruction using the RealFusion [24] dataset, which comprises 15 diverse images. Consistent with the settings used in previous studies, we sample 8 views evenly across an azimuth range of [−180, 180] degrees at zero elevation for each object. The cosine similarity is then calculated using the CLIP features of these rendered views and the reference view. Table 1 highlights that Vista3D-S attains a CLIP-similarity score of 0.831, with an average generation time of just 5 minutes, thereby surpassing the performance of the Magic123 [30]. Furthermore, when compared to another optimization-based method, DreamGaussian [41], Vista3D-S may take longer at 5 minutes, but it significantly improves consistency, as evidenced by the higher CLIP-Similarity score. For Vista3D-L, we apply an enhancement-only setting. By employing angular diffusion prior composition, our method achieves a higher CLIP-Similarity of 0.868. The capabilities of Vista3D-L, especially in generating objects with more detailed and realistic textures through prior composition, are demonstrated in Figure 4. Additionally, we conduct quantitative experiments on the Google Scanned Object (GSO) [7] Dataset, following the setting in SyncDreamer [20]. We evaluate each method using 30 objects and computed PSNR, SSIM, and LPIPS [54] between the rendered views of the 3D object and 16 ground-truth anchor views. The results, as shown in Tab. 2, reveal that our Vista3D-L achieves SOTA performance among these methods with a large margin. Vista3D-S also demonstrates competitive performance, albeit with a single diffusion prior."
    },
    {
        "id_": "7e3d966f-3e2f-45c9-91c3-b0081df690c6",
        "text": "# Vista3D\n\n|Method|PSNR ↑|SSIM ↑|LPIPS ↓|\n|---|---|---|---|\n|RealFusion [24]|15.26|0.722|0.283|\n|Make-it-3D [42]|15.79|0.741|0.245|\n|Zero-1-to-3 [19]|18.93|0.779|0.166|\n|One-2-3-45 [18]|17.47|0.768|0.184|\n|SyncDreamer [20]|20.05|0.798|0.146|\n|DreamGaussian [41]|23.43|0.832|0.092|\n|Magic123 [30]|24.89|0.875|0.084|\n|Vista3D-S|25.42|0.912|0.073|\n|Vista3D-L|26.31|0.929|0.062|\n\nTable 2: Quantitative Comparison on the GSO [7] dataset"
    },
    {
        "id_": "74800209-a81d-4ff7-88e3-966a5b18de94",
        "text": "# 4.4 User study\n\nIn our user study, we evaluate reference view consistency and overall 3D model quality [41]. The evaluation encompasses four methods: DreamGaussian [41], Magic123 [30], and our own Vista3D-S and Vista3D-L. We recruited 10 participants for this user study. Each was asked to sort generated 3D object from different methods in terms of view consistency and overall quality respectively. Thus, the scores presented for each metric range from 1 to 4. The results, presented in Table 3, reveal that our Vista3D-S outperforms the previous methods in both view consistency and overall quality. Furthermore, the adoption of the angular prior composition in Vista3D-L leads to additional improvements in both the consistency and quality of the generated 3D objects.\n\n|Method|View Consistency ↑|Overall Quality ↑|\n|---|---|---|\n|DreamGaussian [41]|1.78|2.02|\n|Magic123 [30]|2.11|1.83|\n|Vista3D-S|2.87|2.81|\n|Vista3D-L|3.24|3.33|\n\nTable 3: User study of Vista3D. We conduct user study in terms of view consistency and overall quality, the score ranges from 1 to 4, the higher the better."
    },
    {
        "id_": "c6d40d9c-52dc-4acd-b369-e7841568fbac",
        "text": "# 4.5 Ablation Study\n\nCoarse-to-fine framework. Our framework integrates a coarse stage to learn initial geometry then a fine stage to refine geometry and shade textures. We validate the necessity of such a coarse-to-fine pipeline in Figure 5 (a). We first commence with isosurface representation to learn geometry directly, finding the geometry optimization is prone to collapse without preliminary geometry initialization. Thus, a coarse initialization becomes imperative. Beside, we present the normal map of a rough mesh extracted from 3DGS from the coarse stage. It is observed that the coarse stage tends to generate rough even non-watertight geometry, both difficult to mitigate. These findings demonstrate that combining both stages is crucial for the optimal performance of Vista3D.\n\nDisentangled Texture. For validating the effectiveness of the disentangled texture, we compare adopting both hash encodings with single hash encoding."
    },
    {
        "id_": "84700876-b118-41b8-a9d9-2a2296799837",
        "text": "# 14 Shen et al.\n\nRef\nFlexiCubes\n3DGS\nOurs\nSingle hash-encoding\nBoth hash-encoding"
    },
    {
        "id_": "3c6fb1bc-1844-4110-92d0-542192d1ccac",
        "text": "# a) Ablation study of overall framework"
    },
    {
        "id_": "75013e2b-5491-450f-88a2-e89a3d1ec1be",
        "text": "# b) Ablation study of angular hash-encoding\n\nFig. 5: Ablation study of overall framework and disentangled texture.\n\nin Figure 5 (b). With both hash-encodings, the artifacts on the reconstructed robot are notably reduced, especially at the backside. Further, we visualize the disentangled texture in supplementary Figure 6(b). Specifically, when visualizing Href, Hback is set as 0 in Equation 4, and vice versa. From the shown visualization, we can clearly find that the facing-forward hash encoding Href mainly encodes the detail features consistent with the given reference view. While the back hash encoding Hback mainly encodes the features in the unseen views. The textures of the facing-forward view and back views are disentangled and learned in two separate hash encodings, which can facilitate learning better textures near the reference view and in unseen views."
    },
    {
        "id_": "537d0823-2b34-4b13-8339-61e6fdad4e0e",
        "text": "# 5 Conclusion\n\nIn this paper, we present a coarse-to-fine framework Vista3D to delve into the 3D darkside of a single input image. This framework facilitates user-driven editing through text prompts or enhances generation quality using image captions. The generation process begins with a coarse geometry obtained through Gaussian Splatting, which is subsequently refined using an isosurface representation complemented by disentangled textures. The design of these 3D representations enables the generation of textured meshes within a mere 5 minutes. Additionally, the angular composition of diffusion priors empowers our framework to reveal the diversity of unseen views while maintaining 3D consistency. Our approach surpasses previous methods in terms of realism and detail, striking an optimal balance between generation time and the quality of the textured mesh. We hope our contributions will inspire future advancements and foster future exploration into the 3D darkside of single images."
    }
]