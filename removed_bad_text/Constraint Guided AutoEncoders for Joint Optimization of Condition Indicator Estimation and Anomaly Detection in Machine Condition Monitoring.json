[
    {
        "id_": "a7db9189-1b5d-4838-aa7a-5b38218ada8a",
        "text": "# 1 Introduction\n\nThe main goal of machine condition monitoring is, as the name implies, to monitor the condition of industrial applications, and one of the crucial parts in these applications is rotating machinery. When considering this type of machinery, it has been determined that the main cause of failure is attributed to Rolling Element Bearings (REB) (Nabhan et al, 2015). Currently, the most common method to monitor these REB is currently by capturing and analyzing the vibrations produced by REB (Hoang and Kang, 2019). However, acoustic signals have also been used as a promising alternative (Pacheco-Chérrez et al, 2022), and allow for easier data collection.\n\nThe work regarding this condition monitoring can be mainly split into two branches. The first branch considers a diagnostic problem, where a signal is either classified as normal or anomalous, otherwise called Anomaly Detection (AD), or it can be further categorized into specific fault types, depending on the problem and considered method (Jiang et al, 2019). The other branch considers a prognostic problem, where a signal is given a condition score, or Condition Indicator (CI), that assesses the condition of a machine (She et al, 2020). This CI can then be used as a basis to predict the Remaining Useful Life (RUL) of a machine (component) or can be used to determine if the state of the machine is considered normal or anomalous based on some predefined threshold.\n\nFor both AD and CI estimation research has been done using signal processing, e.g. traditional clustering (Knorr et al, 2000) or shallow Machine Learning (ML) (Widodo and Yang, 2007), and data-driven, mainly Deep Learning (DL) (Ionescu et al, 2017; Said Elsayed et al, 2020), methods, with the latter receiving more attention in recent years (Zhou et al, 2022; Zhang et al, 2020). This work will focus on using data-driven DL methods.\n\nAD is an important task in a broad range of domains, such as, network intrusion detection (Yang et al, 2022), finance (Ahmed et al, 2016), and machine condition monitoring (Kamat and Sugandhi, 2020). The most common DL approach used for AD is an AutoEncoder (AE) or an AE-based variant (Chalapathy and Chawla, 2019). These methods work by encoding data into a compact latent representation in a manner that allows the original data to be reconstructed as well as possible (Hinton and Salakhutdinov, 2006). By applying this on normal data, it is expected that the reconstruction of anomalous data is worse, as there are differences between both types of data.\n\nThis can be seen in (Oh and Yun, 2018), where an AE was used to perform AD on acoustic data from a surface-mount device assembly machine. A similar methodology was used in (Givnan et al, 2022), where a stacked autoencoder was used to perform AD on sensor data from industrial motors. A downside of this type of approach is the inability to use anomalous data during training, which can provide valuable information, hence, approaches that allow the use of anomalous data are of interest (Primus et al, 2020; Liu and Gryllias, 2020). This additional information can be used by either semi- or fully supervised methods. However, it is indicated in (Arunraj et al, 2017) that fully"
    },
    {
        "id_": "45bd3a26-072a-4474-b2d3-bea274673c31",
        "text": "# 3\n\nSupervised methods only have a better performance in case all types of anomalies are known during training. A similar conclusion is made in (Zhang et al, 2021), where it was indicated that incorrect labeling reduces the performance of a supervised method, while keeping these label uncertainties unlabeled in a semi-supervised approach mitigates this issue. Since it is unlikely that all types of anomalies are known during training and label uncertainties are possible, semi-supervised methods are likely to, in general, outperform fully supervised methods.\n\nA commonly used semi-supervised method is Deep Support Vector Data Description (DSVDD) (Ru↵ et al, 2018, 2019), which attempts to map normal data close to a center and abnormal data further away from the same center, and is an adaptation of the original Support Vector Data Description (SVDD) method (Tax and Duin, 2004) into the DL framework. However, this adaptation requires to restrict the DL network used by the method, so that trivial solutions are avoided (Ru↵ et al, 2018, 2019): (i) the center used by DSVDD cannot be initialised as 0, (ii) the network cannot use biases, and (iii) the network cannot have bounded activation functions.\n\nIn (Liu and Gryllias, 2021) a DSVDD model was used to perform AD on vibration data from helicopters. They made a comparison between the data-driven DSVDD method and a traditional SVDD method with engineered features, showing an improved performance for the DSVDD method. A similar approach was used in (Peng et al, 2023) to monitor the condition of wind turbines. The DSVDD method was also compared to traditional SVDD as well as to a regular AE, again showing an improved performance, although the difference with the AE was not large.\n\nThese restrictions were alleviated in a combined AE and DSVDD approach, where the method will still learn to map normal data close to a center, as per the DSVDD objective. However, a trivial solution, where all the normal data is mapped to a single point, will be prevented by the added reconstruction objective of the AE (Zhang and Deng, 2021; Zhou et al, 2021; Hojjati and Armanfard, 2024).\n\nAs discussed earlier, next to the diagnostic AD problem, a prognostic CI estimation problem can also be considered. DL methods have already been applied to this problem in previous works. In (Su et al, 2020) a Variational AutoEncoder (VAE) is combined with different types of Recurrent Neural Networks (RNN) to estimate the RUL of aircraft engines. A pretrained VGG16 model was used in combination with SVDD in (Mao et al, 2020), by first adapting the VGG16 model to the bearing data, after which the features obtained from this model were passed to the SVDD algorithm to estimate the CI.\n\nA Multi-Task DSVDD approach was used in (Shi et al, 2021) to estimate the CI for different bearings. This was done by creating a hypersphere around a center for each different bearing and introducing a hypersphere similarity loss that attempts to overlap these hyperspheres as much as possible. A similar approach was used in (Kou et al, 2022). However, instead of attempting to create overlapping hyperspheres, a set of centers is chosen using prototype clustering and these are used to determine the probability of a point to be assigned to one of these centers. In (Wang et al, 2023) a combination of a CNN and RNN was used to perform CI estimation based on the vibration data and an enhanced empirical mode decomposition algorithm.\n\nThere are also methods that detect anomalies when the CI goes above some predefined threshold (assuming a low CI corresponds to normal behavior while a high CI corresponds to deviating behavior) (Liu and Gryllias, 2020). However, determining"
    },
    {
        "id_": "dff20302-a62d-4f50-af12-63702004a026",
        "text": "# Threshold Setting in Machine Learning\n\nThis threshold is a nontrivial problem as it often depends on the specific application, e.g. false negatives might be undesirable, which evidently influences the threshold. Different methods to set this threshold exist. They can be split into three groups:\n\n1. Methods that rely on statistics from the training data, such as the mean and standard deviation or the maximum (Wang et al, 2023; Kou et al, 2022).\n2. Methods that use additional data, e.g. the test set, to set a threshold to optimize a metric (Garg et al, 2022).\n3. Methods that dynamically determine the threshold based on the data that is being evaluated (Jia et al, 2022).\n\nIn this work, it is opted to focus on methods belonging to the first group, as these do not require additional data or analysis. It has been empirically observed that choosing a threshold based on these methods, as is expected, shows a strong dependency on the training and initialization of a model, especially when DL models are used. A more robust thresholding method, called Constraint-Guided AutoEncoders (CGAE) was proposed in (Meire et al, 2023a), where a fixed threshold was defined and constraints were applied to the training so that an encoder is enforced to map normal and anomalous data to the correct side of this threshold. It was indicated that this method of training with constraints resulted in a robust threshold that is less dependent of the distribution of the training data in the latent space."
    },
    {
        "id_": "1c6bad79-660c-4df9-991f-586a7b5c433f",
        "text": "# Monotonic Behavior in Machine Condition Monitoring\n\nThe aim of CGAE is to separate normal and anomalous data based on a fixed, predefined threshold. However, when considering machine condition monitoring as a prognostic problem, it is expected that this CI shows a monotonically increasing behavior, which is not directly linked to the objective of CGAE. Previous works have already applied different methods to learn or enforce this monotonic behavior. In (Wu et al, 2019) 4 different monotonic curves, that emulate an expected RUL curve, were used as target for a learning objective. A similar approach was used in (Hu and Li, 2020), where features were selected based on both a correlation metric between the feature and the running time of an asset and a monotonicity metric. The selected features were then used to construct a single CI with the normalized life of an asset as target. While these methods allow for the construction of a monotonic CI, they require a target or label that matches the degradation pattern of the bearing, which might not be trivial, as each bearing can show a different degradation pattern (Sikorska et al, 2011).\n\nA two step approach was used in (Qi et al, 2024), where a CI was selected by first splitting a raw vibration signal into multiple frequency bands, then calculating multiple CIs per frequency band, and finally selecting the CI that attains the highest score based on the monotonicity, trendability, and prognosability. This CI is then provided to a SVDD model to perform AD, and when an anomaly is detected the RUL is estimated using a statistical model. A two step approach was also used in (Nieves Avendano et al, 2022) to first estimate a CI using a stochastic gradient descent regressor, and second, optimize an isotonic regression to fit this CI onto a monotonic curve. As these methods use two steps, a change in one of the steps results in the other step needing to be reoptimized, based on that change."
    },
    {
        "id_": "d504a03d-7b4a-4e1b-86b5-f89bb4b44956",
        "text": "# Proposed Method: Monotonically Constraint Guided AutoEncoder (MCGAE)\n\nThis work proposes an extension to CGAE, which will be called Monotonically Constraint Guided AutoEncoder (MCGAE), where an additional monotonicity constraint is included to enforce the monotonic behavior of the CI estimations over time. In this way the proposed method causes an AE to learn to both map normal and"
    },
    {
        "id_": "cc65a534-945e-4e66-9f8f-a4802d6b4238",
        "text": "# Current Page Title\n\nAnomalous data to the correct side of some threshold, as per the CGAE, objective, as well as to map data from earlier in the life time of a bearing closer to the origin than data from later in the life time of a bearing. Since the CI is defined as the distance of the latent representation of the current measurement to the latent space origin the latter will cause the desired monotonic behavior. Using this approach, our proposed method does not require a target that matches the expected degradation pattern of a bearing, as the monotonicity is enforced by the constraint. The optimization of both the AD and CI estimation is also integrated into a joint optimization, meaning that a change in either will automatically also optimize the other."
    },
    {
        "id_": "9a8429d8-6a7f-47e3-ab34-f7b48a73bf2f",
        "text": "# The main contributions of this work are:\n\n- An extension of the CGAE algorithm that retains the ability to separate normal and anomalous data, while having an improved underlying CI by enforcing it to have a monotonic trend.\n- A joint modelling of CI estimation and AD which allows the use of both labeled and unlabeled data, without needing to assume that the latter can be considered (mostly) as normal data.\n- A comparison of the proposed method with existing methods using the data from two sets of life time tests of bearings: one where the health is monitored via an accelerometer and another where a microphone was used.\n\nThe rest of this paper is structured as follows. The methods that will be used for comparison are described in Section 2. This is followed by detailed discussion of the proposed MCGAE method in Section 3. Then, the experimental setup, including the used datasets, preprocessing, model architectures, and evaluation metrics are explained in Section 4. The performed experiments and corresponding results are discussed in Section 5. An ablation study is presented in Section 6. Finally, a conclusion to this work is given in Section 7 as well as possible directions for future work."
    },
    {
        "id_": "559c62d6-88f9-4ec1-8bb6-9d03821daf28",
        "text": "# 2 Methods\n\nThis section will introduce the methods that will be used for comparison with the proposed MCGAE method, more specifically, AutoEncoder Deep Support Vector Data Description (AE-DSVDD) and Constraint Guided AutoEncoders (CGAE). The data used in this work consists of normal, unlabeled, and anomalous points. The normal points correspond to measurements taken when the machine was in a healthy condition, the unlabeled points correspond to a situation where the machine condition is considered to be unknown, and anomalous points correspond to faulty machine conditions, this will be explained in more detail in Section 4.1."
    },
    {
        "id_": "5402fd15-0af5-46c6-a6ce-7162917a2eb1",
        "text": "# 2.1 AutoEncoder Deep Support Vector Data Description\n\nThe learning objectives of both AE and DSVDD are combined in AutoEncoder Deep Support Vector Data Description (Zhang and Deng, 2021). The resulting objective learns to both reconstruct normal points as good as possible, as per the AE objective, and also to map the normal points as close as possible to a center point c ∈ RDl, with Dl the amount of dimensions in a latent space, and anomalous points as far away as."
    },
    {
        "id_": "21f3e8fc-0755-401f-bedf-d4cb35db51b7",
        "text": "# 1. Introduction\n\nThis is done by learning both the weights of an encoder E(·|✓E), where ✓E are the weights of the encoder of an AE, and those of a decoder D(·|✓D), where ✓D are the weights of the decoder of an AE, by solving the following objective:\n\nargmin ✓E,✓D 1 X ⟨kx D (E (x | ✓E ) | ✓D )k2 + kE(x|✓E ) ck2⟩ N x∈N\n\n+ Ax2A1 X ⟨kE(x|✓E ) ck2⟩1,\n\nwhere N=1 and N A are the amount of normal and anomalous points, respectively, N = {n}i is the set of normal points, A = {a}i=1 is the set of anomalous points, i A > 0 is a hyperparameter that balances the weight of anomalous data in comparison with normal data, and || · || is the L2-norm. Do note that the objective in (Zhang and Deng, 2021) only uses normal points, whereas (1) also incorporates anomalous data. This addition of anomalous data has already been used in (Meire et al, 2023a).\n\nDetermining the CI for an input point x can then be done by using the distance of the encoding of x to the center c as follows:\n\nfCI(x) = kE(x|✓E ) ck2.\n\nThis CI can then be used to determine if x should be considered as normal or anomalous by comparing it to a threshold T using the following classification rule:\n\nˆ(x) =\ny 1, fCI(x) > T\n1, fCI(x) ≤ T,\n\nwhere ˆ = 1 and ˆ = 1 corresponds to a normal and anomalous point, respectively.\n\nAs already mentioned in Section 1, there are different methods to determine T, and in this work three methods, that were described in (Meire et al, 2023a), will be used. The first method determines an \"optimal\" threshold Topt. The value is chosen such that a chosen metric is optimized for the considered test set. As this is the best possible threshold, the performance can be regarded as an upper bound. The second method determines a threshold Ttrain by computing the mean μn and standard deviation σn of the estimated CI for the normal training data and then sets Ttrain to μn + 3σn. This is one of the more simple and basic methods to determine a threshold. The third and final method attempts to fit a sigmoid on the estimated CI of the normal training data, such that the median and 99-th percentile are receiving values 0.25 and 0.5 at the sigmoid’s output, respectively. The supremum, the maximum value of the sigmoid, is set to 1. By setting the threshold to 0.6 on this sigmoid and then inverting the function, the value of this threshold Tsigmoid can be determined. Note that the latter has similarities with the output layer of DL models as they typically employ sigmoid activation functions.\n\n6"
    },
    {
        "id_": "e5130979-6ffb-4f2d-8d1f-ae686a4f3828",
        "text": "# 2.2 Constraint Guided Autoencoder\n\nIn the Constraint Guided AutoEncoder (CGAE) (Meire et al, 2023a) the learning objective of an AE is subjected to a set of constraints: (i) normal data should be mapped inside of a sphere, with radius R1, around the origin, and (ii) anomalous data should be mapped outside of a larger sphere, with radius R2, around the origin. This learning objective is made with the assumption that at least some anomalous data is available, so that the model can learn to distinguish between normal and anomalous data, and it is also assumed that R1 + < R2 with some positive constant, so that there is a separation between the mapping of the normal and anomalous data, which is expected to lead to a better generalization. The resulting constrained learning objective is as follows:\n\nargminθE,θD 1/N Σx∈N ||x - D(E(x | θE) | θD)||2,\n\nwhere B[0, R1] and B[0, R2] denote the closed balls, with radius R1 and R2, respectively, around the origin. The learning objective is optimized using CGGD (Van Baelen and Karsmakers, 2023).\n\nTo determine the CI for an input point x a similar procedure as for AE-DSVDD can be followed, with the difference being that for CGAE the origin is considered as the center instead of a point c:\n\nfCI(x) = ||E(x | θE) - 0||2.\n\nBy using the constrained learning objective, it is expected that a (near) “optimal” threshold can be found somewhere in the interval [R1, R2], as this lies between the normal and anomalous data. Following this expectation, the threshold associated with CGAE is defined as:\n\nT := R1 + (R2 - R1) N + A .A\n\nDo note that this threshold only depends on the amount of known normal and anomalous points that are available at training time and the choice of R1 and R2. Furthermore, it is not influenced by the actual training of the model, as is the case for Ttrain and Tsigmoid. This threshold can then be used in (3) to determine if a point should be considered normal or anomalous."
    },
    {
        "id_": "15894718-a626-4271-a94b-f30cfc28b663",
        "text": "# 3 Monotonically Constraint Guided Autoencoder\n\nIn the application of condition monitoring of machines, it is natural to assume that the health of a machine cannot improve over time. Equivalently, the machine can over time only become less healthy or, equivalently, the CI can only increase over time. Therefore, it is desirable to predict a monotonically increasing sequence of norms for the encodings of the life time of a single machine, hereafter called a run. More"
    },
    {
        "id_": "a24a3bb6-a4b5-4fbe-ab21-4150c39e67ba",
        "text": "specifically, for a given time series of data {xj}j (with the data chronologically ordered from left to right) consisting of normal and unlabeled data points of a single recording, it should hold that\n\nj1 &lt; j2 ) kE(xj1 |✓E)k &lt; kE(xj2 |✓E)k.\n\nTo this end we propose Monotonically Constraint Guided AutoEncoder (MCGAE), which extends CGAE with a constraint that enforces this monotonous behavior. This results in the extension of the optimization problem (4) to\n\nargmin✓E,✓D Xx∈N D (E (x | ✓E) | ✓D)2,\n\nwhere the constraints are:\n\n- s.t. ∀x ∈ N : E(x|✓E) ∈ B[0, R1],\n- ∀x ∈ A : E(x|✓E) / B[0, R2],\n- ∀r, ∀xt1, xt2 ∈ Sr : t1 &lt; t2 &lt; tr kE(xt1 |✓E)k &lt; kE(xt2 |✓E)k.\n\nwhere r denotes the considered run, tar is the time at which the first anomalous point occurs for run r, and Sr = Nr ∪ Ur | [ Ur, with Nr the set of normal points of run r, Ur = {ui | i=1} being the set of unlabeled points of run r, with Ur the amount of unlabeled points of run r, and xti is the sample on time ti of run r. For simplicity, we omit the time index if it is not required. Note that only points in a single run can be compared to each other for the monotonicity constraint, because, there is, in general, no relation between the encodings of certain time points from different runs.\n\nThis optimization problem can be optimized using CGGD (Van Baelen and Karsmakers, 2023). The latter framework enables training a neural network by minimizing a loss function while satisfying constraints. This is realized by prioritizing satisfying the constraints over minimizing the objective function during every step of the optimization. In order to do so, it is required to define the direction by which a certain model variable needs to be adjusted when it violates a corresponding constraint. This direction needs to be chosen such that when following it during the update the resulting updated model at least comes closer to the feasible region (that contains model solutions that satisfy the constraint). In case a constraint is satisfied the direction is considered to be 0, otherwise, the different directions can be computed as follows.\n\nSimilar to CGAE, in case (8) is not satisfied, the direction to update the model when normal training points are considered (dirN) is computed so that these points will be mapped inside of B[0, R1]\n\ndirN(x, E) := kE (x | ✓E)k .E (x | ✓E)\n\nConsidering (11) it can be observed that a scaled version of direction (dirN) is added to the considered weight ✓j which will cause the solution to move towards the inside of the ball B[0, R1]. The latter occurs as the computed direction will be followed in the opposite direction, due to the update procedure. In this case, the computed direction is the vector from the origin to E (x | ✓E), and hence, following this vector in the opposite direction causes the solution to move inwards.\n\nIn the previous equation a direction was calculated for a single point x. To evaluate a set of points, this direction is aggregated, by summation. As mentioned earlier,"
    },
    {
        "id_": "8aad34bd-ffdd-4c53-9ba7-7c513e41b214",
        "text": "dirN (x, E) is considered to be 0 if the constraint is satisfied for point x. In case E (x | ✓E ) = 0, thus when the encoding of x is the origin, the direction to the origin is not computed, instead a random direction is chosen, that is, by sampling from a uniform distribution on the unit sphere.\n\nIn a similar manner, in case needed, to satisfy (9), the direction for the anomalous training points (dirA) is computed so that these points will be mapped outside of B[0, R2]\n\ndirA (x, E) := kE (x | ✓E )). E (x | ✓E k\n\nTo satisfy (10), the direction that causes the normal and unlabeled points to be mapped so that the norm of their corresponding encodings monotonically increases is computed as follows. For a given run, a function argsort(·) is used that takes in a set of points and returns a vector of indices that are ranked in ascending order according to the norm of their corresponding encodings.\n\nvector ⇥0 ... This vector of indices can then be compared to the expected order expressed by⇤ by performing an element-wise subtraction to get the difference |Sr |  1 between the rankings. The resulting difference can then be normalized, using the L2 norm, to obtain the direction. More formally this becomes:\n\nargsort⇣nkE(x|✓E )k2 | x 2 Sro⌘ ⇥0 ... ⇣n |Sr |  1⇤\n\ndirmono (Sr , E) := argsort kE(x|✓E )k2 | x 2 Sro⌘ ⇥0 ... |Sr |  1⇤.\n\nNote that, if the ranking for a point is far from the desired ranking, this point will more strongly influence the direction than if the ranking of a point is only 1 position different from the desired ranking.\n\nThe weights of a model can then be updated using the objective function and the directions for the different constraints\n\n✓j+1 = ✓j ⌘ [rL (N , (N ))\n\n+ R dirN (N , E) max {kreL (N , (N ))k , ⇣}\n\n+ R dirA (A, E) max {kreL (N , (N ))k , ⇣}\n\n+R dirmono (Sr , E) max {kreL (N , (N ))k , ⇣}] ,\n\nwhere (N ) = D (E (N | ✓E ) | ✓D ), ⌘ is the learning rate, R > 1 a fixed real number called the rescale factor, ⇣ > 0 a fixed real number, which is chosen small, L is the objective function from (7), reL is the gradient of the loss function with respect to the encodings, and ✓ is the union of ✓E and ✓D . (N ) denotes the evaluation of the set of normal points N by the model. This evaluation is done separately for each point in the set, and results in a new set that contains these evaluations. The objective function from (7) only uses normal data, this is denoted in (11) by the use of the set of normal points N . As it is not feasible to use an entire run at once during training,\n\n9"
    },
    {
        "id_": "d9cbf3e3-cd92-41ef-968e-7c255ddf91bc",
        "text": "this process will be performed on batches. While this direction can be computed with only 2 points, it is expected that this will not perform well, and that more points are needed for this constraint to be used to compute a better direction. Therefore, batches are used. Note that these are created in a stratified way, where it is guaranteed that, if a run is present in a batch, sufficient, e.g. 10 or more, points of that run are present in that batch.\n\nAs MCGAE is an extension of CGAE, the CI for an input point x can be determined using (7), and the associated threshold can also be acquired using (6). This threshold can then also be used in (3) to distinguish between normal and anomalous points."
    },
    {
        "id_": "15d03a87-1c53-4f8c-bb39-b44136acee80",
        "text": "# 4 Experimental setup\n\nThis section will discuss the datasets used in this work as well as the associated preprocessing, the model architectures and hyperparameter settings, and the relevant metrics."
    },
    {
        "id_": "cf8350dd-6149-4d1d-9031-edb1dd992a75",
        "text": "# 4.1 Datasets\n\nThe experiments in this work will be performed on two datasets. Firstly, a dataset containing vibration data from accelerated run-to-failure tests of bearings, hereafter called the Smart Maintenance (SM) dataset. Secondly, a dataset that contains acoustic data from accelerated run-to-failure tests of bearings, hereafter called Acoustic Bearing Monitoring (ABM) dataset."
    },
    {
        "id_": "c6f94df3-26d5-4aa6-ae54-16dd343ce572",
        "text": "# 4.1.1 Smart Maintenance\n\nThe Smart Maintenance (SM) dataset contains vibration data of a total of 70 accelerated run-to-failure tests, hereafter called runs, of bearings and was already used in (Meire et al, 2023a) and (Meire et al, 2022). One of the 7 bearing rigs used to perform the tests is shown in Figure 1.\n\nThe runs were performed under fixed operation conditions, more specifically a high radial load of 9 kN and with the shaft rotating at 2000 rpm, and the stopping condition was set to 20g peak acceleration, which is the assumed end-of-life. The high radial load and a very small indentation in the inner race of the bearing were used to significantly accelerate the bearing lifetime. Radial accelerations were measured by an accelerometer, attached to the bearing housing, at a sampling frequency of 50 kHz. As the aim is to work with a run-to-failure test under fixed conditions, data collected at a rotational speed lower than 2000 rpm during the initial run-up or during short measurement interruptions was omitted during the experiments.\n\nOnly data from 1 of the 7 bearing test rigs was selected to reduce the computing time needed for the ablation studies in this work. The data of this single setup contains 6 runs that start with a small initial indentation in the inner race of the bearing and end with a severe fatigue fault. The length of these runs ranges from roughly 6600 to 14500 seconds, and the fault occurrence ranges from roughly 100 to 3000 seconds prior to the end of the run."
    },
    {
        "id_": "ff65abb6-8445-45b2-a2af-2ba8230a36b8",
        "text": "# 4.1.2 Acoustic Bearing Monitoring\n\nThe Acoustic Bearing Monitoring (ABM) dataset contains both vibration and acoustic data of a total of 64 accelerated run-to-failure tests, hereafter called runs, of bearings and was collected on the same bearing test rig as shown in Figure 1. The placement of the internal microphone can be seen in Figure 2. This dataset was already used in (Meire et al, 2023b). In this work the data collected by the internal microphone, from runs with a fixed speed of 2000 rpm, and a fixed load were selected, resulting in 5 runs being retained. The length of these runs ranges from roughly 5000 to 18500 seconds, and the fault occurrence ranges from roughly 700 to 4300 seconds prior to the end of the run.\n\nData was captured with a sampling frequency of 50 kHz for each sensor. Each run was accelerated in the same manner as the SM dataset, by creating a small indent on the inner race of the bearings in addition to a high radial load and the same stopping criteria of 20g peak accelerations as for the SM dataset was used.\n\nNext to the raw data, a ground truth was also provided. An important note is that this labelling was not based on the physical state of the bearings, but on analysis of the data collected by the internal microphone. This dataset was also split into 3 segments along the time axis, similar to the SM dataset, by using two cuto↵ points ph and pf. ph was determined in the same way as for the SM dataset and pf was determined as the point in time where the faulty behavior was first detected."
    },
    {
        "id_": "64a2cac6-5995-436f-9c86-81e273f06125",
        "text": "# 4.2 Preprocessing\n\nThe acceleration and acoustic signals are first transformed into a time-frequency representation, more specifically log mel spectra, as was done in (Meire et al, 2022, 2023a). This transformation is performed by first acquiring time-frequency spectra from the raw signals, using a window and hop size of 1 second. From these spectra mel bands are then extracted, 64 and 512 bands for the ABM and SM datasets, respectively, and these are then log scaled. Finally, 8 consecutive seconds were concatenated into an input frame, with shapes (64,8) and (512,8) for the ABM and SM dataset, respectively, which was then used as an input to the models. It was noticed that the distribution of the values of the mel spectra is different between runs. Therefore, each run was separately standardized to have zero mean and unit variance prior to ph."
    },
    {
        "id_": "ddc1f93a-4a95-44a0-a391-71c6a9aa6ff5",
        "text": "# 4.3 Experimental details\n\nThe models used in the experiments all use a similar AE architecture, consisting of an encoder and decoder. The encoder uses convolutional blocks, consisting of a convolutional layer, followed by batch normalisation (Ioffe and Szegedy, 2015), an activation function and finally a max pooling with stride and size of 2. The decoder uses\n\nFig. 2: The microphone setup used in the bearing run-to-failure tests."
    },
    {
        "id_": "6c513e70-f4f8-4e28-8359-589f87615381",
        "text": "# 4.4 Metrics\n\nThe alternative methods studied in this paper will be compared in two ways: (i) the ability to discriminate between normal and anomalous behavior, and (ii) the quality of the models for the SM model were trained for 300 epochs, and the models for the ABM dataset for 500 epochs, both with the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 1e3. The best performing model was saved during training, and if the performance did not improve for 30 epochs, the learning rate was halved, up to a limit of 1e6. The training performance is determined by the learning objective associated with the algorithms, as described in Sections 2 and 3. For CGAE and MCGAE the satisfaction ratio of the constraints, the amount of points for which the constraints are satisfied with respect to the total amount of points, is also considered. An improvement in model performance for AE-DSVDD corresponds to the lowering of the learning objective on the validation set. When considering CGAE or MCGAE, the improvement corresponds to both the lowering of the learning objective on the validation set and either an increase of the satisfaction ratio on the associated constraints or the satisfaction ratio being above 95%.\n\nIt was previously mentioned that the monotonicity constraint can only be applied to data from the same runs, and hence it is crucial that the runs in each batch are represented by multiple points. To ensure this is the case, the batches were created in a structured manner. This is done by first setting a number of runs that should be present in a batch and a number of points that should be selected from each run. Based on these settings a batch is created by first randomly selecting runs and then selecting points from each of these runs. For the latter, a stratified sampling was used that first selects a fixed number of points of the normal and, if applicable, anomalous data in a run and then another fixed number of points from the unlabeled data. For the SM dataset each epoch a total of 80 batches were created in this way, with 5 runs in each batch, with 25 normal or anomalous points and an additional 10 unlabeled points for each of these runs. For the ABM dataset the amount of batches was increased to 100. However, each batch only contained 4 runs, with the same amounts of points as the SM dataset. Do note that this procedure will cause some points to be included in multiple batches in a single epoch. However, this is not expected to have much impact on the training of the models."
    },
    {
        "id_": "b7dd9385-8a19-42c8-853d-d61b25b63566",
        "text": "the CI in terms of its monotonicity. The performance with regards to the discrimination between normal and anomalous data is evaluated using the balanced accuracy (BA) (12), which is the mean of the recall obtained on each class. While it is common to use the F1 score to evaluate discrimative performance, it was opted to use the BA instead in this work for two reasons. Firstly, both the F1 score and BA were used in (Meire et al, 2023a), where it could be seen that both showed similar results. Secondly, the different runs show a varying amount of normal and anomalous data, which is not ideal for the F1 score as it is biased towards correctly classifying the positive class. In this work anomalous data is considered as the positive class, leading to\n\nBA = 1 - (T N + F P) + (T P + F N) T N T P\n\n2\n\nwhere T P, F P, T N, and F N correspond to the amount of true positives, false positives, true negatives, and false negatives, respectively. To evaluate the BA, a threshold is required. Three thresholds, Topt, Ttrain, and Tsigmoid were described in Section 2.1 and will be used in the evaluation.\n\nThe second evaluation looks into the quality of the CI. A high quality CI is expected to show an increasing trend up to a point where a fault occurs. In (Kim et al, 2016) and (Meire et al, 2022) this was done by using the Spearman’s ρ (Kokoska and Zwillinger, 2000), which correlates the rank of two variables, A and B, and is calculated as follows,\n\nρ = cov(r(A), r(B))\n\nr(A)r(B)\n\nwith r(A) and r(B) being the ranking of A and B, in ascending order, and cov and σ being the covariance and the standard deviation, respectively. In the context of this work, A and B correspond to the CI and the time, respectively."
    },
    {
        "id_": "baa75399-53e2-490f-9d0c-c9806bfdd1ea",
        "text": "# 5 Experiments and results\n\nIn this section a comparison is made between CGAE, AE-DSVDD, and our proposed method, MCGAE."
    },
    {
        "id_": "68d774bd-1709-4c28-b072-6ad72c8ae9c3",
        "text": "# 5.1 Experiments\n\nTo compare alternatives, a leave-one-run-out scheme using both the SM and ABM dataset was employed. All experiments were repeated 3 times, using different seeds for model initialisation and batch generation. As indicated earlier the selected subset of the SM dataset has 6 faulty runs. This results in 6 folds, as per the leave-one-run-out scheme, with each fold using 5 runs for training and validation, split 75% and 25%, respectively, and the remaining run for testing. The amount of anomalous data was incrementally increased by selecting the anomalous data from either 1, 2, 3, 4, or 5 runs used in the training and validation sets. For each of these results the mean and standard deviation over the different folds was computed."
    },
    {
        "id_": "129e931c-870e-4fcd-9399-f3f35edae648",
        "text": "# 5.2 Results\n\nThis section will discuss the results of the performed experiments, as described in Section 5.1. Firstly, the discriminative performance with regards to the BA was evaluated. Secondly, the quality of the generated CI was evaluated using the Spearman’s ρ. When evaluating the discriminative performance, a trivial predictor, that predicts all data as anomalous, is also provided for reference. Next to the fixed thresholds and the trivial predictor, an “optimal” threshold is also considered. Its corresponding discrimination performance can be considered as an upper bound. As MCGAE extends CGAE, it is expected that the former outperforms the latter, mainly with regards to the quality of the CI, as this is the focus of the additional monotonicity constraint."
    },
    {
        "id_": "78dd1c52-c80c-4e9d-8b4d-35952b02c22d",
        "text": "# 5.2.1 Discriminative performance\n\nThe results of the comparison of the different methods in terms of the BA are shown in Figure 3.\n\nWhen considering the results that are obtained when an optimal threshold was used it can be observed that MCGAE has equal or slightly improved performance compared to the alternatives. For the SM dataset this better performance is only noticeable when using anomalous data from 1 run, while for the ABM dataset MCGAE shows a consistent (near) perfect performance, and only when anomalous data from all 4 runs is used the performance of CGAE is similar. CGAE does perform slightly worse than AE-DSVDD when anomalous data from 1 or 2 runs is used on the ABM dataset. However, when more anomalous data is used it performs slightly better. However, do remember that these thresholds are determined using the test data, and should be considered as the upper bound for the performance.\n\nWhen not using an optimal threshold, both MCGAE and CGAE attain a performance that is significantly better than AE-DSVDD using a threshold based on training statistics, except for the case where the anomalous data from 1 run was used on the SM dataset. Upon closer inspection of the generated CI, it was noticed that this lower score is attributed to 2 test runs where the CI for anomalous data is estimated to be lower than the threshold, whereas the other test runs perform closer to the behavior."
    },
    {
        "id_": "e8f15c3b-93b1-42fc-9984-853ad4f08a73",
        "text": "# Smart Maintenance"
    },
    {
        "id_": "baa4627b-face-40c0-bb80-ab232075cea9",
        "text": "# Acoustic Bearing Monitoring\n\n|MCGAE|CGAE|AE-DSVDD optima|AE-DSVDD sigmoid|\n|---|---|---|---|\n|MCGAE optimal|CGAE optimal|AE-DSVDD train|Trivial predictor|"
    },
    {
        "id_": "82e42f20-d74e-4778-a51b-789e885c36e5",
        "text": "# Performance Metrics\n\n|1.0|1.0|\n|---|---|\n|0.9| |\n|0.8| |\n|0.6|0.200|\n|0.20|0.175|\n|0.15|0.150|\n|0.10|0.100|\n|0.075| |\n|0.05|0.050|\n|0.025| |\n|0.00|0.000|\n\nAnomalous data from number of runs\n\nFig. 3: The mean (a) and standard deviation (c) of the BA on the SM dataset, and the mean (b) and standard deviation (d) on the ABM dataset, for the different methods and different thresholds. The thresholds are Ttrain, Tsigmoid, and Topt, as described in Section 4.4. The x-axis indicates from how many runs the anomalous data is used.\n\nthat is observed when more anomalous data is used. This lower performance might be due to a combination of 2 reasons. The first being that the model has only seen data from 1 type of anomaly, as only 1 run was used to select anomalous data. The second being that it is possible that the model was trained with a limited amount of anomalous data for 1 or more folds, as anomalous data is selected from only 1 run and some runs only contain a limited amount of anomalous data. When using anomalous data from more runs this is no longer the case, and the BA remains relatively constant."
    },
    {
        "id_": "f2178c5b-a930-4692-9813-521490865bc0",
        "text": "# 5.2.2 Condition indicator\n\nNext to discrimination performance also the quality of the CI estimation is assessed. More specifically, the quality of the CI is evaluated using the Spearman’s ⇢, described in Section 4.4, for which the results can be found in Table 1."
    },
    {
        "id_": "40047a92-5768-41ed-8471-a58f0df36000",
        "text": "# Table 1: The Spearman’s ⇢ results for the both datasets on the test set. The best results for each amount of anomalous data are shown in bold.\n\n|Anomalous data from number of runs|SM data set|SM data set|SM data set| | |\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.658 ± 0.304|0.439 ± 0.229|0.284 ± 0.285|\n|2|0.667 ± 0.233|0.439 ± 0.419|0.205 ± 0.597|\n|3|0.588 ± 0.369|0.538 ± 0.432|0.238 ± 0.415|\n|4|0.582 ± 0.377|0.515 ± 0.452|0.475 ± 0.319|\n|5|0.558 ± 0.403|0.475 ± 0.467|0.518 ± 0.350|\n\n|Anomalous data from number of runs|ABM data set|ABM data set|ABM data set| | |\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.228 ± 0.308|0.157 ± 0.247|0.184 ± 0.456|\n|2|0.249 ± 0.285|0.099 ± 0.251|0.116 ± 0.428|\n|3|0.213 ± 0.451|0.034 ± 0.255|-0.052 ± 0.516|\n|4|0.174 ± 0.438|0.007 ± 0.319|0.053 ± 0.400|\n\nIt can be seen that MCGAE shows the highest Spearman’s ⇢ overall for both datasets. When looking at the results for MCGAE for the SM dataset more closely, a better performance is noticed when anomalous data from only 1 or 2 is used. The lower performance when using more anomalous data is caused by 2 test runs performing poorer while the other test runs show a more constant performance. This slight decrease in the Spearman’s ⇢ when anomalous data from more runs is used can also be seen on the ABM dataset, albeit to a lesser extent."
    },
    {
        "id_": "b740266e-a00a-4322-b575-1aa2bbe1a557",
        "text": "While the behavior of CGAE and AE-DSVDD is similar, it can be seen that the former does perform slightly better when considering the SM dataset. However, as was also the case for MCGAE, the performance difference is not always clear when considering the ABM dataset. This different behavior between the datasets could be due to the ABM dataset being more noisy, as it contains acoustic data, in comparison to the SM dataset, which contains vibration data.\n\nWhile MCGAE does show a better performance, it also shows a decreasing trend when using anomalous data from more runs. As mentioned earlier, this is mainly due to 2 test runs showing a strong decrease in Spearman’s ρ. However, when observing the other methods for the SM dataset, an increasing trend can be seen. This could be due to the nature of the constraint not being simple to generalize to different unseen test runs, as an increasing trend in a CI over time is strongly tied to the run itself, and bearings can show different degradation patterns, even when operating under the same conditions (Sikorska et al, 2011). The overall lower Spearman’s ρ on the ABM dataset is likely also due to poor generalization, which is further worsened due to increased noise in the dataset.\n\nTo investigate the effect of the added constraint without the difficulties of the generalization, the Spearman’s ρ is also evaluated on the training set, and the corresponding results are shown in Table 2."
    },
    {
        "id_": "eace3441-05c0-4567-9125-26a47697dd3b",
        "text": "# Table 2: The Spearman’s ρ results for the both datasets on the training set. The best results for each amount of anomalous data are shown in bold.\n\n|Anomalous data from number of runs|Smart Maintenance|Smart Maintenance|Smart Maintenance| | |\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.891 ± 0.062|0.235 ± 0.554|0.396 ± 0.437|\n|2|0.926 ± 0.040|0.536 ± 0.445|0.547 ± 0.305|\n|3|0.931 ± 0.037|0.626 ± 0.336|0.579 ± 0.343|\n|4|0.943 ± 0.029|0.677 ± 0.341|0.608 ± 0.254|\n|5|0.945 ± 0.030|0.631 ± 0.361|0.658 ± 0.211|"
    },
    {
        "id_": "ce5fb587-6636-49b2-ac5a-6f76f49ee085",
        "text": "# Acoustic Bearing Monitoring\n\n|Anomalous data from number of runs|MCGAE|CGAE|AE-DSVDD|\n|---|---|---|---|\n| |MCGAE|CGAE|AE-DSVDD|\n|1|0.860 ± 0.089|0.216 ± 0.341|0.463 ± 0.318|\n|2|0.872 ± 0.091|0.266 ± 0.354|0.473 ± 0.301|\n|3|0.883 ± 0.096|0.355 ± 0.274|0.553 ± 0.253|\n|4|0.898 ± 0.080|0.433 ± 0.221|0.555 ± 0.251|\n\nIt is clear that MCGAE attains the highest Spearman’s ρ when considering the runs used during training, and that the difference with the other methods is significantly larger than on the test set. This indicates the effectiveness of the constraint and that the generalization to an unseen test run is indeed not simple. When comparing the difference in performance for MCGAE on the training and test set between the SM and ABM dataset, it is clear that this difference is noticeably larger for the latter. This indicates that the generalization is indeed more difficult on the ABM dataset, likely due to the increased acoustic noise in the dataset, as mentioned earlier.\n\n18"
    },
    {
        "id_": "cce06c37-f732-4371-9eb2-69f7fd542397",
        "text": "# 6 Ablation study\n\nThis section discusses the results obtained when reconstructing additional data next to only the normal data. Experiments were performed with 4 different combinations of reconstructed data: (i) only the normal data (MCGAE-n), these are the same as in Section 5.2, (ii) additionally reconstructing unlabeled data (MCGAE-nu), (iii) additionally reconstructing anomalous data (MCGAE-na), and (iv) additionally reconstructing both unlabeled and anomalous data (MCGAE-nua)."
    },
    {
        "id_": "66aac38f-8d10-4b44-9dcb-2b58946a31d5",
        "text": "# 6.1 Discriminative performance\n\nThe results of the comparison of the different reconstruction based on the BA is shown in Figure 4. It can be seen that the results on the SM and ABM datasets are different. When considering a chosen threshold on the SM dataset, it is clear that there are no large differences in performance for the different reconstructions. However, MCGAE-n seems to perform slightly worse, mainly when using anomalous data from up to 3 runs. This is due to 1 or 2 test runs, across the different folds and seeds, performing worse, while the other test runs perform the same. It also seems that MCGAE-na and MCGAE-nua perform slightly better overall. However, the difference is very minimal. When considering an optimal threshold this performance difference is even further reduced, as all reconstructions attain a near perfect BA, except for when anomalous data from 1 run is used. This indicates that the lower performance of MCGAE-n using the chosen threshold is due to a mismatch between the chosen and optimal threshold, and not due to the model performing worse.\n\nWhen considering a chosen threshold on the ABM dataset, it is indicated that MCGAE-n and MCGAE-nu perform better overall, except when anomalous data from 1 or 2 runs is used. This behavior is slightly different when an optimal threshold is considered, as MCGAE-n outperforms MCGAE-nu when anomalous data from less than 3 runs is used. It is indicated that MCGAE-na performs the worst, when considering a chosen threshold, although the difference with MCGAE-nua is minimal when."
    },
    {
        "id_": "5b95bc60-aa37-43b6-8055-df00131408b9",
        "text": "# Smart Maintenance"
    },
    {
        "id_": "3b83086c-6b71-4811-9dad-1c8491497c86",
        "text": "# Acoustic Bearing Monitoring\n\n|MCGAE-n|MCGAE-na|MCGAE-n optimal|MCGAE-na optimal|\n|---|---|---|---|\n|1.00| |0.95| |\n|0.90| |0.90| |\n|0.85| |0.85| |\n|0.80| |0.80| |\n|0.75| |0.75| |\n|0.20| |0.175| |\n|0.15| |0.150| |\n|0.10| |0.100| |\n|0.05| |0.050| |\n|0.00| |0.025| |\n\nFig. 4: The mean (a) and standard deviation (c) of the BA on the SM dataset, and the mean (b) and standard deviation (d) on the ABM dataset, for the different reconstructions. The x-axis indicates the number of runs from which anomalous data is used.\n\nAnomalous data from more than 2 runs is used. When selecting an optimal threshold both MCGAE-na and MCGAE-nua also perform worse than MCGAE-n and MCGAE-nu. This could be due to the CI estimated by either MCGAE-na or MCGAE-nua, and to a lesser extent MCGAE-nu, being less distinct for the normal and anomalous data in comparison to MCGAE-n. The cause of this lesser distinction is mainly split into 2 parts. Firstly, it seems that reconstructing unlabeled data, MCGAE-nu and"
    },
    {
        "id_": "248f9568-49eb-4f16-9c67-b328059126cf",
        "text": "# 6.2 Condition indicator\n\n|Anomalous data from number of runs| | |Smart Maintenance| | |\n|---|---|---|---|---|---|\n|MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua| | |\n|1| |0.658 ± 0.304|0.601 ± 0.343|0.516 ± 0.401|0.569 ± 0.364|\n|2| |0.677 ± 0.233|0.622 ± 0.283|0.463 ± 0.035|0.479 ± 0.347|\n|3| |0.588 ± 0.369|0.631 ± 0.330|0.513 ± 0.377|0.476 ± 0.347|\n|4| |0.582 ± 0.377|0.620 ± 0.315|0.497 ± 0.389|0.539 ± 0.346|\n|5| |0.558 ± 0.403|0.553 ± 0.357|0.540 ± 0.343|0.569 ± 0.306|\n\n|Anomalous data from number of runs|Acoustic Bearing Monitoring|Acoustic Bearing Monitoring|Acoustic Bearing Monitoring|Acoustic Bearing Monitoring| | | |\n|---|---|---|---|---|\n|MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua| |\n|1|0.228 ± 0.308|0.160 ± 0.325|0.175 ± 0.280|0.066 ± 0.254|\n|2|0.249 ± 0.285|0.162 ± 0.315|0.153 ± 0.255|0.112 ± 0.332|\n|3|0.213 ± 0.451|0.175 ± 0.280|0.100 ± 0.366|0.086 ± 0.352|\n|4|0.174 ± 0.438|0.212 ± 0.408|0.162 ± 0.320|0.151 ± 0.382|\n\nThe evaluation of the quality of the CI using the Spearman’s ⇢ for the different reconstructions can be found in Table 3. Overall, it can be seen that MCGAE-n and MCGAE-nu show a higher Spearman’s ⇢ in comparison to MCGAE-na and MCGAE-nua, except for the SM dataset when using anomalous data from all runs. As mentioned in Section 5.2 the decreasing trend for MCGAE-n on the SM dataset is mostly due to 2 test runs. For MCGAE-nu the Spearman’s ⇢ shows a more increasing trend, expect for the SM dataset when anomalous data from all runs is used, where a noticeable decrease in performance can be seen. This is mainly due to a significantly lower performance of 4 test runs, across all folds and seeds, while the rest of the runs show a more stagnant Spearman’s ⇢. A small difference between the results for both datasets can be seen for MCGAE-na and MCGAE-nua. When considering the SM dataset, both reconstructions perform quite similarly. However, on the ABM dataset MCGAE-nua performs worse than MCGAE-na. As was also done when comparing the different methods, the quality of the CI on the training set will also be evaluated to omit the potentially difficult generalisation. The results are shown in Table 4."
    },
    {
        "id_": "47488585-b3f3-44e0-9982-3aa145eaf497",
        "text": "# Table 4: The Spearman’s ⇢ results for the both datasets on the training set. The best results for each amount of anomalous data are shown in bold.\n\n|Anomalous data from number of runs|Smart Maintenance|Smart Maintenance|Smart Maintenance|Smart Maintenance| | | |\n|---|---|---|---|---|\n| |MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua|\n|1|0.891 ± 0.062|0.906 ± 0.086|0.870 ± 0.088|0.877 ± 0.100|\n|2|0.926 ± 0.040|0.939 ± 0.044|0.899 ± 0.066|0.910 ± 0.082|\n|3|0.931 ± 0.037|0.952 ± 0.026|0.927 ± 0.049|0.931 ± 0.052|\n|4|0.943 ± 0.029|0.955 ± 0.035|0.944 ± 0.036|0.922 ± 0.129|\n|5|0.945 ± 0.030|0.965 ± 0.018|0.954 ± 0.024|0.956 ± 0.032|"
    },
    {
        "id_": "9a666c17-f66f-469e-9dc5-e7c773f2e0b5",
        "text": "# Acoustic Bearing Monitoring\n\n|Anomalous data from number of runs|MCGAE-n|MCGAE-nu|MCGAE-na|MCGAE-nua| | | |\n|---|---|---|---|---|---|---|---|\n| | | | |1|2|3|4|\n|1|0.860 ± 0.089|0.938 ± 0.052|0.886 ± 0.082|0.951 ± 0.037| | | |\n|2|0.872 ± 0.091|0.946 ± 0.052|0.907 ± 0.066|0.951 ± 0.050| | | |\n|3|0.883 ± 0.096|0.963 ± 0.039|0.925 ± 0.061|0.955 ± 0.049| | | |\n|4|0.898 ± 0.080|0.963 ± 0.035|0.936 ± 0.053|0.968 ± 0.024| | | |\n\nIt can be seen that these results show a different behavior than those on the test set. For the SM dataset, MCGAE-nu shows the overall highest Spearman’s ⇢ instead of MCGAE-n. Both MCGAE-na and MCGAE-nua still show a slightly lower Spearman’s ⇢, except for when anomalous data from all runs is used, although the difference is not always significant. For the ABM dataset, MCGAE-n shows the lowest Spearman’s ⇢ out of the different reconstructions, and MCGAE-nu and MCGAE-nua show the highest Spearman’s ⇢. This indicates that additionally reconstructing the unlabeled data seems to improve the effect on the constraint on the training set, likely due to a similarity between the normal data and most of the unlabeled data. The influence of reconstructing anomalous data is not as clear, for the SM dataset there is no real benefit, while for the ABM dataset there is some gain in Spearman’s ⇢ in comparison to MCGAE-n. However, MCGAE-nua performs similar to MCGAE-nu, indicating that the reconstruction of the unlabeled dataset has a stronger influence on the constraint.\n\nAs was done in Section 5.2, the evaluation with regards to how similar the fixed threshold are to an \"optimal\" threshold was also performed for this ablation study, and the detailed results are shown and discussed in Appendix B."
    },
    {
        "id_": "08fa8339-aa72-41b1-b690-907827a0788e",
        "text": "# 7 Conclusion and future work\n\nIn this work an extension to the CGAE algorithm, MCGAE, was proposed to jointly optimize a model for CI estimation and AD. This extension adds a constraint that enforces a monotonic behavior in the estimated CI, while retaining the ability of CGAE to discriminate between normal and abnormal data. The proposed MCGAE algorithm was evaluated and compared to AE-DSVDD and CGAE in terms of the discriminative performance and the quality of the CI. This comparison was performed on two datasets, the SM dataset containing vibration data from run-to-failure tests of bearings, and the ABM dataset containing acoustic data from similar run-to-failure."
    },
    {
        "id_": "e8a5bbd2-bad2-4356-aec5-d307fe4c3f52",
        "text": "# Tests\n\nNext to the comparison with AE-DSVDD and CGAE, an ablation study was also performed to evaluate the reconstruction of unlabeled and/or anomalous data. The results show that MCGAE attains a similar or slightly better discriminative performance than CGAE, depending on the considered dataset, while also providing a higher quality CI. However, it should be noted that, while increasing the amount of anomalous data improves the discriminative performance, it does result in a decrease in the quality of the CI. It is possible that this is due to the monotonicity constraint not being simple to generalize to unseen runs, as the increasing trend in CI over the life of a bearing is strongly tied to the bearing itself. Additionally, as the runs are highly accelerated, the variation in the degradation trends is also higher. Specifically for the ABM dataset, as acoustic data is more prone to noise than vibration data, this could result in the lower monotonicity. When evaluating the quality of the CI on the training set, to investigate the effect of the monotonicity constraint without the generalization, MCGAE showed a noticeably higher quality of the CI, and increasing the amount of anomalous data also improved the CI.\n\nThe results of the ablation study show that, depending on the dataset, the discriminative performance is affected differently based on what data is reconstructed. On the SM dataset, reconstructing additional data shows a slight improvement, while on the ABM dataset it noticeably lowers the performance. The quality of the CI on an unseen run is highest when reconstructing either only normal data, or normal and unlabeled data. However, when evaluating the quality of the CI on the training set, additionally reconstructing unlabeled data does show a clear increase in performance.\n\nA possibility for future research is the improvement of the function (6) that determines the threshold for (M)CGAE, either through including some (limited) test data for calibration, a dynamic thresholding mechanism, a more advanced function, or incorporating insights from e.g. industry, such as considering false negatives to be more important than false positives, or vice versa.\n\nAnother possibility could be investigating whether a MCGAE model could be fine-tuned to an unseen test set by using some data from this set. This could be seen as a more advanced calibration in comparison to what was mentioned in the previous point."
    }
]