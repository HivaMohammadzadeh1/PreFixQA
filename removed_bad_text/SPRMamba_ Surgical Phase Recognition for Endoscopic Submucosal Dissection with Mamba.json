[
    {
        "id_": "158ea364-29bb-45b3-b905-21af01bc7a2a",
        "text": "# 1. Introduction\n\nEndoscopic Submucosal Dissection (ESD) is a groundbreaking minimally invasive procedure that has transformed the management of early gastrointestinal cancers. Compared to traditional surgery, ESD offers patients reduced trauma, faster recovery times, and lower rates of cancer recurrence [1]. However, the intricacy of lesion characteristics and the delicate nature of the gastrointestinal tract pose a risk of unexpected complications during ESD, making proficiency in this technique limited to a few experienced endoscopists. With the increasing detection of gastrointestinal cancers through routine gastric endoscopy screenings, there is a growing demand for skilled endoscopists capable of performing ESD. Computer-assisted surgery (CAS) systems are an advanced medical technology that can significantly enhance surgical efficiency and reduce the risk of complications [2]. Surgical phase recognition (SPR) is a critical and challenging task within CAS systems, aimed at identifying surgical activities from surgical videos [3]. In ESD, the phases carry distinct risks, necessitating accurate phase recognition for real-time monitoring, surgical process optimization, context-aware decision support, and early anomaly detection [4]. Additionally, automated SPR can significantly improve postoperative reporting and video indexing, providing valuable educational resources for novice endoscopists [5]. Therefore, developing efficient and accurate video-based SPR algorithms is essential to meet the demands of modern surgical practice and education.\n\nLimited inter-phase variance and high intra-phase variance are the most critical challenges for automatically recognizing surgical phases from video. Even though a lot of surgical phase recognition methods have been proposed to address this issue, for example, Jin et al. [6] introduced TMRNet, which includes a repository to store remote information to learn the relationship between the current frame and previous frames. Czenpiel et al. [7] proposed TeCNO, an enhanced Multi-Stage Temporal Convolutional Network (MS-TCN) [8]. Gao et al. [9] devised Trans-SVNet, addressing fine-grained"
    },
    {
        "id_": "1e6fb45b-9e4b-451a-8e7d-638fd51e8be3",
        "text": "# Temporal Features Loss in TCNs with a Compact Transformer Model\n\nHowever, ESD surgical phase recognition is still challenged since their surgical phases are extremely complex and temporally imbalanced. Therefore, applying existing surgical phase recognition methods directly to ESD data leads to performance degradation [10, 11, 12]. A more efficient temporal modeling approach is needed to address the insufficient temporal modeling capabilities of traditional temporal models.\n\nRecently, a state-space model called Mamba has demonstrated substantial potential in modeling temporal contexts. Unlike traditional temporal models, the Mamba model shows higher robustness and accuracy in dealing with complex surgical phase transitions while avoiding the problem of the Transformer’s high computational complexity. Making Mamba well suited for modeling the temporal context in ESD videos. To fully utilize Mamba’s temporal modeling ability, we propose a Mamba-based surgical phase recognition framework called SPRMamba.\n\nSPRMamba leverages a ResNet-50 backbone for spatial feature extraction from ESD videos, followed by the application of four LSTContext modules to effectively combine short-term and long-term temporal contexts for phase recognition. Central to SPRMamba is the Scale Residual TranMamba (SRTM) module, which combines Mamba’s long-term temporal modeling capabilities with the Transformer’s ability to capture fine-grained details, addressing the complex phase relationships inherent in ESD surgeries.\n\nFurthermore, according to [13], allowing the model to operate on the complete input sequence is more beneficial compared to only accessing a subset of the input. Therefore, considering the quadratic complexity of the transformer and the high memory usage of O(L2) (where L represents the input sequence length), it is inappropriate to directly apply the transformer to uncut surgical videos. To address this issue, we designed two sampling strategies, a Long-range Sample and a Window Sample, to reduce computational complexity and support the online phase."
    },
    {
        "id_": "f8232d36-0e70-4298-9f5d-8d01be11994f",
        "text": "# Main Contributions\n\n- We propose a novel surgical phase recognition framework (SPRMamba), leveraging the Scaled Residual TranMamba (SRTM) module to efficiently model short-term and long-term temporal contexts.\n- Introducing Temporal Sample Strategy (TSS) with Window and Long-range sampling to reduce computational burden and support online phase recognition."
    },
    {
        "id_": "42389069-325d-4521-9805-9baee8d11820",
        "text": "# Conducting qualitative and quantitative experiments on ESD385 and Cholec80 datasets, demonstrating significant improvements over existing techniques."
    },
    {
        "id_": "8fd165f5-2aab-42b0-aab8-b365eb0da81a",
        "text": "# 2. Related Work"
    },
    {
        "id_": "59cc1fcb-0228-4f93-b498-b37a94d3e06e",
        "text": "# 2.1. Video understanding\n\nIn the field of video understanding, researchers are dedicated to developing models that can effectively capture spatiotemporal features to extract semantic information from dynamic scenes. Traditional video understanding methods often rely on 3D Convolutional Neural Networks (3D-CNNs) [14], which extend 2D convolutions to simultaneously process spatial and temporal dimensions. However, 3D-CNNs tend to have high computational complexity, limiting their application in long video analysis.\n\nIn the field of video understanding, effectively capturing spatiotemporal features is crucial for extracting semantic information from dynamic scenes. Early approaches, such as 3D Convolutional Neural Networks (3D-CNNs) [14], extended 2D convolutions to process spatial and temporal dimensions simultaneously, achieving reasonable performance in action segmentation tasks. However, the high computational complexity of 3D-CNNs makes them impractical for analyzing long videos, particularly in medical applications like ESD surgery, where procedures can last several hours. To address computational limitations, Temporal Convolutional Networks (TCNs) [15] emerged, which focus on capturing temporal dependencies through one-dimensional convolutions. While TCNs are more efficient and capable of handling longer video sequences, they struggle to model complex action variations and capture the fine-grained details needed for surgical phase recognition, where subtle movements and transitions between phases play a critical role.\n\nTo address this, Temporal Convolutional Networks (TCNs) [15] were proposed, focusing on capturing temporal information through one-dimensional convolutions, thereby modeling long-term dependencies with lower computational costs. However, TCNs face limitations in handling complex action variations and capturing fine-grained features. Transformer-based models have gained attention in recent years for their ability to model long-range dependencies using self-attention mechanisms [16]. While Transformers achieve state-of-the-art performance in video understanding tasks by capturing spatiotemporal relationships across frames, they are constrained by their quadratic computational complexity, which poses challenges when applying them to"
    },
    {
        "id_": "2e03e669-f629-433a-b564-a2e199bed45a",
        "text": "# 2.2. Surgical Phase Recognition\n\nResearch in the field of Surgical Phase Recognition mainly focuses on automatically detecting and classifying different phases of surgery by analyzing surgical videos. Early surgical phase recognition methods relied heavily on hand-designed features and statistical learning methods [17, 18, 19, 20, 3, 21, 22, 23]. However, these methods are limited by empirical design features and usually rely on predefined dependencies, making it difficult to accurately capture subtle motion features with strongly nonlinear dynamics, and their performance suffers greatly.\n\nTwinanda et al. [3] proposed the EndoNet model, a CNN-based approach that automatically extracts features and performs phase classification from laparoscopic surgical videos. This approach opened the way for SPR research using deep learning. Since then, researchers have sought to enhance SPR by incorporating more advanced temporal modeling techniques. For instance, long short-term memory (LSTM) networks have been widely adopted in models like PhaseNet [24], EndoLSTM [25], and SVRCNet [26], enabling the capture of both spatial and temporal dependencies.\n\nThese methods, by combining deep residual networks (ResNet) with LSTM modules, have shown promising improvements in recognizing surgical phases by effectively modeling temporal series data. However, these methods are computationally expensive when dealing with long time series and prone to misclassification when dealing with complex phase transitions.\n\nTo address these limitations, Transformer-based architectures have been introduced into SPR due to their capability of capturing long-range temporal dependencies. For example, Czempiel et al. [27] proposed a Transformer-based surgical phase recognition method, which significantly improves the model’s performance in complex surgical scenarios through the self-attention mechanism. More recently, Graph Neural Networks (GNNs) have been employed in SPR, with studies such as Padoy et al. [28] demonstrating their effectiveness in modeling complex spatiotemporal relationships between surgical tools and anatomical structures. Although existing techniques have made significant"
    },
    {
        "id_": "0fbaed56-efa6-4847-9134-9de69fa8d4a1",
        "text": "# 2.3. State Space Models\n\nRecently, State-Space Models (SSMs) have been proven to have transformer-level performance in capturing long sequences in the field of natural language processing [29, 30]. [29] introduced a new model for Structured State-Space Sequences (S4), specifically designed to model long-range dependencies exhibiting the well-established property of scaling linearly with sequence length. Based on this, [31] proposed an advanced layer called S5, which integrates MIMO SSM and efficient parallel scanning into the S4 architecture. This development aims to overcome the limitations of SSMs and improve their efficiency. In addition, [32] contributed a novel SSM layer H3, significantly narrowing the performance gap between SSM and transformer-based attention in language modeling. [33] Expand the S4 model by introducing additional gating units in the gated state space layer to enhance its expressiveness. More recently, [30] developed a universal language model called Mamba by introducing a data-dependent SSM layer and a selection mechanism using parallel scanning. Compared to transformers based on quadratic complexity attention, Mamba excels at handling long sequences with linear complexity.\n\nIn the field of vision, [34] proposed Visual Mamba (Vim), which combines position encoding and bi-directional scanning to efficiently capture the global context of an image. Pioneered the application of Mamba in vision tasks. Li et al. [35] constructed a generic framework called Video Mamba Suite to develop, validate, and analyze Mamba’s performance in video understanding. Besides, the great potential of Mamba has inspired a series of works [36, 37, 38, 39, 40], which demonstrated Mamba’s has better performance and higher GPU efficiency than Transformer on visual downstream tasks such as semantic segmentation and video understanding. In our work, we integrate Mamba into the surgical phase recognition model to efficiently capture the temporal context in surgical video."
    },
    {
        "id_": "dbd0c796-b668-4f93-a185-649e791938eb",
        "text": "# Window Sample\n\n|Insmance|LicMI LincaI|Resampie|\n|---|---|---|\n|TranMamba|Conild|Soltthix|\n|Instance|GELU|Long-range Sample|\n|MLP|Resampic|Sle Residual Tranhamba lar&r|\n|Scale Residual|TranMamba|TranMamba|\n\nFigure 1: A schematic overview of the LSTContext block; (a) This represents the Scale Residual TranMamba (SRTM), which is composed of the TranMamba and its main components; (b) The Temporal Sample Strategy is illustrated with a window of size 4. For window sampling, the sequence is partitioned into small windows, and the SRTM is computed for each window. For long-term sampling, the sequence is reordered such that the SRTM is computed over the entire, but sparsely sampled sequence. After the SRTM computation, the output is reordered to preserve the original sequence order."
    },
    {
        "id_": "d6cb1dc5-1618-4f56-ba32-7952bf60bc4a",
        "text": "# 3. METHODOLOGY"
    },
    {
        "id_": "a5e12645-fef8-471a-b633-15f52daaae92",
        "text": "# 3.1. Preliminaries\n\nState Space Models (SSMs) are typically considered linear time-invariant systems that map a 1-D function or sequence x(t) → R ↑ y(t) → R through a hidden state h(t) → R N. These systems are commonly represented by linear ordinary differential equations (ODEs) with A → RN ↑N as the evolution parameter, and B → R N ↑1, C → R 1↑N as projection parameters.\n\nh↓ (t) = Ah(t) + Bx(t), y(t) = Ch(t). (1)\n\nStructured State Space Sequence Models (S4) and Mamba are discrete versions of continuous systems, which include a time scale parameter ¯, B. A converts continuous parameters A and B into discrete parameters A! that common conversion method is Zero-Order Hold (ZOH), defined as follows:\n\nA¯ = exp(!A), B = (!A) ↔1 (exp(!A) ↓ I) · !B. ¯ (2)\n\nAfter the discretization of A¯, B¯, Eq.(1) can be expressed as discrete parameters:\n\nh t = A¯h t↔1 + B¯x t, y t = Ch . t (3)"
    },
    {
        "id_": "8c491e65-dabc-4db3-bf03-0f3da5300e8e",
        "text": "# 3.2. Scale Residual TranMamba\n\nFor the surgical phase recognition task, temporal information plays a crucial role in accurate recognition. However, due to the complexity of ESD surgeries, existing surgical phase recognition algorithms based on traditional temporal models are difficult to apply directly to ESD data. We aim to explore a simple and effective structure combining Mamba that simultaneously models both long-term and short-term temporal contexts. A straightforward approach is to combine the short-term temporal context modeled by the transformer with the long-term temporal context modeled by Mamba. Therefore, we propose the Scaled Residual TranMamba module, as shown in Fig. 1a.\n\nGiven the input feature Fin → RL↑C, the SRTM module first applies Instance norm[41]. Then, it uses the TranMamba module to capture long-term and short-term temporal context, thereby generating Fls → RL↑C. In addition, to obtain more comprehensive context information, the fusion of Fin and Fls was achieved through scale residual connections. The fused feature is followed by normalization utilizing Instance Norm and then MLP to learn deeper features. The entire process can be described as follows:\n\nF = ωFin + TranMamba(IN(Fin))(5)\n\nFout = MLP(IN(F))(6)\n\nSpecifically, there are three branches in the TranMamba module. The first branch takes the first quarter portion of the input feature C Fin along the channel dimension as input Fb1 → RL↑4, then expands the dimension to εC by a linear transformation, and then activates it using the SiLU[42] function. The second branch in the TranMamba module has inputs similar to the first branch. It takes the second quarter of the input features C Fin along the channel dimension as input Fb2 → RL↑4. Subsequently, these features are sequentially expanded through the dimensional expansion of the"
    },
    {
        "id_": "49189a22-689a-4e7a-8f97-7afa2f774cb7",
        "text": "# Figure 2\n\nA schematic overview of the proposed SPRMamba architecture, which consists of a ResNet-50, and four LSTContext blocks (top).\n\n|Dilated|GELU|Short-term Context|Long-term Context|Linear|\n|---|---|---|---|---|\n|ConvID|1|XN|1|XN|\n| |XN|1|9| |\n| |LSTContext|LSTContext|LSTContext|LSTContext|\n| |Block|Block|Block|Block|\n| | |Lincur Layer| | |\n| | |linenrLayet| | |\n| | |LnurLaye| | |\n\nAfterward, the features extracted from both branches are fused using the Hadamard product, which is designed to capture the long-term temporal context information in this way[30]. The third branch takes the last half of the input features F in C along the channel dimension as input Fb3 → RL↑2. Subsequently, Fb3 is input into the Attention layer and activated using the GELU function[43], aiming to capture short-term temporal context features. Finally, the short-term temporal features achieved from the third branch will be fused with the previously captured long-term temporal features via a linear mapping to obtain feature Fls. The entire process takes the following form:\n\nF1 = SiLU (Linear(Fb1)) (7)\n\nF2 = LN (SSM (Conv1d(Linear(Fb2)))) (8)\n\nAttention(Q, K, V) = softmax(QKTV) ↗ K (9)\n\nF3 = Dropout(Linear(GELU (Attention(Fb3)))) (10)\n\nFls = Linear(F1 ↘ F2 + F3) (11)\n\nwhere ↘ represents Hadamard product, Q, K, V → RL↑D are linearly transformed form Fb3."
    },
    {
        "id_": "ddbb8256-1fbb-4d4b-8339-23ec7fda01db",
        "text": "# 3.3. Temporal Sample Strategy\n\nDue to the quadratic complexity of self-attention blocks, it is impractical to apply attention to long sequences of untrimmed videos. This is because the L of the video sequence is very large, so we need to resample it to achieve modeling of long-term and local temporal contexts, as shown in Fig. 1b."
    },
    {
        "id_": "bc8899ea-f27f-47da-97ee-9be20a2dab8e",
        "text": "# Temporal Window Sample\n\nFor temporal window sampling (WS), we divide the sequence into non-overlapping windows of size W. Fig. 1b illustrates the case of W=4. Given that different tasks may have different time-dependent ranges, W is task-specific. We used W=64 in practice. The impact of W was evaluated in Section 4. Instead of modeling the temporal context over the entire sequence of length T, we model the temporal context T times, where each F b1 → R W ↑ 4, F b2 → R W ↑ 4, F b3 → R W ↑ 2 corresponds to each window."
    },
    {
        "id_": "5ad826fe-61e6-453f-8ba1-d407cabf8fce",
        "text": "# Temporal Long-range Sample\n\nFor the temporal long-range sample, we sample the input every G to divide G non-overlapping sequences and model the temporal context information of each sequence. In G = 4 shown in Fig. 1b, we model the temporal context information of each of the four downsampled sequences. In general, we model the temporal contexts for GC F b3 → R W ↑ 2 where the F b1 and F b2 are the same, i.e., F b1, F b2 → R W ↑ 4. The parameter G provides the flexibility to adjust the sparsity based on the available memory budget, e.g., G=1 corresponds to the case where the attention is applied to the entire sequence. In practice, we use G=64 and evaluate the impact of G in Section 4."
    },
    {
        "id_": "24736f3a-7cdf-400c-8e7b-96a2ba1faa43",
        "text": "# LSTContext Block\n\nThe top of Fig. 2 illustrates the entire LSTContext block. As in previous work [7], we use a 1D dilated convolution with kernel size 3. This is because the dilated convolution increases the receptive field without the need to increase the parameter number by increasing the kernel size. Where the dilation factor for each layer increases by a factor of 2 and the receptive field exponentially expands as the number of layers increases. Therefore, with a few parameters, we achieved a significantly large receptive field in the temporal sequence, which mitigated model overfitting and effectively promoted the accuracy of surgical phase recognition. The dilated convolution is followed by a Gaussian Error Linear Unit (GELU). In the LSTContext block, we first use the window sample and then the long-range sample, as shown in Fig. 1. Finally, we use a linear layer with residual connectivity to output the features for each frame, F → R L↑D."
    },
    {
        "id_": "ae25cde2-65c9-416b-8285-cb5dee8752ba",
        "text": "# 3.4. Overview\n\nThe architecture of the proposed SPRMamba is shown in Fig. 2. For a video feature sequence V → RL↑C↑H↑W RL↑D F → of length L, we first extract the spatial frame-level from a fixed ResNet-50[44], where D=2048 is the spatial dimension, and then SPHMamba uses a linear layer to reduce the feature dimension to 64. As in previous works, we repeated each LST-Context block N times, where the dilation factor of the dilation convolution was increased in each layer. After the first N layers of the LTContext block, we use an additional linear layer to further reduce the dimensionality D to 32. The dimensionality reduction method reduces the number of parameters from 2.49 million to 1.23 million without degrading the accuracy. We also use an additional Conv1D followed by a softmax layer to generate the frame-level class probabilities P → R L↑C. We proceed with three additional stages, each consisting of N layers of LSTContext blocks. At the beginning of each stage, we reset the dilation factor of the temporal convolution to 1 and compute the frame-level class probabilities P → RT ↑C a multi-stage loss. We use cross-entropy loss and the mean squared error smoothing loss introduced by [8] for supervised training."
    },
    {
        "id_": "a6febc47-3f6c-4d40-b1b3-11bc6ced5125",
        "text": "# 4. Experiments\n\nIn this section, we first describe the dataset used in our study, as well as the detailed experimental setup and evaluation metrics employed. Subsequently, we validated the effectiveness of our proposed method in ESD and cholecystectomy by comparing it with the SOTA method and conducting ablation experiments."
    },
    {
        "id_": "0e7c49a9-3399-4cf0-ae56-4393813fbeac",
        "text": "# 4.1. Dataset\n\n|ESD Surgical phases|Preparation|Estimation|Marking|Injection|Incision|ESD|Vessel-treatment|Clips|Total|\n|---|---|---|---|---|---|---|---|---|---|\n|Train|91739|43198|15585|44464|59695|174201|44317|10817|484016|\n|val|14193|5822|2820|9886|11917|41807|4865|3985|95295|\n|test|14323|16860|6583|15831|22181|75968|14707|4748|171201|"
    },
    {
        "id_": "c4a0ba31-1736-4093-b51e-987827764256",
        "text": "# 4.1.1. ESD385\n\nThere is no publicly available dataset on ESD surgery. To validate the effectiveness of the proposed algorithm in this study, we retrospectively selected"
    },
    {
        "id_": "a6e91bbe-06f3-487c-8a69-ecadf478c7ab",
        "text": "patients who underwent ESD from August 16, 2023, to January 8, 2024, in the endoscopy unit of the Department of Gastroenterology, Renji Hospital, Shanghai Jiao Tong University. A total of 385 videos of ESD procedures were collected. All procedures were performed by experienced endoscopists. The instruments used for ESD procedures included: gastroscope GIF-Q260, GIF-H260, GIF-HQ290, mucosal incision knife KD-612L/U, KD-655L/U, KD-620UR, endoscopic water pump OFP-2, mucosal injection needle NM-400L-0423, NM-400U-0423, thermal biopsy forceps HDBF-2.4-230-S, hemostatic forceps FD-410LR, FD-412LR, soft tissue clips ROCC-D-26-195-C, ROCC-D-26-235-C, hemostatic clips M00521242, methylene blue injection, indigo rouge solution.\n\nAll endoscopic surgical videos were recorded using Image Management Hub, IMH-200, and Olympus and stored in MP4 format. The video resolution was 1920 × 1080 with a frame rate of 50 fps. After collection, all videos were anonymized to remove identifiable patient information, procedure timestamps, and other identifying details. This work was approved by the Renji Hospital affiliated with the Shanghai Jiao Tong University School of Medicine ethics committee with number RA-2024-457(2024.3.20).\n\nThe ESD video is divided into eight surgical phases, including (1) Estimation; (2) Marking; (3) Injection; (4) Incision; (5) ESD; (6) Vessel-treatment; (7) Clips(portion); and (8) Preparation. Phase (6) Preparation included elements unrelated to the ESD procedure, such as gastric insufflation and device replacement. Four endoscopists independently annotated the video, labeling each frame in the video. After the initial annotation, quality control was performed by two additional experienced endoscopists. Any uncertainties that arose during the quality control process were resolved through collaborative discussions among the six experts. The number of annotated frames in the dataset varied at each phase, with the ESD phase occupying most of the procedure time, which is the most important and skill-demanding phase of ESD. Detailed statistics for each phase are shown in Table 7. Overall, a total of 484016 frames, 95,295 and 171,201 frames were annotated for training, validation, and verification, respectively."
    },
    {
        "id_": "7b468535-5085-4761-9ecb-1a0f864d4eb4",
        "text": "# 4.1.2. Cholec80\n\nFurthermore, to verify the robustness of our proposed algorithm, we validated it on the Cholec80 dataset[3] for cholecystectomy procedures. The Cholec80 dataset is a large-scale surgical benchmark dataset containing 80 cholecystectomy videos performed by 13 surgeons. These videos are recorded at 25 frames per second, with each frame having a resolution of either"
    },
    {
        "id_": "b64aa9ae-4658-4562-a058-3590f46b74d0",
        "text": "# 4.2. Evaluation Metrics\n\nFor surgical phase recognition, we employed four commonly used evaluation metrics to quantitatively assess model performance, which have been used in previous phase recognition work as well[3, 26, 45, 6]. These metrics include Precision, Recall, Jaccard Index, and Accuracy. Accuracy is defined as the percentage of frames across the entire video correctly predicted to be in their ground truth phase. Given the imbalanced phase presented in the video, Precision, Recall, and Jaccard Index refer to phase-level evaluations, calculated within each phase and then averaged across all phases."
    },
    {
        "id_": "71de5905-b62b-4dc1-99a5-e2b1d63d130e",
        "text": "# 4.3. Implementation details\n\nOur approach is implemented in Python using Pytorch framework and training is conducted on a workstation equipped with an NVIDIA RTX 4090. In the first stage, we use a ResNet-50 model pre-trained on ImageNet-22K[44]. We then fine-tune the model on our data. To ensure a fair comparison with SOTA methods, we downsample all videos to 1 fps, which is also the approach used in previous works[45, 6]. This operation has additional benefits, including enriching temporal information and saving memory. During training, image frames are further downsampled from the original resolutions of 1920 ≃ 1080 and 854 ≃ 480 to a resolution of 250 ≃ 250 pixels to further reduce memory usage. Data augmentation is performed through 224 ≃ 224 cropping, flipping, and random mirroring to expand the training dataset. The ResNet-50 model is fine-tuned with a batch size of 160 images. In the second stage, LSTContext is trained end-to-end from scratch. The W and G for each task in the LSTContext module are initially chosen based on estimates of task average durations on the training dataset and are then fine-tuned through ablation studies. For both stages, we adopted the same"
    },
    {
        "id_": "e5184c96-cccf-4255-aed8-484ac46ee8e2",
        "text": "# 4.4. Comparison with State-of-the-Art Methods"
    },
    {
        "id_": "604118c9-b5cf-4800-a43e-328bd1e7d31a",
        "text": "# 4.4.1. Result on the ESD385 Dataset\n\n|Method|Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|FLOPs|Params|\n|---|---|---|---|---|---|---|\n|ResNet-50 [44]|72.31|78.75|66.63|55.39|4.1G|24.56M|\n|SV-RCNet [26]|75.58±13.46|81.27±17.23|70.78±17.46|60.12±17.77|41.1G|28.76M|\n|SAHC [48]|86.64±10.63|86.35±18.07|83.75±17.55|75.14±18.20|9.5G|26.27M|\n|Furube et al. [12]|84.79±11.58|84.85±18.75|82.11±17.52|72.60±18.25|4.5G|24.69M|\n|AI-Endo [11]|83.38±12.09|84.74±17.35|82.17±17.23|72.09±17.23|5.7G|24.72M|\n|SPRMamba(Ours)|87.64±9.83|86.72±16.54|86.76±15.66|77.51±17.83|7.5G|25.42M|\n\nOn the ESD385 dataset, we compared our proposed method with the SOTA method. This includes the method proposed by Furube et al. [12], which fine-tunes ResNet-50 as a feature extractor and then uses MS-TCN for hierarchical prediction refinement for surgical phase recognition. The method proposed by Cao et al. [11]. An Intelligent Surgical Workflow Recognition Suite for ESD is based on Trans-SVNet [9] implementation. In addition, we compared our method with SV-RCNet [26] and SAHC [48], two SOTA methods for cholecystectomy surgical phase recognition. The comparative results are presented in Table 2. We found that applying existing surgical phase recognition methods directly to ESD leads to performance degradation because ESD surgery has more complex workflows compared to traditional surgeries. In the case of image classification using ResNet-50 alone, the average accuracy is 72.31%, the average precision is 78.75%, the average recall is 66.63%, and the average Jaccard is 55.39%. After including the SRTM module and Temporal Sample Strategy, the present method achieves an average accuracy of 87.64%, an average precision of 86.72%, an average recall of 86.76%, and an average Jaccard of 77.51%, increasing the average accuracy from 72.31% to 87.64%. These results demonstrate the effectiveness of the method for automatic phase recognition. Compared to the SOTA method, our method outperforms the second-best method in all four metrics by 1.00%, 0.37%, 3.01%, and 2.37%, respectively. Furthermore, to demonstrate the algorithm efficiency of the proposed method, we also compare the number of parameters and computational cost of our method with the."
    },
    {
        "id_": "26bcf2e0-06c4-43d0-b621-d121066e516b",
        "text": "# 4.4.2. Result on the Cholec80 Dataset\n\n|Method +|Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|\n|---|---|---|---|---|\n|MTRCNet-CL [45]+ Endonet [3]|81.70±4.20|73.70±16.10|79.60±7.90|-|\n| |89.20±7.60|86.90±4.30|88.00±6.90|-|\n|LAST [49]+|93.12±4.71|89.25±5.49|90.10±5.45|81.11±7.62|\n|SV-RCNet [26]|85.30±7.30|80.70±7.00|83.50±7.50|-|\n|TeCNO [7]|89.35±6.70|83.24±7.21|81.29±6.61|70.08±9.08|\n|TMRNet [6]|90.10±7.60|90.30±3.30|89.50±5.00|79.10±5.70|\n|Trans-SVNet [9]|90.27±6.48|85.23±6.97|82.92±6.77|72.42±8.92|\n|SAHC [48]|91.80±8.10|90.30±6.40|90.00±6.40|81.20±5.50|\n|SPRMamba(Ours)|93.12±4.58|89.26±6.69|90.12±5.61|81.43±6.90|\n\nIn the Cholec80 dataset, we conducted a performance comparison of our methods with state-of-the-art (SOTA) methods, including Endonet [3], MTRCNet-CL [45], LAST [49], SV-RCNet [26], TeCNO [7], TMRNet [6], Trans-SVNet [9], and SAHC [48]. Please note that we re-implemented TeCNO and Trans-SVNet using the model weights provided in the original manuscript. The results of the other state-of-the-art methods were extracted verbatim from their respective published works. Our comparison results are presented in Table 3. Our method outperforms the other methods in most of the evaluation metrics, except for the average precision, where it ranks third behind MTRCNet-CL and SAHC. Specifically, for average accuracy, SPRMamba matches the high accuracy of LAST (a multi-task learning method that requires additional information in the form of extra labels) but displays a lower standard deviation of approximately 0.13%. Additionally, for average recall."
    },
    {
        "id_": "174a0908-a01a-4ec4-a4cb-81d8d189b03b",
        "text": "# SPRMamba"
    },
    {
        "id_": "1e69a502-9ffa-4333-84e6-6d9faa914e3b",
        "text": "# Figure 3\n\nQualitative comparisons with some other methods on ESD385 dataset. The following four rows show the ground-truth labels, the predictions by the proposed SPRMamba, the predictions by Furube et al., and the predictions by AI-Endo. P1 to P8 indicate the phase label.\n\nand average Jaccard, SPRMamba achieves the best performance with 90.12% and 81.43%, respectively.\n\nFurthermore, to illustrate the performance of our approach compared to state-of-the-art methods, in Fig. 4 we qualitatively compare two examples from the Cholec80 testing dataset. We compare the proposed method with two SOTA methods, TeCNO [7] and Trans-SVNet [9]. As shown in Fig. 4, our method predicts higher frame-level accuracy, especially on the boundary between two different phases. In addition, we can notice that Trans-SVNet and TeCNO make a lot of mistakes, i.e., they are unable to accurately classify ambiguous frames in different phases. For example, some frames within P3 are misclassified into P4 and some frames within P5 are misclassified into P4 in Video 1. On the contrary, our proposed method performs very accurately compared to ground truth values."
    },
    {
        "id_": "cf534de7-0dda-406c-8744-fb3c6997a5d1",
        "text": "# 4.5. Ablation Study\n\nWe conducted a series of ablation experiments on the ESD385 dataset to validate the effectiveness of each component and parameter setting in our proposed method on the model."
    },
    {
        "id_": "2ab534c1-1762-4a3a-ba8a-48fb20ffa769",
        "text": "# 4.5.1. Scale Residual TranMamba\n\nTable 4 shows the importance of the SRTM modules. Transformer and Mamba denote the use of transformer branches alone and Mamba branches alone, respectively. To keep the number of parameters constant, we still use F → RL↑64 as input. SRTM improves the accuracy by 1.61% and 0.73%\n\n16"
    },
    {
        "id_": "ba51fbc1-5410-4ac0-b6fd-44f6e8f27762",
        "text": "# SPRMamba"
    },
    {
        "id_": "091e5272-9181-4c1b-ac21-5b221fcafdd7",
        "text": "# TeCNO"
    },
    {
        "id_": "90c88a6b-9f54-4f7c-a634-8bd151cda84e",
        "text": "# Trans-SVNet"
    },
    {
        "id_": "4972a389-d1ca-4e04-a183-1dfe6a1ba3b7",
        "text": "# Video"
    },
    {
        "id_": "dbdaf623-5bd9-4f55-8b95-c86ec623606a",
        "text": "# Video_\n\nFigure 4: Qualitative comparisons with some other methods on Cholec80 dataset. The following four rows show the ground-truth labels, the predictions by the proposed SPRMamba, the predictions by TeCNO, and the predictions by Trans-SVNet. P1 to P7 indicate the phase label."
    },
    {
        "id_": "110cff18-d1d2-42f5-a5ef-e4d54f935f8d",
        "text": "# Table 4: ABLATION STUDY ON THE SCALE RESIDUAL TRANMamba\n\nIn the case of Transformer, we only use transformer branches instead of SRTM. In the case of Mamba, we only used the Mamba branch.\n\n| |Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|FLOPs|Params|\n|---|---|---|---|---|---|---|\n|Transformer|86.03±10.89|86.02±17.40|84.89±17.03|75.44±18.41|8.5G|26.15M|\n|Mamba|86.91±12.06|86.59±17.24|86.05±16.74|77.19±18.42|6.5G|25.28M|\n|SRTM|87.64±9.83|86.72±16.54|86.76±15.66|77.51±17.83|7.5G|25.42M|\n\nCompared to Transformer and Mamba, respectively, demonstrating the crucial role of modeling short-term and long-term temporal context for ESD surgical phase recognition."
    },
    {
        "id_": "d4f1cdf9-15c0-4d07-a981-d794c549b3ef",
        "text": "# 4.5.2. Temporal Sample Strategy"
    },
    {
        "id_": "590bb6ab-9c9c-4e0d-ae3e-ddfbf0fab01b",
        "text": "# Table 5: ABLATION STUDY ON THE TEMPORAL SAMPLE STRATEGY\n\nIn the case of STContext SRTM, we use two STContext SRTM blocks instead of a combination of STContext SRTM and LTContext SRTM. In the case of LTContext SRTM, we used two LTContext SRTM blocks.\n\n| |Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|\n|---|---|---|---|---|\n|baseline|86.58±11.58|86.60±17.48|84.16±16.80|75.65±18.70|\n|STContext SRTM|87.03±11.73|86.95±17.57|85.20±17.32|76.86±17.82|\n|LTContext SRTM|87.38±11.15|87.27±17.20|85.42±16.43|77.07±17.89|\n|Ours|87.64±9.83|86.72±16.54|86.76±15.66|77.51±17.83|\n\nTo demonstrate the effectiveness of the Temporal Sample Strategy, we designed four ablation experiments. We also configured the original SRTM framework matching the SPRMamba structure as a benchmark for comparison. As shown in Table 5, baseline achieves an average accuracy of 86.58%, an average precision of 86.60%, an average recall of 84.16%, and an average."
    },
    {
        "id_": "ed6fe2ff-0f04-41d9-aff3-7bc988f15076",
        "text": "# 4.5.3. Effect of Different value of W and G\n\n|W|G|Accuracy(%)|Precision(%)|Recall(%)|Jaccard(%)|\n|---|---|---|---|---|---|\n|8|64|87.53±11.10|87.73±16.56|85.39±16.21|77.41±17.16|\n|16|64|87.37±9.63|86.56±16.23|86.23±16.30|77.35±17.17|\n|32|64|86.91±11.52|87.62±17.82|85.10±15.87|76.80±18.23|\n|64|64|87.64±9.83|86.72±16.54|86.76±15.66|77.51±17.83|\n|64|32|87.33±10.41|86.50±16.74|85.62±16.32|76.85±17.65|\n|64|16|87.08±10.98|87.09±17.19|85.15±16.42|76.77±17.91|\n|64|8|87.11±10.76|86.35±18.76|85.88±17.40|76.60±18.76|\n\nTo further explore the effect of the Temporal Sample Strategy with different W and G on the model performance, we conducted the experiments shown in Table 6. The parameter W controls the size of the local window and the parameter G controls the range of the long-term temporal context modeling. The experimental results show that the performance of the models (Accuracy and Jaccard) increases as G increases until the optimal performance is reached for G=64. The performance fluctuates as the size of the local window W becomes smaller, but W=64 works best."
    },
    {
        "id_": "b1187b5f-7e26-4c2d-9ab2-5997e8852a02",
        "text": "# 4.5.4. Effect of using convolutions\n\n|Jaccard(%)| | | |Accuracy(%)| |Precision(%)|Recall(%)| | |\n|---|---|---|---|---|---|---|---|---|---|\n| | |Conv without Dilation| | |85.97±11.48|85.75±17.70| |83.96±18.06|74.74±18.92|\n| | |Without Conv| | |86.60±10.95|85.71±17.94| |84.99±17.56|75.46±18.87|\n| | |Conv with Dilation| | |87.64±9.83|86.72±16.54| |86.76±15.66|77.51±17.83|\n\nTo explore the impact of dilated convolution in LSTContext blocks on model performance. We compared 1D dilated convolution with 1D convolution using the same size but without dilation factor and without 1D convolution."
    },
    {
        "id_": "1ead45a6-ae88-4af3-b518-16be32a7c84b",
        "text": "# 5. Discussion\n\nThe accurate recognition of surgical phases in ESD within CAS systems is crucial for enhancing surgical efficiency, minimizing patient complications, and providing valuable training material for novice endoscopists. However, considering the duration and complexity of the ESD process, the challenge of handling long sequences of video frames and effectively modeling temporal context with limited computing resources remains significant.\n\nTo address these challenges, we propose SPRMamba, a Mamba-based framework for online ESD surgical phase recognition. Our approach leverages the Scale Residual TranMamba module to effectively model both long-term and short-term temporal contexts, offering superior temporal modeling capabilities compared to traditional methods. Additionally, we introduce a novel Temporal Sample Strategy to mitigate the computational resource constraints, making the framework feasible for real-time surgical phase recognition.\n\nThe advantages of our approach are as follows:\n\n1. Long-Term Modeling Superiority: Mamba enhanced capabilities for long-term temporal modeling with lower computational complexity compared to Transformers. This makes it particularly suitable for handling the extended duration and complexity of ESD surgeries.\n2. Effectiveness of the SRTM Module: The proposed SRTM module achieves the best performance due to its ability to capture complex phase relationships in ESD surgeries. While Mamba ensures overall phase recognition accuracy, the challenge of accurately recognizing short-duration phases remains, which our SRTM module addresses effectively.\n3. Advantages of the Temporal Sample Strategy: The proposed Temporal Sample Strategy outperforms direct long sequence processing and mitigates the secondary complexity typically associated with Transformer models, enhancing the efficiency of temporal context modeling and improving overall recognition performance. In addition, our proposed method allows for flexible modification of the temporal modeling length and specific adjustments for different surgical tasks.\n\nFinally, our proposed method not only surpasses state-of-the-art (SOTA) methods in ESD surgical phase recognition but also demonstrates robustness for other surgical tasks. Specifically, our method achieves an average accuracy of 87.64%, an average precision of 86.72%, an average recall of"
    },
    {
        "id_": "7428ea6a-9920-4c24-b544-389ee27cabc0",
        "text": "86.76%, and an average Jaccard of 77.51% on the ESD dataset, outperforming the next best method by significant margins. Additionally, validation on the cholecystectomy Cholec80 dataset further confirms our method’s robustness, achieving superior results in most metrics compared to the SOTA methods[3, 45, 49, 26, 7, 6, 9, 48], except for precision. The qualitative results, as illustrated in Fig. 3 and Fig. 4, further substantiate the effectiveness and reliability of our approach.\n\nAlthough our method shows high surgical phase recognition performance in ESD and cholecystectomy videos, it has some limitations. First, this study primarily focuses on ESD procedures, and while we validated the robustness of our method on the Cholec80 dataset, further experiments are needed to assess its generalizability across a broader range of surgical tasks. In the future, we plan to extend our method to other surgical video analysis tasks, potentially incorporating other innovations such as uncertainty analysis to enhance the robustness and accuracy of SPRMamba across different surgical. Second, the dataset used in this study was limited in size and diversity, which may affect the model’s performance in more diverse clinical settings. Future work will focus on expanding the dataset to include multicenter data and a wider variety of surgical procedures."
    },
    {
        "id_": "bc79dfdd-f68f-41f2-9d14-c9a12f3cc4b6",
        "text": "# 6. Conclusion\n\nIn this paper, we address the critical need for accurate surgical phase recognition in Endoscopic Submucosal Dissection, a key procedure for treating early-stage gastrointestinal cancers. Achieving precise real-time phase recognition in ESD is essential for improving surgical outcomes and efficiency. However, traditional SPR methods face significant challenges, particularly in effectively capturing the temporal context over extended surgery durations and managing the high computational demands of video analysis required for real-time applications. To overcome these challenges, we propose SPRMamba, a novel framework designed to enhance the accuracy and efficiency of ESD surgical phase recognition. Specifically, our method proposes the Scale Residual TranMamba module, which combines the ability of Mamba to extract the long-term temporal context with the ability of the Transformer to extract the short-term temporal context and excels in capturing complex temporal relationships in surgical videos. In addition, considering the real-time requirements of surgical phase recognition, a temporal sampling strategy is designed to optimize computational resources by efficiently modeling."
    },
    {
        "id_": "d559dac5-9bd4-4030-ad36-59cea7709423",
        "text": "# Current Research on Surgical Video Analysis\n\nBoth short-term and long-term temporal contexts. Extensive experiments demonstrate that SPRMamba not only outperforms current state-of-the-art methods in accuracy and robustness but also significantly reduces the computational burden. In the future, our approach has the potential to advance the field of surgical video analysis, offering a valuable tool for both clinical practice and surgical education."
    }
]