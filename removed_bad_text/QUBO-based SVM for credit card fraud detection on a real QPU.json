[
    {
        "id_": "4bcfb8a3-972e-4c6c-a423-6e6a9e09453d",
        "text": "# 1 Introduction\n\nThe last decade has experienced a more and more intense research activity in the field of quantum computing as well as an increased interest of companies in its use within their businesses. Indeed, it gave rise to the experimental realization of different physical platforms for the realization of Quantum Processing Units (QPUs). Some of the most promising ones include: superconductors,"
    },
    {
        "id_": "dd43f358-d597-4076-b8cc-1efd771da158",
        "text": "# Trapped Ion and Neutral Atom Devices\n\nTrapped ion, spins in semiconductors, NV centers in diamond, photons, and neutral atoms trapped in optical tweezers [1–6]. However, all these systems share the fact that they are afflicted by noise introducing errors during the computation. In fact, we refer to today’s quantum processor as Noisy Intermediate-Scale Quantum (NISQ) devices [7]. In addition to noise, the number of operations performed and their duration must also be kept under control so that good results can be achieved. The number of qubits, which is relatively low, is also a problem because as their number increases, noise does increase. For this reason, numerous studies are being carried out to find methods to reduce the effects of noise: these may be based on the addition of extra qubits to realize quantum error correction, still unfeasible today, [8–10] or on ML protocols [11–13]."
    },
    {
        "id_": "e30795b7-8ab2-4d23-9873-b3f82e050f5b",
        "text": "# Growth of Neutral Atom Devices\n\nIn this context, neutral atom devices have experienced a significant growth in interest for both the academic and non-academic community. This intense growth culminated in the realization of error corrected logical qubits [14], placing neutral atom devices among the most promising systems at the moment. The 48 logical qubits are obtained from 280 physical qubits (atoms) grouped into logical blocks. Leveraging this, the authors show how it is possible to perform operations while maintaining high fidelity (99.8%)."
    },
    {
        "id_": "6ed9990f-4bbc-4cd8-bd99-88e857831304",
        "text": "# Advantages of Neutral Atom Devices\n\nIn addition to this, atoms are naturally identical, hence free from manufacturing errors. Moreover, their scalability is obtained via the realization of spatially extended potentials being able to create multiple optical tweezers. In these devices, excited and fundamental states of atoms are used for the realization of the two (qubit) computation states. They generally benefit also from relatively long coherence times and allow working in both digital (gate mode) and analog mode (Hamiltonian mode) [15]. In the latter, one directly manipulates the Hamiltonian of the system, for instance by applying global laser pulses on the atoms. This greatly reduces the duration of quantum protocols."
    },
    {
        "id_": "65e752b0-2c99-4a45-bbdd-187a7029416f",
        "text": "# Applications in Optimization Problems\n\nFinally, neutral atom devices are naturally suited for efficiently solving combinatorial optimization problems, which makes them particularly popular in various fields: routing [16–18], scheduling [19, 20], energy distribution [21, 22], finance [23–25] and also ML applications [26–29]. Optimization problems that can be solved naturally with such devices are the Quadratic Unconstrained Binary Optimization (QUBO) problems [30, 31]. Such problems, apart from being very difficult to solve exactly as they belong to the NP-hard class, find many applications in industry. Among these there are graph clustering (quantum community detection problems) [32], traffic-flow optimization [33], vehicle routing problems [34], maximum clique problems [35], and financial portfolio management problems [36]. Therefore, many attempts have been made to find possible ways of solving them."
    },
    {
        "id_": "e9d07179-468e-4699-9b81-aa62bca6fc48",
        "text": "# Quantum and Classical Approaches\n\nGiven their relation with the Ising model, the possibility of solving them in an approximate manner using quantum computers has been explored. Such problems can also be solved in a classically approximate manner by simulated annealing, where temperature fluctuations are exploited to try to reach a solution to the problem. However, in some cases a quantum advantage can be obtained by exploiting quantum fluctuations and the tunneling effect to find the minima of optimization problems [37]. This is the basis of adiabatic."
    },
    {
        "id_": "404a2e64-746b-433f-a53f-88935722061d",
        "text": "# Computing and Quantum Annealing\n\nFor such problems, Quantum Adiabatic Algorithm (QAA), i.e. a process in which a quantum Hamiltonian is evolved very slowly into the one representing the desired problem, can be exploited. The state of the system, in turn, changes from the ground state of the initial Hamiltonian to the one of the target Hamiltonian, thus obtaining the solution of the problem. Potentially, the high connectivity and flexibility of atom topology seem to make them more advantageous than other quantum annealing platforms [38].\n\nSince it is possible to reformulate the training process of a ML model as a QUBO problem [39], a neutral atom device can be used to train ML models via QAA. Specifically, the model that will be trained is a QUBO-based version of Support Vector Machine (SVM) [40], which will be called QUBO SVM from here on."
    },
    {
        "id_": "222c8015-3437-4029-8a6f-0d99787bc9ff",
        "text": "# Supervised Learning\n\nSupervised Learning, as a branch of ML, is characterised by the presence of ground truths (labels) that are associated with the data. These ground truths play a crucial role in the training and testing phase of the model. For this purpose, the dataset is divided into two parts (at least), one for training and one for performance evaluation, i.e. training and test sets, respectively. Within supervised learning, it is possible to identify classification and regression tasks. The former concerns the training of a model capable of assigning data to two (or more) classes. In the latter, a model is trained to predict a continuous value of one or more parameters.\n\nSVMs are a family of widely used classifier models that are appreciated for their stability [41, 42], meaning that small differences in the training set do not cause significant changes in the results. In general, these models are used when the dataset is small, but there are applications where they are used on top of neural networks with significant gains in performance [43–45]. In particular, SVM versions based on digital quantum computing have been developed [46] which, however, suffer from the limited number of qubits, noise, and the fact that the quantum processors must be used for both training and test set.\n\nThis makes them unusable on large datasets because of the cost and access limitations of QPUs. In this manuscript, we will compare the performance of the QUBO SVM models trained via QAA on a dataset of particular relevance: CCF, i.e., a scenario in which someone other than the owners makes an unlawful transaction using a credit card or account details. The problem is of crucial relevance given the exponential growth of electronic payments, resulting in annual losses of billions of dollars. Automated detection of CCFs is therefore a hot topic of research [47].\n\nThis problem has already been addressed in an unsupervised manner by Ref. [48], however, this approach currently restricts its applicability because it cannot be implemented on current QPUs due to its unsupervised nature. In fact, it is necessary to provide the entire dataset to the QPU by encoding it into a single QUBO matrix, whose size scales with the number of qubits. Although the model shows very good performance, current QPUs do not have sufficiently enough number of qubits for application-relevant data sets. Our approach, on the other hand, requires that only the training part exploits a QPU, while the testing phase exploits classical hardware."
    },
    {
        "id_": "353ce279-90cb-4608-b378-a9af7ff4c9ae",
        "text": "since the QUBO matrix is obtained only from the training data, it is sufficient that the size of the training set is chosen in such a way as to respect the limits of today’s QPUs. Thus, what is proposed in this manuscript is applicable on today’s NISQ devices.\n\nThe QUBO SVM model is first implemented and simulated on classical hardware, then tested on a real QPU by extending the number of qubits (or similarly atoms) used. For the simulation on classical hardware, we exploit the Python library Pulser [49], which allows ideal and noisy simulations. Pulser is developed by Pasqal [50], a company that builds QPUs based on neutral atoms, which, in some versions, can leverage more than 100 atoms. Via Pulser, it is possible to run simulations either on real machines or on classical hardware. As far as the implementation on QPUs is concerned, the real Fresnel neutral atom QPU developed by Pasqal is used, which, in the experimental configuration available at the time of the experiment, can use up to 25 atoms.\n\nThis manuscript is organised as follows: in Sec. 2, a brief introduction to QUBO problems is given, and in Sec. 3, a quick overview of the phenomenon of Credit Card Frauds (CCFs) and their detection is given. The sections 4 and 5 respectively introduce the SVM model with some of its applications and modified versions. In Sec. 6, the used CCF dataset is presented and described, while Sec. 5 discusses the implementation of the QUBO problem concerning the training of an SVM model on a neutral atom QPU. Finally, in Secs. 8 and 9, more general comments on our results are presented and some final conclusions and outlooks are drawn."
    },
    {
        "id_": "080125ed-ab21-451b-9f54-429ab3f3e66c",
        "text": "# 2 QUBO problem\n\nQUBO problems are combinatorial optimization problems of particular importance for adiabatic quantum computing given the connection with Ising models. They fall among the NP-hard problems, and therefore alternative ways to the classical ones are being explored for their solution. Quantum computers have shown advantages in their approximate solution. In particular, in adiabatic quantum computing QUBO problems are solved by QAA. Formally, a QUBO problem is defined as the minimization of the cost function\n\n∑n E = aTQa = ∑i,j=1n ai Qij aj,\n\nwhere n↑n aT Bn is a binary vector, B = {0, 1} is the set of binary variables and Q ∈ R is known as QUBO weight matrix. The goal of the minimization is to find a binary vector a↓ such that\n\na↓ = arg min E.a↔Bn\n\n4"
    },
    {
        "id_": "68a400b1-c3a0-4397-a785-6cfc234d90cb",
        "text": "# 3 CCF detection\n\nRecent years have seen a growth in electronic payments based on credit cards, both in physical shops and in online payments. With them, the number of identity thefts is also on the rise. This poses a major new problem, as CCFs are responsible for financial losses for credit card holders and credit institutions themselves. Confirming this, in 2018 the value of fraudulent transactions related to cards issued in the eurozone alone amounted to approximately 1.8 billion euros [51]. In 2025, worldwide fraud-related losses are expected to reach approximately 35 billion dollars [52]. For this reason, financial institutions are prioritising the development of algorithms capable of detecting and preventing losses due to such activities. In particular, ML and Deep Learning (DL) algorithms are being applied to decide whether an incoming transaction is related to a fraud attempt or not. However, given its importance, it is a very active field of research, with constant improvements and in which many techniques from different backgrounds are applied."
    },
    {
        "id_": "9a86fb53-8384-4879-ab59-165e964ff93a",
        "text": "# 4 SVM\n\nBinary classification is one of the most famous and common ML tasks: it involves training a parametric model that becomes able to assign data to two distinct categories via supervised training. SVM is a very famous classical ML model used to perform binary classifications. With mapping data in high-dimensionality spaces, it is possible to classify both linearly separable data and non-linearly separable data. However, this results in computational costs that can grow exponentially when dealing with large amounts of data. This problem can be circumvented by using kernels. Indeed, when data appears in the form of a scalar product, instead of mapping the data into another space and calculating their scalar product, it is possible to replace it directly with the inner product of the data in the new space. The use of kernels leads to an overall less computationally expensive problem and has the advantage of strong mathematical foundations.\n\nIn the field of supervised learning, it is well known that SVM can offer good performance even with small datasets, unlike neural networks requiring a lot of data to be trained. We now consider a dataset\n\nD = {(xn, yn) : xn → Rd, yn = ±1} n=0,...,M ↗1, (3)\n\nwhere xn is a sample (i.e. the feature vector) and yn the associated target. We call the classes yn → {1, ↑1} ↓n respectively as “positive” and “negative”. The reader interested in more technical details of the SVM model can find them in the appendix 10.1."
    },
    {
        "id_": "3bd6b5ec-a8e1-430d-b6f2-c1155741e7f9",
        "text": "# 5 QUBO Support Vector Classifier\n\nAfter introducing the SVM model and the QUBO problem we can show that is possible to reformulate the training process of a SVM model into a QUBO optimization problem. From now on we will refer to this model as QUBO SVM.\n\nThe reformulation involves rewriting Eq. 9 in a form similar to Eq. 1. At the end of some manipulations, it is possible to obtain the following QUBO matrix:\n\nQKn+k,Km+j = 1Bk+j yn ym (k(xn, xm) + ω) ↑ εnm εkj Bk, (4)\n\nwhere B, K and ω are constants, k(·) is a kernel function and εji is Kronecker’s delta. It can be observed that the square matrix Q has a linear dimension equal to K ↔ N, where N is the number of samples in the training set. This is related to the fact that the encoding of continuous data in the form of binary sequences requires their discretization. In our case, the encoding is done via the Eq. 15.\n\nFor the interested reader, the full derivation can be found in appendix 10.2.\n\nAs previously mentioned, each bitstring obtained as output from the quantum device measurement can be used as the decision hyperplane of a SVM model. Hence, there is a close connection between the matrix Q and the objective function to be minimised. We do not actually use Q directly, we modify the position of the in such a way as to replicate Q as closely as possible in the Hamiltonian HQ. Since this optimisation is the training of a SVM model, finding a solution means finding the best SVM model for the specified training set (hence for that given matrix Q). Since we obtain a distribution of solutions (bitstrings resulting from the measurement process), we have at our disposal a distribution of SVM models that we can potentially aggregate to obtain performance gains.\n\nThe aforementioned protocol is schematised in the Fig. 1.\n\n|Data|Training|Quantum register|\n|---|---|---|\n|Test|QUBO SVM|Score|\n| | |Simulation|\n\nFigure 1: Schematic representation of our protocol."
    },
    {
        "id_": "9247628f-ab6f-4e94-97c0-6e3015e40659",
        "text": "# 6 Dataset and metrics\n\nThe CCF detection is a binary classification with an unbalanced dataset, namely a supervised anomaly detection. By definition, anomalies are rare events that have great impact. The considered dataset concerns CCF detection, found on Kaggle [53]. We chose to search for a dataset on this website since there are many datasets of applicational relevance, often companies organise challenges on Kaggle offering monetary rewards to the winners. The CCF detection dataset was chosen because, among the datasets present, it represents a problem of considerable relevance. It has a very marked imbalance between classes: 284315 “normal” transactions and 492 “anomalous” transactions (about 0.17% of transactions are the positives). In addition, it contains only numerical input variables which are the result of a PCA transformation. Due to confidentiality issues, the original features and more background information about the data are not provided.\n\nSpecifically, we divide the dataset into two equal parts. A part is then used as a test set, so it is ignored until that point. Instead, the other part is divided into a validation set and a training set. The latter contains 6 training samples. These training and validation sets are first resampled, and then used for training and validation procedures. We repeat this process 10 times, collecting metrics and providing estimates with associated errors in the form of mean values and standard deviations calculated across the 10 analyses on different datasets. In addition, to provide performance and error scaling, we repeat the analysis using training sets of 7 and 8 training samples.\n\nSince this is an unbalanced classification problem in which the focus is on the minority class, accuracy is not a good metric to use. Recall and balanced accuracy are used instead:\n\n|recall|=|T rue P ositives|=|T rue P ositives + F alse N egatives|,|T rue P ositives|\n|---|---|---|---|---|---|---|\n|All P ositives| | | | | | |\n\n|balanced accuracy|=|1|[|T rue P ositives + F alse N egatives + T rue P ositives|2|]|\n|---|---|---|---|---|---|---|\n| | |T rue N egatives|.| |T rue N egatives + F alse P ositives| |\n\nRecall is the fraction of relevant instances that are retrieved by the model, while balanced accuracy is a metric specifically developed to generalize accuracy in the case of unbalanced datasets. A high recall means that the number of false negatives is low, i.e., the number of frauds that the model fails to recognize is low compared to those that it does recognize. The number of false positives (when the model classifies a transaction as fraud by mistaking it) is not as important as false negatives, since with subsequent checks it can be determined whether it was fraud or a false alarm (e.g., by sending verification messages on the credit card owner’s smartphone, etc.). This is because the scenario where the bank loses money is the false negative case. In this scenario, with such an imbalance of classes, the application of resampling algorithms is one of the most"
    },
    {
        "id_": "1871b94f-37ff-4a63-9026-93d679c8c37e",
        "text": "adopted practices. Initially, the SMOTE oversampling algorithm [54] is applied to increase the minority class data (the frauds), then random undersampling [55] is used to reduce the size of the dataset to balance the classification problem. At the end, a balanced dataset containing 500 samples is obtained. This will then be divided into training and validation sets. The test set, instead, has not been rebalanced and has the same class ratio as the original data set (0.17% fraud). This is because the test set only needs to simulate a scenario that is as realistic as possible, unlike the other splits that can instead be modified to attempts making the model to learn better. Treating the test set separately avoids data leakage that would lead to overestimation of performance.\n\nTo be sure about the performance of the QUBO SVM models, they are compared with the main classical models in the literature: classical SVM and Decision Tree. In addition, various versions of the QUBO SVM models are trained using ideal and noise simulations (denoted respectively with the letter “i” and “N”). Furthermore, two types of models are tested: QUBO SVM and a stacked ensemble configuration in which the QUBO SVM model is used as a meta-model, i.e. it is trained using the outputs of other models as data. After ensuring that the models actually work, both configurations are tested on a real QPU, exploring regimes that are not easily simulated with local clusters.\n\nThe reader interested in all the information on the various model configurations (including the number of measurements used for each training) that have been used can find it in the appendix 10.4."
    },
    {
        "id_": "2e460a7b-ab49-4a7b-aebe-79911f8ee296",
        "text": "# 7 Neutral atom implementation\n\nWe now describe the model implementation on a neutral atom NISQ device. We distinguish three phases: encoding, training and testing. Let us point out that only the second phase takes place on a QPU, while the first and third ones are pre- and post-processing via traditional CPU. Encoding is achieved by computing the QUBO matrix of Eq. (18): this is done by specifying all the training data together. This means that the linear dimension of the (square) matrix will be K ↔N. Hence, the parameter K plays an important role in terms of the number of qubits required to implement the algorithm. K ↔ N are the required atoms. The encoding in this case takes place on the quantum register.\n\nWe recall that for a QPU of neutral atoms the (global) Hamiltonian has usually the form\n\nH = ⊋$(t) ∑ϑ i x ↑⊋ε(t)∑ϑ i z + ∑U ij n n j . i\n\nHere, z)/2 and with ϑ i z representing the component along the z-axis of the pauli$ and ε are, respectively, the laser amplitude and the detuning, n i = (1 + ϑ i vector applied to the i-th atom. By tuning the positions of the atoms, the value of $ and ε, we can make H as close as possible to Q. This is done using the COBYLA optimizer [56] and imposing the physical constraints of relative"
    },
    {
        "id_": "3f9d5651-0067-4b2d-82eb-fd1ab05b884e",
        "text": "# Distance Between Atoms\n\nThe distance between atoms that are feasible on the specific real QPU. As far as the QPU is concerned, due to the prototype available at the time of the simulations, the atoms in the quantum register can only be arranged on the vertices of a triangular lattice with a fixed lattice constant. Therefore, now the co-ordinate optimisation no longer takes place on continuous but discrete space, which is why now the best spatial configuration of atoms on a triangular lattice is found by simulated annealing. Although this appears to be a more disadvantageous condition for satisfying HQ ↗ Q, we will show that from the results it does not appear that the model suffers from such a limiting constraint."
    },
    {
        "id_": "a8307172-2040-4235-87eb-eae7a1adfbdc",
        "text": "# Quantum Register Topology\n\nOnce the encoding is done, the desired quantum register topology is realized on the QPU, the pulse is applied, and the final measurement of the entire quantum system is taken. The applied control consists of a pulse sequence with each pulse of duration equal to 10 μs and maximum amplitude equal to:\n\n$ = \\begin{cases}\n\\text{median(Q)}, & \\text{if median(Q) < } \\$_{max} \\\\\n\\$_{max}, & \\text{otherwise}\n\\end{cases}\n\nwhere \\$max = 15.71 rad/μs is the maximum amplitude being possible on the Pasqal device. At the same time, a linear detuning ramp is applied, starting at -10 rad/μs and ending at 10 rad/μs after 10 μs. The obtained sequence is shown in Fig. 2. The sequence applied in the real quantum device is the same, except made for the duration that is reduced to 5 μs."
    },
    {
        "id_": "78738031-be1e-40e2-9b55-19f19d6792cc",
        "text": "# Hamiltonian and Measurement\n\nWe denote with HQ the Hamiltonian obtained after tuning the coordinates and fixing the sequence. In any case, we recall the goal of this procedure is to obtain a Hamiltonian such that HQ ↗ Q. Finally, we repeat the preparation-pulse-readout cycle N shots times. This yields a distribution of atomic states with the corresponding probabilities: if the problem has been well encoded in the Hamiltonian, the highest probability states are the best solutions of the QUBO problem."
    },
    {
        "id_": "c20602ef-11d3-4d1d-9344-01e5a78cb0c2",
        "text": "# QUBO Problem and SVM Model\n\nBut the problem framed as a QUBO is the training of an SVM model, i.e. finding a hyperplane that best separates the points of the two classes while committing as few errors as possible. Therefore, each quantum state obtained (a string of binary variables) is a possible decision boundary defined by ϖn. The state with the largest probability is the best solution to the QUBO problem and it contains the coefficients of the best model. Since a particular SVM model is completely defined by the hyperplane (which is defined)."
    },
    {
        "id_": "cc9aaa48-4497-4b44-8b4f-acca609c246e",
        "text": "# Figure 2\n\nPulse sequence of the protocol, the height of the smooth green curve depends specifically on the QUBO matrix."
    },
    {
        "id_": "3c72522e-a296-41dc-8f41-5365010d8f06",
        "text": "# 10\n\nBy the coefficients ϖ), to test a model it is sufficient to use a particular ϖ n in the decision function (12) and provide new data as input to the model. This testing operation is done on a classical computer. At this point, the predicted class can be derived from the decision function. By repeating this it is possible to compute the predictions on a whole test set. After that, one can use the metrics for classification used in classical models: accuracy, precision, recall, f1-score, balanced accuracy, Area Under the Curve of the Receiver Operating Characteristic (AUC ROC) and so on.\n\nHowever, this classifier allows us to experiment different approaches towards the best performing one: (i) using the trained model as it is or (ii) using it as a base model in different kinds of ensemble techniques. The latter is a ML technique that combines the predictions of several trained models together to form a new model that exhibits superior performance, lower bias, robustness, etc. There are various ways of combining models, here we exploit average voting and stacking. In the first case, many models are trained on the same training set and subsequently tested on the same test set. The final predictions are an average of the predictions of the individual models. In stacking, on the other hand, two layers of models are used. In the first layer, several models are trained on the same training set and then tested on the same test set. In this case, however, the outputs of the models in the first layer are used to train and test the model in the second layer, also known as the meta-model. These types of approaches are exploited in CCF detection [57, 58]. More information on ensemble techniques can be found in the appendix 10.3 and in Ref.[41].\n\nIn particular, since a full training of N shots measurements leads to a distribution of states with the relative probabilities, it is possible to set up a full quantum average voting strategy. In fact, remembering that each observed bit-string represents a possible solution of the QUBO problem), one can potentially obtain up to 2N atoms states (and thus solutions of the training problem). It is interesting to see if aggregating multiple models into a single model yields more robust predictions. Then, operationally, the predictions on the test set are computed using each bitstring (state) obtained from the training process. The predictions obtained from the various models are then aggregated by taking an average. In addition, by providing a validation set and specifying with respect to which metric one wants to optimize, it is possible to repeat this procedure by finding the number of aggregate models maximizing that metric (one starts with the highest probability states eventually trying to use all of them). At the end of this procedure, one obtains the classifier ensemble composed of the models that maximize the specified metric.\n\nFurthermore, it is possible to use the proposed QUBO SVM model as a meta-model in a hybrid quantum-classical stacked ensemble learning procedure, in which the quantum model is trained using the outputs of classical models as input."
    },
    {
        "id_": "2c2948ac-d00d-496f-91ad-6ef7e24ab422",
        "text": "# 8 Results and discussion\n\nIn this extreme scenario, all the QUBO SVM models seem to work well (as compared to the classical models) and the error bars are about the same as for the other models, as shown in Figs. 5 and 6. Although SVM-based models do not perform well with unbalanced datasets, we have mitigated this problem with resampling techniques.\n\nFrom Fig. 8 and 9 it can be concluded that there appears to be no substantial difference in performance using different numbers of shots: respectively 1000, 500 and 100. Furthermore, the training simulations count on average 76(±5) shots each. Therefore, there is evidence for the models working even with only a few shots. This results in substantial savings in QPU utilization, a reduction in execution time and related costs for the end user. In addition, there does not seem to be much difference between the optimized and non-optimized versions, probably because the models that perform better than random guessing are very similar to each other. Another observation is that the model seems to be rather robust to noise (both simulated and real); in fact we do not see large differences between the performance of each model in the ideal, noisy and QPU versions.\n\nFinally, the pipeline in stacked configuration seems to provide very interesting results both for the high performance, when referred to the examined models, and for the lower standard deviation with respect to all models. It can also be seen that performance increases as the size of the training set increases, this is interesting because for each additional sample two atoms are used. Therefore, noise does not seem to play such a negative role in the performance of the model. This is true both for simulated data with noise and for data from a real QPU, where noise is certainly present. It is an important result, because it shows that it is possible to make Quantum Machine Learning (QML) algorithms that can really be implemented on actual QPUs to make predictions on real datasets of relevance.\n\nFigures 5b and 6b compare the performance of stack configuration models in the three versions: ideal, with noise and QPU. In particular, while the versions simulated with classical hardware are tested with 8, 10, 12, 14, 16 atoms respectively, the versions trained with QPU are also tested in the following scenarios: 18, 20, 22 and 24 atoms. We can see that, as the training set increases, the performance increases, while the standard deviation decreases. This applies to all models, which show no appreciable performance differences. Surprisingly, there seems to be a greater reduction in standard deviation in the noisy model, especially for the models trained with the QPU.\n\nIn order to validate this intuition quantitatively, we study the scalability of performance when varying noise parameters: laser intensity fluctuations, state preparation and measurement errors, temperature of the atoms in the quantum register and laser width. The models whose scaling is evaluated are the ones that are additionally trained on the QPU, i.e. the QUBO SVM model in stack configuration and the one trained through 100 shots only and optimised (as described above).\n\nThe scaling analysis is done by fixing the number of atoms and varying the noise level. The obtained results are shown in Fig.3 Fig 4 for 5 and 7 training"
    },
    {
        "id_": "b7979dea-6dff-465b-ae40-7983fdef090d",
        "text": "# Scaling of performances with 5 training samples, 10-atom simulations.\n\n|Noise Level (% respect to QPU level)|Recall|Balanced Accuracy|\n|---|---|---|\n|100|0.8|1|\n|200|0.6|1|\n|300|0.4|0.8|\n|400|0.2|0.6|\n|500|0.0|0.4|\n|600|-0.2|0.2|\n|800|-0.2|0.0|\n|1000|-0.2|0.0|"
    },
    {
        "id_": "198d8fd2-9a3d-42d8-b63e-68e540b0a94e",
        "text": "# Scaling of performances with 7 training samples, 14-atom simulations.\n\n|Noise Level (% respect to QPU level)|Recall|Balanced Accuracy|\n|---|---|---|\n|100|0.8|1|\n|200|0.6|1|\n|300|0.4|0.8|\n|400|0.2|0.6|\n|500|0.0|0.4|\n|600|-0.2|0.2|\n|800|-0.2|0.0|\n|1000|-0.2|0.0|\n\nare reported as a percentage of those used with respect to Fig. 5b and 6b (and present on the actual QPU). This allows us to see how performance varies from ideal simulations to simulations with 1000% of the noise levels compared to what is claimed on the QPU. What we can see in general is a bell-shaped trend, i.e. higher performance and lower errors are obtained when some noise is present compared to its total absence. Therefore, there is evidence of performance enhancement due to noise. This is a new phenomenon in the context of quantum machine learning, but known for some time in other contexts. For example, there are cases in which the presence of noise has a beneficial effect, such as quantum transport over complex networks and quantum maze escapes [59–61].\n\nSummarizing, an analysis of the results in Figures 8d and 9d shows that the best models in terms of maximum performance and minimum error are"
    },
    {
        "id_": "237b0268-6bdf-435f-b170-e97c4e76cae8",
        "text": "# 8.1 Computational complexity\n\nIn this section we aim to analyze time-qubit complexity of the introduced QUBO SVM model, and compare it with the commonly used classical SVM models. We have to distinguish three different phases in which we will derive the complexity:\n\n1. time required to convert the algorithm into a QUBO problem\n2. time required to embed the QUBO problem on the quantum register\n3. theoretical time complexity of QAA.\n\nThe former can be derived from Eq. (17) and it is estimated by O(N2 ↔ K2 ↔ Nfeatures) operations if a linear kernel is exploited. Regarding the number of operations required by the embedding, it is constant, since we specify a maximum number of iterations in the COBYLA optimizer. Therefore, there is no real scaling. Moreover, the number of features Nfeatures of the dataset is fixed and can be reduced via dimensionality reduction techniques. Finally, although the theoretical time complexity of QAA is O(1/%2), where % is the minimum eigenvalue gap between the ground state and the first excited state of the Hamiltonian H. In any case, since in the current neutral atom QPUs the maximum sequence duration is upper bounded, we consider it to be constant and equal to this limit. Therefore, even in this case, there is no real scaling to take into account. Therefore, the time complexity of the proposed quantum algorithm is O(N2 ↔ K2). In contrast, the time complexity of SVM is O(N3), although recently algorithms have been proposed to lower the complexity to O(Nω), with 2 < ϱ < 3. Finally, regarding the qubit footprint, our algorithm shows a O(N ↔ K) qubit scaling. However, since K is a fixed hyperparameter, the complexity reduces to O(N2) and O(N) for time complexity and qubit footprint respectively.\n\nTherefore, the algorithms proposed in this manuscript have: better scaling (N2 < ω < 3); better (or at best) performance than their classical counterparts (NQUBO2 vs SVM) and lower error bars; the possibility of being implementable on atom-neutral NISQ devices; little use of the QPU; and the privacy of the data is preserved because only atomic coordinates and laser parameters reach the QPU, the model test is done locally because only training requires a QPU."
    },
    {
        "id_": "a19c6ff4-0784-48ce-9316-478e29ff98e5",
        "text": "# QUBO SVM opt QPU"
    },
    {
        "id_": "f51d7233-e11e-4b34-acd4-5a99f7da1028",
        "text": "# QUBO SVM Stack QPU"
    },
    {
        "id_": "0c425aad-6636-4d4c-8999-aaf3040df7af",
        "text": "# SVC Linrecall"
    },
    {
        "id_": "89c4deac-e8d1-44e2-ba0b-d497457e7f6b",
        "text": "# QUBO SVM i opt"
    },
    {
        "id_": "751fa2a6-4bca-4e42-9f89-02c693ec8a5c",
        "text": "# QUBO SVM i Stack"
    },
    {
        "id_": "bce892bf-85d7-4dbd-b02e-2d206937c190",
        "text": "# QUBO SVM N opt"
    },
    {
        "id_": "ab3a57b3-7344-43f0-b50f-54410029572b",
        "text": "# QUBO SVM N Stack"
    },
    {
        "id_": "c820e130-d03c-4bd9-87d8-9e537d0bd4cb",
        "text": "# Decision Tree\n\n|Recall|0|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1.0|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|(a) 8 training samples, 16 qubits simulations.|1|0.9|0.8|0.7|0.6|0.5| | | | | |\n\nNoisy simulations\n\nIdeal simulations\n\nQPU real data\n\n|Number of qubits|0.3|8|10|12|14|16|18|20|22|24|\n|---|---|---|---|---|---|---|---|---|---|---|\n|(b) Recall scaling with the number of qubits.| | | | | | | | | | |\n\nFigure 5: Comparison of recall between classical and QUBO SVM models using 8 training samples (16 qubits), subfigure (a). Subfigure (b) shows the recall scaling in both ideal, noisy and QPU QUBO SVM stack models. In all subfigures, data with error are intended as mean and standard deviation calculated on 10 different splittings of the dataset. For the complete list of the used models see Tab. 1."
    },
    {
        "id_": "bb78fbc0-b81e-4a3d-bff0-be0b63698eb7",
        "text": "# QUBO SVM opt QPU"
    },
    {
        "id_": "4777cf3a-ec9d-431c-9635-db12c4df57f9",
        "text": "# QUBO SVM Stack QPU"
    },
    {
        "id_": "3d1a88e7-9353-47f7-833e-7749416d816f",
        "text": "# SVC Linbalanced accuracy"
    },
    {
        "id_": "4caa0eb5-d9f0-41c7-ad98-574431d5bbae",
        "text": "# QUBO SVM i opt"
    },
    {
        "id_": "7d6a0e15-8023-4e77-a40c-bba0696b9d31",
        "text": "# QUBO SVM i Stack"
    },
    {
        "id_": "aab8708e-f7ee-4861-a5d2-66b4e29c21ee",
        "text": "# QUBO SVM N opt"
    },
    {
        "id_": "52cde81f-170a-4ba1-ae8a-8ea6fa12c078",
        "text": "# QUBO SVM N Stack"
    },
    {
        "id_": "7a5b1e6c-d43a-4b97-8553-431553f58276",
        "text": "# Decision Tree\n\n| |0|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1.0| |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Balanced accuracy|1|0.9|0.8|0.7|0.6|0.5| | | | | | |\n\n(a) 8 training samples, 16 qubits simulations.\n\nNoisy simulations\n\nIdeal simulations\n\nQPU real data\n\n| |8|10|12|14|16|18|20|22|24|\n|---|---|---|---|---|---|---|---|---|---|\n| | | |Balanced accuracy scaling with the number of qubits.| | | | | | |\n\n(b) Balanced accuracy scaling with the number of qubits.\n\nFigure 6: Balanced accuracy analysis using 4, 5, 6, 7, and 8 training samples, subfigure (a). Subfigure (b) shows the recall scaling in both ideal, noisy and QPU QUBO SVM stack models. In all subfigures, data with error are intended as mean and standard deviation calculated on 10 different splittings of the dataset. For the complete list of the used models see Tab. 1.\n\n15"
    },
    {
        "id_": "81fc81f3-c7de-4c05-b668-df291c31c76b",
        "text": "# 9 Conclusion\n\nWe have shown a model of SVM whose training is reformulated through in a QUBO problem. These kinds of problems are naturally implementable on neutral atom devices using analog processing, that is, performing only global laser operations on the atoms. The linear classifier model can be easily used in the presence of nonlinear data in a variety of ways: using a nonlinear kernel, via a kernel PCA, or via feature discretization.\n\nThe model was trained and tested on a very unbalanced CCF detection dataset. This required first the application of all the techniques used in the case of highly unbalanced datasets: namely, the use of resampling techniques and the choice of appropriate metrics (other than accuracy, which is definitely obsolete in these cases). The model we considered was tested in several variants considering also ensembling strategies. For each strategy, the trained model was tested by both ideal and noisy simulations. To provide robust estimates with associated measures of error, each configuration was trained and tested on 10 different splits of the dataset finally calculating mean and standard deviation of the chosen metrics.\n\nIn addition, in order to numerically study the scalability of performance, these analyses were performed by running ideal and non-ideal simulations with training sets of 4, 5, 6, 7 and 8 samples, using quantum registers of 8, 10, 12, 14 and 16 atoms, respectively. Finally, the same analysis was then carried out using a real QPU with neutral atoms. In addition to the configurations already used and listed above, quantum registers of 18, 20, 22 and 24 atoms were also used; corresponding to training sets of 9, 10, 11 and 12 samples.\n\nAll QUBO SVM models were compared with the main binary classifier (classical) models in the literature, resulting in performance comparisons. Finally, the same analysis was then carried out using a real QPU with neutral atoms. In addition to the configurations already used and listed above, quantum registers of 18, 20, 22 and 24 atoms were also used; corresponding to training sets of 9, 10, 11 and 12 samples.\n\nWe have faced that our proposed models have performances that do not deviate much from the classical models and are compatible with them within the error bars. However, a stacked ensemble variant we proposed seems to suggest superior performance and smaller error bars, both in terms of recall and balanced accuracy. In particular, focusing only on this type of model and analyzing its scaling of both the ideal, noisy and QPU versions, some interesting conclusions can be drawn.\n\nThe first is that the performance seems to increase as the number of atoms (and thus training samples) increases. Based on this, we believe that by using a few hundred training samples, it may be possible to achieve superior performance to classical models. This means that the model could lead to higher metric values and lower associated errors. This is important because it motivates interest in investigating its performance by increasing the number of atoms even more.\n\nFor this reason, it would be interesting to test it on a neutral-atom QPU with 100-200 qubits, in order to verify the actual robustness under real conditions. Another interesting consideration concerns the fact that both the noisy and QPU model, in addition to having almost identical performances as the ideal one, shows slightly lower error. To confirm this observation, we performed"
    },
    {
        "id_": "11c92aad-0a9e-44fc-a16b-4dede4118762",
        "text": "# Quantum Algorithms and Applications\n\nand compared simulations at different noise levels, showing improvements in the presence of noise compared to the ideal case. This could be related to the fact that quantum noise in QAA can help to find better solutions, i.e. global minima of the cost functions. Finally, here there are some remarks on the QUBO SVM model. It is trained adiabatically via analog quantum computing and the data is encoded via the Q matrix. This avoids using the various types of encoding used in digital computation that often result in quantum circuits that are too long to execute, hence not feasible on NISQ machines. Therefore, the increase in the number of qubits is not a problem in this respect. In fact, the part of the circuit responsible for encoding, which is absent here, grows in depth with the number of qubits. Moreover, the fact that the model is trained only on the QPU is advantageous because it reduces the use of the QPU and the associated costs. Since only the training is quantum, the model can be saved and used later or multiple times at the same cost. Such a model is suitable also for contexts where data privacy is a crucial factor: in fact, the training data are never disclosed, but are used locally to compute the Q matrix. In turn, the Q matrix is reproduced as closely as possible from the Hamiltonian of the system through device optimization. Therefore, even in case of data leakage during the connection with the QPU, only the coordinates of the atoms and the intensity and detuning values would be disclosed, or at the limit, the results of the measurement without any information on the data used for the training. The analysis of the test data is again performed locally. This study motivates the development and use of model training protocols based on analog processing in real-world scenarios, given the low requirements compared to gate-based versions, in anticipation of fault-tolerant quantum computers. Others possible developments could include the unsupervised version of SVM known as one-class SVM, a version of SVM with a modified loss so as to assign different weights to misclassifications in cases where the datasets are not balanced and the use of counterdiabatic drivings, which are useful to prevent excitation while preventing the adiabatic protocol from becoming too long. Although noise does not seem to be a problem in the explored regimes, we will study what benefits we can get from logical qubits, and how they can be moved to change the topology of the quantum register, further improving our results."
    }
]