[
  {
    "id_": "733d440e-2c60-45cf-b25a-f1ff2e5d51e3",
    "text": "# Applications of Knowledge Distillation in Remote Sensing: A Survey"
  },
  {
    "id_": "50313b26-4c4e-4644-b80a-83cd9ca7aa15",
    "text": "# 1. Introduction"
  },
  {
    "id_": "663a0540-1fac-4b06-87e6-747176c9f0b5",
    "text": "# 1.1. Preliminary\n\nRemote sensing (RS) image analysis plays a pivotal role in interpreting and managing Earth’s natural and human-made environments [1]. This technology harnesses data captured by satellites or high-altitude aircraft, providing crucial insights across a broad spectrum of applications—from agricultural monitoring and disaster management to urban planning and climate science [2]. By enabling timely and efficient observation of vast, inaccessible, or dangerous areas, RS becomes indispensable for tracking environmental changes, predicting weather patterns, and managing natural resources [3]. Consequently, the ability to quickly process and analyze RS images leads to more informed decision-making, enhancing our global capability to respond to challenges such as food security, natural disasters, and climate change [4, 5].\n\nHowever, the complexity of RS tasks varies significantly depending on the specific application, and many of these tasks are inherently challenging and computationally intensive [6, 7]. Key tasks such as image classification, object detection, change detection, and segmentation involve processing high-dimensional data, often characterized by large spatial and spectral resolutions [8, 9]. For instance, distinguishing between different land cover types or detecting minute changes over time in vast geographical areas necessitates sophisticated algorithms capable of handling enormous datasets [10]. Moreover, the presence of noise, variability in lighting conditions, atmospheric distortions, and the need for high precision further compound the complexity of these tasks [11, 12]. These challenges lead to extensive training times, particularly for deep learning models, which require large datasets to achieve high accuracy and generalization. Therefore, optimizing these models to balance accuracy and computational efficiency remains an ongoing challenge in RS [13, 14].\n\nArtificial Intelligence (AI), particularly machine learning (ML) and deep learning (DL), has revolutionized RS image analysis by introducing levels of precision and efficiency previously unattainable with traditional methods [15, 16, 17]. DL models, especially those based on Convolutional Neural Networks (CNNs), are highly adept at handling high-dimensional data from RS imagery [18]. These models excel in tasks such as pattern recognition, object detection, and semantic segmentation, where they can automatically identify features like roads, buildings, or vegetation changes [19]. Furthermore, the deployment of AI enables the processing of large datasets in real-time, significantly improving the accuracy of predictions and analyses. Moreover, DL’s ability to learn feature representations without manual intervention reduces reliance on expert-driven feature design, thus scaling up the analytical capabilities of RS technologies [20].\n\nDespite these advances, the integration of AI and DL into RS presents several significant challenges. One of the foremost issues is the requirement for substantial computational resources, particularly for training large neural network models [21, 22, 23]. This becomes a critical barrier for organizations with limited access to high-performance computing infrastructure [24]. Additionally, DL models often require vast labeled datasets for training, which can be difficult and costly to acquire in the context of RS. Furthermore, these models are prone to overfitting, especially when trained on limited datasets, reducing their ability to generalize well.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 1 of 50"
  },
  {
    "id_": "4001bc66-8433-4efc-997a-32408a61cb2d",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\n|Abbreviation|Full Form|Abbreviation|Full Form|\n|---|---|---|---|\n|KD|Knowledge Distillation|YOLOv8|You Only Look Once version 8|\n|RS|Remote Sensing|MS2RGB|Multispectral to RGB Knowledge Distillation|\n|CNN|Convolutional Neural Network|PseKD|Phase-shift Encoded Knowledge Distillation|\n|S-T|Student-Teacher|GSGNet|Graph Semantic Guided Network|\n|ARSD|Adaptive Reinforcement Supervision Distillation|LPIS|Land Parcel Identification System|\n|RGB|Red, Green, Blue|DOTA|Dataset for Object Detection in Aerial Images|\n|R-CNN|Region-based Convolutional Neural Network|DIOR|Dataset for Object Detection in Remote Sensing|\n|FPN|Feature Pyramid Network|AID|Aerial Image Dataset|\n|MCFI|Multiscale Core Features Imitation|SSKDNet|Self-supervised Knowledge Distillation Network|\n|SSRD|Strict Supervision Regression Distillation|MSKA|Multi-level Semantic Knowledge Alignment|\n|CFKD|Cross-layer Fusion for Knowledge Distillation|ViTs|Vision Transformers|\n|YOLO|You Only Look Once|FPN|Feature Pyramid Network|\n|HSI|Hyperspectral Image|ERKT|Efficient and Robust Knowledge Transfer|\n|CKD|Collaborative Consistent Knowledge Distillation|TWA|Two-way Adaptive Distillation|\n|GKD|Generalized Knowledge Distillation|NLD|Noisy Label Distillation|\n|DKD|Decoupled Knowledge Distillation|CAMs|Class Activation Maps|\n|SSFD|Spatial Feature Blurring for Distillation|RS-SSKD|Remote Sensing Self-supervised Knowledge Distillation|\n|LEVIR|Large-scale Earth Vision Image Recognition|SAR SSDD|Synthetic Aperture Radar Ship Detection Dataset|\n|UCMerced|University of California Merced Land-use Dataset|NWPU-RESISC|Northwestern Polytechnical University Remote Sensing Image Scene Classification|\n|CMD|Class Mean Distillation|MSW|Maximum Sustained Wind|\n\nto new, unseen data [25]. Another pressing concern is the \"black box\" nature of DL models, which often leads to difficulties in interpreting their decision-making processes—a critical requirement in applications where transparency and understanding are paramount, such as in environmental compliance and strategic planning [26, 27].\n\nTo address some of these challenges, knowledge distillation (KD) emerges as a promising technique. KD involves training a smaller, more efficient student model to replicate the performance of a larger, more complex teacher model [28]. By transferring knowledge from a high-performing neural network to a compact model, KD reduces the computational resources required for deployment, making advanced AI-driven RS technologies more accessible [29]. Moreover, the student model can often achieve comparable accuracy with less data, mitigating the issues of extensive data requirements and overfitting [30]. In resource-constrained environments, KD proves particularly advantageous, as it enables energy-efficient deployment, thereby reducing the carbon footprint of AI systems [31]. Additionally, KD facilitates the transfer of pre-trained models to other domains through fine-tuning, extending the versatility of AI applications even in scenarios with scarce data. Furthermore, KD techniques can assist in generating synthetic training data when annotated data is limited, thus addressing one of the critical bottlenecks in RS [32, 30]. The resulting simpler models from the distillation process also offer easier interpretability, providing clearer insights into their decision-making mechanisms [33, 34]. This interpretability is essential for applications requiring transparency, such as environmental monitoring and regulatory compliance. As a result, KD not only democratizes AI capabilities within RS but also enhances the practical utility of these technologies in critical applications, ensuring a balance between performance, energy efficiency, and scalability across diverse domains [31]."
  },
  {
    "id_": "8c217159-89aa-4c84-be7a-82b337282de8",
    "text": "# 1.2. Comparison with Existing Reviews\n\nSeveral recent reviews and surveys have provided comprehensive analyses of various aspects of Knowledge Distillation (KD) and its applications across different domains. These works highlight the evolution, challenges, and future directions of KD, focusing on areas such as computer vision, medical applications, and large language models. For instance, [35] offers an in-depth examination of KD within the framework of the Student-Teacher (S-T) learning model, providing a thorough overview of KD’s core concepts, methods, and applications, particularly in vision tasks. The study also identifies key challenges and potential future research directions. Similarly, [36] explores the significance of cross-stage connection paths between teacher and student networks, introducing a novel approach that enhances the effectiveness of KD while maintaining low computational overhead. This framework is shown to improve performance across various tasks such as classification and object detection. Additionally, [37] presents a survey focusing on KD as a model compression and acceleration technique, categorizing KD methods by knowledge types, training schemes, and architectures. The paper discusses challenges like the trade-off between model size and performance and suggests potential research avenues to advance the field further.\n\nIn another study, Yadikar et al. [38] examine the application of KD in target detection within computer vision, focusing on the challenges of balancing detection speed and accuracy. The study highlights how knowledge compression techniques, particularly knowledge refinement, can enhance the performance of target detection algorithms on edge devices with limited computational power. The authors also propose potential improvements and future trends in integrating distillation learning with target detection [38]. Similarly, Alkhulaifi et al. [39] explore KD as a solution for deploying deep learning models on resource-constrained devices. They introduce a \"distillation metric\" to compare different KD methods based on model size and accuracy, providing a detailed survey of techniques such as soft label distillation and logit and feature map distillation, both offline and online. The study also discusses real-world KD applications in domains such as autonomous vehicles, healthcare, and IoT, outlining current challenges and future research directions. Furthermore, Yu et al. [40] review dataset distillation (DD), a technique related to KD that focuses on creating smaller, synthetic datasets that retain the performance of models trained on larger datasets. The study presents an algorithmic framework for DD methods, categorizes existing approaches, and identifies challenges such as privacy, copyright, and data storage, offering insights into future research directions for this emerging field.\n\nAdditionally, Meng et al. [41] explore the use of KD in the medical field, addressing challenges such as deploying large models on lightweight devices and the difficulty of"
  },
  {
    "id_": "eea3b54e-6d1e-4275-b1b2-95d94eadb947",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nThe study reviews various KD applications in healthcare, demonstrating how KD can compress complex models while improving their performance in medical tasks. It highlights the potential of KD to alleviate issues related to medical resource shortages by optimizing model deployment effectively. Similarly, Li et al. [42] present a survey on KD in object detection (OD), discussing the evolution of KD-based OD models and their advantages in performance and resource efficiency. The study analyzes different distillation techniques and explores their applications in domains like remote sensing (RS) and the management of 3D point cloud datasets, offering a comprehensive comparison of model performance across various datasets.\n\nFurthering the exploration of KD, Luo et al. [43] provide an overview of modern approaches to distilling Diffusion Models (DMs), focusing on distilling DMs into neural vector fields and reviewing stochastic and deterministic implicit generators. The authors also examine accelerated diffusion sampling algorithms as a training-free method for distillation, offering valuable insights for researchers interested in DM distillation. Additionally, Acharya et al. [44] address the emerging field of symbolic KD in large language models (LLMs), emphasizing the transformation of implicit knowledge within these models into a more explicit, symbolic form. This survey categorizes existing research, highlights the importance of symbolic KD in enhancing interpretability and efficiency, and proposes future research directions to advance this growing field.\n\nIn the context of computer vision, Kaleem et al. [45] provide a comprehensive review of KD techniques, covering major methods such as response-based, feature-based, and relation-based knowledge transfer. The study discusses the benefits and challenges of using KD to compress and optimize deep learning models, especially in resource-constrained environments. It explores the application of KD in tasks such as image classification, object detection, and video captioning, and highlights recent developments in multimodal models with KD. Similarly, Habib et al. [46] focus on KD in Vision Transformers (ViTs), addressing the challenges of deploying these models in environments with limited computational resources. The study reviews various KD approaches for compressing ViTs, emphasizing KD’s role in reducing computational and memory requirements while maintaining model performance. It also provides a comparative analysis of different KD techniques for ViTs and identifies unresolved challenges that warrant further research."
  },
  {
    "id_": "c1b8eb1e-c6b4-4397-81c2-63018b982830",
    "text": "# Table 1\n\nComparison of several KD surveys and reviews across various aspects such as focus on vision tasks, the use of teacher-student frameworks, real-world and medical applications, distillation techniques, and future research directions. It highlights which aspects are covered by each reference, along with the proposed study, indicating areas of focus and gaps in the existing literature.\n\nThe proposed review offers a comprehensive and structured analysis of KD, significantly expanding upon previous works by integrating a wide-ranging taxonomy and exploring its diverse applications across various domains, particularly in RS. Unlike existing reviews, which tend to focus on specific aspects of KD such as its role in model compression or its application in computer vision, this review provides a holistic overview, categorizing KD models based on architecture, distillation techniques, and application areas. Furthermore, it delves into advanced topics such as dynamic distillation, layer-wise distillation, and the integration of KD with real-time processing and edge AI—areas that remain relatively underexplored in prior literature. Additionally, the review addresses practical challenges such as data heterogeneity, scalability, and the balance between efficiency and accuracy, offering insights into emerging trends and future directions. This approach not only contextualizes KD within the broader landscape of machine learning but also highlights its potential for innovation in areas like precision agriculture, urban planning, and oceanographic monitoring. Thus, this review serves as a valuable resource for researchers and practitioners aiming to leverage KD in diverse and complex environments. Overall, this review makes several key contributions to the field of knowledge distillation (KD) in RS, which can be briefly summarized into the following:\n\n- Provides a comprehensive and structured analysis of KD, significantly expanding on previous works by integrating a wide-ranging taxonomy.\n- Explores the diverse applications of KD across various domains, with a particular focus on RS.\n- Categorizes KD models based on architecture, distillation techniques, and application areas, offering a holistic overview.\n- Delves into advanced topics such as dynamic distillation, layer-wise distillation, and the integration of KD with real-time processing and edge-AI, which are underexplored in prior literature.\n- Addresses practical challenges, including data heterogeneity, scalability, and the balance between efficiency and accuracy, providing insights into emerging trends and future directions.\n- Contextualizes KD within the broader landscape of machine learning over RS data, highlighting its potential for innovation in areas like precision agriculture, urban planning, and oceanographic monitoring."
  },
  {
    "id_": "471c5e8d-5e75-49b1-887a-655c85e71ed3",
    "text": "# 1.3. Literature Screening Approach"
  },
  {
    "id_": "b723fc8d-a1bc-4b90-bfcc-69f7cd9e7b55",
    "text": "# 1.3.1. Inclusion and Exclusion Criteria\n\nThe inclusion and exclusion criteria have been identified by including studies that are directly relevant to KD, particularly in the context of RS, with a focus on research published within the last 5 years to capture the latest advancements. Peer-reviewed articles, conference papers, preprints from reputable platforms and book chapters published in English are prioritized, in addition to empirical studies, reviews, case studies, and theoretical papers. Studies that do not specifically address KD in RS or focus on unrelated technologies, as well as outdated research published more than 5 years ago unless it is seminal are excluded, as well as non-peer-reviewed sources such as blog posts, opinion pieces, and non-academic publications.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 3 of 50"
  },
  {
    "id_": "e2fdf1f1-ac5e-4ad1-869e-094766571ca0",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "5d60223a-31c5-4be0-8c1f-d2b876ab939f",
    "text": "# Table 1"
  },
  {
    "id_": "391272d6-32c6-4ba8-9584-8ccb63707a5b",
    "text": "# Comparison of KD Surveys and Reviews\n\n|Aspect|Focus on Vision Tasks|Teacher-Student Framework|Real-world Applications|Medical Applications|RS Applications|Distillation Techniques|Discussion on Challenges|Future Research Directions|Model Compression Techniques|Introduction of New Metrics|Multimodal Model Applications|Discussion of Existing Datasets|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|[35]|✓|✗|✓|✓|✗|✗|✓|✓|✓|✓|✓| |\n|[36]|✓|✓|✓|✗|✓|✗|✓|✓|✓|✓|✓| |\n|[37]|✓|✓|✓|✓|✓|✗|✓|✓|✓|✓|✓| |\n|[38]|✗|✗|✗|✗|✗|✗|✓|✓|✗|✓|✓| |\n|[39]|✗|✗|✗|✗|✗|✗|✓|✗|✗|✓|✓| |\n|[40]|✓|✓|✓|✓|✓|✗|✓|✓|✓|✓|✓| |\n|[41]|✓|✗|✓|✓|✓|✓|✓|✓|✓|✓|✓| |\n|[42]|✓|✓|✓|✓|✓|✓|✓|✓|✓|✓|✓| |\n|[45]|✓|✓|✓|✓|✓|✗|✓|✓|✓|✓|✓| |\n|[46]|✗|✗|✗|✗|✓|✗|✗|✗|✗|✗|✓| |\n| |✗|✗|✗|✗|✗|✗|✗|✓|✗|✓|✓| |\n| |✗|✗|✓|✗|✗|✓|✓|✓|✓|✓|✓| |"
  },
  {
    "id_": "b18b05ee-0b1e-4bea-b394-e3d0223a03ba",
    "text": "# 1.3.2. Search Strategy\n\nThe search strategy involves using multiple academic databases such as IEEE Xplore, Scopus, Web of Science, Elsevier, Springer Nature, Wiley, Taylor & Francis, MDPI, Google Scholar, etc. to conduct a comprehensive search using relevant keywords like \"Knowledge Distillation\", \"Model Compression\", \"Model Distillation\", \"Feature Distillation\", \"Data Distillation\", \"Remote Sensing\", \"Urban Planning\", \"Precision Agriculture\", \"Land Cover Classification\", etc., refined with Boolean operators (AND, OR, NOT). Initial screening of titles, abstracts, and keywords is performed manually to identify potentially relevant studies. Studies that meet the inclusion criteria are shortlisted for a full-text review, where a detailed evaluation confirms their relevance and quality. Additionally, reference lists of selected studies are screened to identify any further relevant studies that may have been overlooked.\n\nFig. 1 explains the literature screening approach adopted in this study."
  },
  {
    "id_": "9e83f035-2835-45b4-97e7-36189d07889e",
    "text": "# 1.4. Organization of the Paper\n\nThe organization of the paper is meticulously structured to provide a thorough exploration of KD and its applications in remote sensing. Section 2 lays the groundwork by covering the fundamentals of KD, starting with a brief overview, defining essential concepts, and discussing the historical evolution of KD techniques. This section also delves into the basic principles and mechanisms of KD, including the objective function and overall loss that guide the distillation process. Additionally, the benefits of KD are highlighted, such as model compression, improved efficiency, enhanced performance on smaller models, and the broader implications for various applications. Following this, Section 3 focuses on RS tasks and the public datasets that are pivotal for applying KD in this domain. Section 4 introduces a comprehensive taxonomy of KD models, categorizing them based on variations in the model or input data, the type of transferred knowledge (including response-based, feature-based, and relation-based distillation), distillation targets (data, model, and feature distillation), and structural relationships within network layers (layer-to-layer and cross-layer distillation). In Section 5, the paper transitions to discussing the applications of KD in remote sensing, with a detailed examination of its use in image/scene classification, object detection, land cover classification, semantic segmentation, precision agriculture, urban planning, and oceanographic monitoring. Section 6 then addresses the challenges and limitations associated with KD, including model complexity, data heterogeneity, overfitting, scalability, real-time applicability, dependency on high-quality data, balancing efficiency and accuracy, and integration complexity. Looking ahead, Section 7 outlines future directions for KD research. It suggests advancements such as dynamic distillation, layer-wise distillation, efficient training and inference techniques, low-cost training algorithms, hardware-aware distillation, and improvements in data quality and robustness. The section also discusses scalability solutions like"
  },
  {
    "id_": "4eae0693-a39c-437c-9598-b4b1ef16ad39",
    "text": "# Figure 1\n\nSummary of literature screening approach used in this review.\n\nComprehensive Flowchart of the Literature Screening Process"
  },
  {
    "id_": "1ff58a89-5fd2-4453-81e6-e3f05963a06f",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\ndistributed and incremental distillation, real-time processing enhancements, and the integration of cross-modal and multi-modal distillation. Additionally, it explores the potential for seamless integration with existing workflows through plug-and-play distillation modules, toolkits, and frameworks, as well as enhancing model interpretability through explainable distillation and feature importance preservation. The potential of hybrid approaches, combining KD with other techniques and developing adaptive distillation frameworks, is also considered. Finally, Section 8 offers a comprehensive conclusion, synthesizing the insights gained throughout the paper and highlighting the potential for future advancements in the field of KD in remote sensing."
  },
  {
    "id_": "1d1835aa-700f-46ac-8134-38b75a50a821",
    "text": "# 2. Fundamentals of KD"
  },
  {
    "id_": "82826c67-6306-4c04-b081-385edba9d4b3",
    "text": "# 2.1. A Brief Overview"
  },
  {
    "id_": "49fb2720-c400-4e98-adc2-4d0847055230",
    "text": "# 2.1.1. Definition and Basic Concepts of KD\n\nKD is a ML technique where a smaller, simpler model (known as the student) is trained to emulate the behavior of a larger, more complex model (known as the teacher) [19]. As shown in Fig. 2, KD relies on two deep neural network models, a more complex one that is called the Teacher and a simpler one that is called the Student. The core idea is to transfer the knowledge from the teacher model, which typically performs better due to its greater capacity, to the student model, which is less resource-intensive [47] and tries to mimic the teacher’s behavior. This can be achieved by aligning the student’s outputs with those of the teacher using a Distillation Loss function that compares the two outputs. Usually, the teacher’s soft target probabilities (the outputs from the softmax layer before applying the final decision function, as depicted in Fig. 2) are used for this purpose. These soft targets provide richer information than hard labels, as they contain insights about the relative probabilities of incorrect answers, giving the student model clues about the underlying data structure and feature relationships that the teacher model has learned [48]. However, apart from this response-based knowledge distillation tactic, the student also can learn the output of intermediate teacher layers, or other representations, making the KD approach very flexible and powerful."
  },
  {
    "id_": "82f28eda-5500-4f00-9fd7-bf5048c03599",
    "text": "# 2.1.2. Brief History and Evolution of the KD Technique\n\nThe concept of KD can be traced back to earlier works in model compression and hints training, where simpler models were trained to mimic more complex ones using additional information from those models. However, the term “knowledge distillation” was popularized by Hinton et al. [49] in a seminal 2015 paper, where they demonstrated the effectiveness of using soft targets to train neural networks. Since then, the field has seen rapid development and broad applications across various domains of Artificial Intelligence. Originally, KD was primarily used to reduce the size and computational demands of large neural networks so that they could be deployed on devices with limited hardware capabilities, such as mobile phones and embedded systems. This was particularly valuable for applications that require real-time processing, such as speech recognition and mobile vision [50].\n\nAs research progressed, the scope of KD expanded beyond model compression. Researchers began exploring its potential to improve model generalization by smoothing the decision boundaries, making them more stable and improving generalization. Using ensembles of teacher networks to train a student network further stabilizes training by distilling the collective knowledge of multiple models into a single model, and facilitates transfer learning across different domains or tasks [51]. The technique has been adapted and refined to include not just direct output mimicry, but also feature-based and relation-based distillation, where intermediate representations and relationships between data points are also transferred from the teacher to the student [52]. Today, KD is an active area of research with ongoing innovations that aim to further enhance its effectiveness and expand its applicability. This includes cross-modal distillation for transferring knowledge between different types of data, such as video-to-text, and self-distillation, where a model is iteratively trained on its own softened outputs to refine its capabilities [53]."
  },
  {
    "id_": "229330c4-592b-44b9-aa39-78419f9154a3",
    "text": "# 2.2. Basic Principle and Mechanism\n\nThis section provides the mathematical background of KD [54]. Let us denote the output logits (pre-softmax activations) of the teacher model as 𝑧𝑇 and those of the student as 𝑧𝑆. The softmax function applied to these logits is given by:\n\n𝜎(𝑧, 𝑇 ) =  ∑𝑗 𝑒𝑧𝑗 ∕𝑇 ,𝑒𝑧∕𝑇𝑖\n\nwhere 𝑖 indexes the output classes, and 𝑇 is the temperature parameter that controls the softness of the probability distribution. A higher value of 𝑇 produces a softer probability distribution [55]."
  },
  {
    "id_": "f171c083-aafc-4fc6-9753-7565f8a437ac",
    "text": "# 2.2.1. Objective Function\n\nThe training of the student network involves minimizing a loss function that typically comprises two terms: the distillation loss and the traditional hard target loss [56].\n\n- Distillation Loss: This loss measures the difference between the softened outputs of the teacher and the student, encouraging the student to mimic the teacher’s generalized behavior. It is often computed using the Kullback-Leibler (𝐾𝐿) divergence [57]:\n- 𝐿KD = 𝑇 2 ⋅ 𝐾𝐿(𝜎(𝑧𝑇 , 𝑇 )‖𝜎(𝑧𝑆 , 𝑇 ))\n\nThe factor of 𝑇 2 is used to scale the gradients appropriately, as the gradients produced by the softmax function are scaled by 𝑇 [57].\n\nHard Target Loss: This is a standard loss, such as Cross-entropy (CE), used in training neural networks, calculated between the student’s output (at 𝑇 = 1) and the true labels [58]:\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 5 of 50"
  },
  {
    "id_": "7c8e1a52-b0b9-4c01-baa1-6928fab49fca",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "7a6c1c1e-90a0-45b1-a6ed-2e173b110472",
    "text": "# Teacher Model\n\n|Input Layer|Hidden Layers|Output Layer|\n|---|---|---|\n|Knowledge Distillation|Knowledge Distillation|Knowledge Distillation|\n|Soft Target Probabilities|Soft Target Probabilities|Soft Target Probabilities|\n|Knowledge|Knowledge|Knowledge|\n|Data|Data|Data|\n|Student Model|Student Model|Student Model|\n|Soft Target Probabilities|Soft Target Probabilities|Soft Target Probabilities|\n\nFigure 2: An overview of the knowledge distillation principle."
  },
  {
    "id_": "d8c25816-e4f6-421e-998a-2571a6bd0292",
    "text": "# 2.2.2. Overall Loss\n\nThe total loss function used to train the student model is a weighted sum of the distillation and hard target losses:\n\n𝐿 = 𝛼𝐿CE + (1 − 𝛼)𝐿KD\n\nwhere 𝛼 is a hyperparameter that balances the importance of the two loss components. By optimizing this loss, the student learns not only the explicit knowledge represented by the class labels but also the implicit, richer information embedded in the teacher’s output distribution, thus achieving better generalization from a more compact model [59]. Fig. 3 summarizes the main steps of applying KD in RS applications. Fig. 4 illustrates the architecture of a knowledge distillation (KD) framework based on YOLOv8, designed for precision agriculture applications such as weed recognition and variable rate spraying. In this framework, a YOLOv8l model, which has the highest recognition accuracy, was chosen as the teacher network, while a YOLOv8n model, which has the lowest recognition accuracy and the smallest model size, was selected as the student network. The resulting KD model, named YOLOv8n-DT, is specifically tailored for rice field weed recognition and comprises three main components: the teacher network, the student network, and the distillation loss function module, that performs both target and feature distillation."
  },
  {
    "id_": "b8ab9040-92a7-4535-b2ae-d711701a713a",
    "text": "# 2.3. Benefits of KD\n\nKD offers several compelling advantages that make it an attractive technique in the field of ML, particularly when deploying models in resource-constrained environments."
  },
  {
    "id_": "70ccf17f-812a-49d3-876e-18a789f80be3",
    "text": "# 2.3.1. Model Compression\n\nOne of the primary benefits of KD is model compression. Traditional DL models often require substantial computational resources due to their depth and complexity, which limits their deployment on devices with restricted hardware capabilities such as mobile phones, IoT devices, and embedded systems. KD addresses this challenge by enabling the training of smaller, lighter models (students) that mimic the behavior of larger, more complex models (teachers). This process involves transferring the intricate knowledge and insights learned by the teacher model into a more compact form within the student model. The student thereby learns to approximate the function of the teacher but with fewer parameters and lower computational demands. This compression not only reduces the size of the model but also lessens the energy consumption and heat production, which are critical factors for battery-powered devices."
  },
  {
    "id_": "2384d5c2-4c55-4626-abf0-5c16dcfee968",
    "text": "# 2.3.2. Improved Efficiency\n\nEfficiency in model training and inference is another significant advantage of KD. By distilling a cumbersome model into a smaller one, KD effectively reduces the time and computational power needed for training and deploying AI systems. This improved efficiency is particularly beneficial for applications requiring real-time data processing, such as autonomous driving and real-time surveillance. Smaller models also allow for more frequent updates and easier maintenance, which is crucial for systems that need to adapt to changing conditions or data streams."
  },
  {
    "id_": "688d1e5c-2ed1-451d-92fd-feecbb24fe65",
    "text": "# 2.3.3. Enhanced Performance on Smaller Models\n\nKD not only compresses the size of the models but often also enhances their performance, especially in smaller models. Typically, smaller neural networks are prone to underfitting and may not capture the complex patterns in large datasets as effectively as their larger counterparts. However, when trained through the KD process, these smaller models inherit refined insights from the teacher models, which include soft probabilities and inter-class relationships that are not visible through traditional hard-label training. This enriched training set helps the student models to perform better than if they were trained independently from scratch. Moreover, the nuanced knowledge transferred includes the handling of edge cases and anomalies, which significantly improves the robustness and generalizability of the student models.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 6 of 50"
  },
  {
    "id_": "c1a0dccf-936f-46e1-ad41-090bd770a339",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "40edd573-a65a-4113-9e30-9256e2f4985c",
    "text": "# Start"
  },
  {
    "id_": "e3cdf281-9452-4b25-a361-2941b0ac9e74",
    "text": "# Pre-trained Teacher Model"
  },
  {
    "id_": "bd7cf19a-f434-44e7-957e-855dedcb3fbd",
    "text": "# Initialize Student Model"
  },
  {
    "id_": "ffbdce1f-c403-460b-857f-b580dbe5011f",
    "text": "# Compute Teacher and Student Logits (zT , zS)"
  },
  {
    "id_": "55159a27-d9a7-400b-9319-8da3a5f5f5c3",
    "text": "# Apply Softmax with Temperature T"
  },
  {
    "id_": "3a1bf8d2-94d4-4dc9-91e3-1e307312d906",
    "text": "# Compute Distillation Loss LKD"
  },
  {
    "id_": "82052d30-2c96-471a-b4c3-d4eb8d083540",
    "text": "# Compute Hard Target Loss LCE\n\n- Tune Temperature T\n- Adjust T\n- KL Divergence between Softened Outputs"
  },
  {
    "id_": "65298e68-f05c-483d-aca8-0d509e024b7d",
    "text": "# Compute Overall Loss L = αLCE + (1 − α)LKD\n\n- Adjust α\n- Train Student Model\n- Evaluate Performance on Validation Set"
  },
  {
    "id_": "d1d97ec4-38c9-47c6-97e8-f7c15bb60f66",
    "text": "# End\n\nFigure 3: Principal steps of applying KD in RS applications."
  },
  {
    "id_": "68584345-c3df-464a-970e-8ef9ebd13368",
    "text": "# 2.3.4. Broader Implications\n\nThe advantages of KD extend beyond individual model improvements. In educational settings, distillation techniques can democratize access to advanced AI capabilities by enabling more institutions to deploy high-performing AI solutions without the need for expensive infrastructure. Furthermore, in a research context, KD facilitates greater experimental flexibility and faster iteration speeds, accelerating the pace of innovation in AI."
  },
  {
    "id_": "908e8073-5e85-41fe-9030-bc1d0392ae19",
    "text": "# 3. RS Tasks and Public datasets\n\nRS has become a pivotal tool for monitoring and understanding changes in both urban and agricultural environments. RS involves the acquisition and analysis of data from satellite or airborne sensors to observe and interpret features on the Earth’s surface. The main RS tasks encompass a variety of applications that leverage spectral, spatial, and temporal information. These tasks include image classification, object detection, change detection, segmentation, and data fusion. The primary RS tasks are centered around image classification and analysis.\n\nImage classification categorizes pixels in an image into distinct classes, such as different land cover types, using methods like convolutional neural networks (CNNs) for high accuracy. Object detection identifies and locates specific objects within an image, such as vehicles or buildings, making it crucial for applications in urban planning and agriculture. Change detection focuses on identifying differences in images taken at different times, which is essential for monitoring environmental changes like deforestation. Segmentation further refines this process by partitioning an image into meaningful regions, helping extract detailed information about specific features like roads or rivers. Collectively, these tasks highlight that RS primarily involves sophisticated image classification and analysis.\n\nData fusion tasks arise from the need to integrate and analyze data from various sources, such as multispectral, hyperspectral, or LiDAR data, to enhance the comprehensiveness and accuracy of RS applications. This integration is vital when dealing with the complex nature of environmental features that cannot be fully captured by a single sensor type. Consequently, data fusion is an essential approach to\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 7 of 50"
  },
  {
    "id_": "05e2333a-d9f3-485a-a8c2-690f9148fda3",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "cd7f5e8b-64f0-4ee7-acc9-9c84d48466ba",
    "text": "# Teacher Model\n\n| |Backbone|Head| | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | |CZF| |Detect| |Bbox_| | | |\n| | | | | |PS: 20*20*1024|Conv| | | | | |\n| | | |C2F|C2F|40*40*1022|Detect| | | | | |\n| | | |CZF| |P3: 80*80*512| | | | | | |\n| | | | | | | |Loss|Loss| | | |\n| |160*160*256|320*320*123| | | | | | | | | |\n|Cf| | | |P3|Cz|P4|P5|KT|Output| | |\n| | | | | |Feature Distill| | | | |Logical Distill|Output|"
  },
  {
    "id_": "ca4e5fb4-b4a8-43ec-9ed4-a3633c9a4031",
    "text": "# Student Model\n\n|Backbone|Head| | | | | | |\n|---|---|---|---|---|---|---|---|\n| | | |CZF|20*20*512|Detect|Bbox_| |\n| | | |conv| | | | |\n| | |CZFF|CZF|40*40*512|Detect| | |\n| | | |Conv| | | | |\n| | |C2F| |P3: 80*80*256| |Cls_| |\n| | | | | | |Loss|Loss|\n|80x80+256|P2: 160*160*128| | | | | | |\n|PI: 320*320*64| | | | | | | |\n\nFigure 4: The YOLOv8n DT network architecture is structured into three primary components: the teacher network, the student network, and the distillation loss function module. This architecture incorporates both feature loss and logit loss within the distillation process to effectively transfer knowledge from the teacher to the student network, thereby enhancing the student’s performance while maintaining efficiency.\n\nAddressing the limitations of individual datasets, providing a more holistic view of the Earth’s surface. All the aforementioned tasks are fundamental to various environmental, agricultural, and urban studies, providing essential insights for decision-making and resource management. During the years, diverse datasets have been developed to support the advancement of instance segmentation techniques in this field, each tailored to specific challenges and applications.\n\nSpaceNet 7 [60] and SpaceNet 4 [61] represent significant contributions to urban development analysis. SpaceNet 7 offers insights into the evolution of building footprints across 100 global locations over two years, using Planet imagery. This dataset is crucial for tracking urban expansion and infrastructure development. Conversely, SpaceNet 4 focuses on the technical challenge of detecting buildings from steep observation angles—up to 54 degrees off-nadir. This is particularly valuable in emergency response situations where quick, accurate assessments are necessary. Similarly, the Microsoft BuildingFootprints dataset [62] provides detailed building footprints across several countries, extracted from Bing imagery. This resource supports urban planning and management by offering extensive building delineations. Additionally, the xView 2 Building Damage Assessment Challenge [63] leverages high-resolution Worldview-3 imagery to assess building damage from natural disasters, a critical component of effective disaster response.\n\nIn the agricultural sector, datasets like PASTIS [64] and the Agriculture-Vision Database [65, 66] are invaluable. PASTIS provides panoptic labels for over 124,000 agricultural parcels in France, captured across Sentinel-2 timeseries images. This dataset aids in the precise monitoring and management of agricultural lands. The Agriculture-Vision challenge, on the other hand, focuses on identifying field anomalies from aerial imagery across the United States, promoting enhanced agricultural practices through detailed monitoring.\n\nFor more specialized applications, datasets like RarePlanes [67], which includes both synthetic and real data for plane detection, and iSAID [68], which covers a wide range of categories from planes to bridges, are particularly noteworthy. RarePlanes is essential for developing models that differentiate between aircraft types, useful in both civilian and defense sectors. iSAID facilitates broad applications in aerial image analysis by providing extensive annotations for diverse objects. Furthermore, the introduction of SpaceNet 6: Multi-Sensor All-Weather Mapping [69] combines SAR data and optical imagery to enhance building footprint detection in challenging weather conditions, illustrating the value of multi-sensor data integration in RS. The technological advancements in datasets like Airbus Ship Detection Challenge [70], which focuses on ship detection using satellite imagery, and novel methodologies in the LPIS agricultural field boundaries dataset highlight the industry’s shift towards more sophisticated and fine-grained analysis capabilities.\n\nThe PASTIS dataset [71] provides detailed panoptic labels for over 124,000 agricultural parcels across France, captured in 2,433 Sentinel-2 image timeseries. This dataset is instrumental for applications in agricultural monitoring, allowing for the differentiation of crops at the parcel level through both instance and semantic segmentation. It is particularly useful for tracking changes in agricultural land over time.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 8 of 50"
  },
  {
    "id_": "e81cabb5-dfd3-449f-a42b-cbbd2f099981",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "8bac4687-a657-4a53-9815-c1dcbd8d2136",
    "text": "# Knowledge Distillation Taxonomy\n\n|Type of Knowledge Transferred|Training Methodology|Application Area|Supervision Signal|Distillation Strategies|Performance Optimization|\n|---|---|---|---|---|---|\n|Graph-Based Relations|Progressive Distillation|Model Distillation|Hard Labels|Classical Distillation|Energy Efficiency|\n|Relation-Based|Co-Training|Data Distillation|Soft Labels|Contrastive Distillation|Memory Optimization|\n|Pairwise Distances|Online Distillation| | |Adversarial Distillation|Speed Optimization|\n|Attention Maps|Self-Distillation| | |Cross-Layer Distillation| |\n|Feature-Based|Offline Distillation| | |Layer-to-Layer Distillation| |\n|Intermediate Features| | | | | |\n|Probability Distribution| | | | | |\n|Response-Based| | | | | |\n|Logits Matching| | | | | |"
  },
  {
    "id_": "f0edff01-c2c3-4538-bd4e-6fd2c41899ae",
    "text": "# 4. Taxonomy of KD Models\n\nKD methods in RS (RS) can be categorized into several key approaches, each with unique attributes and applications. As depicted in Fig. 5, the variations may come from the differences in the data or architecture used by the teacher and student networks resulting to Heterogeneous and Cross-modal KD approaches that are based on the Teacher-Student Architecture, or from the different types of knowledge that are distilled between the teacher and the student, resulting to Response-based, Feature-based, and Relation-based approaches. These approaches are tailored to optimize RS models by transferring knowledge from complex teacher models to more efficient student models, using all the available data per case, thereby enhancing performance in tasks such as object detection, scene classification, and image segmentation. Of course, there are several more variations that depend on the training methodology, the application area, the structural representation, the distillation strategy, etc., as shown in Fig. 5 and explained in the following."
  },
  {
    "id_": "04b0213d-35f0-44f7-a0a2-b002061fb4cb",
    "text": "# 4.1. Varying the Model or Input Data"
  },
  {
    "id_": "ff297e28-aba6-45cc-ab3d-a45350100912",
    "text": "# 4.1.1. Heterogeneous KD\n\nHeterogeneous KD (HKD) is a method of transferring knowledge from a teacher model to a student model where the teacher and student models have significantly different architectures [72]. Traditional KD methods typically assume that the teacher and student models have similar architectures, which allows for straightforward layer-by-layer transfer of knowledge. However, in HKD, the architectures may vary greatly, posing a challenge for direct knowledge transfer.\n\nThe study in [73] presents a Generalized KD (GKD) framework for multi-source Earth Observation analysis, specifically for land cover mapping using radar and optical satellite image time series data. This approach tackles data.\n\nY. Himeur, et al.: Preprint submitted to Elsevier"
  },
  {
    "id_": "e94e43c4-812d-40c1-ab39-d2cefb827363",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nmisalignment due to atmospheric conditions or acquisition costs, using radar data consistently and treating optical data as privileged information. This makes it a case of heterogeneous distillation, where different modalities (radar and optical) are involved, requiring the student model to adapt to a less data-rich environment at test time compared to training. The authors in [74] propose using information from deep convolutional networks to guide the training of shallow Grassmannian manifold networks, addressing the need for high-performance yet small-sized networks in resource-limited scenarios. The approach bridges DL with manifold learning, fitting well within the heterogeneous category, as it involves transferring knowledge between fundamentally different architectures. Moving forward, Yang et al. [75] introduce a two-way assistant distillation method for lightweight object detection in RS. This method incorporates compression and multiscale adaptive modules to address feature disparities and background noise, utilizing a heterogeneous distillation approach by applying complex operations from larger models to enhance smaller, simpler ones.\n\nBesides, Nabi et al. [76] propose a compound loss computed on a Transformer-based student and a CNN teacher for single-label scene classification in RS. The use of heterogeneous architectures, where a CNN and a Transformer are involved, leverages the long-range visual capabilities of the Transformer and the inductive biases of the CNN, aiming to enhance classification accuracy in complex scenes. Similarly, the research in [77] involves a teacher-ensemble learning approach using KD in cross-source content-based image retrieval for high-resolution RS images. The method combines source-shared and source-specific classifiers, constructing an effective heterogeneous ensemble of teacher models to transfer useful information to the student model."
  },
  {
    "id_": "ce05e2cf-5f67-40cf-9174-aa4b5ae11ea5",
    "text": "# 4.1.2. Cross-Modal KD\n\nCross-modal KD (CMKD) refers to the process of transferring knowledge from a model trained with superior modalities (e.g., depth maps or point clouds) to another model trained with weaker modalities (e.g., RGB images) [78]. The goal is to improve the performance of the student model trained on the weaker modality by leveraging the knowledge from the teacher model trained on the superior modality. This transfer is achieved by aligning the intermediate feature representations and activation maps between the teacher and student models [78]. In CMKD, the knowledge from the teacher model is used as an additional supervision signal to guide the training of the student model, enhancing its learning process and performance.\n\nExpanding on this concept, Geng et al. [79] propose a topological space network for road extraction, where a denser teacher network focused on topological feature extraction guides a lighter student network. This distillation process transfers knowledge about complex road topology from a heavy network, illustrating a clear case of cross-modal architecture distillation by integrating high-dimensional topological features into a simplified network. Similarly, Xiong et al. [80] introduce a discriminative distillation network for cross-source Content-Based RS Image Retrieval (CBRSIR), addressing the challenge of harmonizing features between multispectral and panchromatic images, thereby further exemplifying cross-modal architecture by handling variations between different types of RS data sources. Additionally, Liu et al. [81] propose a cross-modal KD framework designed to improve multispectral scene classification by transferring knowledge from teacher models pre-trained on RGB images to a student model processing multispectral images. This approach highlights the adaptability of CMKD by addressing the differences between modalities and enhancing the student’s performance, particularly in scenarios with limited samples.\n\nFurthermore, Pande et al. [82] contribute to the field with an adversarial training-driven hallucination architecture for modality distillation in RS image classification, focusing on learning discriminative feature representations from multiple sensor modalities, even in the presence of missing data during the model inference phase. This work aligns closely with cross-modal architectures as it effectively distills features across varying sensor modalities, enhancing model robustness. Lastly, Liu et al. [83] present a universal Super-Resolution-Assisted Learning (SRAL) framework aimed at improving the performance and efficiency of salient object detection in RS images. By incorporating super-resolution techniques into a multitask learning framework, this approach distills domain knowledge from the super-resolution task to significantly boost object detection performance, further showcasing the potential of cross-modal knowledge transfer in enhancing model accuracy and efficiency."
  },
  {
    "id_": "bb8deac8-f1a1-48a1-9908-7c440934d691",
    "text": "# 4.2. Varying the Type of Transferred Knowledge"
  },
  {
    "id_": "d8268b63-e5af-4914-b58c-05170e0b29ca",
    "text": "# 4.2.1. Response-Based (Soft Targets Distillation)\n\nResponse-based KD focuses on the knowledge extracted from the final layer of the teacher model. It aims to align the final predictions between the teacher and the student models. The primary goal is outcome-driven learning, which involves distilling the class probability distribution via a softened softmax function, known as ’soft labels.’ This method guides the student model by matching the output distributions of the teacher and student models using various distance functions such as Kullback-Leibler divergence, mean squared error, or Pearson correlation coefficient [84].\n\nThe study in [85] introduces a KD framework applied to RS scene classification. By using the high-temperature softmax outputs from a large, deep teacher model to train a smaller, shallow student model, the study showcases how KD can improve the performance of less complex models on multiple public datasets, increasing accuracy significantly even on smaller and unbalanced datasets. This approach directly employs the response-based distillation technique by leveraging the teacher’s softened output probabilities to enhance the student’s learning process. Moving on, Zhao et al. [86] introduces a novel pairwise similarity KD method for reducing the complexity of CNN models in RS image scene classification, maintaining accuracy while using less computational resources. This method focuses on distilling discriminative information between sample pairs."
  },
  {
    "id_": "f6758ca4-34f9-46de-9719-45407f991b8d",
    "text": "# 4.2.2. Feature-Based (Intermediate Representations)\n\nFeature-based KD addresses the limitation of response-based KD by providing supervision at intermediate layers of the network. This method focuses on transferring intermediate feature representations, such as feature maps, attention"
  },
  {
    "id_": "a0d90f8b-040d-4580-8cef-694c6bf85678",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "af3d1cac-d6e0-4480-b41b-9c59d82f3aaf",
    "text": "# 4.2.3. Relation-Based (Learning Relationships Between Different Data Layers)\n\nRelation-based KD explores the relationships between different data samples or across different layers within the neural network. Unlike response-based and feature-based KD, which typically handle individual samples, relation-based KD captures cross-sample or cross-layer relationships as meaningful knowledge [84]. This method constructs relational graphs to model dependencies and similarities between instances or layers and uses similarity metrics and distance functions to measure these relationships. The goal is to transfer structured knowledge that encapsulates higher-order dependencies and interactions within the dataset.\n\nChen et al. [96] develop consistency- and dependence-guided KD methods for object detection in RS images. They introduce modules that focus on extracting and transferring discriminative spatial locations and channels, as well as establishing the consistency and dependence of features between the teacher and student models. This approach utilizes relation-based distillation by focusing on the inter-layer and inter-feature relationships to guide the student model’s learning process. Moving on, Li et al. [97] introduce an instance-aware distillation method, which combines feature-based and relation-based distillation techniques. The method enhances the student model’s performance by focusing on instance-related foreground information and constructing relationships between different instances to improve detection accuracy in complex remote-sensing images. Zhao et al. [98] propose a self-supervised KD network (SSKDNet) that uses feature maps of the backbone as supervision signals and transfers the \"dark knowledge\" through KD. This method focuses on enhancing the discriminative feature extraction capabilities by learning the relationships between different data layers in a self-supervised setting.\n\nDong et al. [99] present a cross-model KD framework, distilling segmenters from CNNs and transformers, which uses a channel-weighted attention-guided feature distillation and a target–nontarget KD module to guide the student model in learning complex representations and decision boundaries. This study distinctly focuses on relation-based distillation by leveraging the interdependencies of features and classification decisions between different network architectures. On the other hand, Zhou et al. [100] introduce the Multi-level Semantic Transfer Network (MSTNet), a KD framework designed for dense prediction of RS images. This network utilizes a Multi-level Semantic Knowledge Alignment (MSKA) framework to distill semantic information from a complex teacher model to a more compact student model. The MSKA framework emphasizes cross-layer semantic alignment, dynamic semantic aggregation.\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 11 of 50"
  },
  {
    "id_": "1e7dd77d-61d9-4c70-8668-4b6eaf9ca66f",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "cc4c6fef-ab33-4e55-abf0-31d8b68f2ae9",
    "text": "# 4.3. Varying Distillation Target"
  },
  {
    "id_": "2d4f87fe-f8a9-4fae-a49e-9dc4bfa3159a",
    "text": "# 4.3.1. Data Distillation\n\nData distillation refers to techniques that aim to synthesize small, high-fidelity data summaries which capture the most important knowledge from a given dataset [101]. These distilled summaries are optimized to serve as effective substitutes for the original dataset in various data-usage applications such as model training, inference, and architecture search. The goal is to create a concise representation of the data that maintains its critical characteristics, allowing for faster and more efficient model training and evaluation [101].\n\nBuilding on this concept, Zhang et al. [102] introduce a novel noisy label distillation method within an end-to-end teacher-student framework, which distills knowledge from labels across various noise levels. This approach exemplifies data distillation by effectively utilizing knowledge from noisy data to improve classification performance in RS image scene classification. Extending the application of data distillation, Zhao et al. [86] propose a pair-wise similarity KD method for RS image scene classification. By distilling discriminative information from a cumbersome model to a compact model, this study aims to maintain high accuracy while reducing model complexity, demonstrating another facet of data distillation. Furthermore, Yue et al. [103] contribute to this field with a self-supervised learning method that incorporates adaptive distillation for hyperspectral image classification. Their approach, which focuses on generating adaptive soft labels based on spatial-spectral similarity, underscores the importance of utilizing extensive unlabeled data in the data distillation process."
  },
  {
    "id_": "576af708-c3a2-40a0-af54-68222f4c483b",
    "text": "# 4.3.2. Model Distillation\n\nModel distillation refers to the process of replacing a complex ML model with a simpler model that approximates the original model’s performance [104]. This technique is used to improve computational efficiency by distilling large or ensemble models into smaller, more manageable models that maintain similar accuracy. The primary goal is to reduce the computational cost associated with deploying large models while preserving their predictive capabilities [104]. Model distillation also aids in model interpretability by converting “black-box” models, such as neural networks, into more transparent forms.\n\nIn the context of model distillation for RS applications, a variety of approaches have been developed to enhance the performance and efficiency of lightweight models. Zhang et al. [105] introduce a dynamic knowledge distillation (KD) framework that enables CNN models to be lightweight while maintaining high detection accuracy, with an emphasis on selective learning through a dynamic instance selection distillation module. Building on the concept of model distillation, Yang et al. [106] develop a lightweight semantic segmentation network that combines KD with a multiscale pyramidal pooling module and attention mechanisms, resulting in a pruned model that retains high accuracy. Similarly, Wang et al. [107] propose a change detection method that integrates prototypical contrastive distillation and channel-spatial-normalized distillation, allowing the student model to learn complex feature distributions from the teacher, thereby fitting into the model distillation framework.\n\nFurther advancing the field, Chen et al. [108] propose a multi-teacher collaborative distillation approach that uses adaptive weight and feature knowledge exchange to enhance the robustness of student models, while Gu et al. [109] introduce a Context-aware Dense Feature Distillation (CDFD) strategy for CubeSat-based RS object detection, integrating multiple teacher networks to optimize a lightweight detector. Chai et al. [110] contribute to the model distillation category with their Bidirectional Self-Attention Distillation (Bi-SAD) approach, aimed at enhancing cloud detection models by enabling compact models to learn detailed textural and semantic information.\n\nAddressing the challenge of few-shot learning, Liu et al. [111] present a ranking-preserving KD method that improves the generalization capabilities of student models in RS scene classification. Similarly, Wang et al. [112] explore the enhancement of lightweight models through a Phase-shift encoded KD method (PseKD) that improves object orientation prediction. In a broader application, Chen et al. [113] propose a semi-supervised KD framework for global-scale urban object mapping, emphasizing the handling of urban diversity and large-scale sample growth.\n\nComplementing these efforts, Zhao et al. [114] propose a weakly correlated distillation learning framework for RS object recognition with limited samples, leveraging large-scale natural image datasets to enhance small-scale RS datasets. Lin et al. [115] address the issue of denoising by presenting a lightweight model that uses KD to efficiently extract spatial and spectral features while maintaining computational efficiency. Yu et al. [116] focus on incremental learning, introducing a dual KD method to mitigate catastrophic forgetting, which aligns with the incremental learning approach proposed by Xu et al. [117] and Xu et al. [118], who use KD to enhance multimodal learning and hyperspectral image classification, respectively.\n\nLastly, Zhou et al. [119] introduce a graph semantic guided network (GSGNet) for optical RS scene analysis, utilizing knowledge refinement to maintain high inference speed and contextual inference capability. Zhao et al. [120] propose a target detection model distillation framework that uses feature transition and label registration to improve the learning ability of lightweight networks in RS imagery, further contributing to the body of work on model distillation."
  },
  {
    "id_": "1359543a-43ea-4608-ba4c-398af0998999",
    "text": "# 4.3.3. Feature Distillation\n\nFeature distillation refers to a method in which the student network learns to mimic the hidden feature values of a teacher network [121]. This process involves transferring the intermediate representations (features) learned by the teacher network to the student network. Unlike traditional KD that focuses on the output probabilities (logits), feature distillation emphasizes the transfer of internal activations or feature maps. The primary goal is to improve the student network’s performance by leveraging the knowledge encapsulated in the teacher’s feature representations [121].\n\nBuilding upon this concept, Zhou et al. [122] propose a lightweight student network framework for semantic segmentation of high-resolution RS images. By employing a\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 12 of 50"
  },
  {
    "id_": "140b3c0c-597c-44a9-b8fe-7dda0305fd85",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nGraph attention guidance network, they distill knowledge from a large teacher network to optimize image features, thereby enhancing segmentation accuracy. This method aligns with feature distillation, where the objective is to boost the student’s feature representation capabilities to closely match those of the teacher. Similarly, Zhang et al. [123] introduce a few-shot classification method for RS scene classification, which also falls under the feature distillation category. This approach utilizes a novel two-branch network and incorporates self-KD during training to generate powerful representations, prevent overfitting, and enhance overall performance.\n\nIn parallel, Hu et al. [124] contribute to the field with a variational self-distillation network designed for RS scene classification. This method hierarchically distills class entanglement information from deep to shallow layers, further illustrating the application of feature distillation by refining and transferring feature information across different network layers. Expanding on these ideas, Xing et al. [125] present a collaborative consistent KD method aimed at improving classification accuracy for RS image scenes on embedded devices. Their approach emphasizes feature distillation across multiple network branches, focusing on reducing parameter redundancy and enhancing model efficiency, thus reinforcing the relevance of feature distillation in RS applications."
  },
  {
    "id_": "cb482b25-eb25-4285-8b77-05b2ceb12790",
    "text": "# 4.4. Varying the Structural Relationship of Network Layers"
  },
  {
    "id_": "91bce813-d0d7-4f0c-b686-e3f65ec050fe",
    "text": "# 4.4.1. Layer-to-Layer Distillation\n\nLayer-to-layer distillation refers to the process where the teacher model’s intermediate layers directly guide the corresponding layers of the student model. This method ensures that the student model learns similar feature representations as the teacher model at different stages of its depth [126, 127].\n\nDirect Mapping: In this approach, each layer of the teacher model is aligned with the corresponding layer in the student model. The outputs of each intermediate layer in the teacher model are used as targets for the corresponding layer in the student model. This direct mapping can help the student model learn hierarchical features similar to those learned by the teacher model [128].\n\nFeature Representation: By mimicking the intermediate representations of the teacher, the student model can capture complex features and patterns, which might be difficult to learn solely from the final output. This method is particularly useful when the student model has a similar or reduced architecture compared to the teacher.\n\nLoss Function: Often, additional loss terms are introduced to minimize the difference between the teacher’s and student’s intermediate layer outputs. This can include mean squared error (MSE) or other similarity measures.\n\nSuppose a deep CNN is used as the teacher model with layers: 𝑇1, 𝑇2, 𝑇3, … , 𝑇𝑛. The student model has corresponding layers: 𝑆1, 𝑆2, 𝑆3, … , 𝑆𝑛. During training, the output of 𝑇1 will guide 𝑆1, 𝑇2 will guide 𝑆2, and so on, ensuring that each student layer learns to mimic the feature maps of the corresponding teacher layer."
  },
  {
    "id_": "7696788d-37c9-4d8f-9fba-287375f11ee5",
    "text": "# 4.4.2. Cross-Layer Distillation\n\nCross-layer distillation refers to the process where the teacher and student models do not have a direct correspondence between layers. Instead, the knowledge transfer happens between non-matching layers, for example, higher layers of the teacher model guiding lower layers of the student model or vice versa.\n\nNon-Matching Layers: In this approach, there is no strict one-to-one correspondence between the layers of the teacher and the student. The knowledge from higher (more abstract) layers of the teacher model can be distilled into lower (more detailed) layers of the student model, allowing for flexible guidance. Chen et al. [129] propose Semantic Calibration for Cross-layer Knowledge Distillation (Sem-CKD), which automatically assigns target layers from a teacher model to each student layer using an attention mechanism. This method allows student layers to distill knowledge from multiple teacher layers rather than following a fixed, one-to-one correspondence. Building on this concept, Wang et al. [130] further refine the idea of non-matching layers by using a learned attention distribution to assign appropriate teacher layers to student layers, thereby enhancing cross-layer supervision and subsequently improving student model performance. In addition, Nath et al. [131] introduce Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), which searches for the best teacher layer to supervise each student layer, thus allowing for non-matching layer associations that enhance robustness in neural architectures. Furthermore, Zhao et al. [132] develop Cross-Architecture Knowledge Distillation (CAKD), where non-matching layers are utilized to transfer knowledge from a Transformer-based teacher model to a CNN-based student model, involving the alignment of pixel-wise spatial information across different architectures and expanding the applicability of non-matching layers in cross-architecture scenarios.\n\nLayer Interaction: This method leverages the hierarchical nature of neural networks, where different layers capture different levels of abstraction. By using high-level features from the teacher to guide the student’s learning process, the student can gain a richer understanding of the data. Yao et al. [133] propose Dense Cross-layer Mutual-distillation (DCM), which involves layer interaction by integrating auxiliary classifiers and bidirectional knowledge distillation operations across different layers of the teacher and student models, thereby enhancing knowledge representation and performance. Building on this concept, Su et al. [134] present Deep Cross-layer Collaborative Learning (DCCL), focusing on layer interaction through intermediate cross-layer supervision among peer student models, which integrates features from different layers to enhance representation and learning outcomes. Similarly, Zhu et al. [135] introduce Cross-layer Fusion for Knowledge Distillation (CFKD), which aggregates features from both teacher and student models, allowing for rich layer interactions that further enhance the student model’s learning process. In a related effort, Hu et al. [136] propose an online knowledge distillation method with layer-level feature fusion modules that connect sub-networks, thereby facilitating mutual learning through enhanced layer interaction among student networks. Expanding on the concept, Nguyen et al. [137]"
  },
  {
    "id_": "272f8f75-7d90-4ac5-b241-f1ee5b3c06d4",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "11726c4a-bf4a-4228-8f4d-bd553e985d3c",
    "text": "# Table 2"
  },
  {
    "id_": "f8cc6080-573c-4c75-86aa-5dfef4771948",
    "text": "# Summary of Studies on KD in RS\n\n|Ref.|Model Used|Main Contribution|Database|Task/Appl.|Best Performance Value|Limitation|\n|---|---|---|---|---|---|---|\n|[85]|Small and shallow student models|Introduced a KD framework for scene classification.|AID, NWPU-RESISC, EuroSAT|Scene Classification|Increased accuracy by 1% to 5%|Performance on small and unbalanced datasets|\n|[88]|Lightweight object detector|Developed ARSD to enhance detection capability through feature and regression distillation.|DOTA, DIOR, NWPU VHR|Object Detection|Outperforms SOTA methods|Noise in training due to complicated backgrounds|\n|[96]|Consistency and dependence-guided model (CDKD)|Improved object detection with structured discriminative modules and consistency techniques.|RSOD|Object Detection|92% mean average precision|High model volume and computation in RS images|\n|[87]|Incremental learning model with FPN|Employed feature pyramid and KD for incremental learning in object detection.|Various RS datasets|Object Detection|Comparative performance to SOTA|Challenges with object size diversity and directions|\n|[105]|Dynamic KD (DKD)|Developed a dynamic KD framework to improve model performance on edge devices.|DOTA, NWPU VHR-10|Object Detection|SOTA|Complex model deployment on low-computation devices|\n|[122]|GAGNet with KD|Utilized graph attention and dense fusion for semantic segmentation.|Potsdam, Vaihingen|Semantic Segmentation|Excellent performance on datasets|Resource-intensive model deployment|\n|[123]|RS-SSKD for few-shot classification|Introduced a two-branch network with self-KD for few-shot classification.|NWPU-RESISC45, RSD46-WHU|Scene Classification|Surpasses current SOTA|Requires high model adaptability to new data|\n|[97]|Instance-aware distillation (InsDist)|Combined feature-based and relation-based KD for object detection.|DIOR, DOTA|Object Detection|Noticeable gains over other methods|Integration complexity with existing detectors|\n|[124]|Variational self-distillation network (VSDNet)|Implemented a VKT module for robust and end-to-end optimization.|Multiple RS datasets|Scene Classification|Significant improvement over backbones|Managing uncertainty and perturbation in images|\n|[125]|Collaborative consistent KD (CKD)|Designed a KD method for high classification accuracy on embedded devices.|SIRI-WHU, NWPU-RESISC45|Scene Classification|0.943 and 0.916 average accuracy on devices|Redundancy and parameter management|\n|[89]|DKD Model with DA and SS|Dual KD with dual attention and spatial structure modules|AID, NWPU-45|Scene Classification|Improved accuracy by 7.57% and 7.28%|Model complexity and computational cost|\n|[83]|SRAL Framework|Super-resolution-assisted learning for salient object detection|Multiple datasets|Object Detection in RSIs|Superior to 20+ algorithms|High computational cost of high-resolution processing|\n|[90]|Oriented R-CNN, CF-ORNet|Two-stage fine-grained object recognition with KD|VEDAI, HRSC2016|Object Recognition in HR-RSIs|Competitive performance|Limited by size of geospatial objects|\n|[98]|SSKDNet|Self-supervised KD network for feature learning|Multiple datasets|Scene Classification|Effective feature extraction|Difficulty in training self-supervised networks|\n|[106]|KD-MSANet|Lightweight semantic segmentation with multiscale pooling and attention|Vaihingen, Potsdam|Semantic Segmentation|Accuracy near 99.30% of teacher model|Reduced model size might impact some complex scene parsing|\n|[107]|CDKD Method|Change detection with prototypical contrastive and channel-spatial-normalized distillation|Public CD datasets|Change Detection|Comparable to large models|Requires careful tuning of distillation parameters|\n|[102]|NLD Method|Noisy label distillation for robust training on noisy datasets|UC Merced Land-use, NWPU-RESISC45, AID|Scene Classification|Outperforms fine-tuning methods directly|Performance variability with noise levels|\n|[99]|DSCT Framework|Cross-model KD from CNNs and transformers for semantic segmentation|ISPRS Potsdam, Vaihingen, GID, LoveDA|Semantic Segmentation|Outperforms state-of-the-art KD methods|Complexity of integrating CNNs and transformers|\n|[91]|MS2RGB-KD|MS-to-RGB KD for scene classification using RGB images|EuroSAT|Scene Classification|Effective compared to KD baselines|Dependent on quality of MS teacher model|\n|[100]|MSTNet with MSKA|Dense prediction using multi-level semantic transfer and KD|Vaihingen, Potsdam|Dense Prediction in RSIs|Excellent performance with reduced parameters|Balancing between model complexity and performance|\n\nDevelop CLAFusion, a framework that employs cross-layer alignment for fusing neural networks with different numbers of layers, leveraging layer interaction to improve model accuracy and efficiency. Finally, Zhang et al. [138] propose Patch Aware Knowledge Distillation (PAKD), which emphasizes cross-layer patch alignment and interaction within and across instances, guiding the student’s learning of multi-level information and further reinforcing the importance of layer interaction in knowledge distillation.\n\nHierarchical Guidance: Cross-layer distillation can help in scenarios where the student model is significantly smaller or has a different architecture compared to the teacher. It allows the student to learn abstract representations earlier in its layers. Imagine a teacher model with layers: 𝑇1, 𝑇2, 𝑇3, … , 𝑇𝑛, and a student model with layers: 𝑆1, 𝑆2, 𝑆3, … , 𝑆𝑚. In cross-layer distillation, 𝑇𝑛 (the final layer of the teacher) might guide 𝑆3 (a middle layer of the student), 𝑇3 might guide 𝑆1, and so on, depending on the distillation strategy and the specific architecture of the models. In this regard, Zou et al. [139] develop CoCo DistillNet, which utilizes cross-layer correlations to guide the student model in learning abstract representations from a teacher model in the context of pathological image segmentation, thereby enhancing the student model’s performance in resource-constrained environments. Building on this concept, Zou et al. [140] propose Graph Flow Distillation, a method that transfers cross-layer variations from a large teacher network to a compact student network.\n\nY. Himeur, et al.: Preprint submitted to Elsevier"
  },
  {
    "id_": "f4213296-3db1-4a68-a95e-3e93d6d80430",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "37a9b5e8-d000-42e1-9ed5-c254a290cf4d",
    "text": "# Smart Agriculture"
  },
  {
    "id_": "c0eabe4f-bd19-4c26-8c7b-899675303fc4",
    "text": "# Semantic Image Segmentation"
  },
  {
    "id_": "3f27eaf5-f3f9-48a2-8540-a632301195c3",
    "text": "# Anomaly Detection"
  },
  {
    "id_": "d70ab134-a4cb-4ead-9bf7-94d95d10fc4b",
    "text": "# Super-Resolution"
  },
  {
    "id_": "662b2aca-9ea0-44f1-a4b6-a067b3de1cd4",
    "text": "# Oceanographic Monitoring"
  },
  {
    "id_": "955c5547-a76d-4217-9ba0-c6db8906a284",
    "text": "# Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "372a83ce-3aa1-4d8f-9a2b-1cc7ff32a953",
    "text": "# Domain Adaptation"
  },
  {
    "id_": "8922a215-e027-4cf3-bd3c-d5fe623790d9",
    "text": "# Multi-Sensor Data Fusion"
  },
  {
    "id_": "07b8821e-0823-4b23-be43-c0fbd6079fb0",
    "text": "# Catastrophe Prediction"
  },
  {
    "id_": "4dabf41b-9546-4059-8b6d-27c1ed7de730",
    "text": "# Land Cover Classification"
  },
  {
    "id_": "ffc672d4-b678-4cf6-a0dd-3925c20f72a3",
    "text": "# Urban Planning"
  },
  {
    "id_": "74c275d7-52d5-49d2-8a55-e602a3b1abec",
    "text": "# Figure 1: Applications of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "4d791052-ae93-4785-af2d-06c1eb0c7663",
    "text": "# Figure 6: Applications of KD in RS.\n\nIn medical image segmentation, enabling the student model to learn from both high-level and low-level abstractions of the teacher. In a similar vein, Zhai et al. [141] introduce a method that uses the deepest feature maps from the teacher to guide the shallow layers of the student model, providing hierarchical guidance that effectively balances performance and efficiency. Furthermore, Guo et al. [142] propose Alig-nahead++, an online knowledge distillation framework for GNNs that transfers structure and feature information across layers, facilitating hierarchical guidance and significantly improving performance on edge devices. Together, these studies underscore the importance of hierarchical guidance in enhancing the efficiency and effectiveness of knowledge distillation across various architectures."
  },
  {
    "id_": "5bb5adb1-b715-45e9-b571-1d51b591cb89",
    "text": "# 5. Tasks and Applications of KD in RS"
  },
  {
    "id_": "415584d2-7d0a-47ba-be72-123e995cf811",
    "text": "# 5.1. Tasks\n\nAs previously described, KD has emerged as a transformative approach in RS, enabling the development of more efficient models that handle RS tasks with the same or even better performance across various applications. The main applications of KD is RS are depicted in Fig. 6."
  },
  {
    "id_": "a8181eb7-1194-43fa-9f39-d9c0be193fe4",
    "text": "# 5.1.1. Image/Scene Classification\n\nIn the context of the classification of RS images/scenes, KD can be particularly beneficial. High-resolution satellite or hyperspectral images, which are rich in spatial and spectral information, can be computationally intensive to process online using large models. By employing KD, a large, powerful model (teacher) that has been trained on such images can pass on its learned representations and decision-making capabilities to a smaller, more efficient model (student). This allows the student model to achieve high classification accuracy while significantly reducing computational and storage requirements. Techniques such as spatial feature blurring can be incorporated to enhance the student’s learning by making the training data more challenging, which helps in better generalization and improved classification performance. Various studies have been proposed in the literature to enhance RS image classification, focusing on KD, model efficiency, feature extraction, and handling noisy or incomplete data. Table 3 provides the main features of these works.\n\nBuilding on this, Xu et al. [118] propose a hyperspectral image classification method based on class-incremental learning to learn new land-cover types without forgetting the old ones. This method uses a KD strategy to recall information of old classes and a channel attention mechanism to effectively utilize spatial-spectral information, demonstrating high accuracy on three hyperspectral image datasets. Similarly, Chi et al. [92] introduce a self-supervised learning method with KD for HSI classification, termed SSKD, which generates soft labels for unlabeled samples by considering spatial and spectral distances. This method significantly improves classification accuracy on three public HSI datasets. In addition, Xing et al. [125] address the challenge of using large deep neural networks on embedded devices by proposing a collaborative consistent KD (CKD) method. This method reduces the number of redundant parameters and improves the classification accuracy when tested on the SIRI-WHU and NWPU-RESISC45 datasets. Furthermore, Chen et al. [85] focus on scene classification using a KD framework to improve the performance of smaller and shallower network models. Their method increases the overall accuracy when tested on AID, UCMerced, NWPU-RESISC, and EuroSAT datasets. Along similar lines, Song et al. [143] present ERKT-Net, an efficient and robust knowledge transfer network designed for lightweight yet accurate CNN classifiers, demonstrating superior accuracy and compactness on three RSI datasets. Likewise, Wu et al. [1] propose the TAKD method, which reduces background disturbance and improves the accuracy.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 15 of 50"
  },
  {
    "id_": "6a53d93f-5915-455f-a576-a549d2227e07",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "2f4e3a8a-c774-4a40-a3f0-7de43a462014",
    "text": "# Table 3"
  },
  {
    "id_": "2dcd0d60-84fd-4568-a9dc-64bdf568995d",
    "text": "# Comparison of KD-based RS Image/Scene Classification Studies\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance Value Achieved|Limitation|\n|---|---|---|---|---|---|\n|[118]|learning Class-incremental|PaviaU|KD with channel attention mechanism|99.91% OA|Bias towards new classes|\n|[125]|sistent KDCollaborative con-|SIRI-WHU, NWPU-RESISC45|Multi-branch fused redundant feature mapping|0.943 accuracy (SIRI-WHU)|Parameter redundancy|\n|[92]|Self-supervised learning with KD|Three HSI datasets|Adaptive generation of soft labels|7.09% improvement|Limited labeled samples|\n|[85]|KD framework|AID, UCMerced, NWPU-RESISC, EuroSAT|KD training method for small and shallow models|5% accuracy improvement (UCMerced)|Computationally expensive|\n|[143]|ERKT-Net|Three RSI datasets|Efficient and robust KD network|22.4% OA (NWPU45)|Slight accuracy sacrifice|\n|[98]|SSKDNet|AID|Self-supervised KD network|95.98% accuracy|Complex training|\n|[82]|Adversarial training|HSI datasets|Handling missing modalities with hallucination architecture|98.17% accuracy (Houston)|Modality dependency|\n|[123]|RS-SSKD|NWPU-RESISC45, RSD46-WHU|Few-shot classification with CAMs and KD|86.26% accuracy (NWPU-RESISC45)|Overfitting risk|\n|[102]|NLD|UC Merced, NWPU-RESISC45, AID|Handling noisy labels with end-to-end KD|99.08% accuracy (UC Merced)|Noisy data handling|\n|[73]|GKD framework|Dordogne study site|Handling data misalignment with privileged information|64.27% F-Measure|Incomplete coverage|\n|[75]|TWA distillation|LEVIR, SAR SSDD|Reducing background noise and feature disparities|95.4% AP50 (SAR SSDD)|Background interference|\n|[116]|ing Incremental learning|CIFAR100, RESISC45|Dual KD to prevent catastrophic forgetting|6.9% accuracy improvement|Stability-plasticity dilemma|\n|[2]|DKD with SFB module|Four HSI datasets|Spatial feature blurring for better KD|97.55% OA (Salinas)|Fixed receptive fields|\n\nof student models for RS scene classification on three benchmark datasets. Moreover, Ienco et al. [73] propose a Generalized KD (GKD) framework to manage information misalignment between training and test data, demonstrating improved classification results using radar and optical satellite image time series data. Similarly, Zhang et al. [102] address the challenge of noisy labels in RS image scene classification by proposing a noisy label distillation (NLD) method, which effectively distills knowledge from labels across a range of noise levels, achieving high accuracy on UC Merced Land-use, NWPU-RESISC45, and AID datasets.\n\nIn another approach, Zhao et al. [98] propose a self-supervised KD network (SSKDNet) that uses feature maps as supervision signals and dynamically fuses feature maps to extract discriminating features, showing excellent performance on three datasets. Furthermore, Yang et al. [75] introduce the TWA distillation method for RS object detection, reducing background information and addressing feature disparities, achieving superior performance on the LEVIR and SAR SSDD datasets. Additionally, Pande et al. [82] tackle the problem of missing modalities in RS image classification by proposing an adversarial training-driven hallucination architecture. This method shows that the student model can surpass the teacher model’s performance on HSI datasets. In a similar vein, Yu et al. [116] propose a two-stage training method for incremental learning that includes dual KD to prevent catastrophic forgetting, improving accuracy on CIFAR100 and RESISC45 datasets. Finally, Xie et al. [2] introduce an improved decoupled KD (DKD) strategy for HSI classification using a spatial feature blurring (SFB) module, achieving high overall accuracy on the Salinas dataset.\n\nMoving forward, Zhang et al. [123] present RS-SSKD for few-shot RS scene classification, which uses Class Activation Maps (CAMs) and self-KD to generate powerful representations, achieving high accuracy on NWPU-RESISC45 and RSD46-WHU datasets. As the availability of airborne and satellite imagery increases, the challenge in RS (RS) scene classification has shifted from data scarcity to the lack of ground truth samples. Addressing these challenges, especially in unfamiliar environments with limited training data, few-shot classification offers a promising solution within meta-learning by extracting rich knowledge from minimal data. In [123], the authors introduce RS-SSKD, a method designed for few-shot RS scene classification that focuses on generating robust representations for downstream meta-learners. This approach features a two-branch network that uses three pairs of original-transformed images and incorporates Class Activation Maps (CAMs) to focus on the most relevant category-specific regions, ensuring the creation of discriminative embeddings. Additionally, a self-KD is applied to prevent overfitting and enhance performance (see Fig. 7)."
  },
  {
    "id_": "f6e92383-3aa9-4d12-967d-3c8cf3dfc720",
    "text": "# 5.1.2. Object Detection\n\nIn RS, object detection is crucial for identifying specific features such as buildings, vehicles, and vegetation. KD helps in creating lightweight models that maintain high accuracy, making it feasible to run these models on devices with limited computational power. Several studies focus on the use of KD for improving object detection in RS images, each introducing innovative strategies to address specific challenges. Algorithm 1 outlines a process for KD in RS object detection. It starts by training a teacher model on a dataset, and then defines a student model with a simpler architecture. The teacher model generates soft targets, which are probability distributions over classes, using a softened softmax function. The student model is trained using a combined loss function that includes the cross-entropy loss and the Kullback-Leibler divergence between the teacher’s and student’s outputs. The process iterates over several epochs, optimizing the student model to mimic the teacher while also learning from the original labels. Finally, the trained student model is deployed. The main works on the use of KD in the object detection task in RS images and their main features are summarized in Table 4.\n\nFor instance, Yang et al. [88] propose an adaptive reinforcement supervision distillation (ARSD) framework to enhance lightweight object detectors. This method focuses"
  },
  {
    "id_": "f99c84e2-bd69-46b6-a6bc-9a542c6d115f",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "17c551e3-f9e3-4488-9a29-f93d0730b733",
    "text": "# SSKD embedding module\n\n|M-way classification|Knowledge distill module|ŷ|\n|---|---|---|\n|Base dataset|task 𝒯i|fϕ|\n|support set|fϕ|μ|\n|query set|fϕ|fϕ (x*)|\n|Category representation|Category representation|Similarity metric|\n|sampled tasks|sampled tasks|ŷ*|\n\nFigure 7: The overall framework includes the SSKD module for embedding learning and the meta-learning module based on ProtoNets. The parameter 𝛾 is used to adjust cosine similarity in the meta-learning process. It aims at learning a powerful embedding, without any additional annotation effort that offers more discriminative representations to the downstream meta-learner. The meta-learning module is based on ProtoNets with an additional parameter γ to scale cosine similarity.\n\nOther studies address different aspects of RS. Zhang et al. [53] combine detection and tracking in a joint framework for small objects in complex backgrounds. Zhang et al. [105] introduce a dynamic KD (DKD) framework, leveraging dynamic global distillation and instance selection distillation to enhance object detection in cluttered scenes. Another study by Zhang et al. [144] presents Orientation Distillation (OD) to address issues with boundary discontinuity and spatial feature ossification for detecting arbitrary-oriented objects in RS images. The authors further propose an adaptive composite feature generation (ACFG) strategy to improve feature mapping and handling of foreground and background loss in object detection [145].\n\nFeng et al. [146] introduce an Instance-aware Distillation approach for Class-incremental Object Detection (ID-COD), which helps in preserving old class knowledge while learning new classes, thus mitigating catastrophic forgetting. Chen et al. [3] propose Discretized Position KD (DPKD), which focuses on transferring high-quality bounding box position and pose information to improve object detection performance. Pang et al. [4] present a pyramid KD (PKD) framework to handle the limitations of model compression, utilizing a hybrid online–offline smooth distillation strategy to enhance recognition accuracy while avoiding knowledge explosion and offset.\n\nDu et al. [55] add a detection head specifically for small targets in the YOLOv5 model, proposing a network KD framework for improved small-scale target detection in RS images. Gao et al. [147] design a feature super-resolution fusion framework using cross-scale distillation to improve the detection accuracy of small objects by enhancing feature expression capability. Yang et al. [148] propose a weakly supervised object detection method using self-attention distillation and instance-aware mining to handle varying scales and dense object proximity in RS images.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 17 of 50"
  },
  {
    "id_": "9bb5a81f-f41f-4d37-96c2-0b1c39b332d5",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "e9195b8e-1538-4df2-b9a0-306b7a6ac43d",
    "text": "# Algorithm 1: KD for RS Object Detection\n\nInput: Training data 𝐷, Teacher model 𝑇, Student model architecture 𝑆, Temperature 𝑇𝑒𝑚𝑝, Loss weights 𝛼, 𝛽, Number of epochs 𝑁\n\nOutput: Trained Student Model 𝑆\n\n1. Train the Teacher Model\n\n𝑇 ← TrainTeacherModel(𝐷, 𝑇)\n2. Define the Student Model\n\n𝑆 ← DefineStudentModel(𝑆)\n3. Compute the Soft Targets from the Teacher Model\n\nfor each batch (𝑥, 𝑦) ∈ 𝐷 do\n\n&nbsp;&nbsp;&nbsp;𝑧𝑇 ← 𝑇 (𝑥)\n\n&nbsp;&nbsp;&nbsp;𝑝𝑇 ← Softmax(𝑧𝑇 ∕𝑇𝑒𝑚𝑝)\n\nend\n4. Define the Loss Functions\n\n𝐶𝐸 ← CrossEntropy(𝑆(𝑥), 𝑦)\n\n𝐾𝐷 ← KLDiv(LogSoftmax(𝑆(𝑥)∕𝑇𝑒𝑚𝑝), 𝑝𝑇 ) × 𝑇 𝑒𝑚𝑝2\n\n𝑡𝑜𝑡𝑎𝑙 ← 𝛼𝐶𝐸 + 𝛽𝐾𝐷\n5. Train the Student Model\n\nfor epoch = 1 to 𝑁 do\n\n&nbsp;&nbsp;&nbsp;for each batch (𝑥, 𝑦) ∈ 𝐷 do\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;𝑝𝑇 ← ComputeSoftTargets(𝑇 , 𝑥, 𝑇𝑒𝑚𝑝)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;𝑧𝑆 ← 𝑆(𝑥)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ← ComputeLoss(𝑧𝑆 , 𝑦, 𝑝𝑇 , 𝛼, 𝛽, 𝑇𝑒𝑚𝑝)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update 𝑆 by minimizing \n\n&nbsp;&nbsp;&nbsp;end\n\nend\n6. Deploy the Student Model\n\n𝑆 ← Trained Student Model\n\nOn the other hand, traditional KD-based object detection methods have limitations, such as ignoring crucial background information and focusing solely on global context. To overcome these issues, Attention-based Feature Distillation (AFD) is proposed in [159], which distills both local and global information. AFD enhances local distillation with a multi-instance attention mechanism and reconstructs pixel relationships, resulting in state-of-the-art performance in object detection while remaining efficient. Fig. 8 illustrates the architecture of the proposed Attention-based Feature Distillation (AFD) method. This framework improves upon traditional KD-based object detection by incorporating both local and global information from the teacher network. The multi-instance attention mechanism within AFD allows the model to distinguish between background and foreground elements effectively. Additionally, the method reconstructs pixel relationships, ensuring that both local details and broader context are accurately transferred from the teacher to the student detector, resulting in enhanced detection performance."
  },
  {
    "id_": "abe83b8a-48c0-4f2b-9777-68e874985e81",
    "text": "# 5.1.3. Semantic Segmentation\n\nKD is beneficial for semantic segmentation in RS applications, which involves classifying each pixel in an image into predefined categories. The teacher model is first trained on the segmentation task using high-resolution RS data [160]. Due to its complexity and larger capacity, it can learn intricate patterns and detailed features from the data. Once trained, the teacher model’s predictions, along with its internal representations, are used to guide the training of the student model. The student model, being smaller and more efficient, aims to mimic the performance of the teacher model while maintaining lower computational costs and faster inference times [161]. Moreover, KD is particularly advantageous because it allows for the deployment of effective semantic segmentation models on edge devices or in scenarios with limited computational resources [162]. By leveraging the distilled knowledge from the teacher model, the student model can achieve high segmentation accuracy despite its reduced size. This is crucial for applications such as real-time environmental monitoring, disaster response, and agricultural analysis, where timely and accurate segmentation of satellite or aerial imagery is needed. The distillation process also helps the student model generalize better to new and unseen data, enhancing its robustness and reliability in diverse RS tasks [163].\n\nThe studies on semantic segmentation in RS show significant advancements but also face several limitations. For instance, Gao et al. [164] introduced the FoMA framework, which significantly improves segmentation performance by leveraging foundation models, but it struggles with data scarcity in novel classes and balancing segmentation performance across classes. Similarly, Zhou et al. [122] proposed a lightweight student network (GAGNet-S*) with KD that achieves excellent segmentation performance but faces challenges related to scalability and complexity in deployment on resource-limited equipment. Dong et al. [99] addressed the limitations of CNNs and transformers by proposing the DSCT framework, which enhances segmentation performance through cross-model KD. However, this approach requires high computational complexity and massive data resources.\n\nStudies focusing on KD methods, such as the MGSAD by Zhang et al. [165], and MTKD by Li et al. [166] with MTKD, contribute innovative techniques but encounter challenges such as the need for extensive computation and handling of domain shifts. Liu et al. [167] proposed a three-stage UDA method that shows better performance but relies heavily on large-scale annotated data and struggles with domain shift handling. Similarly, Shi et al. [168] introduced DSANet, which effectively handles spatial and semantic feature enhancement but reduces the model’s characterization ability for these features.\n\nIncremental learning and domain adaptation are other areas where significant contributions have been made but also face limitations. Rong et al. [169] proposed a generalized framework for CSS but struggled with the challenge of old classes collapsing into the background. Rui et al. [170] and Le et al. [171] focused on incremental learning methods but faced high computational costs and the complexity of adapting to incremental domains and partial multi-task learning, respectively. Shan et al. [172, 173] developed class-incremental segmentation methods that address catastrophic forgetting but require balancing old and new class learning and managing feature generation complexity. Li [174] proposed DSSN with weakly-supervised constraints to handle cross-domain segmentation, but the method heavily depends on labeled data and struggles with geographic variation.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 18 of 50"
  },
  {
    "id_": "e5f7b159-e49a-40e4-a2c7-5995d6aa99a3",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "854dd2f1-de03-4d93-a4f2-d04df6c70e6d",
    "text": "# Table 4"
  },
  {
    "id_": "29ce3e55-0792-46cf-b8e4-e4a983b99491",
    "text": "# Comparison of Studies on KD-based Object Detection in RS Imagery.\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance|Limitation|\n|---|---|---|---|---|---|\n|[88]|ARSD framework|DOTA, DIOR, NWPU VHR-10|Adaptive reinforcement supervision distillation for lightweight object detection|Outperforms SOTA methods|High complexity due to adaptive modules|\n|[105]|DKD framework|DOTA, NWPU VHR-10|Dynamic KD for multi-scale feature imitation|Suitable for various detectors|Potential overfitting to specific datasets|\n|[144]|Orientation Distillation (OD)|Multiple datasets|Anti-ambiguous location prediction and feature calibration|Improved performance on non-axially arranged objects|Limited accuracy in complex scenes|\n|[145]|ACFG strategy|DIOR, DOTA|Adaptive composite feature generation for KD|Better performance than SOTA KD algorithms|Complexity in composite mask generation|\n|[146]|IDCOD|DOTA, DIOR, RT-DOD, PASCAL VOC|Instance-aware distillation for class-incremental detection|mAP@0.5 of 74.0% on DIOR|Challenge in handling new classes post-deployment|\n|[3]|DPKD|DOTA, HRSID|Discretized position KD for object detection|mAP of 79.82% on DOTA|Overlooks certain localization knowledge|\n|[4]|PKD framework|Aircraft, FGSC-23|Pyramid KD to avoid knowledge explosion and offset|Effective with ResNet and VGG networks|Complexity in finding optimal configuration|\n|[55]|Enhanced YOLOv5|NWPU VHR-10|KD framework for small-scale target detection|Detection accuracy of 43.9%|High computational cost|\n|[147]|SSRFPN with CSD|NWPU VHR-10, DIOR|Feature super-resolution fusion for small object detection|AP0.5 of 95.0% on NWPU VHR-10|Difficulty in feature extraction for very small objects|\n|[148]|WSOD with SAD and IAM|NWPU VHR-10, DIOR|Weakly supervised learning for object detection|Accurate bounding boxes|Struggles with varying scales and dense objects|\n|[53]|OKD-JDT|JiLin-1|Joint detection and tracking framework|State-of-the-art performance|Limited to certain types of satellite videos|\n|[149]|MGFAFNET|SyluDrone|Efficient detection method for UAV platforms|AP of 52.7%, AP50 of 93.6% on SyluDrone|Balancing detection speed and accuracy|\n|[150]|DC-KD|xView|Distillation scheme for object detection in satellite images|3.88% mAP50 improvement on xView|Data distribution differences|\n|[151]|HMKD-Net|Multiple datasets|Hybrid-model KD with CNN-ViT ensemble|Max accuracy improvement of 22.8%|Handling variances during KD|\n|[152]|Visual knowledge-oriented WSOD|NWPU VHR-10, DIOR|Leveraging visual cues as pseudo labels|mAP of 84.25% on NWPU VHR-10|Handling noise in object proposals|\n|[153]|WSA-GAN, BGNet|Various RS datasets|Multitask learning for image translation and saliency detection|Outperforms other approaches|Complexity in multimodal context learning|\n|[154]|TDKD-Net|Various RS datasets|Tensor decomposition and KD for UAV detection|High generalization and robustness|Handling imbalanced issues|\n|[155]|Coarse-to-fine network|VisDrone, UAVDT|Density-aware scale adaptation for small object detection|Superior detection in UAV images|Issues with scale variation|\n|[156]|Self-distillation YOLO|KITTI|Multi-scale self-distillation for object detection|2.8% accuracy improvement|Inefficiencies in knowledge transfer|\n|[157]|DTCNet|AID|Distillation Transform-CNN for super-resolution|PSNR of 28.73 dB, SSIM of 0.7904|High model complexity|\n|[158]|TGN with KMDN and CDTG|DIOR, FGSC-23, DOTA|Text-guided tail-class generation for long-tailed distribution|Superior performance on tail classes|Data distribution imbalance|\n\nLastly, Guo et al. [175] and Cao et al. [176] proposed methods to balance effectiveness and compactness in segmentation models, but they face high computational demands and challenges in handling noise and redundant features. Zhou et al. [119] introduced GSGNet with high inference speed but had to balance this with contextual reasoning capabilities. Bai et al. [177] and Wang et al. [178] focused on domain adaptation, but they faced difficulties in aligning high-dimensional image representations and managing intermediate domain learning. Michieli et al. [179] addressed incremental learning with various KD techniques but struggled with catastrophic forgetting and internal feature representation complexity. Lastly, Pena et al. [180] introduced DeepAqua for water detection, which improves segmentation accuracy but lacks specific details on datasets and segmentation scenarios. Table 5 provides a summary of the works that use KD for semantic segmentation of RS images.\n\nBesides, [164] produces a Foundation Model Assisted (FoMA) for Generalized Few-Shot Semantic Segmentation (GFSS) in RS images, aimed at improving segmentation performance under data scarcity conditions. FoMA leverages foundation models through three strategies: Support Label Enrichment (SLE) to enhance support labels, Distillation of General Knowledge (DGK) to transfer generalizable knowledge, and Voting Fusion of Experts (VFE) to combine zero-shot and few-shot predictions. The method demonstrates state-of-the-art performance on the OpenEarthMap few-shot challenge dataset. Fig. 9 illustrates the architecture of the FoMA framework, which effectively integrates a vision-language foundation model’s general knowledge into the GFSS task for RS images."
  },
  {
    "id_": "f416d775-4e0d-4ae6-81a2-76c36798c7b9",
    "text": "# 5.2. Specific Applications"
  },
  {
    "id_": "d51d50b5-0c66-4ed3-9eef-e5168cbdcb8c",
    "text": "# 5.2.1. Land Cover Classification\n\nKD improves the classification of land cover types by refining the feature extraction capabilities of student models. This leads to better segmentation and classification of different land cover types, essential for environmental monitoring and urban planning. Several studies have proposed innovative methods to improve land cover classification and other RS tasks using KD and multimodal data fusion. For example, Xu et al. [117] developed a two-branch patch-based CNN with an encoder-decoder (ED) module to fuse multimodal RS (RS) data. They introduced a KD in model (DIM) module for better multimodal data fusion and a cross-model (DCM) module to enhance single-modal classification using multimodal knowledge. Their approach demonstrated superior performance on hyperspectral (HS) and light detection and ranging (LiDAR) data as well as HS and synthetic aperture radar (SAR) data. Fig. 10 depicts the approach proposed in [117]. Wang et al. [182] proposed the cross-modal graph knowledge representation and distillation learning (CGKR-DL) framework, which combines CNN and graph convolutional network (GCN) to enhance land cover classification. Their method addresses the limitations of\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 19 of 50"
  },
  {
    "id_": "1bcaa709-3b8b-42fb-bfb9-1ac274d3ff39",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "70cedca1-0459-4b7a-9c55-c88db3ac29f8",
    "text": "# IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 61, 2023"
  },
  {
    "id_": "94fade5a-3e60-4ae1-9323-a6bb5d6a2140",
    "text": "# Fig. 2. Architecture of our KD method.\n\nThe enhancement of AFD is based on three points. Our new KD approach distills both local and global information from the teacher network. For local distillation, a multi-instance attention mechanism is proposed to identify the background from the foreground. Attention Figure 8: The KD architecture enhances AFD through three key advancements [159]. Firstly, the method extracts both local and global information from the teacher network. For local distillation, a multi-instance attention mechanism is introduced to effectively distinguish foreground elements from the background. Secondly, the approach reconstructs the relationships between different pixels, facilitating a more comprehensive transfer of knowledge from the teacher to the student detector through both traditional CNN-based cross-modal distillation methods and local and global distillation strategies."
  },
  {
    "id_": "30554700-c8dc-4d70-b8f3-24fa8b3fd221",
    "text": "# Equations\n\nWe can write the local channel and spatial masks (Lch and Lsp) as follows:\n\nLch, p = Mcha( f pT) + Mcha( f pS)\n\nLch = ⊗(Lch,1, Lch,2, . . . , Lch,P)\n\nLsp, p = Mspa( f pT) + Mspa( f pS)\n\nLsp = ⊗(Lsp,1, Lsp,2, . . . , Lsp,P)"
  },
  {
    "id_": "911845de-e6ef-4379-a7c0-6838a0209be5",
    "text": "# The Generalized KD (GKD) framework\n\nhas been introduced by Ienco et al. [73] to handle data misalignment between training and test phases. Their method, applied to radar and optical satellite image time series data, improved land use land cover mapping, especially for agricultural classes. A multimodal online KD (MMOKD) framework significantly improves performance on various multimodal RS datasets. Kanagavelu et al. [185] and Gbodjo et al. [186] explored federated learning and multisensor data integration, respectively, to enhance land cover mapping and monitoring. The former work federated teacher-student structure for better generalizability and performance in land cover classification. The latter developed a self-distillation strategy within a CNN framework to combine multitemporal SAR and optical data for improved land cover classification."
  },
  {
    "id_": "07cdde89-c6cd-4fce-9324-41130481a55b",
    "text": "# Table 6\n\nThe works that use KD for the classification of land cover and their main characteristics are summarized in Table 6."
  },
  {
    "id_": "12f9d063-7863-413c-bf0b-04ddb18475ed",
    "text": "# Fig. 3.\n\nActivations for the input image before and after normalization. This process fills the gap between the patterns of the teacher detector and the student detector, providing a more effective and smoother transfer of knowledge.\n\nTo advance this field, a two-branch, patch-based CNN with an encoder-decoder (ED) module for effective multimodal data fusion is proposed in [117]. Typically, a KD in model (DIM) module to guide cth channel in a batch of FPN outputs; therefore, we can obtain the normalized values from the teacher T and the student S."
  },
  {
    "id_": "f990aaf9-6603-4138-a9b6-f7dd03b03102",
    "text": "# Normalization Process\n\nIt is also important that the normalization follows the convolution property, to ensure that features are normalized uniformly at different regions of the feature map. Let V represent the whole set of feature map values that include the components of mini-batch and its spatial locations. Therefore, for a u-size mini-batch and h × w-size feature maps, we take the functional."
  },
  {
    "id_": "1a22ee46-1be5-4d8e-98d1-593740c99b01",
    "text": "# Global Channel and Spatial Masks\n\nThe global channel and spatial masks (Gch and Gsp) can be written as follows:\n\nGch = Mcha(F T ) + Mcha(F S)"
  },
  {
    "id_": "ba0170a3-df24-48fc-9338-405fb8ecb9fc",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "d7751479-17cd-4e28-ae3f-947a6d7abf13",
    "text": "# Table 5"
  },
  {
    "id_": "6594f693-cf21-4d41-bf00-79db76644e85",
    "text": "# Comparison of Various Studies on Semantic Segmentation in RS\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance Value Achieved|Limitation(s)|\n|---|---|---|---|---|---|\n|[164]|FoMA Framework|OpenEarthMap|Introduces GFSS with three strategies: SLE, DGK, and VFE for improved segmentation|Improvement of 28.94% in segmentation performance, with 31.79% for novel classes and 24.64% for base classes|Data scarcity in novel classes and complex balancing in segmentation performance|\n|[122]|GAGNet-S* (with KD)|Potsdam, Vaihingen|Proposes a lightweight student network framework with KD|Achieved excellent segmentation performance on Potsdam and Vaihingen datasets|Scalability and complexity in deployment on resource-limited equipment|\n|[99]|DSCT Framework|ISPRS Potsdam, Vaihingen, GID, LoveDA|Cross-model KD using CNNs and transformers|Outperforms state-of-the-art KD methods on four datasets|High computational complexity and massive data resource requirements|\n|[165]|MGSAD|Not specified|Proposes a multi-granularity semantic alignment distillation method for semantic segmentation|Not specified|Details on datasets and specific performance metrics are not provided|\n|[166]|MTKD|Not specified|Multi-task KD for weather-degraded image segmentation|Achieves 0.038 s in semantic segmentation for a 2048 × 1024 image|Specific performance values not provided, computation-intensive|\n|[167]|Covariance-based Channel Attention Module|ISPRS 2-D Semantic Labeling, Urban Drone Dataset (UDD)|Proposes three-stage UDA method with KD for RS images|Shows better performance compared with state-of-the-art methods|Domain shift handling and reliance on large-scale annotated data|\n|[168]|DSANet|ISPRS Potsdam, Vaihingen|Effective deep supervision-based attention network for RSIs|79.19% mIoU on Potsdam, 72.26% mIoU on Vaihingen with 470.07 FPS on 512 × 512 images|Reduces model characterization ability for spatial and semantic features|\n|[181]|Cross-modal KD|Not specified|Uses optical images to train a student model for SAR images through cross-modal KD|Increase of 5-20% IoU score compared to training from scratch|Small training datasets and complexity in cross-modal learning|\n|[169]|Generalized Framework for CSS|iSAID, GCSS|Proposes historical information-guided modules for CSS in RS images|Outperforms state-of-the-art methods in most incremental settings|Challenge of old classes collapsing into the background|\n|[170]|Domain-Incremental Learning|LoveDA-rural|Proposes domain-incremental learning for multi-source RS data|Achieves mIoU of 0.6233 on LoveDA-rural at step 5|High computational cost and complexity in incremental domain learning|\n|[171]|Partial Multi-Task Learning with KD|ISPRS 2D Semantic Labeling Contest|Enhances partial multi-task learning performance using KD|mIoU of 68.97% on Vaihingen dataset|Lack of all-task annotations and reliance on soft labels|\n|[172]|DFD and LM Modules|Aerial images dataset|Proposes class-incremental segmentation method without old data storage|6.2% and 15% mIoU gains from DFD and LM modules respectively|Catastrophic forgetting and balancing old and new class learning|\n|[173]|PFG and TKD Modules|Not specified|Effective class-incremental segmentation framework without storing old data|More than 4.5% gains compared with state-of-the-art methods|Limited detail on dataset performance and complexity in feature generation|\n|[174]|DSSN with Weakly-Supervised Constraints|Not specified|Proposes DSSN for cross-domain RS image segmentation|Mean F1Score: 60.76%, Mean IoU: 44.53%|High dependency on labeled data and difficulty in geographic variation handling|\n|[175]|CLNet-T and CLNet-S (with KD)|MFNet, PST900|Proposes a balance between effectiveness and compactness using KD|MFNet: mAcc 76.6%, mIoU 58.2%; PST900: mAcc 95.59%, mIoU 80.77%|High computational demands and complexity in terminal device deployment|\n|[176]|C3Net with Multi-Level KD|ISPRS Vaihingen|Proposes efficient C3Net for multi-modal data semantic segmentation|Overall Accuracy: 91.3%, High mean F1 score for car class|Noise and redundant feature handling and high running time|\n|[119]|GSGNet with KD|Vaihingen, Potsdam|Proposes GSGNet for ORSI scenario analysis with high inference speed|Outperforms most advanced methods with 19.61 M parameters|Balancing high inference speed and contextual reasoning capability|\n|[177]|Contrastive and Adversarial Learning|Not specified|Proposes a model for domain adaptation in representation space and spatial layout|Not specified|Specific performance values and dataset details not provided|\n|[178]|TDARS|Three domain adaptation datasets|Proposes transitive domain adaptation for RS images|Effectively handles domain shift problem compared to other methods|High complexity in intermediate domain learning and transfer|\n|[179]|Various KD Techniques|Pascal VOC2012, MSRC-v2|Proposes incremental learning for semantic segmentation with KD|Highest Accuracy: 97.5% (Abisoye et al. 2024), Lowest Error: 0.032 MAE (De 2024)|Catastrophic forgetting and complexity in internal feature representation handling|\n|[180]|DeepAqua|Not specified|Proposes an unsupervised method for water detection in RS|Improves accuracy by 3%, IoU by 11%, F1-score by 6%|Specific details on datasets and segmentation scenarios not provided|"
  },
  {
    "id_": "72784aae-23cc-4768-a17c-25935f17e22c",
    "text": "# 5.2.2. Precision Agriculture\n\nKD is crucial in smart agriculture as it allows for the development of lightweight models that maintain high accuracy while being deployable on resource-constrained edge devices, such as drones or sensors. This is particularly important for precision agriculture tasks like early weed detection and crop monitoring, where efficient and accurate models are needed for real-time decision-making. By transferring knowledge from larger, more complex models to smaller ones, KD helps optimize these tasks, enhancing agricultural.\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 21 of 50"
  },
  {
    "id_": "767738a4-b024-45e6-9dbd-0a3cf35183c9",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "d677d4c6-6cf1-47f4-a70c-5e0f87477a88",
    "text": "# Tunable\n\nA photo of a [cls] \"A photo of a [cls] \""
  },
  {
    "id_": "9787aba0-1b8f-4f27-abcb-abbfef22f5c3",
    "text": "# Vision-Language Foundation Model (VLFM)\n\n|RSI|DGK|SLE|\n|---|---|---|\n|Prediction|Voting|Voting Fusion|\n|Backbone|Pixel|Classifier Base|\n|Encoder|Decoder|Classifier Novel|\n|Update|Self-supervised Optimization|Shared GFSS Learner|"
  },
  {
    "id_": "42a2f613-dece-4588-ac11-65350ecfd380",
    "text": "# Text Prompt\n\nFoundation model as labeler (a) The FoMA GFSS framework Enriched Label Tert Foundation model as teacher\n\nVLFM"
  },
  {
    "id_": "042bbf3d-b294-475e-8b89-103abe29e20b",
    "text": "# Support RSI\n\nVLFM"
  },
  {
    "id_": "a26213a2-fa3a-4945-9cb4-c9ccfcc44ee9",
    "text": "# Query RSI\n\nVLFM"
  },
  {
    "id_": "efedacd8-87cc-45c9-9074-46635ac6296b",
    "text": "# Enc Dec\n\nLSLE"
  },
  {
    "id_": "8d03182a-39ff-44c4-97dc-0d5d54001719",
    "text": "# Original Label\n\nCLS"
  },
  {
    "id_": "aca44b73-d06f-4e54-aa50-221a9153b5a0",
    "text": "# (b) SLE"
  },
  {
    "id_": "3678e9cb-4045-4770-8f17-45fb63fad67b",
    "text": "# (c) DGK\n\nFigure 2. The overall architecture of the proposed Method. This approach introduces the general knowledge of the vision-language foundation model learned from natural images into the remote sensing image GFSS task through two modules: SLE integrates the foundation model’s results on support images as pseudo-labels into the GFSS learner’s training process. Concurrently, DGK distills the exceptional performance achieved by the foundation model on novel classes from query images into the GFSS learner. Furthermore, a voting fusion strategy is used to effectively merge the results of the foundation model and the GFSS learner across all classes, ensuring more accurate prediction results for the model.\n\nThese efforts focus on enhancing model efficiency and accuracy in tasks such as crop monitoring, weed detection, and resource management. For instance, Liangde et al. [199] develop a model distillation approach to enhance agricultural named entity recognition, leveraging a BERT-based model enhanced by BiLSTM and CRF for precise entity detection from a constructed agriculture knowledge graph. Ghofrani and Mahdian Toroghi [200] focus on plant disease detection, using a KD approach to enable smaller CNN architectures, like MobileNet, to achieve near high-end model accuracy on the Plantvillage dataset. On the same line, Hu et al. [201] and Dong et al. [202] address crop disease detection. The former approach optimizes YOLOv5s for maize disease detection, while the latter uses ECA-KDNet for efficient apple disease diagnosis on mobile devices. Finally, Huang et al. [203] develop multistage KD to create lightweight models for diagnosing multiple crop diseases effectively.\n\nIn the task of image segmentation, Angarano et al. [204] introduce a method for robust crop segmentation using KD, aimed at improving the generalization across different environmental conditions for robotic field management. Similarly, Li et al. [205] employ KD for efficient panoptic segmentation, creating lightweight networks capable of detailed scene understanding at high speeds, whereas Jung et al. [206] improve plant leaf segmentation using KD to maintain high-quality instance segmentation. The work of Pagé-Fortin [207] investigates class-incremental learning methods to address the challenge of learning new plant species and diseases incrementally, focusing on mitigating catastrophic forgetting."
  },
  {
    "id_": "90a6e06f-0b85-48a7-ad5a-44c26623ca92",
    "text": "# 3.2.1 Support Label Enrichment\n\nGiven a support image I, in our challenge setting, only one novel class is labeled. It indicates that even though some other novel classes appear in the image I, the pixels are also labeled as background, making it semantic ambiguous during the training of the novel classifier. Therefore, we aim to leverage off-the-shelf foundation models to enrich the information of current limited support labels. We name it Support Label Enrichment (SLE).\n\nTo obtain accurate pseudo labels, we utilize off-the-shelf lightweight semantic segmentation model for identifying grape picking points, enhancing the picking efficiency in vineyard environments, whereas Hollard and Mohimont [209] apply KD to enhance grapevine detection for early yield prediction, focusing on lightweight model deployment for embedded devices. As far as it concerns disease detection, Musa et al. [210] propose a low-power DL model for detecting plant diseases in hydroponic systems, aiming at efficiency and reduced resource consumption and Zhang and Wang [211] improve plant leaf disease recognition using a.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 22 of 50"
  },
  {
    "id_": "71836d82-1621-4045-82b0-526749f02c31",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "9b969460-d40b-4681-b6b0-e06a19f15d79",
    "text": "# Table 6"
  },
  {
    "id_": "db591678-2ad2-4197-902f-27bfa94ec462",
    "text": "# Comparison of Studies on KD-based Land Cover Classification\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|\n|---|---|---|---|\n|[117]|Two-branch patch-based CNN with ED and DIM modules|Hyperspectral (HS) and LiDAR data (Houston2013)|Developed a KD in model (DIM) and cross-model (DCM) module for better LC classification|\n|[182]|CGKR-DL framework with CNN and GCN|HS-LiDAR, SAR, HS-SAR-DSM datasets|Proposed cross-modal graph knowledge representation and distillation learning|\n|[73]|Generalized KD (GKD) framework|Radar (Sentinel-1) and optical (Sentinel-2) SITS|Managed information misalignment between training and test data|\n|[11]|MMOKD framework|Optical and SAR images|Developed multimodal online KD framework for land use/cover classification|\n|[185]|Federated UNet model with KD|Satellite and street view images|Improved efficiency and privacy of real-time climate tracking|\n|[183]|DH-ADNet with MSIS|Coregistered optical and SAR datasets|Introduced dynamic-hierarchical attention distillation for land cover classification|\n|[54]|KD-MSI with CAMs|WHU-CD, DSIFN-CD, LEVIR-CD datasets|Weakly supervised change detection using KD|\n|[184]|Transfer learning framework with CMD and high-temperature softmax|Various RS datasets|Improved land cover classification using teacher-student structure|\n|[187]|DAGDNet with IG-FGM and MS-ADL|Coregistered optical and SAR datasets|Efficient dense adaptive grouping distillation network for MLCC|\n|[186]|Patch-based multibranch CNN|Multitemporal SAR/optical data|Integrated multisensor RS data using self-distillation strategy|\n|[188]|Hallucination network with KD|PAN-MS image pairs, hyperspectral dataset|Provided robust solution for missing modalities using hallucination module|\n|[189]|CloudSeg framework with multi-task learning|M3M-CR, WHU-OPT-SAR datasets|Addressed semantic segmentation under cloud cover using KD|\n|[190]|Segment Anything (SAM) model|Planetary images|Rapid annotation for geological mapping using KD|\n|[191]|Distill and refine strategy with CNN|Sentinel-1 data|Addressed spatial transfer challenge for mapping irrigated areas|\n|[192]|Lightweight model with KD|UC Merced Land Use dataset|High accuracy and efficiency for RS image retrieval|\n|[193]|MRF-NAS with self-training UDA|OpenEarthMap, FLAIR #1 datasets|Lightweight neural networks for UDA in RS|\n|[194]|Cross-modal distillation framework|Sen1Floods11 dataset|Improved flood detection with cross-modal distillation|\n|[195]|GCPNet with GCN and ASPM|Various satellite datasets|Enhanced pansharpening using GCN and KD|\n|[196]|Domain knowledge-guided self-supervised learning|Onera Satellite Change Detection dataset|Improved unsupervised change detection using domain knowledge|\n|[197]|VGG13 (teacher), ResNet8 (student)|SMAP satellite data|Improved soil moisture prediction using KD|\n|[198]|LSAW with adaptive weights|CCF, Potsdam, Vaihingen datasets|Addressed catastrophic forgetting in incremental learning|"
  },
  {
    "id_": "371d16d8-7e97-4908-bafb-a65bdf8a6490",
    "text": "# Best Performance Value Achieved\n\n|Best Performance Value|Limitation|\n|---|---|\n|Improved LC classification performance on two multimodal RS datasets|The study mainly focuses on LC classification; does not cover other RS applications|\n|Significant improvement in land cover classification accuracy|Focuses on classification; not on other types of RS tasks|\n|Accuracy: 65.01%, F-Measure: 64.27%, Kappa: 0.5775|Limited to cases where radar data is always available|\n|Outperformed other networks in both full- and missing-modality scenarios|Large semantic gap between modalities poses a challenge|\n|Accuracy above 95%|Focus on semantic segmentation, not other RS tasks|\n|State-of-the-art results in the privileged information scenario|Limited to privileged information scenarios|\n|F1-score: 0.854 on WHU-CD|Focuses on change detection; not applicable to other RS tasks|\n|Average increase in mIoU: 9.9%, 2.1%, 4.3%|Requires large datasets for teacher model training|\n|Superior performances on representative datasets|Limited to scenarios with privileged modality|\n|Accuracy: 94% (Reunion island), 88% (Dordogne)|Requires sparsely annotated ground-truth data|\n|Overall accuracy: 97.01%|Focused on scene recognition and image classification|\n|mIoU improvement: 3.16% (M3M-CR), 5.56% (WHU-OPT-SAR)|Focuses on cloudy conditions; not applicable to cloud-free scenarios|\n|Comparable to state-of-the-art on mapping planetary skylights|Limited to geological mapping tasks|\n|Best performance in spatial transferability|Focused on spatial transfer; not on other RS tasks|\n|mAP: 0.9680 with 3.8M parameters|Limited to image retrieval tasks|\n|mIoU: 59.38% (OpenEarthMap), 51.19% (FLAIR #1)|Focus on UDA; not on other RS tasks|\n|IoU improvement: 6.53% on test split|Limited to flood detection; not other RS tasks|\n|Outperformed state-of-the-art visually and quantitatively|Limited to pansharpening tasks|\n|Kap: 53.34%, F1: 55.69%|Focused on change detection; not other RS tasks|\n|High prediction accuracy with efficient student model|Focused on soil moisture prediction; not other RS tasks|\n|Best datasets results on three novel data augmentation-based KD framework, enhancing recognition accuracy in natural environments.|Focus on incremental learning; not other RS tasks|\n\nMore advanced and complex tasks have also been addressed using KD. In the aquaculture domain, Yin et al. [212] propose a novel fish individual recognition method using KD within a vision transformer framework, improving accuracy. In the same application domain, Li et al. [213] focus on underwater fish species classification using a novel two-tier KD method to enhance model accuracy and reduce computational demands. Back to the plants and trees images, Yang et al. [214] developed a fast pest detection algorithm using lightweight feature extraction and KD to enhance performance on edge devices and Wu et al. [215] presented Deep BarkID, a lightweight CNN for tree species identification from bark images, tailored for use in forest environments with limited computing resources. Finally, Yamamoto [216] utilized CNNs to distill crop models to accelerate understanding of plant physiology, applying DL to evaluate environmental impact on grain yield.\n\nResearchers have also contributed to the development and deployment of lightweight student models on low capacity devices on the edge. Wenjie et al. [217] discuss structured model compression via KD, transferring knowledge from a complex VGG16 model to a lightweight MobileNet. This approach significantly reduces model size and improves performance, making it suitable for deployment on devices with limited resources. Wang et al. [218] explore.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 23 of 50"
  },
  {
    "id_": "f731deff-4600-4ddf-9d20-06240c5d6e6b",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "edbe6b7e-4b0b-448e-8385-52d67ed9a7c0",
    "text": "# IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024"
  },
  {
    "id_": "c9867f95-62dc-4f15-9e74-a9ebb35b678d",
    "text": "# Figure 10\n\nIllustration of the framework proposed in [117]. The \"Conv\" block comprises a 3 × 3 convolutional layer, followed by batch normalization, a 2 × 2 max-pooling layer, and a ReLU activation function. The \"FC\" block includes a fully connected layer, batch normalization, and a ReLU activation function. Both the \"Shared Classifier\" and \"Classifier\" share the same structure, composed of \"FC\" blocks and a softmax layer for final classification."
  },
  {
    "id_": "b99193cc-dff7-4a4c-82e4-528e945dffe3",
    "text": "# 1) Knowledge\n\nDIM: The self-distillation component that supports our network to learn from itself. The per-source output layer is trained to mime the behavior of the final fused output with the goal of distilling knowledge from fused information to the per-source stream. Using VGG as a teacher network, a student network is trained with KD, achieving high recognition accuracy and speed, particularly for coffee leaf pest and disease identification.\n\nIn detail, to align the dimension v, z1, and z2, a FC layer is applied to z1 and z2:\n\nzs,i = f(Ws zs,i + bs)"
  },
  {
    "id_": "224fe23a-c722-4dfd-af7d-e0ebf8d14b4a",
    "text": "# 5.2.3. Urban Planning\n\nKD has emerged as a vital technique in urban planning, particularly in the context of enhancing the efficiency and accuracy of models used for complex tasks such as environmental monitoring, infrastructure management, and resource allocation. For instance, KD has been effectively used to improve the real-time detection of building defects, optimize building extraction from noisy datasets, and enhance the accuracy of traffic flow prediction and travel time estimation.\n\nThis technique is particularly valuable in scenarios involving large-scale urban data, where it enables the deployment of sophisticated models on resource-constrained devices, such as UAVs and edge computing frameworks, facilitating more efficient management of urban infrastructure and services."
  },
  {
    "id_": "298a5b3c-519a-4262-bf30-cb0ba6bf0f66",
    "text": "# 2) Knowledge DCM\n\nThe DCM module leverages a distillation component that transfers the knowledge of the multimodal from powerful teacher models to more efficient student models. KD supports the development of robust, scalable solutions that are essential for modern urban planning and the creation of smarter, more responsive cities.\n\nFor instance, Rithanasophon et al. [228] proposed a method that leverages deep CNNs (DCNNs) and KD to evaluate QoL for pedestrians using walkability data collected through virtual reality tools, achieving significant improvements in model performance."
  },
  {
    "id_": "fd48ac12-73db-4b8d-b353-08b1331e2b80",
    "text": "# Table 7\n\nSummary of the main works that employ KD in the agriculture domain.\n\n|Reference|Application|Model Type|Performance Improvement|\n|---|---|---|---|\n|[219]|Instance-based semantic segmentation|DCNN|High accuracy and speed|\n|[220]|Animal behavior classification|Compact model|Real-time performance|\n|[221]|Weed mapping using UAVs|Vision Transformer|Effective weed mapping|\n|[222]|Plant growth monitoring|Transformer-based network|Enhanced performance|\n\nAuthorized licensed use limited to: University of Dubai. Downloaded on August 14, 2024 at 12:26:54 UTC from IEEE Xplore. Restrictions apply.\n\nY. Himeur, et al.: Preprint submitted to Elsevier"
  },
  {
    "id_": "e925e2fc-f37e-4f56-9f2a-d1b4752e0a5e",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "bde40923-723a-4318-8d86-5ce75f5c4125",
    "text": "# Table 7"
  },
  {
    "id_": "6be886b5-7eec-4ae5-bfde-35e2b924317a",
    "text": "# Comparison of Studies on KD in Agriculture\n\n|Ref.|Model(s) Used|Dataset/Data Type|Main Contribution|\n|---|---|---|---|\n|[199]|BERT-ALA + BiLSTM + CRF|Agriculture named entity data|Enhanced agricultural entity recognition using model distillation|\n|[200]|MobileNet, Xception|PlantVillage dataset|Plant disease recognition with KD|\n|[201]|Improved YOLOv5s|Maize leaf disease dataset|Lightweight model for maize leaf disease detection|\n|[205]|ResNet-34|Various datasets for panoptic segmentation|KD for panoptic segmentation|\n|[206]|Identical architecture for teacher and student|Large dataset for plant leaf segmentation|Improved instance segmentation using spatial embedding and KD|\n|[202]|ECA-KDNet|Apple leaf dataset|Lightweight model for apple leaf disease diagnosis|\n|[203]|YOLOR model variants|PlantDoc dataset|Multistage KD for plant disease detection|\n|[223]|Multilevel distillation framework|CIFAR100 and CIFAR10|Addressing low resolution identification problems|\n|[208]|Lightweight semantic segmentation model|Custom dataset for grape picking point localization|Efficient grape picking point localization in complex environments|\n|[210]|Low-power DL model|Hydroponic systems|Plant disease detection in low-power IoT devices|\n|[211]|Data augmentation-based KD framework|PlantDoc dataset|Enhanced recognition accuracy for plant leaf diseases|\n|[209]|Knowledge-distilled models|Datasets for grapevine detection|Early grape detection and yield prediction with KD|\n|[218]|Lightweight model using VGG for KD|Coffee leaf dataset|High accuracy in coffee leaf disease identification with a lightweight model|\n|[220]|GRU-MLP models, ResNet|Animal behavior datasets|In-situ animal behavior classification on wearable devices|\n|[217]|Distilled-MobileNet|Common diseases of crops|Lightweight disease recognition model for limited-resource devices|\n|[219]|Instance-based semantic segmentation with Mask2Former|Agricultural datasets|KD for instance semantic segmentation|\n|[221]|Lightweight Vision Transformer|WeedMap dataset|Mapping weeds with drones using KD|\n|[222]|PA-RDFKNet|Various datasets for plant growth monitoring|RGB-depth fusion for plant age estimation with KD|\n|[224]|KD from Multi-head Teacher (KDM)|Bio-HSI|Efficient hyperspectral image segmentation with a compact student network|\n|[225]|UNet with various backbones|On-field images of pomegranate fruit|Effective segmentation of pomegranate fruits for agricultural automation|\n|[212]|Vision Transformer with chunking method|DlouFish dataset|Enhanced fish individual recognition using a novel KD strategy|\n|[214]|C3Faster with KD|CropPest6 dataset|Fast and efficient crop pest detection suitable for edge devices|\n|[215]|Lightweight CNN models|Indiana Bark Dataset|Portable tree species identification system for smartphones|\n|[216]|CNN|Crop growth dataset|Learning plant physiology from crop models generated by a crop model to enhance model portability|\n|[213]|Two-tier KD (T-KD)|Fish37 dataset|Improved accuracy and reduced parameters for underwater fish species classification|\n|[226]|KD from multispectral to RGB models|Mullus Marbatus family dataset|Fish quality estimation using RGB cameras with knowledge from multispectral images|\n|[227]|ResNet50 and a lightweight student model|Dataset of Ethiopian medicinal plants|Accurate identification of medicinal plants using a distilled knowledge approach|"
  },
  {
    "id_": "565ad03c-861e-444b-a59e-deba5ad56d0b",
    "text": "# Best Performance\n\n|Performance|Limitation|\n|---|---|\n|Macro-F1 increased by 3.3%|High time and space complexity|\n|Accuracy of 97.58%|Limited to small architectures|\n|mAP(0.5): Increased by 3.8%|Only focuses on maize; may not generalize to other crops|\n|Improved panoptic quality by up to 4.1 points|Requires extensive fine-tuning of balancing weights|\n|Enhanced segmentation accuracy|High dependency on the quality and size of the dataset|\n|Accuracy of 98.28%|Focused only on apple leaves, might not generalize|\n|60.4% mAP@.5|Model complexity and distillation stages may be challenging to manage|\n|Improved low-resolution recognition accuracy|Specific to low-resolution datasets|\n|91.08% accuracy in picking point localization|Limited to grape picking, may not extend to other fruits|\n|Accuracy of 99.4%|Focus on hydroponics; broader application unknown|\n|Improved accuracy by up to 3.06%|Performance heavily dependent on data augmentation quality|\n|Improvement in various metrics, e.g., 13.63% in mAP50-95|Predominantly focused on early detection stages|\n|Accuracy of 96.73%|Generalization to other crop diseases not demonstrated|\n|MCC of 0.882 (ResNet)|Mainly applicable to animal behavior, not crops|\n|Accuracy of 97.62%|Limited to specific diseases and crops|\n|AP improvement of 1.8 for ResNet-50|Focused on specific types of segmentation|\n|F1 score of 0.863|Specific to drone-based RS|\n|MSE reduced from 2 to 0.14 weeks|Focused on plant growth, might not extend to other agricultural tasks|\n|mIoU of 90.03%|Over-compression degrades performance without medium-sized teacher assistants|\n|F1 score of 90.35% for VGG19 backbone|Dependency on the choice of backbone for performance|\n|Accuracy of 93.19%|Specific to underwater environments|\n|97.5% mAP|Reduced feature extraction capability in lightweight models|\n|96.12% accuracy|Limited to specific tree species in Indiana|\n|MSE of 52.9 during training|Limited by synthetic data generation from crop models|\n|Top-1 accuracy of 97.20%|Requires large model sizes for initial training|\n|Classification accuracy of 84.3%|Limited to specific types of fish and conditions|\n|96.91% accuracy|High accuracy dependent on extensive data preprocessing|\n\nperformance and computational efficiency. Similarly, Liu et al. [229] introduced UrbanKG, a knowledge graph system that integrates KD for urban data fusion, showing promising results in boosting performance across various urban computing applications. Xu et al. [230] also addressed the challenges of limited training samples in building polygon extraction by proposing BPDNet, a KD-based framework that effectively integrates generalization knowledge from large datasets with task-specific characteristics, resulting in superior performance in complex urban environments.\n\nFederated learning frameworks have also benefited from KD, particularly in the context of land use monitoring and environmental impact assessment. Kanagavelu et al. [185] demonstrated the potential of integrating KD with federated UNet models for the semantic segmentation of satellite and street view images, achieving high accuracy and significant."
  },
  {
    "id_": "b15391ba-c600-4ea6-90a5-53c7436c8ae1",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nModel compression. In a similar vein, Xu et al. [231] developed a KD-based building extraction method that reduces the impact of noise on model performance while maintaining generalization, achieving notable improvements in precision, recall, and IoU metrics. In the context of transportation systems, KD has been utilized to enhance travel time estimation (TTE) models and improve traffic flow prediction. Yang et al. [106] proposed KDTTE, a deep neural network model that employs KD to reduce computation and memory costs while increasing accuracy, significantly outperforming state-of-the-art baselines in TTE tasks. In a different but related task, Li et al. [232] applied deep KD to traffic flow prediction in spatio-temporal networks, demonstrating improvements in both local and global feature perception and achieving better accuracy in traffic predictions.\n\nIn the autonomous driving domain and the task of off-road environment segmentation, KD has been instrumental in improving model efficiency and accuracy. Pan et al. [233] developed an end-to-end lane detection method using KD to guide polynomial regression under complex road conditions, achieving competitive results in efficiency and accuracy. Similarly, Kim and An [234] proposed a KD method for segmenting off-road environment range images, resulting in a favorable trade-off between segmentation performance and computational cost, highlighting its effectiveness for autonomous systems.\n\nLee et al. [235] proposed a high-speed detection method for multi-class defects on residential building façades using KD. The study demonstrated that applying KD to a lightweight DL model significantly improved mean average precision (mAP) by approximately 20% and reduced inference time by 2.5 times, making it more suitable for real-time applications. Moving on, Chen et al. [108] introduced a novel approach to building extraction that utilizes KD to enhance the robustness of the distilled student model. The study employed a multi-teacher collaborative distillation strategy to transfer comprehensive feature knowledge from teacher networks to the student model. The approach demonstrated state-of-the-art performance on multiple datasets, including the Massachusetts Roads Dataset, LRSNY Roads Dataset, and WHU Building Dataset, achieving high IoU scores and improving learning capabilities. Geng et al. [79] developed a lightweight topological space network for road extraction from optical RS images, leveraging KD. The study addressed the challenge of extracting topological features from complex road networks by proposing a topological space loss calculation model. The method resulted in significant improvements in accuracy and computational efficiency, demonstrating a good balance between performance and model size.\n\nBesides, Li et al. [236] proposed an off-policy imitation learning method for autonomous driving that employs task KD. This approach was designed to clone human driving behavior and transfer driving strategies to new, unseen scenarios. The method showed promising results in transferring knowledge to different illumination and weather conditions, enhancing route-following performance in realistic urban driving scenes. Hong et al. [237] introduced a hierarchical edge-decision framework for intelligent transportation systems (ITS) that incorporates KD. The framework enables vehicle-road-cloud cooperation to enhance real-time motion planning by distilling complex spatial-temporal event reasoning into efficient decision-making processes. The method was validated on autonomous driving scenarios, demonstrating improved adaptability to complex environments. Luo et al. (2022) [238] presented the KeepEdge framework, which integrates deep neural networks into an edge computing system for UAV-assisted parcel delivery. By employing KD, the study created a lightweight model that maintained high accuracy while reducing the computational load on UAVs. This approach proved effective in complex environments where traditional GPS-based positioning might fail. Pelizari et al. (2023) [239] developed a deep multitask learning (MTL) architecture for building characterization using street-level imagery. The study incorporated KD to encode cross-task interdependencies, which improved the generalization capabilities of the model across multiple natural hazards. The proposed MTL methods outperformed traditional single-task learning (STL) models, achieving higher accuracy and efficiency. The aforementioned studies that demonstrate the versatility of KD in enhancing the efficiency, accuracy, and scalability of models in various prediction and classification tasks in urban planning and intelligent transportation systems, using RS data are summarized in Table 8."
  },
  {
    "id_": "b677f0c2-2c03-4487-88c2-dd8bff18d0c2",
    "text": "# 5.2.4. Oceanographic Monitoring\n\nKD can significantly enhance the efficiency and practicality of AI applications in ocean and sea studies by simplifying complex models for deployment on resource-constrained devices. This includes improving marine wildlife detection, real-time oceanographic monitoring, and underwater object detection by compressing large models into smaller, more efficient versions without compromising accuracy. Additionally, it can aid in climate change prediction and fisheries management, making advanced AI models more accessible and effective for monitoring and analysis in remote or resource-limited environments. In this direction, the authors in [244] explore the application of CNNs in ocean RS, highlighting their effectiveness in tasks such as 3D ocean field reconstruction, image super-resolution, and ocean phenomena forecasting. The study demonstrates significant improvements in classification accuracy for sea ice and open water areas in SAR images and a notable enhancement in image resolution using CNN-based models.\n\nSeveral studies focus on underwater environments, where detection and analysis face unique challenges due to poor visibility and environmental complexities. Chen et al. [245] propose an online KD framework, Online-XKD, to enhance the accuracy and generalizability of underwater object detection models while maintaining their lightweight nature. Similarly, Ben Tamou et al. [246] present a CNN-based approach for classifying live reef fish species in underwater environments, using incremental learning to maintain high accuracy as new species are added. Another underwater-focused study introduces WaterMono [247], a framework for depth estimation and image enhancement in underwater scenes, leveraging KD to address challenges such as dynamic scenes and image degradation.\n\nIn the domain of geophysical field reconstruction, Adapt-Deep [248], a self-supervised framework designed, has been proposed to reconstruct fine-grained spatial structures from coarse-scale geophysical data. The proposed method effectively identifies and recovers detailed information in sea.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 26 of 50"
  },
  {
    "id_": "c3f31ccc-e080-439c-a925-6aa385e90508",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "0bcd25a0-23e0-4430-bd5a-ba2ded1581b5",
    "text": "# Table 8"
  },
  {
    "id_": "27f68152-ed4b-4b53-acaf-c9bee8db28fe",
    "text": "# Comparison of KD Studies for Urban Planning\n\n|Citation|Model(s) Used|Dataset/Data Type|Main Contribution|Best Performance Value Achieved|Limitation|\n|---|---|---|---|---|---|\n|[228]|DCNNs, LSTM, KD|VR-based questionnaire data|Evaluates walkability using AI and enhances real-time performance through KD|MSE of 7.19 × 10−3 (within-city) and 9.73 × 10−3 (across-cities)|Limited to VR data, may not generalize to all environments|\n|[229]|FedUKD, UNet|Satellite and street view images|Integrates knowledge graphs for urban data fusion|97% accuracy on Chennai land use dataset|May struggle with dynamic, heterogeneous urban data|\n|[230]|BPDNet|Building polygons|Distills knowledge for generalization in building extraction tasks|IoU of 66.54%|Performance may drop in complex urban settings|\n|[185]|FedUKD|Satellite and street view images|Reduces communication costs in land use classification via federated learning|Above 95% accuracy with significant compression|Scalability to other urban data types may be limited|\n|[106]|KDTTE|Travel time estimation datasets|Improves travel time estimation with KD|86.8% accuracy improvement on Porto dataset|Limited generalization to diverse traffic conditions|\n|[240]|UrbanKG|Urban spatial-temporal data|Develops an urban knowledge graph for data fusion|Effective in various urban applications|Requires extensive setup and integration|\n|[231]|UPerNet, Swin Transformer|Noisy RS images|Enhances building extraction from noisy images with KD|IoU of 81.61%|Dependent on noisy label quality|\n|[235]|DCNN|Building façade images|Accelerates defect detection on building façades using KD|20% mAP increase, 2.5x faster inference|Limited to façade defects, may not generalize|\n|[108]|U-Net, DeepLabV3Plus|Road and building datasets|Enhances model robustness via multi-teacher distillation|IoU scores: 48.56%, 79.51%, 81.35%|Teacher weight optimization is still needed|\n|[232]|Deep KD Model|Traffic flow datasets|Improves spatio-temporal traffic flow prediction using KD|Accuracy improvement of 0.19 and 0.18 on respective datasets|Focused on local data, may miss global patterns|\n|[233]|End-to-End Lane Detection with KD|TuSimple, CULane Datasets|Lane detection method using auxiliary supervision|Competitive accuracy, high efficiency|Post-processing still needed for some tasks|\n|[241]|Interaction-aware Trajectory Planning with KD|Real-world driving scenarios|Combines DL with optimization for trajectory planning|Fivefold improvement in computation time|Integration with control paradigms is complex|\n|[242]|Lightweight Location Prediction Model|Next Mobility data|Efficient next-location prediction with reduced inference time|6.57% error reduction, 99.8% faster inference|Focuses on reducing computational load|\n|[243]|MJPNet-S*|RGB-T/D data|Trimodal joint-perception network for crowd density estimation|92% faster, 83% fewer parameters|Reduced resource consumption may impact generalization|\n|[79]|TSKD-Road|RS images|Topological network for road extraction with KD|Road IoU: 59.16%, mIoU: 78.49%, F1: 74.15%|Limited to road extraction tasks|\n|[234]|MobileNet_v2 DeepLabV3+ with SLKD|Off-road environment dataset|Lightweight model for off-road segmentation using KD|mIoU of 57.28%, low computational cost|Trade-off between accuracy and efficiency|\n|[237]|GSCNN|Autonomous driving scenarios|Edge-decision framework for motion skill enhancement|Improved adaptation to dynamic environments|Complexity in real-time implementation|\n|[238]|DNN|UAV delivery environments|Edge intelligence framework for UAV positioning|High accuracy with reduced model complexity|Dependent on visual data quality|\n|[239]|Deep MTL|Street-level imagery|Cross-task interdependency modeling for building characterization|accuracy = 88.43%|Complexity in MTL model training|\n\nSurface temperature fields, demonstrating the potential of domain adaptation techniques in enhancing data resolution and accuracy. Moving on, Tropical cyclone (TC) wind radii estimation is the focus of Jin et al. [249], who propose a multimodal fusion network, MT-TCNet, and its distillation variant, MT-TCNet-Distill. These models utilize a combination of satellite infrared images, wind field reanalysis, and maximum sustained wind speed data to estimate TC wind radii, achieving superior performance even in scenarios with incomplete data.\n\nIn the domain of water segmentation, the challenge of accurately segmenting water areas for unmanned surface vehicles (USVs) has been presented in [250]. The study introduced a multimodal fusion method combining 2D camera images and 3D LiDAR point clouds, utilizing transformers and KD to improve segmentation accuracy and processing speed. Lastly, Yang et al. [251] focus on sea ice segmentation, proposing a CNN-based method enhanced with data augmentation, a novel loss function, and multiscale strategies. Their study achieves high segmentation accuracy using the HRNet-W48 backbone, demonstrating the effectiveness of innovative DL techniques in environmental monitoring. Table 9 provides a summary of the studies that employ KD techniques to improve model performance in the oceanographic remote imaging domain."
  },
  {
    "id_": "e4137e43-e57b-40e5-ab3e-c02e88897358",
    "text": "# 6. Challenges and Limitations\n\nDespite the many advantages that KD techniques offer and the wide range of their applications, they still face several limitations as portrayed in Fig. 11. These challenges are mainly related to the deployment of the models to resource-constrained devices and to keeping the performance of these models high when handling heterogeneous data or data from new, unseen distributions. Finding the balance between model efficiency and prediction accuracy is the key challenge as explained in the following."
  },
  {
    "id_": "4fb145f2-ec40-4cdc-af46-72e7d89d2e32",
    "text": "# 6.1. Model Complexity and Deployment\n\nIn RS applications, KD is often employed to create smaller, more efficient student models by transferring knowledge from a larger, more complex teacher model. The main goal of KD is to retain the high accuracy of the teacher model while reducing the computational load and model size.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 27 of 50"
  },
  {
    "id_": "4423ba93-cc4a-43d7-9f9f-63741298342d",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "e7f5d255-9fd1-499b-8dbf-80fce3682a19",
    "text": "# Table 9\n\n|Ref.|Model(s) Used|Dataset/Data Type|Brief Description of Main Contribution|Best Value Achieved|Performance|Limitation|\n|---|---|---|---|---|---|---|\n|[244]|CNNs|Various ocean RS data|Applied CNNs across multiple ocean RS tasks, including 3D ocean field reconstruction and image super-resolution.|ACC=92.36% for sea ice and open water areas in SAR images|High computational cost|and model interpretability challenges.|\n|[245]|Online-XKD|URPC2020 dataset|Enhanced feature extraction and generalization in underwater object detection using mutual knowledge transfer in a distillation framework.|3.6 mAP improvement in student model detection accuracy|Complexity may hinder deployment in low-resource environments| |\n|[246]|CNN with incremental learning|LifeClef 2015 Fish dataset|Developed an incremental learning strategy for live reef fish species classification, maintaining high performance on previously learned species.|81.83% accuracy on LifeClef 2015 Fish benchmark dataset|Scaling to larger datasets|or complex environments could be challenging.|\n|[248]|AdaptDeep|Coarse and fine-scale geophysical field data|Proposed a self-supervised framework for fine-grained reconstruction of geophysical data using domain adaptation and contrastive learning.|Recovered 81.2% detailed information in sea surface temperature fields|Performance depends on|the availability of coarse-scale data and temporal correlations.|\n|[247]|WaterMono|Underwater images|Introduced a self-supervised depth estimation framework with image enhancement for underwater environments using KD.|RMSE: 0.945, RMSE log: 0.152|Limited generalization to|diverse camera angles and extreme conditions.|\n|[249]|MT-TCNet, TCNet-Distill|Multimodal data including satellite IR images, reanalysis wind fields, and MSW speed|Developed a multimodal fusion network and distillation method for robust TC wind radii estimation with both complete and missing modalities.|R34 estimation: RMSE 22.458 nmi, MAE 16.577 nmi, R-value 0.855; RMW estimation: RMSE 7.958 nmi, MAE 5.689 nmi, R-value 0.738|Reliance on reanalysis data|limits real-time applicability.|\n|[250]|Transformer-based multimodal fusion|2D camera images, 3D LiDAR point clouds|Proposed a water segmentation method using transformers and KD for improved 2D image-based segmentation with faster speed.|Approx. 1.5% improvement in accuracy and MaxF, speed of 15-110 fps|High computational load|during training phase, though reduced with distillation.|\n|[251]|CNN with HRNet-W48 backbone|Large sea-ice segmentation dataset|Introduced innovative data augmentation, loss function, and multiscale strategies for accurate sea ice segmentation with KD for real-time application.|FWIoU score of 97.8439|High computational resource requirement|for real-time processing.|\n|[252]|Tiny YOLO-Lite|SSDD, HRSID, large-scene SAR images|Developed a lightweight SAR ship detector using network pruning and KD to reduce model size and computation while maintaining high accuracy.|Average Precision (AP)MB model size, of 89.07%, 2.8|inference speed >200 fps|Performance may decline with further model size reduction.|"
  },
  {
    "id_": "55fa610c-91c4-4751-b57c-14affed0df4e",
    "text": "# Knowledge Distillation Challenges in Remote Sensing\n\n- Mathematical Optimization and Loss Computation\n- Model Complexity and Deployment\n- Data Heterogeneity\n- Heterogeneity Error in Multi-Modal Data\n- Generalization Error due to Limited Data\n- Overfitting and Generalization\n- Scalability\n- Computational Cost for Large Datasets\n- Inference Time and Real-Time Constraints\n- Limited Real-Time Applicability\n- Dependency on High-Quality Data\n- Impact of Data Quality on Model Performance\n- Efficiency-Accuracy Trade-offs\n- Balancing Efficiency and Accuracy\n- Integration Complexity\n- Complexity in Integration with Other Techniques"
  },
  {
    "id_": "b39fb65b-d229-4cae-b897-be620a8b957f",
    "text": "# End\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 28 of 50"
  },
  {
    "id_": "1aeb7dea-d7fb-469e-954b-c265bf30ee29",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nwhich is crucial for deployment on resource-constrained devices commonly used in RS. However, the process of optimizing the distillation loss function to achieve this balance between model size and performance is inherently complex and computationally demanding. The distillation loss *ℓKD, which combines the cross-entropy loss ℓCE and the Kullback-Leibler divergence ℓKL*, needs to be carefully minimized to ensure that the student model effectively approximates the teacher model. This optimization process becomes more challenging as the complexity of the teacher model increases, leading to a higher computational burden during training [253]. Furthermore, when deploying the distilled student model on resource-constrained devices, such as those used in RS for on-board data processing, the reduced model complexity must still meet the real-time processing requirements and maintain high accuracy. The complexity of optimizing the KD process for deployment is expressed by the computational cost associated with the gradient of the distillation loss with respect to the student model parameters. As this complexity increases, it can lead to longer training times, higher energy consumption, and potentially suboptimal model performance, particularly when deployed in environments with limited computational resources.\n\nIn this regard, given a teacher model *T with parameters θT and a student model S with parameters θS*, the distillation loss is defined as:\n\n*ℓKD(θS) = αℓCE(y, S(x; θS)) + (1−α)ℓKL(T(x; θT), S(x; θS))* (5)\n\nThe challenge related to the complexity of optimizing this loss function for deployment is defined as follows:\n\n*Complexity ∝ O(∂ℓKD)* (6)\n\nThis complexity can affect the feasibility and effectiveness of deploying KD models in real-world RS scenarios, where computational efficiency and model robustness are critical."
  },
  {
    "id_": "157e117e-2a64-4c49-8d72-b080a4987e3b",
    "text": "# 6.2. Data Heterogeneity\n\nRS data often comes from multiple modalities, such as optical, SAR, and multispectral sensors. Integrating knowledge across these heterogeneous data sources while maintaining accuracy is challenging, as the characteristics of the data can vary significantly. For multi-modal RS data *x1, x2, … , xm* from different modalities, the aggregate loss is:\n\n*ℓmulti-modal(θS) = ∑i=1m wi ⋅ ℓKD(T(x; θT), S(x; θS))* (7)\n\nTypically, the heterogeneity error is defined as:\n\n*Heterogeneity Error = ∑i=1m |ℓKD(T(x; θT), S(x; θS))| - ℓKD(Tj(xj; θT), S(xj; θS))|* (8)"
  },
  {
    "id_": "9cfa85b2-df9e-4023-958e-65e38816a94b",
    "text": "# 6.3. Overfitting and Generalization\n\nWhile KD helps reduce the size of models, it can also lead to overfitting, particularly when the student model is trained on a limited dataset. This results in poor generalization to new, unseen data, which is critical for the success of RS applications [35, 254]. The generalization error is given by:\n\n*Generalization Error = ℓKD(θS; Dtest) − ℓKD(θS; Dtrain)* (9)\n\nOverfitting occurs when: Generalization Error ≫ 0. The impact of overfitting on the performance of knowledge distillation in RS applications includes: Overfitting causes the student model to perform well on the training data but poorly on unseen test data, which can significantly reduce the model’s effectiveness in real-world RS applications where data variability is high; Overfitted models are often overly sensitive to noise in the training data. This sensitivity can lead to incorrect predictions when the model encounters noisy or outlier data in RS, where data quality can vary widely across different sensors and conditions; In RS, data is often collected from multiple modalities (e.g., optical, SAR, multispectral). An overfitted student model might fail to generalize well across these different modalities, leading to inconsistent performance and reduced reliability in practical applications; Overfitting can limit the ability of the student model to transfer learned knowledge to new tasks or domains within RS, reducing the versatility and adaptability of the distilled model."
  },
  {
    "id_": "746d53f5-5316-4dc3-b2ba-5c5a0897062e",
    "text": "# 6.4. Scalability\n\nAs the size of RS datasets increases, the computational complexity of applying KD also increases. This scalability issue can limit the practicality of deploying distilled models on large datasets.\n\nFor a dataset of size *N*, the computational complexity scales as:\n\n*Scalability ∝ O(N ⋅ C(ℓKD))* (10)\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 29 of 50"
  },
  {
    "id_": "da2d4d0c-6b32-4a0e-82c6-24277e69782e",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "7a1e80a1-451a-415e-b920-6afe4b2ac5c7",
    "text": "# 6.5. Limited Real-Time Applicability\n\nIn RS applications, the need for real-time processing is critical, as delays in data processing can render the information outdated and less useful for immediate decision-making. Knowledge Distillation (KD) aims to create more efficient models, but even with distilled models, achieving the required inference speed can be challenging, especially when dealing with complex student models. The inference time tinference, which depends on both the size of the student model θS and the computational complexity of evaluating the distillation loss ℭ(ℒKD), must be kept within the real-time processing limit treal-time. If tinference exceeds treal-time, the performance of the KD model is compromised, as it may not be able to process data quickly enough to be useful in time-sensitive RS applications. This limitation can reduce the effectiveness of KD in scenarios where immediate data analysis and decision-making are required [35, 254].\n\ntinference ≤ treal-time (11)\n\nwhere tinference is the inference time and treal-time is the allowable time for real-time processing:\n\ntinference ≈ O(θS) + O(ℭ(ℒKD)) (12)"
  },
  {
    "id_": "546cdcff-8ecd-4264-ad6d-89f2ec538386",
    "text": "# 6.6. Dependency on High-Quality Data\n\nThe effectiveness of Knowledge Distillation (KD) is highly contingent on the quality of the training data, which plays a critical role in the success of the distillation process. In RS applications, the data is often noisy, sparse, or collected under varying conditions, leading to inconsistencies that can adversely impact the KD process. The distillation loss, which combines the cross-entropy loss ℒCE between the student model’s predictions and the true labels, and the Kullback-Leibler divergence ℒKL between the teacher and student model outputs, assumes that the input data is of high quality. However, when the data is of poor quality, the student model may struggle to learn effectively from the teacher model, leading to increased errors and reduced generalization capability. The formula for the impact of data quality indicates that as the proportion of low-quality data increases, the overall performance of the KD model diminishes. This reduction in performance can result in less robust models, which may fail to accurately process and interpret RS data, ultimately hindering the effectiveness of KD in real-world RS tasks [255, 256, 96].\n\nℭKD(θS) = ∑i=1n(ℒCE(yi, S(x; θS)) + ℒKL(T(xi; θT), S(x; θS))) (13)\n\nTypically, the impact of data quality is defined as follows:\n\nData Quality Impact = n1 ∑i=1n I(quality(x, y) < ε) (14)"
  },
  {
    "id_": "b628d613-8d08-474b-9540-a2d65c0dc08f",
    "text": "# 6.7. Balancing Efficiency and Accuracy\n\nOne of the key challenges in Knowledge Distillation (KD) is striking the right balance between model efficiency and accuracy. In the context of RS applications, where the stakes are often high, such as in disaster monitoring or environmental protection, compressing the student model too much in the pursuit of efficiency can lead to a significant loss in accuracy. This reduction in accuracy could result in the failure to correctly interpret RS data, leading to erroneous decisions [257, 258]. The trade-off between efficiency and accuracy is represented by the following relationships:\n\nEfficiency ∝ Model Size(θS), Accuracy ∝ 1/ℭKD(θS) (15)\n\nwhere Model Size(θS) refers to the number of parameters in the student model, and ℭKD(θS) is the distillation loss, which is inversely proportional to the accuracy of the student model. The optimization problem, therefore, involves maximizing the product of efficiency and accuracy:\n\nmaxθS Model Size(θS) ⋅ 1/ℭKD(θS) (16)\n\nHowever, this optimization is complex because increasing efficiency (i.e., reducing model size) often leads to a rise in distillation loss ℭKD, which in turn decreases accuracy. Conversely, maintaining high accuracy may require a larger model size, reducing efficiency. In RS, where both computational resources and model performance are critical, failing to achieve an optimal balance can limit the effectiveness of KD. This trade-off must be carefully managed to ensure that the compressed model performs adequately in practical RS scenarios, where both speed and accuracy are essential."
  },
  {
    "id_": "a26a8d3c-3bf1-4c83-abdc-3b0bd7e5488b",
    "text": "# 6.8. Integration Complexity\n\nIntegrating Knowledge Distillation (KD) with other techniques such as multi-modal fusion or domain adaptation introduces significant complexity to the model and its training process, affecting its performance in RS applications. Integrating these techniques requires careful balancing of multiple loss functions, as the overall performance now depends on the combined effectiveness of KD, multi-modal learning, and domain adaptation. This added complexity can make the training process more computationally expensive.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 30 of 50"
  },
  {
    "id_": "64c6a0f3-c0c7-4c3f-92fd-67c835f458ff",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nHarder to optimize, and more prone to issues like overfitting or convergence to suboptimal solutions. For instance, when integrating KD with other techniques, the overall loss function is expressed as a combination of multiple loss components, each weighted by a coefficient (e.g., 𝛼, 𝛽, 𝛾). The need to fine-tune these coefficients to achieve the desired balance between KD, multi-modal fusion, and domain adaptation further complicates the training process [116, 259]. Moreover, the integration complexity, represented by the derivative of the total loss function concerning the student model parameters, reflects the increased difficulty in optimizing the student model. As the complexity increases, the risk of inefficient training or suboptimal performance also rises, making it challenging to achieve the desired accuracy and efficiency in real-world RS applications."
  },
  {
    "id_": "88a52c63-8d2b-40a7-aa3d-321c00f3e36b",
    "text": "# 7. Future Directions"
  },
  {
    "id_": "fcc3246b-32fc-4c4e-b0cf-06a0366c7181",
    "text": "# 7.1. Advanced Model Compression Techniques"
  },
  {
    "id_": "f5ebd131-cf6b-4871-bbfa-3c4dbb4e7eb9",
    "text": "# 7.1.1. Dynamic Distillation\n\nDynamic Distillation is a technique within the broader category of advanced model compression. It aims to optimize the student model’s performance by dynamically adjusting its complexity based on the specific characteristics of the input data or the task at hand. The core idea behind dynamic distillation is to create a flexible and adaptive student model that can efficiently learn from the teacher model without being overly constrained by a fixed architecture or predetermined level of complexity [260, 261]. In many RS applications, the complexity of the input data can vary significantly. For example, a satellite image of a dense urban area may contain more intricate features than an image of a rural landscape. A static, one-size-fits-all student model may struggle to balance performance across such diverse inputs. Dynamic distillation allows the student model to adapt its architecture or parameters based on the specific task, ensuring that it allocates resources efficiently [262, 263]. Not all inputs require the same level of processing. Dynamic distillation enables the student model to adjust its complexity (e.g., the number of layers, the size of feature maps, or the degree of feature extraction) depending on the input. For instance, simpler inputs might be processed with a reduced version of the student model, while more complex inputs trigger a more detailed processing approach [264]. The term \"dynamic\" implies that these adjustments occur in real-time or near-real-time, during the inference phase. This is particularly useful in resource-constrained environments, such as edge devices or real-time RS applications, where computational resources are limited. By making on-the-fly adjustments, the model can maintain high performance while conserving resources [105]. The dynamic distillation process can be expressed as an optimization problem:\n\nmin𝔼𝑥∼[𝜆(𝑥) ⋅ KD(𝑇 (𝑥; 𝜃𝑇 ), 𝑆(𝑥; 𝜃𝑆 ))] 𝜃𝑆\n\nWhere 𝜃𝑆 represents the parameters of the student model that need to be optimized. The input sample, denoted as 𝑥, is drawn from the data distribution . The teacher model, parameterized by 𝜃𝑇, produces an output 𝑇 (𝑥; 𝜃𝑇) for the input 𝑥, while the student model, with parameters 𝜃𝑆, generates an output 𝑆(𝑥; 𝜃𝑆) for the same input. The KD loss function, KD, typically quantifies the difference between the teacher’s and student’s outputs. Additionally, 𝜆(𝑥) is a dynamic weighting function that adjusts the contribution of each input 𝑥 to the overall loss, depending on its complexity or the specific requirements of the task.\n\nSpecifically, in dynamic distillation, the factor 𝜆(𝑥) acts as a critical gatekeeper, adjusting the influence of each input on the student model’s training. For complex or critical inputs, 𝜆(𝑥) increases, prompting the student model to allocate more resources, such as deeper layers or enhanced feature extraction. Conversely, simpler inputs lead to a lower 𝜆(𝑥), allowing the student model to process the data more efficiently with reduced resources. The KD loss function KD is central to this process, focusing on minimizing the difference between the teacher and student models’ outputs to ensure the student effectively mimics the teacher. This approach is generalized across the entire dataset, as captured by the expectation 𝔼𝑥∼, optimizing the student model’s performance across diverse inputs.\n\nDynamic distillation enhances resource efficiency by dynamically adjusting the student model’s complexity based on the input, ensuring that computational resources are used optimally, especially in resource-constrained environments. This adaptability allows the student model to maintain or even surpass the performance of static models, particularly when dealing with heterogeneous datasets like those in RS. Additionally, the scalability of dynamic distillation makes it a versatile solution that is suitable for deploying ML models across various environments, from cloud-based systems to edge devices. Typically, in RS, dynamic distillation could be applied to urban monitoring where satellite images vary significantly between dense urban areas and sparse rural regions. For instance, when analyzing satellite imagery for urban heat island detection, the student model could dynamically adjust its complexity, using more layers and features for complex urban environments with varied structures, while simplifying its approach for less complex rural landscapes, thus optimizing processing efficiency and accuracy."
  },
  {
    "id_": "d7c5a21a-3fd0-44ae-85ab-60d53710ba4f",
    "text": "# 7.1.2. Layer-Wise Distillation\n\nLayer-wise distillation is an advanced technique in model compression that focuses on transferring knowledge from a teacher model to a student model at a more granular level [265]. Unlike traditional KD, which typically focuses on aligning the final outputs of the teacher and student models, layer-wise distillation involves aligning the outputs of corresponding layers in both models [266]. This approach ensures that the student model learns not only the final output distribution but also the intermediate representations that the teacher model uses to arrive at that output [267]. Typically, layer-wise distillation enables a more effective transfer of knowledge by focusing on the outputs of different layers within a complex model, where each layer captures varying levels of abstraction—from basic features like edges (17) to more complex patterns [268]. This approach allows for tailored compression by assigning different importance to each layer, ensuring that critical features are preserved while less important layers are compressed more aggressively. As a result, the student model becomes more compact and maintains or improves performance, particularly in tasks requiring detailed understanding, such as detecting and\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 31 of 50"
  },
  {
    "id_": "b3705a34-beec-488a-b0ae-4bbd7944855e",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nclassifying intricate patterns in RS applications [269]. The layer-wise distillation process can be expressed as follows:\n\n𝐿\n\nKD-layer =∑𝑤𝑙 ⋅ KD(𝑇 𝑙(𝑥), 𝑆𝑙(𝑥)) (18)\n\nwhere 𝑇 𝑙(𝑥) and 𝑆𝑙(𝑥) denote the outputs of the 𝑙-th layer in the teacher and student models, respectively, while KD represents the knowledge distillation loss function applied to these corresponding layer outputs. The term 𝑤𝑙 is a weight assigned to each 𝑙-th layer, indicating its significance in the distillation process, and 𝐿 denotes the total number of layers in the model.\n\nIn layer-wise distillation, 𝑇 𝑙(𝑥) and 𝑆𝑙(𝑥) represent the outputs of the 𝑙-th layer for a given input 𝑥 in the teacher and student models, respectively, ensuring the student model learns the same feature representations as the teacher model at each stage. The KD loss function, KD, measures the difference between these outputs, and when applied layer-wise, it ensures close alignment between the corresponding layers of both models. The layer-specific weight 𝑤𝑙 allows for fine-tuning the importance of each layer, with critical layers in the teacher model being given higher weights to ensure their knowledge is effectively transferred. Finally, the summation across all layers 𝐿 ensures that the distillation process comprehensively covers the entire model, enabling the student model to replicate the full range of the teacher model’s capabilities.\n\nLayer-wise distillation offers several practical benefits, including enhanced feature preservation, where focusing on each layer ensures that the student model retains the critical features learned by the teacher, leading to greater accuracy and capability. The flexibility in compression, enabled by layer-specific weights, allows for optimizing the trade-off between model size and performance, depending on the application’s needs. Additionally, this approach fosters better generalization, as the student model is trained to replicate the hierarchical representations of the teacher model, making it more adept at handling new data, particularly in tasks that require detailed feature extraction and classification. In RS, layer-wise distillation can be applied to multispectral image classification, where different spectral bands capture varying levels of detail. By aligning the outputs of corresponding layers in both the teacher and student models, this technique ensures that the student model effectively learns the intermediate features critical for distinguishing complex land cover types, such as differentiating between various crop types or identifying subtle changes in vegetation health."
  },
  {
    "id_": "e57838f7-17ff-4913-9e1a-66a4f7e1b7f5",
    "text": "# 7.2. Efficient Training and Inference"
  },
  {
    "id_": "407af447-3af1-4dd3-a180-10b48ddb25b8",
    "text": "# 7.2.1. Low-Cost Training Algorithms\n\nLow-cost training algorithms aim to reduce the computational burden associated with training both teacher and student models, which is particularly important in the context of KD where the goal is to make the student model as efficient as possible. These algorithms focus on optimizing various aspects of the training process to minimize costs while maintaining or even enhancing the performance of the distilled models [270]. Developing more efficient training algorithms that reduce the computational burden of training both teacher and student models is crucial. This can involve leveraging techniques such as federated learning, transfer learning, or smaller proxy datasets for initial training to minimize the overall training cost [271].\n\nFederated learning is a distributed approach that allows training to occur across multiple devices or servers without the need to centralize the data. This can significantly reduce the computational cost associated with data processing and model training by distributing the workload [272]. Each device trains a local model using its data and periodically shares updates with a central server, which aggregates these updates to improve the global model. This approach not only reduces the computational burden on individual devices but also enhances privacy since raw data is not shared [273, 274].\n\nTransfer learning involves taking a pre-trained model (often trained on a large dataset) and fine-tuning it on a smaller, task-specific dataset. This approach can drastically reduce the training cost because the model has already learned general features from the larger dataset, and only minimal additional training is required to adapt it to the new task. In the context of KD, transfer learning can be used to initialize the teacher model, which then distills its knowledge to a student model with minimal additional training [275, 276].\n\nUsing smaller proxy datasets for initial training can also reduce costs. Proxy datasets are subsets of the original data or synthetic datasets that approximate the characteristics of the full dataset but are much smaller in size. Training on these datasets requires fewer resources and can provide a good initial model that can be further refined with the full dataset. This approach is particularly useful in scenarios where obtaining labeled data is expensive or time-consuming [277, 278].\n\nThe cost of training both the teacher and student models can be expressed as:\n\n𝑁\n\n𝐶train =∑(𝐶data(𝑥𝑖) + 𝐶model(𝑇 , 𝑆)) (19)\n\nwhere 𝐶data(𝑥) denotes the cost associated with processing each data sample 𝑥𝑖, which encompasses activities such as data loading, augmentation, and preprocessing, while 𝐶model(𝑇 , 𝑆) refers to the cost incurred during the training process for updating both the teacher model 𝑇 and the student model 𝑆. This formulation highlights that the total training cost 𝐶train is the sum of the costs associated with processing all data samples and updating the models. By optimizing these components—such as by reducing the size of the data samples with proxy datasets, distributing the training workload with federated learning, or leveraging pre-trained models with transfer learning—the overall training cost can be significantly reduced [271].\n\nLow-cost training algorithms enhance resource efficiency, allowing organizations to train effective models even in environments with limited computational resources, which is particularly valuable in fields like RS that involve large datasets and complex models. These algorithms also support scalability, enabling the handling of extensive datasets and sophisticated models without a proportional increase in resource demands, making them adaptable to various environments from cloud servers to edge devices [279]. Additionally, by reducing training costs and time, these algorithms facilitate faster development cycles, allowing for quicker iteration and deployment of ML models, which is essential in rapidly evolving fields like AI [280]."
  },
  {
    "id_": "f873268d-11ee-4d93-82bc-cadfe59ede50",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\napplied to disaster response scenarios, where real-time analysis of satellite imagery is crucial. For example, federated learning can be used to train models across multiple local servers situated near disaster zones, enabling rapid analysis of satellite images for damage assessment without the need for extensive centralized computing resources. Transfer learning can further enhance this process by fine-tuning pretrained models on smaller, region-specific datasets, ensuring swift deployment and effective monitoring during critical events."
  },
  {
    "id_": "f0a90164-5beb-48a4-beef-9f121d2e1128",
    "text": "# 7.2.2. Hardware-Aware Distillation\n\nIntegrating KD with hardware-aware design principles can optimize models specifically for the hardware on which they will be deployed, such as edge devices or GPUs. This approach aims to balance the distillation process with the computational capabilities of the target hardware [281]. Hardware-aware distillation integrates KD with hardware-specific optimization strategies to create models that are not only efficient in terms of performance but are also tailored to the computational constraints of the hardware on which they will be deployed. This approach is particularly useful for scenarios where the model needs to be run on edge devices, GPUs, or other specialized hardware, ensuring that the distilled model operates within the physical and computational limits of the target platform [282].\n\nHardware-aware distillation seeks to balance the effectiveness of the knowledge transfer process with the computational efficiency required by the target hardware. The goal is to ensure that the student model retains as much of the teacher model’s performance as possible while also fitting within the hardware’s resource constraints [283]. This involves careful consideration of factors such as memory usage, processing speed, and power consumption, which are critical in environments like mobile devices, embedded systems, or cloud-based GPUs [284]. Different hardware platforms have varying capabilities and limitations. For example, GPUs excel at parallel processing but may have limited memory bandwidth, while edge devices often have strict power and computational limits. Hardware-aware distillation tailors the student model to leverage the strengths of the target hardware while minimizing its weaknesses. This could involve optimizing the model’s architecture to reduce the number of parameters, simplify computations, or increase parallelism, depending on the hardware’s characteristics [285].\n\nThe regularization parameter 𝜆 in the hardware-aware distillation framework controls the trade-off between the accuracy of the distilled model and its hardware efficiency. A higher 𝜆 places more emphasis on minimizing computational costs, potentially sacrificing some accuracy for greater efficiency. Conversely, a lower 𝜆 prioritizes accuracy, allowing for more complex models that may require more computational resources. The choice of 𝜆 depends on the specific requirements of the application and the hardware. The optimization objective for hardware-aware distillation can be expressed as:\n\nmin KD(𝑇 , 𝑆) + 𝜆 ⋅ 𝐶hardware(𝜃S)\n\nwhere KD(𝑇 , 𝑆) refers to the knowledge distillation loss function, which quantifies the discrepancy between the outputs of the teacher model 𝑇 and the student model 𝑆. The term 𝐶hardware(𝜃S) represents the computational cost associated with running the student model 𝑆 on specific hardware, encompassing factors such as inference time, memory usage, and power consumption. The regularization parameter 𝜆 is introduced to balance the trade-off between reducing the distillation loss and optimizing hardware efficiency. This formulation ensures that the student model is not only accurate but also optimized for the computational environment in which it will be deployed. Typically, hardware-aware distillation enhances the feasibility of deploying advanced ML models in resource-constrained environments by tailoring the student model to the specific hardware, making it particularly valuable for edge computing scenarios with strict power, memory, and processing limits. This approach leads to improved performance on the target hardware, offering faster inference times, lower power consumption, and more efficient memory usage, thereby optimizing model deployment in real-world applications. Additionally, hardware-aware distillation allows for customization to meet the unique requirements of various deployment environments, ensuring that models are optimized whether deployed on high-performance GPUs in data centers or low-power microcontrollers in IoT devices [286].\n\nHardware-aware distillation can be applied to real-time monitoring on edge devices, such as drones used for precision agriculture. By optimizing the student model to operate efficiently within the power and computational constraints of these drones, the model can quickly process high-resolution imagery to detect crop health issues or identify weeds, enabling swift, in-field decision-making without relying on cloud-based resources. This approach ensures that advanced analysis can be performed directly on the edge, even in remote or resource-limited environments."
  },
  {
    "id_": "229029dd-d0c1-41a1-a740-531870d204cc",
    "text": "# 7.3. Improving Data Quality and Robustness"
  },
  {
    "id_": "bc984714-3cc2-4a11-9a3f-23c4cf241ae4",
    "text": "# 7.3.1. Robust Distillation Against Noisy Data\n\nThe effectiveness of KD heavily relies on the quality and quantity of training data. Robust distillation techniques need to be developed to handle noisy, sparse, or imbalanced datasets, which are common in RS. Robust Distillation Against Noisy Data focuses on enhancing the resilience of the KD process when dealing with imperfect data [287]. In real-world applications, particularly in RS, datasets often contain noise, inconsistencies, or imbalances that can degrade the performance of ML models. This approach aims to mitigate the impact of such issues by incorporating robustness into the distillation process, ensuring that the student model can still learn effectively even when the data is not ideal [288].\n\nIn RS, noisy data is common due to various factors like sensor errors, atmospheric interference, or mislabeling during data collection. Standard KD techniques may struggle with such data, leading to poor model performance [289, 290]. Robust distillation techniques address this by explicitly modeling and compensating for the noise during the training process. This can involve using techniques that identify and either correct or down-weight the influence of noisy samples on the student model [291, 292]. The key to robust distillation is modifying the loss function to account for the presence of noise. The traditional KD loss"
  },
  {
    "id_": "313b435f-062a-4379-aefa-92833e5ccd29",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nfunction, which measures the difference between the teacher and student models, is augmented with a term that penalizes the model based on the amount of noise in the data. This term is controlled by a weighting factor 𝜂, which determines how much influence the noise has on the overall learning process. By doing so, the student model becomes more robust to the effects of noise, learning to focus on cleaner, more reliable data [293]. Robust distillation techniques must strike a balance between learning from noisy and clean data.\n\nWhile it is important to minimize the negative impact of noise, completely ignoring noisy data could result in a loss of valuable information [294]. Therefore, these techniques aim to optimize the learning process by allowing the student model to still extract useful knowledge from noisy data while minimizing the distortion it causes [295]. The formulation for robust distillation against noisy data can be expressed as:\n\n&#x1D4C8;KD-robust = 𝔼𝑥,̃𝑥 ∼&#x1D4C8;[&#x1D4C8;KD(𝑇 (̃ ), 𝑆(𝑥)) + 𝜂 ⋅ Noise(𝑥,̃𝑥 )]\n\nwhere &#x1D4C8;KD(𝑇 (̃ ), 𝑆(𝑥)) denotes the standard knowledge distillation loss, capturing the discrepancy between the teacher model’s output on noisy data 𝑥 and the student model’s output on corresponding clean data 𝑥. The weighting factor 𝜂 regulates the influence of the noise term in the overall loss function, with a higher 𝜂 placing greater emphasis on noise correction, thereby enhancing the model’s robustness to noise. The term Noise(𝑥,̃𝑥 ) quantifies the noise level between the clean and noisy data, using metrics such as mean squared error (MSE) or other relevant measures of data corruption.\n\nThis aforementioned formulation ensures that the distillation process remains effective even in the presence of noisy data by integrating a mechanism to handle noise directly within the training objective. Robust distillation techniques enhance model robustness by enabling student models to handle real-world data imperfections, resulting in more reliable performance in practical applications like RS where data quality can vary [296]. These techniques also improve generalization across diverse data conditions by incorporating noise handling into the training process, ensuring that models perform well in both clean and noisy environments.\n\nAdditionally, robust distillation is particularly beneficial for sparse or imbalanced datasets, allowing student models to learn effectively from limited or unevenly distributed data while minimizing the risk of overfitting to noisy or rare examples [297]. In RS, robust distillation against noisy data can be applied to cloud detection in satellite imagery, where cloud cover often introduces noise that obscures ground features. By employing robust distillation techniques, a student model can be trained to accurately detect clouds even in images with varying levels of noise caused by atmospheric conditions, ensuring more reliable and consistent results for subsequent analyses, such as land use classification or vegetation monitoring."
  },
  {
    "id_": "b999f7c1-1449-4eeb-9dd6-eac780581f5b",
    "text": "# 7.3.2. Semi-Supervised and Unsupervised Distillation\n\nSemi-Supervised and Unsupervised Distillation represents an emerging area of research in KD, particularly relevant for fields like RS where labeled data is often scarce or expensive to obtain. The traditional KD process relies heavily on labeled datasets to transfer knowledge from a teacher model to a student model [298]. However, in many practical scenarios, especially in RS, obtaining a large volume of labeled data is challenging. To address this, semi-supervised and unsupervised distillation techniques aim to leverage the abundant unlabeled data available, reducing the dependency on labeled datasets and making the distillation process more robust and scalable [299].\n\nWhile it is important to minimize the negative impact of noise, completely ignoring noisy data could result in a loss of valuable information [294]. Therefore, these techniques aim to optimize the learning process by allowing the student model to still extract useful knowledge from noisy data while minimizing the distortion it causes [295]. The formulation for robust distillation against noisy data can be expressed as:\n\n&#x1D4C8;KD-semi = 𝛼⋅&#x1D4C8;KD(𝑇 , 𝑆; &#x1D4C8;labeled)+(1−𝛼)⋅&#x1D4C8;KD(𝑇 , 𝑆; &#x1D4C8;unlabeled)\n\nThe overall loss function includes &#x1D4C8;KD(𝑇 , 𝑆; &#x1D4C8;labeled), which represents the KD loss computed using the labeled dataset &#x1D4C8;labeled, and &#x1D4C8;KD(𝑇 , 𝑆; &#x1D4C8;unlabeled), which is the distillation loss calculated from the unlabeled dataset &#x1D4C8;unlabeled where the teacher model’s predictions serve as pseudo-labels. The weighting factor 𝛼 adjusts the influence of the labeled and unlabeled data on the overall loss function.\n\nThe above formulation allows the student model to learn from both labeled and unlabeled data, making the training process more flexible and less dependent on extensive labeled datasets. Semi-supervised and unsupervised distillation techniques offer significant practical advantages by reducing the reliance on large, high-quality labeled datasets, making model training more feasible in resource-constrained environments. These methods enhance generalization by exposing models to a wider variety of data patterns, allowing them to perform well on new, unseen data–an essential capability in fields like RS, where data diversity is high.\n\nAdditionally, these techniques provide a scalable approach to model training, enabling organizations to leverage vast amounts of unlabeled data to build robust models without the need for extensive manual labeling efforts. Semi-supervised and unsupervised distillation can be applied to satellite imagery for land cover classification, where acquiring labeled data for every land type is impractical. By using semi-supervised distillation, a model can initially learn from a small set of labeled images and then leverage the large pool of unlabeled satellite images, where the teacher model provides pseudo-labels to refine the student model’s performance.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 34 of 50"
  },
  {
    "id_": "4ec8a4e3-a20c-483d-9014-d7db996825f5",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "29683be6-fa16-4c01-bf20-75b514911738",
    "text": "# 7.4. Scalability Solutions"
  },
  {
    "id_": "1150290d-4a01-4dbc-94eb-ccd7b4920564",
    "text": "# 7.4.1. Distributed Distillation\n\nDistributed distillation is an advanced approach designed to scale the KD process across larger datasets by leveraging distributed learning frameworks. This method spreads the computational load of both the teacher and student models across multiple nodes or devices, making it more feasible to handle large-scale data. The key idea is to perform distillation in a parallel or distributed manner, where each node or device processes a subset of the data, thus reducing the individual computational burden and allowing for more efficient training [303]. In distributed distillation, the overall training task is divided among multiple nodes in a distributed learning framework. Each node independently performs a portion of the distillation process, working with its local subset of data. This division of labor helps in managing the computational load effectively, enabling the distillation process to scale with the size of the dataset. The use of multiple nodes allows for parallel processing, which significantly speeds up the training process [304]. Each node in the distributed system hosts a teacher model and a student model, denoted as 𝑇𝑘 and 𝑆𝑘, respectively, where 𝑘 represents the node index. These models operate on the local data available to that node. The student model on each node learns from its corresponding teacher model, capturing knowledge specific to that subset of the data. This localized learning allows the student models to collectively capture diverse knowledge from the entire dataset when combined [305]. After the distributed distillation process is complete, the student models from all nodes are aggregated to form a comprehensive model that incorporates the knowledge distilled from all parts of the dataset. The aggregation can be done by averaging the model weights, or by combining the outputs of the student models. The objective function KD-distributed is averaged over all nodes, ensuring that the final student model reflects a balanced learning from the entire distributed dataset [306]. The primary advantage of distributed distillation is its scalability. By distributing the workload, it becomes feasible to train on extremely large datasets that would be otherwise impractical to process on a single machine. This approach is particularly useful in scenarios like RS, where data is often collected in large quantities from multiple sources and needs to be processed efficiently. Distributed distillation allows for more rapid training and deployment of models, making it an effective solution for handling big data in ML [307, 308].\n\nThe loss function for distributed distillation is given by:\n\n𝐾\n\nKD-distributed = 1 ∑KD(𝑇𝑘, 𝑆𝑘) (23)\n\n𝐾  𝑘=1\n\nwhere KD(𝑇𝑘, 𝑆𝑘) represents the KD loss computed on the 𝑘-th node, with 𝑇𝑘 and 𝑆𝑘 being the teacher and student models on that node. The term 𝐾 denotes the total number of nodes involved in the distributed learning process. By averaging the distillation losses across all nodes, the overall objective function ensures that the student model benefits from the collective knowledge distributed across the dataset [306]. Distributed distillation can be utilized in the analysis of global satellite imagery, where the vast amount of data is processed across multiple computational nodes. For instance, in mapping land use changes over large geographic regions, distributed distillation allows each node to handle different segments of the satellite images, collectively training a student model that integrates insights from all segments, leading to a comprehensive and scalable approach for detecting and classifying land cover changes [309]."
  },
  {
    "id_": "6eb7b3e4-6a6e-4cc5-9a2c-83ee8c2683a6",
    "text": "# 7.4.2. Incremental Distillation\n\nIncremental distillation is a technique designed to efficiently handle the continuous influx of new data without the need to retrain the student model from scratch each time new information becomes available. This method is particularly beneficial in scenarios where datasets grow over time, such as in real-time applications or when new data is periodically added, as it allows for the model to be updated incrementally [310]. The key idea behind incremental distillation is to enable the student model to learn from new data as it arrives, while also retaining the knowledge gained from previous training sessions. At each time step 𝑡, the student model 𝑆𝑡 is trained to mimic the behavior of the teacher model 𝑇, which is typically derived from the latest data [311]. A critical aspect of incremental distillation is the preservation of previously learned knowledge. As the student model is updated with new data, it is important to ensure that it does not forget what it learned in earlier stages. This is achieved by incorporating a historical distillation term history in the loss function, which measures the difference between the current student model 𝑆𝑡 and its previous version 𝑆𝑡−1. This regularization term helps in maintaining continuity and stability in the learning process, preventing catastrophic forgetting [312]. The overall loss function for incremental distillation, KD-incremental, includes two components:\n\n- KD Loss: KD(𝑇, 𝑆𝑡) ensures that the student model learns from the current teacher model at time 𝑡.\n- Historical Distillation Loss: history(𝑆𝑡, 𝑆𝑡−1) penalizes deviations from the knowledge previously acquired by the student model, helping it retain important information from past iterations.\n\nThe parameter 𝛽 controls the balance between learning new information and preserving old knowledge. A higher 𝛽 places more emphasis on retaining historical knowledge, while a lower 𝛽 allows the model to adapt more quickly to new data [313]. Incremental distillation provides significant practical benefits by allowing the student model to be updated incrementally, thereby avoiding the high computational costs associated with full model retraining. This approach is especially advantageous for large-scale applications where datasets are continuously evolving, such as in RS or streaming analytics [314]. It ensures that the model remains adaptive to new data trends, maintaining its relevance and accuracy over time. Additionally, the use of a historical distillation term enhances the stability of the learning process, minimizing the risk of performance degradation as new data is incorporated [315]. Incremental distillation can be used for real-time monitoring of deforestation in satellite imagery. As new satellite images become available, the student model is updated incrementally to learn from the latest data while preserving its ability to recognize previously identified deforestation patterns, thus allowing continuous and efficient.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 35 of 50"
  },
  {
    "id_": "1b30e5dd-3c0f-43eb-a5d4-c04d22ceda5f",
    "text": "# Application of Knowledge Distillation in Remote Sensing"
  },
  {
    "id_": "a0a2cf24-4b41-4651-a2dd-b20e5a5d450e",
    "text": "# 7.5. Real-Time Processing Enhancements"
  },
  {
    "id_": "755656fd-45d1-49a9-9f5d-ebad17abb8be",
    "text": "# 7.5.1. Real-Time Distillation Algorithms\n\nReal-time distillation algorithms are critical in scenarios where timely decision-making is essential, such as in RS applications that involve disaster monitoring or autonomous systems. These algorithms are designed to optimize both the accuracy and speed of the inference process, ensuring that the student model can deliver predictions within the stringent time constraints required for real-time operations [307]. In real-time applications, there’s a trade-off between model accuracy and inference speed. A highly accurate model may be too slow for real-time processing, while a faster model might sacrifice accuracy. Real-time distillation algorithms aim to strike a balance by training the student model to achieve an acceptable level of accuracy while ensuring that it can make predictions quickly enough to meet real-time requirements [317]. The inference time, denoted as 𝑡inference, is the time it takes for the model to process an input and produce an output. For real-time applications, this must be less than or equal to a predetermined threshold 𝑡real-time. The distillation process incorporates this requirement into the training by adding a penalty term in the loss function if the student model’s inference time exceeds the threshold. This ensures that the final model is optimized not only for accuracy but also for speed [318]. The penalty term 𝛾 ⋅ 𝕀(𝑡inference ≤ 𝑡real-time) in the loss function introduces a strong disincentive for any delay in inference time. Here, 𝕀(⋅) is an indicator function that activates the penalty whenever the inference time 𝑡inference is greater than the real-time threshold 𝑡real-time. This encourages the model to adhere strictly to time constraints, making it suitable for deployment in environments where every millisecond counts, such as in disaster response or real-time traffic management [319]. In RS, where data often needs to be processed and acted upon quickly (e.g., detecting natural disasters, monitoring climate changes, or guiding autonomous vehicles), real-time distillation ensures that models are both accurate and fast enough to be practical. This is particularly important in disaster monitoring, where delays in data processing could result in missed opportunities to mitigate damage or save lives [320]. The loss function for real-time distillation, 𝓛KD-real-time, combines the traditional KD loss with a penalty for exceeding the inference time threshold:\n\n𝓛KD-real-time = 𝓛KD(𝑇 , 𝑆) + 𝛾 ⋅ 𝕀(𝑡inference ≤ 𝑡real-time) (24)\n\nwhere 𝓛KD(𝑇 , 𝑆) represents the standard Knowledge Distillation (KD) loss, which measures how effectively the student model replicates the teacher model’s behavior. The hyperparameter 𝛾 controls the strength of the penalty applied when the inference time exceeds the real-time threshold. Additionally, the indicator function 𝕀(⋅) activates this penalty if the inference time 𝑡inference surpasses the allowed real-time limit 𝑡real-time. This formulation ensures that the student model not only learns effectively from the teacher model but also adheres to the necessary timing constraints for real-time deployment [321]. The development of real-time distillation algorithms provides several practical benefits, including the ability to make timely decisions, which is critical in applications such as disaster monitoring where delays can have severe consequences. These algorithms efficiently balance the trade-offs between speed and accuracy, ensuring models are both effective and practical for real-time use. Additionally, their versatility makes them suitable for a wide range of fields that require real-time processing, from RS to autonomous driving and other time-sensitive applications [322]. Real-time distillation algorithms can be utilized in monitoring wildfires through satellite imagery, where immediate detection and response are crucial. The student model is trained to rapidly process satellite images and detect fire outbreaks in real-time while adhering to stringent time constraints, ensuring timely alerts for emergency services and minimizing potential damage [323]."
  },
  {
    "id_": "58c592a1-b38b-43d2-a212-310a195284b0",
    "text": "# 7.5.2. Edge-AI Distillation\n\nEdge-AI distillation focuses on optimizing ML models specifically for deployment on edge devices, such as sensors, drones, or other low-power, resource-constrained hardware commonly used in RS applications [324]. These devices often have limited computational power, memory, and battery life, which necessitates the development of highly efficient models that can perform complex tasks with minimal resources. The goal of edge-AI distillation is to ensure that the distilled student model is not only accurate but also energy-efficient and capable of running with low latency on edge devices [325].\n\nThe key to achieving this lies in balancing the KD process with the energy consumption constraints of the target hardware. The distillation loss function denoted as 𝓛KD(𝑇 , 𝑆), is typically used to align the outputs of the student model 𝑆 with those of the teacher model 𝑇. However, in the context of edge-AI, an additional term, Energy(𝜃S), is introduced into the objective function to account for the energy consumption of the student model on the edge device [326].\n\nThis approach involves minimizing not just the distillation loss but also the energy consumption associated with running the student model. The regularization parameter 𝛿 is used to balance the importance of energy efficiency against the need to maintain high model performance. By carefully tuning this parameter, it is possible to develop models that are both effective in their predictive capabilities and efficient in terms of energy usage [327].\n\nEdge-AI distillation can be employed in drones for real-time wildlife monitoring, where models need to process and analyze video feeds on the device itself. By optimizing the model for low power consumption and fast inference, drones can efficiently identify and track animal movements without draining their batteries or relying on constant data transmission to central servers."
  },
  {
    "id_": "dc6fab84-b8db-48bd-8e89-f2efab566029",
    "text": "# 7.6. Cross-Modal and Multi-Modal Distillation"
  },
  {
    "id_": "5afdf0ae-847b-4ff5-8aa3-62eb01d2e41c",
    "text": "# 7.6.1. Cross-Modal Knowledge Transfer\n\nCross-modal distillation involves transferring knowledge from one modality (e.g., optical images) to another (e.g., SAR or multispectral images). This approach can improve model generalization across different data sources [328, 329].\n\nY. Himeur, et al.: Preprint submitted to Elsevier"
  },
  {
    "id_": "28225d9c-c6e0-477c-837b-61b9df8e52ab",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\n𝑀\n\nKD-cross = ∑𝑚=1𝑤𝑚 ⋅ KD(𝑇𝑚(𝑥𝑚), 𝑆(𝑥𝑚)) (25)\n\nwhere 𝑀 is the number of modalities, and 𝑤𝑚 is the weight associated with each modality 𝑚.\n\nCross-modal knowledge transfer is a specialized technique within the broader context of KD, focusing on transferring knowledge from one data modality to another. In RS, data is often collected from various sources, each providing unique and complementary information about the environment. For instance, optical images capture visible light, SAR (Synthetic Aperture Radar) images provide microwave data, and multispectral images cover a range of wavelengths beyond the visible spectrum. Each modality offers distinct advantages and limitations, making it valuable to develop models that can generalize across these diverse data types [330]. The essence of cross-modal distillation lies in the ability to leverage a teacher model trained on one modality (e.g., optical images) to improve the performance of a student model operating on a different modality (e.g., SAR or multispectral images). This process enhances the student model’s ability to generalize and perform well across different data sources, which is crucial in RS tasks that require robust performance across varying environmental conditions and sensor types [329].\n\nThe formulation of cross-modal distillation is encapsulated in the loss function KD-cross. This loss function aggregates the KD process across multiple modalities, denoted by 𝑀. For each modality 𝑚, a specific weight 𝑤𝑚 is assigned, reflecting the importance or relevance of that modality in the distillation process. The objective is to minimize the weighted sum of the KD losses KD for each modality, where 𝑇𝑚(𝑥𝑚) represents the output of the teacher model for modality 𝑚, and 𝑆(𝑥𝑚) is the corresponding output of the student model [331]. By carefully selecting and tuning the weights 𝑤𝑚, the cross-modal distillation process can be tailored to emphasize certain modalities over others, depending on the specific requirements of the application. For example, in scenarios where optical images are more informative, the weight 𝑤𝑚 for optical data can be increased, ensuring that the student model learns more effectively from that modality [332]. Cross-modal knowledge transfer is particularly beneficial in RS, where data from different modalities may be abundant but not always consistently available. By enabling models to transfer knowledge across modalities, this approach ensures that the student model remains robust and effective even when some data sources are missing or degraded. This capability is critical in applications such as environmental monitoring, disaster response, and resource management, where reliable and accurate information from diverse data sources is essential for decision-making [333].\n\nAs mentioned above, cross-modal knowledge transfer can be applied in integrating SAR and optical satellite imagery for flood monitoring. By leveraging a teacher model trained on high-resolution optical images, a student model can be optimized to accurately interpret SAR images, enhancing the detection of flood extents and water levels in areas where optical data might be obstructed or unavailable."
  },
  {
    "id_": "e113f7e9-baee-4f6a-be5b-47b84083c525",
    "text": "# 7.6.2. Multi-Task Distillation\n\nMulti-task distillation is an advanced technique in KD where a single student model is trained to perform multiple tasks simultaneously, such as classification, segmentation, and object detection. This approach aims to create a more versatile and efficient model that can handle a variety of tasks without compromising performance in any of them. By distilling knowledge from teacher models specialized in different tasks, the student model learns to balance these tasks effectively, making it highly valuable in applications where multi-functionality is essential, such as in RS or autonomous systems [334]. Multi-task distillation involves training a student model to handle multiple tasks concurrently. Each task has its own teacher model, and the student model learns from these teachers to perform all tasks simultaneously. For example, in RS, one teacher model might be specialized in land cover classification, while another is focused on detecting specific objects like vehicles or buildings. The student model is designed to learn from both these tasks, allowing it to perform land cover classification and object detection within the same framework [335].\n\nThe loss function for multi-task distillation, denoted as KD-multi-task, is a weighted sum of the distillation losses from each task. The weight 𝜆𝑡 assigned to each task allows for prioritization based on the importance or difficulty of the task. For instance, if segmentation is more critical for a specific application than classification, a higher weight can be assigned to the segmentation task to ensure the student model focuses more on that aspect [336].\n\n𝑇\n\nKD-multi-task = ∑𝑡=1𝜆𝑡 ⋅ KD(𝑇𝑡, 𝑆𝑡) (26)\n\nWhere 𝑇 represents the total number of tasks, and KD(𝑇𝑡, 𝑆)𝑡 is the KD loss for task 𝑡 between the teacher 𝑇𝑡 and the student model 𝑆. One of the key challenges in multi-task distillation is balancing the learning of different tasks. Since each task might require different levels of focus or complexity, the student model must be carefully trained to avoid underperforming in one task while excelling in another. The weight 𝜆𝑡 helps manage this balance by allowing certain tasks to have more influence on the model’s learning process [337]. By combining multiple tasks into a single model, multi-task distillation reduces the need for deploying separate models for each task, saving computational resources and simplifying the deployment process. This makes the distilled model more efficient and versatile, particularly in environments where resources are limited or where real-time processing of multiple tasks is required [338].\n\nIn practical scenarios, such as RS, a multi-task distilled model could simultaneously analyze satellite images for land classification, detect changes over time, and identify specific objects or features. This approach is not only more efficient but also enables the model to leverage shared information across tasks, leading to better overall performance and more coherent results across the different tasks [339]. While multi-task distillation offers numerous advantages, it also comes with challenges, such as the potential for task interference, where learning one task might negatively impact another. Careful design of the distillation process and appropriate weighting of tasks are essential to ensure that the student model performs well across all tasks [340]. A multi-task distilled model can be used for comprehensive.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 37 of 50"
  },
  {
    "id_": "ace8a3bb-c9c8-4d36-a489-d0ee1dac64b9",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nUrban monitoring from satellite images, where it simultaneously performs land cover classification, detects infrastructure changes (such as new buildings or roads), and identifies specific objects (like vehicles or trees). This enables efficient and integrated analysis of diverse data, enhancing the ability to track urban development and infrastructure changes in a single model [341]."
  },
  {
    "id_": "b67ee32f-9522-4d0f-909d-3aee146596f0",
    "text": "# 7.7. Seamless Integration with Existing Workflows"
  },
  {
    "id_": "4e7c5f4a-1c0e-487e-876f-56ca884d366e",
    "text": "# 7.7.1. Plug-and-Play Distillation Modules\n\nCreating plug-and-play distillation modules that can be easily integrated into existing RS workflows offers several benefits [342]. These modules are designed to be seamlessly incorporated with minimal adjustments to the current infrastructure, reducing the barriers to adopting knowledge distillation (KD) in RS applications [33]. The formulation of this integration can be expressed as:\n\n𝑀\n\nKD-plug = KD(𝑇 , 𝑆) +∑mod(𝑆, 𝑀𝑚)\n\n𝑚=1\n\nwhere KD(𝑇 , 𝑆) represents the standard KD loss between the teacher model 𝑇 and the student model 𝑆, and mod(𝑆, 𝑀𝑚) denotes the additional loss terms introduced by integrating existing modules 𝑀𝑚 with the student model 𝑆. The modularity of this approach allows different components to be swapped or upgraded independently, facilitating the adoption of KD in diverse scenarios. This scalability ensures that KD remains applicable across a wide range of RS tasks, from small UAV datasets to extensive satellite imagery. By leveraging pre-built modular KD techniques, developers and researchers can save time on implementation, accelerating the development and deployment of RS models, which ultimately enhances their performance [343, 344]."
  },
  {
    "id_": "669d4db6-7be2-46f8-888a-84833b41412b",
    "text": "# 7.7.2. Toolkits and Frameworks\n\nDeveloping comprehensive toolkits and frameworks for KD in RS can significantly enhance its performance and adoption. These toolkits provide standardized implementations of KD methods, ensuring consistency and reliability across different RS tasks [345]. The complexity of integrating various modules within the KD process can be expressed as:\n\n∑𝐶integration(𝑀, KD)\n\nToolkit Complexity ∝ 𝑁\n\n𝑖=1\n\nwhere 𝐶integration(𝑀, KD) represents the complexity of integrating module 𝑀𝑖 with the KD process. This standardization reduces the variability in performance that can arise from ad-hoc implementations, leading to more predictable results and making KD a more reliable option for RS applications. Additionally, these toolkits lower the technical barriers for practitioners by providing user-friendly interfaces and comprehensive documentation. Frameworks often include optimized routines for tasks such as hyperparameter tuning or data preprocessing, which can lead to better model performance and faster training times, especially in computationally intensive RS tasks. The community-driven development of these toolkits and frameworks leads to faster identification of bugs, new feature additions, and overall better support for the technology. This collective improvement makes KD more practical and effective for RS tasks, contributing to enhanced model performance and more efficient applications [346]."
  },
  {
    "id_": "2a9fae06-8ea4-4048-942e-782fb85fe1ef",
    "text": "# 7.8. Enhancing Model Interpretability"
  },
  {
    "id_": "63bc25b9-d579-4449-b92d-e60f664fa71b",
    "text": "# 7.8.1. Explainable Distillation\n\nExplainable distillation is an advanced approach in KD that not only focuses on transferring the predictive performance of the teacher model to the student model but also ensures that the student model’s decision-making process is interpretable. The goal is to create models that are not just accurate but also transparent, providing insights into how they arrive at their predictions [347]. This is particularly important in critical applications like RS, healthcare, and autonomous systems, where understanding the model’s reasoning is crucial for trust and accountability [348]. Explainable distillation mainly relies on the following key concepts. Traditional KD emphasizes performance, often at the cost of interpretability. However, in many applications, it is essential to know why a model makes a certain decision. Explainable distillation aims to bridge this gap by integrating interpretability into the distillation process. The student model is trained not only to mimic the teacher’s outputs but also to generate explanations for its decisions that are comprehensible to humans [349]. The process of explainable distillation involves an additional term in the loss function, denoted as:\n\nKD-explain = KD(𝑇 , 𝑆) + 𝜉 ⋅ explain(𝑆) (27)\n\nwhere, KD(𝑇 , 𝑆) is the standard KD loss, and 𝜉 is a regularization parameter that controls the trade-off between accuracy and interpretability. The term explain(𝑆) guides the student model toward generating explanations that are either inherently interpretable or match the explanations provided by the teacher model if available [348]. In domains such as RS, where decisions based on model predictions can have significant real-world impacts, the ability to explain model outputs is critical. For example, when a model identifies a potential disaster area in satellite imagery, it is not enough to simply flag the area; stakeholders need to understand the reasoning behind the decision, such as the specific patterns or features that led to the prediction [349]. Various explainable AI techniques can be incorporated into the distillation process, such as saliency maps, attention mechanisms, or feature attribution methods [351]. These techniques help to visualize and understand which parts of the input data are most influential in the model’s decision-making process. By integrating these techniques into the distillation process, the student model becomes not only a distilled version of the teacher but also a more interpretable and transparent model [352].\n\nAll in all, explainable distillation holds significant potential in fields where trust in AI systems is paramount. By producing models that are both accurate and interpretable, it enhances the usability and acceptance of AI systems in sensitive areas. Moreover, it aids in regulatory compliance.\n\nY. Himeur, et al.: Preprint submitted to Elsevier Page 38 of 50"
  },
  {
    "id_": "d84b4084-acad-43d4-bbbf-439fa0aa4879",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nAs interpretable models can more easily meet legal and ethical standards regarding AI transparency [353]. Besides, while explainable distillation is a promising approach, it comes with challenges, such as defining and quantifying interpretability in a way that aligns with both human understanding and model performance. Future research may focus on developing more sophisticated explainability metrics and integrating them seamlessly into the distillation process, ensuring that models are not only effective but also understandable and trustworthy [354]."
  },
  {
    "id_": "d990d5df-818c-42f7-8f7a-34842c9dc9bb",
    "text": "# 7.9. Hybrid Approaches"
  },
  {
    "id_": "ae8d892e-8ff6-4b25-9fe6-90a30f463da5",
    "text": "# 7.9.1. Combining Distillation with Other Techniques\n\nHybrid approaches in ML involve the integration of multiple learning paradigms to create more powerful and versatile models. When applied to KD, these hybrid approaches can significantly enhance the performance and efficiency of models, especially in complex and data-rich fields like RS [360]. For instance, a hybrid approach relies on combining KD with transfer learning and federated learning. Typically, transfer learning involves leveraging knowledge gained from one task or domain to improve the performance on a related but different task. In the context of hybrid approaches, transfer learning can be integrated into the distillation process to help the student model acquire additional knowledge from pre-trained models on related tasks [361]. The transfer learning loss component, 𝓛Transfer(𝑆), represents the cost associated with adapting the student model to the new task or domain. Moving on, reinforcement learning is a paradigm where models learn to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties. In hybrid distillation approaches, RL can be incorporated to optimize the student model’s performance through trial and error, especially in scenarios where decision-making under uncertainty is critical. The RL loss component, 𝓛Reinforce(𝑆), measures how well the student model performs in achieving its objectives within the environment [362].\n\nBesides, active learning is another technique that can be integrated with KD. It involves selectively querying the most informative data points for training, thereby improving the model’s performance with fewer labeled examples. While not explicitly included in the equation, active learning can complement the distillation process by ensuring that the most critical data points are used for training the student model [363, 364]. The hybrid loss function in this approach combines the losses from KD, transfer learning, and reinforcement learning:\n\n𝓛KD-hybrid = 𝛼⋅𝓛KD(𝑇 , 𝑆) + 𝛽⋅𝓛Transfer(𝑆) + 𝛾 ⋅𝓛Reinforce(𝑆) (29)\n\nIn this context, 𝛼, 𝛽, and 𝛾 are weighting factors that determine the contribution of each component to the overall loss function. The term 𝓛KD(𝑇 , 𝑆) ensures that the student model closely mimics the behavior of the teacher model, while 𝓛Transfer(𝑆) aids the student model in adapting to new tasks or domains by utilizing previously acquired knowledge. Additionally, 𝓛Reinforce(𝑆) enhances the student model’s decision-making abilities through interaction with an environment, thereby optimizing its performance [365].\n\nBy integrating multiple learning paradigms, hybrid approaches can create models that are not only smaller and faster (through distillation) but also more knowledgeable and capable (through transfer learning) and more adaptive (through reinforcement learning). Hybrid models are better.\n\nY. Himeur, et al.: Preprint submitted to Elsevier\n\nPage 39 of 50"
  },
  {
    "id_": "0a71a139-42e0-4dd8-8433-3277291cfa74",
    "text": "# Application of Knowledge Distillation in Remote Sensing\n\nEquipped to handle a variety of tasks and environments, as they combine the strengths of different learning methods. This is particularly useful in RS, where data can vary widely in type, quality, and context. The ability to combine different learning strategies makes hybrid approaches highly versatile. For example, a hybrid model might use transfer learning to understand basic image recognition tasks while using reinforcement learning to make real-time decisions based on that understanding [366].\n\nHybrid distillation approaches are particularly relevant in complex fields such as RS, where models need to process and analyze vast amounts of data from multiple sources. For instance, in disaster monitoring, a hybrid model could use transfer learning to recognize different types of terrain, reinforcement learning to predict the spread of a fire, and KD to ensure the model is efficient enough to run on edge devices deployed in the field [367]. However, combining multiple learning paradigms can increase the complexity of the model and its training process. Careful tuning of the weighting factors (𝛼, 𝛽, 𝛾) is necessary to achieve the desired balance between the different learning objectives. Moreover, while the goal of distillation is to create efficient models, the integration of transfer learning and reinforcement learning can demand additional computational resources during training, which may limit the scalability of the approach [368]."
  },
  {
    "id_": "7915cd98-2312-484a-bc7e-259434692a06",
    "text": "# 7.9.2. Adaptive Distillation Frameworks\n\nDeveloping adaptive distillation frameworks that can adjust the distillation process based on the complexity of the task or the availability of data could lead to more flexible and robust models. Traditional knowledge distillation methods apply a fixed strategy for transferring knowledge from the teacher model to the student model, which might not be optimal for all scenarios, particularly in RS where data heterogeneity and varying task requirements are common [269]. An adaptive distillation framework dynamically adjusts the distillation process by incorporating additional mechanisms that account for task complexity, data quality, or computational constraints. This dynamic adjustment allows the distillation process to be more responsive to the specific needs of the application, improving both the efficiency and effectiveness of the student model [369].\n\nFor instance, in scenarios where the task is relatively simple or where data is abundant and high-quality, the framework could prioritize rapid model convergence by assigning higher weights to the knowledge distillation loss. Conversely, in more complex tasks or when dealing with sparse or noisy data, the framework could allocate more resources to the adaptation process, fine-tuning the student model to handle these challenges more effectively [370]. The adaptive distillation process can be represented as:\n\n𝓛KD-adaptive = ∑t=1T 𝛼t ⋅ 𝓛KD(𝑇t, 𝑆) + 𝜇t ⋅ 𝓛adaptive(𝑆)\n\nwhere 𝛼t and 𝜇t are time-varying weights for the KD and adaptation loss components, respectively. These weights are not static but rather evolve over time 𝑡 based on factors such as the current performance of the student model, the difficulty of the current task, and the quality of the data available at each step [371].\n\nThe term 𝓛KD(𝑇t, 𝑆) represents the traditional knowledge distillation loss, which measures the discrepancy between the outputs of the teacher and student models. The term 𝓛adaptive(𝑆) is an additional adaptation loss that could include penalties for model complexity, regularization terms to prevent overfitting, or other factors that enhance the student model’s ability to generalize across different tasks or datasets [372]. By allowing the distillation process to adapt to varying conditions, these frameworks can produce student models that are not only smaller and faster but also more versatile and capable of maintaining high performance across a range of tasks and environments. This adaptability is particularly valuable in RS, where the conditions under which models operate can vary significantly, and the ability to generalize effectively is crucial [373]."
  },
  {
    "id_": "8caee388-1178-4dfb-a94e-577be873bd1e",
    "text": "# 8. Conclusion\n\nThis review provides a comprehensive analysis of knowledge distillation (KD) and its applications in remote sensing (RS), making significant contributions to the understanding and advancement of this technique. The review begins by offering a detailed overview of the fundamentals of KD, including its definition, basic concepts, historical evolution, and underlying mechanisms. The advantages of KD, such as model compression, improved efficiency, and enhanced performance in smaller models, are highlighted, particularly in the context of RS tasks like image classification, object detection, land cover classification, and semantic segmentation. A key contribution of this review is the taxonomy of KD models, which categorizes the variations in the models and input data, the type of knowledge transferred, the distillation target, and the structural relationships between network layers. This taxonomy serves as a valuable resource for researchers and practitioners, providing a clear framework for understanding the diverse applications and implementations of KD in RS.\n\nHowever, the review also addresses the challenges and limitations of applying KD in remote sensing. These challenges include the complexity of model deployment, the heterogeneity of data sources, overfitting and generalization issues, scalability, real-time applicability, dependency on high-quality data, and the difficulty in balancing efficiency and accuracy. These challenges underscore the need for continued innovation and refinement of KD techniques to meet the unique demands of remote sensing applications. Moving forward, the review identifies several important future directions for the field. These include the development of advanced model compression techniques, dynamic and layer-wise distillation, efficient training algorithms, and hardware-aware distillation. The importance of improving data quality and robustness through robust distillation against noisy data and semi-supervised or unsupervised approaches is also emphasized. Additionally, the review suggests scalability solutions like distributed and incremental distillation, real-time processing enhancements, cross-modal and multi-modal distillation, and the seamless integration of KD into existing workflows through plug-and-play modules and standardized toolkits.\n\nFinally, the review calls for efforts to enhance model interpretability through explainable distillation and feature."
  }
]
