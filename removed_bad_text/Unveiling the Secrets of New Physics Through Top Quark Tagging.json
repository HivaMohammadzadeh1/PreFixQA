[
    {
        "id_": "fb9f8f60-24b5-4653-a572-607f4065dbcb",
        "text": "# 1 Introduction\n\nThe Large Hadron Collider (LHC) [1] at CERN represents the pinnacle of scientific engineering, dedicated to unraveling the fundamental constituents of nature. This proton-proton collider, designed to probe the tiniest structures within a controlled laboratory setting, delivers collisions at unparalleled center-of-mass energies: 7 and 8 TeV for Run I, 13 TeV for Run II, and 14 TeV from Run III onwards. Although protons are not elementary particles themselves, they are composed of quarks and gluons. Quantum Chromodynamics (QCD), the theory governing strong interactions within the Standard Model (SM) of particle physics, describes the interaction of quarks and gluons. Despite the challenge of directly detecting free quarks and gluons due to color confinement, they remain pivotal in discussions regarding high-energy hadron collider phenomenology. When quarks or gluons are produced at high energies, they promptly fragment and hadronize, resulting in a collimated spray of energetic particles known as a jet. By measuring the energy and direction of these particles (the jet) one can glean insights into the properties of the original parton. Defining a jet involves a prescription (jet algorithms) [2–9] to group hadrons into jets and assign momentum to the resulting jet. In addition to the collimated beam of hadrons resulting from the hadronization of light quarks and gluons (light jets), the hadronic decay of boosted heavy SM particles such as the W/Z-boson, Higgs boson (h), or top quark also leads to a collimated spray of hadrons that can resemble a single jet."
    },
    {
        "id_": "8e03bb20-aa04-45ba-b917-70ae79b91ed2",
        "text": "# Review of Fat Jets from Boosted Top Quarks\n\nSince its inception, the LHC has pursued evidence of physics beyond the SM (BSM). Despite the remarkable discovery of the Higgs boson [10, 11], which validated aspects of the SM, the absence of concrete evidence supporting BSM physics has prompted researchers to venture into higher energy regimes. These enhanced energies enable the production of boosted heavy SM particles like the top quark, W/Z-boson, and Higgs boson. The hadronic decays of these boosted SM particles result in a collimated cluster of quarks, forming large-radius (large-R) jets known as “fat jets” with distinctive sub-structure features. There are several advantages to designing search strategies for heavy BSM resonances that decay into highly Lorentz-boosted massive SM particles when hadronic decays of these boosted particles are considered. For one, the hadronic decays of these particles result in a higher signal rate compared to their leptonic decays. Further, the visibility of the hadronic decay products of these particles at the LHC detectors enables the kinematic reconstruction of the decay cascade. More importantly, only a small fraction of the SM background events would give rise to fat jets in the final state. Consequently, fat jet final state signatures are beset with considerably less SM background than those with resolved jets. As a result, the analysis of jet substructure at the LHC resulting from the hadronic decay of boosted top quarks, W/Z-bosons, or the Higgs boson has been instrumental in searching for heavy BSM resonances across various new physics scenarios, including supersymmetry [12–16], extra-dimensional models [17, 18], leptoquark models [19, 20], and other extensions of the SM [18, 21–29]. Efficiently identifying the particle origin of fat jets is crucial to enhancing the sensitivity of the LHC and future colliders. This necessitates a significant shift in analysis strategy and the development of new innovative methodologies for tagging the particle origin of the fat-jets.\n\nIn this review, our focus is on classifying fat jets resulting from the hadronic decay of boosted top quarks and distinguishing them from light quarks and gluon jets (hereafter referred to as QCD jets). Top quarks at the LHC are particularly intriguing due to the substantial tt¯ production cross-section, essentially making the LHC a ”top factory”. The millions of top quarks produced at the LHC are expected to provide insights into the SM and its potential extensions. While most top quarks are produced near threshold and can be identified using traditional top reconstruction algorithms similar to those used at the Tevatron, some top quarks produced at the LHC are highly boosted. Theoretical interest in top quarks is heightened by their large Yukawa coupling. The large top Yukawa coupling not only plays a critical role in computing electroweak precision observables [30] and determining the vacuum stability [31] of the Standard Model (SM), but it also has a notable impact on the masses and interactions of various BSM resonances. Many of these resonances exhibit enhanced couplings with the top quark, contributing to a final state rich in top quarks at the LHC. Over the past decade, considerable efforts have been made in the literature to develop effective methods for efficiently distinguishing boosted top quark jets from QCD jets. While early literature introduced cut-based strategies for boosted top tagging [32, 33], leveraging substructure information from fat jets resulting from the hadronic decay of boosted top quarks, recent years have seen a surge in the adoption of machine learning-based approaches for top-jet classification. In this article, we provide a comprehensive review of various cut-based and machine learning-based approaches proposed in the literature over the past couple of decades for top-tagging. This review aims to synthesize the advancements in top-jet classification methodologies, highlighting their evolution and effectiveness in distinguishing top quark jets from QCD jets at high-energy collider experiments like the LHC."
    },
    {
        "id_": "0d38a63a-6843-4f93-845f-1d27cfe0e623",
        "text": "# Organization of the Review\n\nThe review is organized as follows: In the next section, we review high-level feature (HLF) based top classifiers. Sections 3 and 4 are dedicated to Image-based Classifiers and Graph Neural Network (GNN) classifiers, respectively. In section 5, we provide a list of BSM scenarios that result in boosted top quark final states at the LHC. Finally, we summarize our findings in section 6."
    },
    {
        "id_": "4faf82f3-1f82-4ab5-abd3-2e67b6b03de2",
        "text": "# 2 High-Level Feature (HLF) based classifiers\n\nTagging boosted objects is a long-pursued quest dating back to the eras of the Tevatron. These boosted objects (or, in the context of our discussion, boosted fat jets) can have different origins ranging from decays of SM bosons (W/Z/H), the top quark, light quarks/gluons, or some BSM particles. As discussed in the introduction, we will focus our attention on the tagging of boosted top jets, i.e., identifying fat jets originating from hadronic decay of top quarks from those originating from QCD-initiated light quarks and gluon jets. Initial works in this direction rely on identifying b-jets inside the top jet and the reconstruction of the invariant mass of the W-boson inside the top and top quark mass as a whole. The problem with this approach was the isolation of the b-jet and light jets, which are highly collimated due to the boosted mother particle. This makes the procedure inefficient in scenarios where the production cross-section of these fat jets is small. This led to an in-depth investigation of the jet sub-structure and resulted in the development of several jet substructure variables/high-level features (HLFs).\n\nThe literature on jet substructure variables is vast. Some interesting examples include the jet energy moments [34], the energy correlation functions (ECFs) [35], the generalized energy correlation functions (ECFGs) [36], N-subjettiness variables [37–39], and Energy Flow Polynomials [40]. For a comprehensive discussion, we direct the interested readers to reference [41–47]. We have hand-picked some of these and some physics-inspired algorithmic approaches for the subsequent discussion.\n\nThe Johns Hopkins Top Tagger (JHTT), originally introduced in reference [33], looks into the subjet structure of the fat jet, applying additional kinematic criteria to identify fat jets originating from top quarks. To begin with, C/A fat jets with a given R parameter are considered. These fat jets undergo a sequential declustering procedure. First, the fat jet J is declustered into two subjets, j1 and j2. If the softer subjet is discarded if it has pT,j < ωp → pT,J, for some predefined ωp while the harder subjet undergoes further declustering until certain conditions are met (if both subjets do not fulfill above criteria they both are considered for further analysis). These conditions include the subjet being comprised of a single calorimeter cell, the daughter jets from declustering being too close (|ε| + |ϑ| < ωr), or both daughter jets satisfying the initial criteria of pT,j < ωp → pT,J. Fat jets with 3 or 4 subjets are considered for further analysis. For their final analysis, reference [33] uses SM top pair production and QCD di-jet production for generating signal and background fat jets in Pythia [48]. To incorporate detector effects, final state visible particles are combined in grids of size 0.1 → 0.1 and passed onto the clustering algorithm. Only fat jets with pT > 500 GeV and |ε| < 2.5 are retained. Different R-parameter values are adopted depending on the event’s scalar ET. For ET > 1000, 1600, 2600 GeV they select R=0.8, 0.6, 0.4, ωp = 0.1, 0.05, 0.05, and ωr = 0.19, 0.19, 0.19, respectively. Moreover, the fat jets must also satisfy pT,J > 0.7 → ET/2. Once the final subjets are identified, they pass through some kinematic requirements. For jets with pT < 1000 GeV, these amounts to the requirement that the invariant mass of the final subjets must be within 145-205 GeV, and there must be two subjets with invariant mass in the window of 65 to 95 GeV. For fat jets with pT > 1000 GeV, the upper window is shifted to pT/20 + 155 GeV and pT/40 + 70 GeV respectively for top and W mass reconstruction. Additionally, the reconstructed W helicity angle must adhere to cosϖh < 0.7 in both scenarios. For a detailed discussion, we encourage the interested reader to refer to the original paper [33]. For completeness, we present their final results in Figure 1.\n\nN-subjettiness is an inclusive jet shape originally introduced in reference [37]. It is designed to identify the energy deposition pattern inside a fat jet and quantify the major sources of these energies. In simpler terms, it quantifies the number of prongs of a fat jet. To define N-subjettiness,"
    },
    {
        "id_": "2af4b6e8-be7e-457c-bdba-40682c60f257",
        "text": "# Figure 1:\n\nThe variation of top tagging efficiency and the mis-tagging efficiencies of gluon and light-quark jets with the transverse momentum of the fat jets [33].\n\n|pT (1 GeV)|Tagging Efficiency|\n|---|---|\n|600|0.5|\n|800|0.4|\n|1000|0.3|\n|1200|0.2|\n|1400|0.1|\n|1600|0.0|\n|1800| |\n|2000| |\n\nOne needs information on the N candidate subjets inside a fat jet. To remain IRC safe, reference [37] employs the exclusive-k T algorithm for this task. Once these subjets are known, the N-subjettiness can be calculated via the relation [37]:\n\nϱ N = ∑k p T,k min{!R 1,k , ...!R N,k } / ∑k p T,k R 0 (2.1)\n\nHere, R 0 is the R-parameter of the fat jet clustering algorithm. The sum runs over all the jet constituents of momentum p T,k and !R N,k represents the separation in the rapidity-azimuth plane between the N th subjet and the kth constituent. In an ideal setting, ϱ N ↑ 0 when all the constituents are aligned with some candidate subjet.\n\nAs argued in reference [37], the ratio of N-subjettiness variables is a better candidate for fat-jet tagging and will be used in the subsequent discussion. To test the effectiveness of the ϱ N variable, reference [37] has demonstrated its application in separating top jets from QCD background jets. After generating top-pair events in Pythia-8.135 [49], a simple detector simulation is performed where visible final state particles with ε < 4 are combined to form 0.1 → 0.1 sized calorimeter cells. These cells are assumed massless and are used in Fastjet 2.4.2 [50] for reconstructing the fat jets.\n\nFat jets in different ranges of transverse momenta (with ε < 1.3) clustered using different values of R 0 are considered for the final analysis. To effectively suppress the QCD background, an additional criterion of 145 GeV < mjet < 205 GeV is imposed on the fat jet mass. For a detailed discussion on the signal and background event generation, we direct the interested reader to reference [37]. For our discussion, we present their results in figure 2.\n\nThe HEPTopTagger (HTT), initially introduced in reference [53], has undergone several modifications in subsequent works [54, 55]. For our discussion, we adopt the version outlined in reference [55], known as HEPTopTagger2 (HTT2). While HTT2 shares some steps with the Johns Hopkins Top Tagger, like using C/A jets and gradual declustering to identify relevant subjets, it incorporates several physics-motivated procedures to enhance top tagging performance. Unlike the JHTT, HTT2 addresses both low (p T > 200 GeV) and high (p T > 600 GeV) p T fat jets. A complete description"
    },
    {
        "id_": "c7cc92b7-e63d-4a1d-9719-d7955de5cac2",
        "text": "# Top jets vs. QCD jets"
    },
    {
        "id_": "e9b05781-4b11-440b-94bf-07f2564d2642",
        "text": "# Top jets vs. QCD jets, 450 GeV < p < 600 GeV\n\n| |Tagging efficiency| | |\n|---|---|---|---|\n|0.7|0.7|0.7| |\n|0.6|0.6|0.6| |\n|0.5|0.5|0.5| |\n|0.4| |0.4|B|\n|0.3|0.3|0.3| |\n|0.2|pT = 450−600 GeV|0.2| |\n|0.1|pT = 600−750 GeV|0.1|R = 0.6|\n| |pT = 750−900 GeV|R = 0.8|0.1|"
    },
    {
        "id_": "5e3c28ab-d421-4f1c-acb1-737adcd83d41",
        "text": "# Mistagging rate\n\n| | |Mistagging rate| | | | | | | | | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |0|0.05|0.1|0.15|0.2| | | | | | | | | | |\n| |0|0.05|0.1|0.15|0.2| | | | | | | | | | |\n| |0| | | | | | | | | |0.02|0.04|0.06|0.08|0.1|\n\nFigure 2: The variation of top tagging efficiency with the mis-tagging rate of gluon and light-quark jets. The plot on the left shows the variation of classification performance with pT of the fat jet. The plot in the middle shows the dependence on the radius of reconstruction for fat jets in the transverse momentum range of 450-600 GeV. The rightmost plot compares the performance of the N-subjettiness tagger with that of the Johns Hopkins Top Tagger (JHTT) and the YSplitter method.\n\nThe algorithm is beyond the scope of our discussion, and we direct the interested reader to ref [55] for a comprehensive discussion. Here, we only highlight its important features. The algorithm starts with a large radius (R=1.8) C/A fat jet and implements a mass drop criterion to identify the relevant subjets. It then enforces a resolution cut and selects the five hardest subjets, which are further clustered into three candidate subjets meeting specific mass criteria mrec ↓ [150, 200] GeV. To qualify as top constituents, the paired invariant masses of these three subjets are required to satisfy some additional conditions. If multiple triplets satisfy these criteria, the one with mrec closest to the top pole mass is kept for further analysis. The transverse momentum of the reconstructed top jets must satisfy pT > 200 GeV. To optimize performance, four Boosted Decision Trees (BDTs) are trained, each focusing on distinct input features, and accordingly, they are named variable masses, OptimalR, N-subjettiness, and Qjets. For more details, see ref [55]. We present their classification performance for pT > 200 GeV and pT > 600 GeV fat jets in figure 3."
    },
    {
        "id_": "9040b949-d3dd-4a34-bf5a-244f356eaabe",
        "text": "# Figure 3: The ROC curves for the four BDT classifiers\n\nFor comparison, the performance of the previous version of HTT is shown as a star pointer. The plot on the left is for fat jets with pT > 200 GeV while that on the right is for pT > 600 GeV.\n\n| |variable masses (17)| |variable masses (17)|\n|---|---|---|---|\n|10|optimalR (18)|10|optimalR (18)|\n|10|N-subjettiness (20)|10|N-subjettiness (20)|\n|10|Qjets (21)|10|Qjets (21)|\n|2| |2| |\n|10| |10| |\n|10| |10| |\n|0|pT > 200 GeV|0|pT > 600 GeV|\n\n– 5 –"
    },
    {
        "id_": "ea6031b5-da80-4b7c-8c29-da5b5f890603",
        "text": "# Energy Flow Polynomials (EFPs) for jet tagging\n\nEnergy Flow Polynomials (EFPs) for jet tagging were originally introduced in reference [40]. They form a linear basis for IRC-safe observables, making them suitable for use with linear classifiers. As demonstrated in ref [40], EFPs have a one-to-one correspondence with loopless multigraphs. This characteristic makes constructing the basis of EFPs much simpler. It also allows the truncation of the infinite number of EFPs at any particular order determined by the number of vertices in the multigraph. In terms of a multigraph G with N vertices, the EFPs take the form [40]:\n\n∑...M    ∑M                 ∏\nEF P G =                   z i 1 ...zi N       ϖ i k li                              (2.2)\ni 1 =1  i N =1            (k,l)→G\n\nwhere (k, l) ↓ G are the edges of the multigraph. M denotes the number of particles in the jet, z i is the energy fraction, and ϖ ij denotes the angular distance. The choice of z i and ϖ ij depends on the specific collider and the problem. For the subsequent discussion, the following choices are made [40]:\n\nz i = p T,J,\n=∑p T,iM\np T,i             p T,J                                               (2.3)\ni=1\n\nϖ ij = (!y ij+ !ϑ ij 2 2) ω/2                                                       (2.4)\n\nwhere !y and !ϑ denote the difference in rapidity and azimuth of the two particles, the exponent ς is fixed at 0.5. To determine the effectiveness of EFPs in discriminating top fat jets from quark/gluon-initiated jets, SM top pair and dijet events are generated using Pythia 8.226 [56]. The final state visible particles are clustered into R = 0.8 anti-k T jets. The leading fat jet in an event satisfying 500 GeV < p T < 550 GeV and |ε| < 1.7 are kept for the final analysis. All EFPs up to degree d ↔ 7 are computed for these events. These EFPs are used as input in a Fisher’s linear discriminant [57] as well as a three-layered DNN. Additional classifiers based on the N-subjettiness variable and jet images are also designed to compare the classification performance. We direct the interested readers to reference [55] for a complete description of these classifiers. Here, we present the performance of the classifiers mentioned above in Figure 4."
    },
    {
        "id_": "cf5bbe49-e346-4e1a-949c-fcdd9a8c04e6",
        "text": "# 3 Image-based Classifiers"
    },
    {
        "id_": "33ce38eb-2a19-4b71-aad3-d89ad23fc052",
        "text": "# 3.1 Image representation of jets and Preprocessing\n\nJet image, like the images we see in our day-to-day lives, is a grid of numbers. Each grid cell is called a pixel, and the number associated with each pixel is called the pixel intensity. Pixels in jet images can have various sources. The simplest and most common source is the calorimeter tower, defined as the sum of energy deposits in a given rapidity-azimuthal angle bin of the Electromagnetic and hadronic calorimeter. Another source can be the topoclusters [58], i.e., the clusters of calorimeter cells. The topoclusters help as most jet images are sparse with many empty pixels, and combining these empty pixels helps build a smaller image that requires a simpler neural network architecture. Finally, tracks can also be used as a source of pixels. Now, tracks have a much finer granularity than the towers and hence can provide much better recognition of physical features present in a jet. At the same time, analyzing a large image is computationally expensive. Therefore, a bunch of tracks in a given ε ↗ ϑ direction are usually grouped to form simpler jet images. The CMS collaboration also uses ParticleFlow [59] algorithms to identify calorimeter energy deposits originating from charged particles (i.e., with associated charged tracks). These particle flow candidates can also be used to construct jet images."
    },
    {
        "id_": "05da350f-0b90-41ad-bb89-76cedc1a71f5",
        "text": "# Inverse QCD Jet Mistag Rate"
    },
    {
        "id_": "9fbe9b02-443d-410a-9cb1-d2a02d18ea0a",
        "text": "# Top vs. QCD"
    },
    {
        "id_": "307d4364-4922-45b1-a542-4f020985ddc5",
        "text": "# Pythia 8.226, ps = 13 TeV"
    },
    {
        "id_": "de934f08-e6da-4898-8257-848548da6bc1",
        "text": "# R = 0.8, pT 2 [500, 550] GeV\n\n| |EFPs, Lin.|EFPs, DNN|gray CNN|color CNN| | |\n|---|---|---|---|---|---|---|\n|Top Jet Efficiency| | | | | | |\n| |10.0|0.2|0.4|0.6|0.8|1.0|\n\nFigure 4: The ROC curves for the EFP-based (red) liner (solid) and DNN (dotted) classifiers and the ROC curves of N-subjettiness (blue) and image-based (gray) classifiers [40].\n\nJet images can be uni-layered (grayscale images) or multi-layered (colored images). Colored images usually contain several layers originating from the sources mentioned above. Each layer corresponds to a different part of the detector, providing better insight into the jet content. Construction of such images can be complicated as different parts of the detector can have different resolutions, and all layers of the images must be of similar shape. In such situations, the pixels coming from the high-resolution source are usually grouped to match the resolution of the other source. This may lead to the loss of important information.\n\nAfter the images are formed, they are passed through a series of pre-processing steps before being passed into any neural network. These preprocessing steps remove any redundant information in the images, thereby helping the neural network learn important characteristics of the jet more efficiently. Apart from stabilizing the training process, it improves the classifier’s performance by simplifying the training data. However, these preprocessing steps can sometimes lead to the loss of important information and may hamper the classifier’s performance.\n\nFour common pre-processing steps are usually employed in jet image studies: Translation, Reflection, Rotation, and Normalization. In translation, all the jet constituents are shifted so that the constituent with the highest transverse momentum lies at the origin of the (ε, ϑ) coordinate system. Now, translation along the ϑ direction does not cause any additional trouble as there is no preferred direction in the transverse plane, and the physics should remain the same when moving the system as a whole along ϑ. Translation in the ε direction, on the other hand, is equivalent to a longitudinal boost along the beam axis. Therefore, if quantities like energy deposit are used as pixel intensities, which are not invariant to longitudinal boost, important jet characteristics, like the invariant mass, may change. Therefore, quantities like transverse momentum or transverse energy are usually preferred. In rotation, the entire jet is rotated such that the second highest pt constituent or the principal component of the pixel intensity distribution becomes vertical."
    },
    {
        "id_": "4702115f-9ea5-47b8-bf55-db6c0dfc4b35",
        "text": "# 3.2 Convolutional Neural Network\n\nThough image representation of jets can be used as inputs to several NN-based architectures (See [60, 61] for the use of locally connected layers on image data), the most common and efficient ones are CNNs. This popularity is due to the translational invariance of convolution operations and the weight-sharing technique employed by CNNs (see the discussion below).\n\nCNNs use convolutional filters/grids containing weights that scan the input image. This scanning involves performing the inner product between different patches of the image (of the same dimension as the filter) and the filter. The output of this inner product serves as a new pixel for the output image. Once an inner product is performed, the filter is moved by certain units called stride, producing another pixel. This process is repeated till the full image is covered. The output pixels are combined to produce the response map. For uni-layered images, these filters are two-dimensional. On the other hand, for colored images, three-dimensional filters with depth equal to the number of image layers are used. The output of each convolution operation is one single layer of the response map; the response map’s depth is determined by the number of filters used. The use of the same filter for different parts of the image is the weight-sharing mechanism mentioned earlier, and it is highly efficient in reducing the number of trainable parameters. After the convolution operation, the response map is passed through a non-linear activation to produce the activation map.\n\nThe convolution operation produces output images with sizes smaller than the input. This can cause trouble in constructing very deep networks. To overcome this, padding is performed where some empty pixel layers are added to the edges of the input image. This is usually achieved with the help of some pooling operation, usually max pooling or mean pooling. The pooling operation resembles the convolution, where a squared grid is moved across the image to produce the final output. However, the difference is the pooling filter does not contain any weights. Its only job is to combine the pixels through the pooling operation. After many such convolution and pooling operations, the output image is linearised and is passed through a series of fully connected layers called the decoding network to produce the final output (either a classification score or a regression value).\n\nAnother problem commonly encountered in deep CNNs is the so-called vanishing gradient problem (VGP). The updation of network parameters (weights and biases) in NNs happens through backpropagation, which implements the chain rule to determine the gradients of the loss function with respect to the network parameters. The network uses these gradients with the learning rate (which determines the step size) to update the parameters that minimize the loss function. In very deep NNs, the updating of parameters in initial nodes involves a product of a large number."
    },
    {
        "id_": "b660366d-af86-4583-a5d1-c1bf1703c322",
        "text": "# 3.2 Techniques to Address Vanishing Gradient Problem\n\nThis may generate vanishing gradients, where gradients in earlier layers become very small during training. This phenomenon can slow down the learning process or hinder the training of deep networks. While parameter updating typically continues in neural networks despite vanishing gradients, they can affect convergence speed and overall training effectiveness. Over the years, several techniques have been developed to address the vanishing gradient problem (VGP) in neural networks. A brief discussion on some of these techniques is in order.\n\n- Unitary matrices for weight initialization [62–64]: Since the eigenvalues of a unitary matrix are unimodular and thus can be raised to arbitrary power without vanishing, using unitary matrices helps stabilize the training process.\n- Artificial Derivatives [65]: For sigmoid and ReLU activation functions, artificial derivatives can amplify the loss function’s derivatives when they approach zero, thereby addressing the VGP effectively.\n- Specialized RNN architectures: Long short-term memory (LSTM) networks [66] and gated recurrent units (GRUs) [67] use a gating mechanism to maintain and control the flow of gradient over long sequences, facilitating stable learning.\n- Weight initialization methods: Using methods like Xavier [68] or He [69] appropriate to the activation function ensures proper gradient scaling during backpropagation.\n- Gradient clipping: This technique limits the gradients’ magnitude, preventing them from becoming too large or too small.\n- Batch Normalization [70]: By normalizing the inputs of each layer, batch normalization improves the stability and convergence during training.\n- Residual connections [71]: The input to the network is passed to the output through a skipped connection. This means instead of the usual output f(x) (x being the input and f being a combination of operations like pooling, convolution, etc.), the new output becomes x + f(x). Note that sometimes the input has to undergo additional operations to change its size to be combined with f(x).\n\nOut of these techniques, appropriate weight initialization, batch normalization, gradient clipping, and residual connections are commonly employed with CNNs."
    },
    {
        "id_": "ad02f63b-8bac-4894-aa1e-5e51e3c3fb36",
        "text": "# 3.3 CNN Architectures\n\nThis section discusses some of the state-of-the-art CNN architecture for top tagging. We plan to present our study in chronological order, mainly focusing on five image-based analyses: DeepTop [72], Upgraded Deeptop [73], ResNext-50 [74], CapsNet [75], and Bayesian networks [76]. In all the studies discussed below, the SM top pair production is used to generate the signal images, and QCD di-jet production provides the background sample unless mentioned otherwise.\n\nThe DeepTop CNN was originally introduced in reference [72]. It uses single-layered top images based on the calorimeter energy deposit as input. Signal and background fat jets in the transverse momentum range of 350-450 GeV and |ε| < 1 are considered for the analysis. After detector simulation, the anti-kT fat jets with R=1.5 are reconstructed, and the calorimeter towers are used for image construction with the transverse energy serving as pixel intensity. The images are passed through a series of pre-processing steps: pixelization, translation, rotation, reflection, and scaling. We discussed translation, reflection, and rotation in the previous sections. In pixelization, the image is divided into 40 × 40 pixels, while scaling ensures all pixel intensities are in the range of 0."
    },
    {
        "id_": "02685d13-a689-4725-bcfe-208370483488",
        "text": "# 1. Analysis of DeepTop Tagger\n\nTwo types of images are considered for the final analysis: one that does not incorporate the reflection and rotation preprocessing steps (DeepTop minimal) and another that does (DeepTop full). To better understand the effectiveness of the DeepTop tagger, its performance is compared with two baseline BDT-based taggers called SoftDrop + N subjettiness and MotherOfTaggers1. The architecture and model implementation details can be found in reference [72]; we only showcase their final results in figure 5. To summarise their results, the CNN-based taggers perform slightly better than the High-level feature-based taggers, proving their effectiveness. Between the two CNN taggers, the DeepTop minimal performs slightly better than the DeepTop full. This behavior can be ascribed to the loss of information, as discussed in section 3.1, in the additional pre-processing steps used in the DeepTop full images.\n\n|SoftDrop + N-subjettiness|SoftDrop + N-subjettiness|MotherOfTaggers|DeepTop full|DeepTop minimal|\n|---|---|---|---|\n|104|103|102|10|1|\n|0 0.2 0.4 0.6 0.8 1|0 0.2 0.4 0.6 0.8 1|0 0.2 0.4 0.6 0.8 1|0 0.2 0.4 0.6 0.8 1|0 0.2 0.4 0.6 0.8 1|\n|Signal efficiency ‘S|Signal efficiency ‘S|Signal efficiency ‘S|Signal efficiency ‘S|Signal efficiency ‘S|\n\nFigure 5: The ROC curves for the minimal and full version of the DeepTop tagger compared with the HLF-based Softdrop + N-subjettiness and MotherOfTaggers [72].\n\nAn upgraded version of the DeepTop analysis was presented in reference [73]. The authors made several improvements to the previous analysis [72]. These involve a change in loss function from Mean Square Error to Cross Entropy (better suited for classification problems), a change of optimizer from Stochastic Gradient Descent to AdaDelta[77] (to take advantage of the insensitivity of the latter to noisy gradients), introducing a learning rate scheduler, improved architecture with an increased number of nodes and more feature maps, change in the pre-processing2, increased sample size, and introduced multi-layered / color images. This analysis also considered two types of training data. The first set resembles the sample of the DeepTop analysis with top/QCD jets in the pT range of 350-450 GeV, reconstructed with R=1.5 anti-kT jets after detector simulation. These jets are matched with a truth level top by demanding !R(t, j) < 1.2. This sample only uses single-layered images generated from the calorimeter and utilizes the transverse momentum as pixel intensity. After preprocessing, the images are pixelized into a 40 → 40 grid. The second dataset (CMS jets) uses top/QCD jets in the pT range 800-900 GeV, reconstructed with a R = 0.8. Apart from the matching condition of !R(t, j) < 0.6, an additional merging condition is demanded that\n\n1 Mainly consisting of the HEPTopTagger[54, 55] variables.\n\n2 Instead of creating the image pixels before the preprocessing step they moved it to the end. As a result, the preprocessing steps can now utilize the fine granularity of the tracks. They also performed a two-fold flipping, first left-right, then up-down."
    },
    {
        "id_": "e1f316a1-f400-4b1a-a243-dab71ebba2fb",
        "text": "# DeepTop jets"
    },
    {
        "id_": "c5d7bb67-fd4d-4a56-92ac-e1a2d30a60dc",
        "text": "# CMS jets\n\n|DeepTop minimal|Training|DeepTop minimal|Training|\n|---|---|---|---|\n|105|Architecture|105|Architecture|\n|104|Preprocessing|104|Preprocessing|\n|Sample size| |Sample size|Color|\n|1000| |1000| |\n|1/ϵB100| |100| |\n|10| |10| |\n|1| |1| |\n|0.0|0.2|0.0|0.2|\n|0.4|0.6|0.4|0.6|\n|0.8|1.0|0.8|1.0|\n\nFigure 6: The ROC curves for the DeepTop jets (left) and CMS jets (right) samples demonstrating the gradual improvement in performance as modifications are incorporated to the minimal DeepTop model [73].\n\nThe ResNeXt architecture was implemented in reference [74] for top tagging. The model used was a smaller version of the original ResNeXt-50 [78] architecture, with the number of channels in all layers except the first reduced by a factor of four. This modification is due to the smaller size of the jet images used. The jet images are reconstructed from particle flow candidates collected in a 64 → 64 grid. Reference [74] studied top/QCD jets in the p T range of 550-650 GeV, reconstructed as R=0.8 anti-k T jets after dedicated detector simulation. Both matching and merging requirements are imposed to ensure the selection of properly reconstructed fat jets. The performance of ResNeXt was compared with several other DNN and GNN-based architectures. We present their final result in figure 7.\n\nThe application of Capsule Networks for collider analysis was first demonstrated in reference [75]. The model implementation follows the original paper [79]. Though the analysis [75] mainly focuses on the application of capsules for event-level analysis in separating a resonance decaying into a top pair from continuum top and dijet backgrounds and emphasizing the usefulness of capsules for overlying images, they have also studied the use of CapsNet for di-top and single top tagging. The usefulness of capsules for event-level analyses stems from their ability to learn the geometric position and orientation of objects in an image. In CNNs, the final classification/regression is done by linearising the images and using a decoding layer to generate the network prediction. CapsNet converts the images into capsule vectors in signal/background feature space. For instance, a 12-layer 8 → 8 image in CNN gets transformed into a linear array of 12 → 8 → 8 numbers, while in a CapsNet, it can transform into 8 → 8 = 64 capsules of dimension 12 each. These capsules are further processed."
    },
    {
        "id_": "e475e93d-bb88-4f7e-b90b-3b27dcf0e91a",
        "text": "# Figure 7\n\nROC curves showcasing the performance of ResNext and several other image-based and four-vector-based classifiers [74].\n\nTill they reach the final layer, with the number of capsules equal to the number of classes under study. The length of these capsules encodes how likely it is for the event in that particular class. More details on the model implementation and signal/background samples can be found in ref [75]."
    },
    {
        "id_": "2f116b29-4c80-44c3-b251-421b78f5ddc4",
        "text": "# Figure 8\n\nIn figure 8, we present the results of ref [75] for di-top and single-top tagging. The di-top samples are generated following the process pp ↘ Z ↑ ↘ tt¯, and the corresponding background comes from QCD dijet events. After detector simulation, the top jets are reconstructed as R = 1.0 C/A jets. Jets with pT > 350 and |ε| < 2.0 are considered for further analysis. The whole calorimeter energy deposit is converted into size 180 → 180 images with transverse energy as the pixel intensity. For the single top analysis, the public dataset [80] was used. For the sake of comparison, the analysis also considered the performance of the DeepTop tagger [73] for both tasks.\n\nAs can be seen from Fig. 8, both DeepTop and CapsNet demonstrate comparable performance. The blue-shaded region in Fig. 8 represents the uncertainty stemming from the use of different estimators. In conventional CNNs with a softmax activation function in the final layer, the output of the signal and background neurons are not independent, and one usually uses the output of the signal neuron as an estimator to build the ROC curve. However, the same is not true for the CapsNet. Capsules are vectors that are designed to encode the geometric position and orientation of objects in an image. Consequently, the signal and background capsules carry independent information. This gives one the freedom to try out different estimators and test their effect on the tagging performance. Ref. [75] used two such.\n\n– 12 –"
    },
    {
        "id_": "84f73b8f-e955-4e91-a1af-56e8f7f6f216",
        "text": "# 4 GNN"
    },
    {
        "id_": "4d295606-e8de-4bd4-81c0-06bc3328e6c6",
        "text": "# 4.1 Graph representation of jets\n\nIn the previous section, we discussed the remarkable success of CNNs in separating boosted jets originating from the hadronic decay of tops from those originating from light quarks and gluons. The reason for this success is twofold: firstly, CNN architecture respects translational symmetry and can identify patterns in di\"erent parts of the image. Second, CNN’s weight-sharing mechanism helps drastically reduce the number of trainable parameters. This suggests that NN architectures"
    },
    {
        "id_": "25bf12f3-5c0b-4b5d-8ea3-20f11e489d7b",
        "text": "# Figure 8\n\nThe ROC curves for CapsNet and Rutgers DeepTop CNN for di-top tagging (left) and single top tagging (right) [75]. The blue-shaded region represents the uncertainty stemming from the use of di\"erent estimators to build the ROC curve."
    },
    {
        "id_": "9d093e0b-ac28-46e7-8532-946427b2fa6d",
        "text": "# Figure 9\n\nIn reference [76], top/QCD jets in the p T range 550-650 GeV are used to check the performance of Bayesian DeepTop and compare it with the corresponding deterministic version. After reconstruction, the transverse energy of the calorimeter cells is used to construct the jet images. These images are passed through the updated preprocessing steps suggested in reference [73]. In addition to the image-based DeepTop tagger, the Bayesian version of the four-vector-based DeepTopLoLa tagger [81] was also considered. We present the ROC curves for the Bayesian and deterministic versions of DeepTop and DeepTopLoLa taggers."
    },
    {
        "id_": "6e4de6be-6500-434a-a15a-d28c0f9b4b43",
        "text": "# Estimators\n\nEstimators (see Ref. [75] for detailed discussion), and the region in blue represents the area between the ROC curves resulting from these two estimators. It is important to note that the choice of the estimator a\"ects the tagging performance of realistic training.\n\n| |di-jet top tagging| |reference data top tagging| | |\n|---|---|---|---|---|---|\n|8|103|8|103| | |\n|102|Di-DeepTop|102|Rutgers DeepTop| | |\n|101| |101| | | |\n|100.0|0.2|0.4|0.6|0.8|1.0|\n\nThe Bayesian version of the DeepTop tagger was originally introduced in reference [76]. Bayesian neural networks (BNNs) have the advantage that in addition to the network score, they also provide the score distribution, which can be used to estimate the error band on the score. BNNs help determine statistical uncertainties due to the limited size of the data sample and other systematics originating from pile-up interactions and jet energy scale. The goal is to determine the posterior distribution of the network weights given an assumed prior. The Kullback-Leibler divergence is used to estimate the approximate shape of the posterior from the training data. Once the posterior distribution is estimated, it can be used to determine the mean network output and the associated error."
    },
    {
        "id_": "c87a2c36-275b-4cfe-9c6f-786fb1e87d0e",
        "text": "# Figure 9: ROC curves for the Bayesian and deterministic versions of DeepTop and DeepTopLoLa taggers [76].\n\n| |B-CNN|CNN|B-LoLa|LoLa|\n|---|---|---|---|---|\n|0.0| | | | |\n|0.2| | | | |\n|0.4| | | | |\n|0.6| | | | |\n|0.8| | | | |\n|1.0| | | | |\n\nThat respect the intrinsic symmetry of the problem can not only excel in performance but can also provide a more economical network design.\n\nReturning to the case at hand, the constituents of a fat jet originate from the showering, fragmentation, and hadronization of the initial parton. We observe these constituents as charged tracks and calorimeter energy deposits at detectors. Though most analyses try to order these constituents by their transverse momentum, energy, or angular position, no theoretical reasoning supports this assumption. Therefore, architectures that respect this inherent unordered nature of jet constituents possess the potential to capture intricate jet-level features adeptly. This calls for the graph representation of fat jets, facilitating the development of permutation-equivariant architectures and enabling the modeling of intricate interrelations among jet constituents.\n\nGraph Neural Networks (GNNs) are architectures designed to learn functions on these graphs. For a comprehensive understanding of GNNs, we encourage the interested reader to consult reference [82, 83]. For our discussion, we only noted some key features of graphs.\n\nA graph is a collection of nodes and the pairwise relationship between these nodes (edges). In the case of a fat jet, these nodes can represent the jet constituents, with the constituent four vectors playing the role of the node coordinates in Minkowski space. Each node can also be characterized by node-specific features like the constituent mass, charge, or ID. In addition, we can also associate each node with some global features characteristic of the fat jet. In other words, we can represent the graph node as fi = xi ≃ hi ≃ gi, with the xi representing the constituent four-vector, hi the..."
    },
    {
        "id_": "7c4ac067-85ae-468e-8d99-404c38e88284",
        "text": "# 4.2 GNN architectures\n\nThis section will discuss some of the leading GNN architectures for top tagging. Despite the presence of many well-to-do and physically motivated architecture, we have decided to focus our discussion on five models: PFN [84], ParticleNet [85], LGN [86], LorentzNet [87], and PELICAN [88–90]. Each of these models has used the public dataset [80] to access the classifiers’ performance, giving us a common ground to compare their performance.\n\nThe Particle Flow Network (PFN) [84] operates on a point cloud representation of jet constituents, disregarding any inherent ordering among them. The theoretical basis of PFN is the Deep Sets theorem, which suggests that any observable associated with a jet can be approximated by [84]:\n\nO(p1, ....pM) = F (∑ϑ(pi))i=1M\n\nHere, pi (i = 1...M (the number of constituents)) represents some characteristics of the jet constituents like their four-momentum, mass, identification, etc. ϑ acts on each particle transforming its feature to the latent space. The summation over the particle label ensures permutation equivariance and converts the per-particle latent representation into the latent representation of the event. Finally, the map F converts the event latent representation into the final observable. In PFN, these functions ϑ and F are realized through neural networks. The model performance is evaluated on the public top-tagging dataset [80]. Before passing through the network, the dataset undergoes some preprocessing where the jet is first centered in the ε ↗ ϑ plane, the pT of the constituents are normalized, and undergo rotation and reflection. For more details on the model implementation and dataset used, we refer the interested reader to the original paper [84]. Here, we present their results in Figure 10 and Table 1. Note that the architecture discussed here is named PFN-r.r. in reference [84]. In their work, PFN denotes a model excluding reflection and rotation preprocessing steps. However, as evidenced in reference [84], these preprocessing steps help in enhancing the classifier’s performance. Finally, we also want to mention that reference [84] also implements an infra-red and collinear (IRC) safe version of the network called Energy Flow Network (EFN). Though the network makes more physical sense3, it performs slightly less on the top-tagging dataset. We refrain from discussing EFN here and urge the interested reader to consult [84].\n\n3 IRC safety ensures that the network output (the so-called topness of the jet) is invariant under soft and collinear emissions. Such observables are calculable within the paradigm of perturbative QCD (see the discussion in Ref. [91]) and thus are more sensible."
    },
    {
        "id_": "0cceb4b1-a72a-40a0-be71-7ef6f36abae8",
        "text": "# ParticleNet\n\nLike PFN [84], ParticleNet [85] also employs the point cloud representation of jet constituents (particle cloud). However, unlike PFN, which follows the Deep Sets approach, ParticleNet implements edge convolution [92] that helps the architecture utilize the local structure of the constituents. The edge convolution operation is inspired by the weight-sharing and hierarchical learning features of CNNs.\n\nHowever, unlike images, the point clouds can have uneven shapes and do not have a grid-like representation. This makes it difficult to construct local patches for the convolution kernel to operate. Edge convolution solves this problem by using the k nearest neighbors to construct local patches in a point cloud. Analytically, for a given point x, the operation of edge convolution results in:\n\nHere, the sum runs over the k-nearest neighbors of the point x, i.e. {xi1 .. xik}. \"s are some learnable parameters parametrizing the function h, and ↭ denotes the aggregation operator, designed to respect permutation equivariance (ParticleNet uses a mean aggregator). ParticleNet implements h using neural networks with parameters shared across the edges. The main blocks of ParticleNet are the EdgeConv blocks. These blocks first determine the k nearest neighbors of a particle using their position in the rapidity-azimuth plane. The edge features are then passed through the EdgeConv operation. ParticleNet also uses residual connections to pass the input features to the output of the EdgeConv operation. The combined output is passed through the subsequent EdgeConv block. Note that the second EdgeConv block utilizes the latent representation of the point cloud while determining the k nearest neighbors. In other words, the point clouds are dynamically updated, making ParticleNet a Dynamic Graph Convolutional Neural Network (DGCNN). ParticleNet implements three such EdgeConv blocks, each with k=16. After the EdgeConv blocks, the output undergoes a global average pooling followed by a decoding layer to produce the predictions. For complete details on the architecture, we urge the interested reader to consult the original paper [85]. The model performance is evaluated on the public top tagging dataset [80], and we present their results in Figure 10 and Table 1."
    },
    {
        "id_": "5d308b35-5ae5-45e2-ba3a-0c594521038d",
        "text": "# Table 1: Performance and architectural complexity of different GNN top taggers [89]\n\n|Architecture|Accuracy|AUC|1/↼ B|# Params|\n|---|---|---|---|---|\n|LGN|0.929(1)|0.964(14)|424 ± 82|4.5k|\n|PFN|0.932|0.982|891 ± 18|82k|\n|ResNeXt|0.936|0.984|1122 ± 47|1.46M|\n|ParticleNet|0.938|0.985|1298 ± 46|498k|\n|LorentzNet|0.942|0.9868|2195 ± 173|220k|\n|PELICAN|0.9425(1)|0.9869(1)|2289 ± 204|45k|\n\nThe results are calculated from the average of several training runs (the number of runs varies across networks, see Ref. [89]) with random network initialization. The numbers in the parenthesis represent the uncertainties over these runs.\n\nLorentz invariance is the fundamental symmetry of space-time governing elementary particle interactions. NN architecture respecting this symmetry can provide a relatively simple and physically interpretable design. Lorentz Group Network (LGN) [86] is based on the theory of finite dimensional representation of the Lorentz group and demonstrated for the first time the usefulness of including Lorentz equivariance in the construction of efficient GNN architectures with significantly fewer trainable parameters. LGN is based on the G-equivariant universal approximation theorem."
    },
    {
        "id_": "1eead8c3-74d6-49b1-8646-5ff450a1af4d",
        "text": "that suggests that any equivariant map between two completely reducible representations of a lie group G can be realized through NNs using vector activations belonging to finite-dimensional representations of the group G. The permutation equivariant and Lorentz equivariant architecture is built by stacking several Clebsch-Gordon layers [86] on top of one another that perform CG decompositions on the activations of the previous layers. It first performs tensor products representing self-interactions and interactions among different particles. The CG operator acts upon these tensor products to decompose them into irreducible representations of the Lorentz group. Finally, an equivariant learnable operator mixes these decompositions to form the channels of the next layer. Apart from the CG layer, the architecture also includes an input layer, several MLP layers, and an Output layer. For the details of the model implementation, see [86]. The performance of the model is evaluated on the public dataset [80], and we present the model performance in Figure 10 and table 1."
    },
    {
        "id_": "fa06ebbd-1919-4ca2-aba1-fb886ecc7d8e",
        "text": "# Figure 10: ROC curves for the GNN classifiers [89].\n\nLorentzNet [87] is another permutation equivariant and Lorentz equivariant GNN architecture. Like the CG layers in LGN [86], the main building blocks of LorentzNet are called Lorentz Group Equivariant Blocks (LGEB). Their role is to define the edges of the graph and update the node coordinates and the node embeddings while respecting Lorentz equivariance. If we denote the node embeddings of the lth layer as hli (i=1,2...N) and the corresponding node coordinates in the Minkowski space as xli (I = 1,2...N), where N denotes the number of nodes then the action of the LGEBs can be summarised in three simple steps [87]:"
    },
    {
        "id_": "8c1d730d-c1c1-4d76-ac5a-e5c69c09fe43",
        "text": "# Neighbourhood aggregation:\n\nmlij = ϑ e(h i l, hj l, ↽(||x i l↗ xj l || 2 , ↽(⇐x , x j l⇒))) li (4.5)\n\nwhere ϑ e is a NN, ↽(a) = sign(a)log(|a| + 1) is introduced to normalize large entries, ||a|| 2 is the Minkowski norm, and ⇐a⇒ is the Minkowski inner product. LorentzNet does not assume any prior knowledge of interaction among the nodes; in other words, the graphs in LorentzNet are fully connected."
    },
    {
        "id_": "bccd3817-cd32-4592-8365-7dd2cd93dedd",
        "text": "# Updating the node coordinates:\n\nl+1 = xl i + c ∑ ϑ x (ml ij ).x j\n\nx i (4.6)\n\nWhere the constant c is introduced to control the scale of x i l+1, ϑ x is another NN, and the sum is over the neighborhood of the node i."
    },
    {
        "id_": "410244be-5bd8-4341-abb1-9ccc411160d0",
        "text": "# Updating the node embeddings:\n\nl+1 = hi l + ϑ h(hi l , ∑ w ij mlij)\n\nx i (4.7)\n\nHere, w ij = ϑ m (m ij ) is introduced to code the significance of the edge between node i and j, ϑ m and ϑ h are Neural Networks.\n\nApart from the LGEBs, LorentzNet also contains an input/encoding layer that transforms the input node embedding scalars to the latent space and a final decoding layer that uses the output of LGEBs to generate Network predictions. The model’s performance is tested on the publicly available dataset [80], and we present the model’s performance in Figure 10 and Table 1.\n\nSo far, PELICAN [88, 89] is the best-performing NN architecture for top-tagging. It is also a permutation equivariant and Lorentz equivariant GNN. Notably, PELICAN adeptly constructs a comprehensive set of Lorentz invariant functions, as per ref [94], from the four-momentum of jet constituents. These invariants can be constructed from the pairwise dot products of the four vectors, and for PELICAN, these dot products serve as the primary inputs. To ensure permutation equivariance, PELICAN followed reference [95, 96] to construct the complete list of equivariant aggregators (15 to be exact) that can transform the input dot products. Following aggregation, PELICAN further refines the aggregated values through scaling, employing a factor of (N/ N˜ ) ε, where ⇀ represents a learnable parameter, N signifies the count of particles in the event, and N˜ denotes a constant indicative of the typical number of particles expected in such events. The main building block of PELICAN is the equivariant block that contains the layers for message formation, followed by the aggregation block that applies the 15 aggregation functions discussed earlier. After five such equivariant blocks, the output passes through a decoding block that generates the model predictions. For a comprehensive understanding of the model architecture, interested readers are directed to reference [89]. Reference [89] also uses the public dataset [80] for the performance assessment of PELICAN. We present their results in Figure 10 and Table 1."
    },
    {
        "id_": "923c67a9-7bbf-495c-af84-180d1139da75",
        "text": "# 5 Top Quarks and Physics Beyond the Standard Model\n\nAttempts to alleviate various shortcomings of the SM—theoretical problems like the hierarchy problem, the flavour problem, the strong CP problem, the vacuum instability problem, as well"
    },
    {
        "id_": "ddfc4503-49f5-4cec-85d2-68a5927c7461",
        "text": "# as experimental inconsistencies like the nonzero neutrino masses and mixing, the baryon asymme-"
    },
    {
        "id_": "9df5dbc8-64cb-4792-8fa8-e5f178d2e3c5",
        "text": "# try of the universe, the presence of cold dark matter (DM) in the universe, and various flavour"
    },
    {
        "id_": "8532abed-79e0-4556-81fd-533e00c8107f",
        "text": "# anomalies—have led to a plethora of theories or models going beyond the SM. Examples of such"
    },
    {
        "id_": "c42db25f-0f1b-4622-a283-1f8120b9fbac",
        "text": "# theories or models are Grand Unification Theories (GUTs) [97–103], supersymmetry [104–109],"
    },
    {
        "id_": "c99c65b6-2185-407d-8189-f5535c95b381",
        "text": "# wrapped extra-dimensions [110–113], technicolour models [114–117], little Higgs theories [118–122]"
    },
    {
        "id_": "91159824-fb90-4510-8479-7050c8f3604f",
        "text": "# and theories featuring dynamical or elaborated spontaneous symmetry breaking [109, 123–133]."
    },
    {
        "id_": "010ea26e-c002-4f1f-9f39-f9bf5bd83db5",
        "text": "# As briefly discussed below, such models introduces various new particles: extra gauge bosons, ex-"
    },
    {
        "id_": "33ede120-c684-416b-aef8-44eeb5dda477",
        "text": "# tra scalars, leptoquarks, vector-like quarks, etc. In many scenarios, these new particles can have"
    },
    {
        "id_": "7f093899-3b53-42b0-83b3-423bb42fffdc",
        "text": "# preferential couplings to the third-generation fermions, in particular, the top quark. As such, at"
    },
    {
        "id_": "03d3eeaa-1f17-4dfe-b1d9-d8ba32cb652a",
        "text": "# the LHC, their production and decay could result in various top-enriched final states: exclusive"
    },
    {
        "id_": "8f74a0a8-3c5a-4e34-90b0-c6f70ef3c4dd",
        "text": "# single top, top pair and multi-top, or those in association with the SM gauge bosons, leptons and"
    },
    {
        "id_": "71a3745d-3ea6-423d-8c36-aa01b602d5e6",
        "text": "# quarks. Motivated by such scenarios, numerous searches have been performed by various exper-"
    },
    {
        "id_": "cb28718f-964e-45e1-8fbf-5301636b53f8",
        "text": "# imental collaborations, particularly the CMS and ATLAS. At the LHC, with the exception of a"
    },
    {
        "id_": "efa48158-40cb-44aa-8b5b-4f992867f7fa",
        "text": "# few mild excesses (see [134] for a review), the observations are found to be consistent with the SM"
    },
    {
        "id_": "85a07baf-4357-4512-ab6e-d3d2256e9e8a",
        "text": "# expectation. This has led to stringent limits on the new particles’ masses and couplings. In fact, in"
    },
    {
        "id_": "ef2ddf07-7cb2-4028-8522-3ae115cb4dfb",
        "text": "# most of the cases, the limits have been pushed to the TeV scale, although not in full generality. For"
    },
    {
        "id_": "39da40db-99f0-4a2b-a97b-6eaf5185b5ac",
        "text": "# TeV scale states, their decay products—SM leptons, quarks and bosons—could be highly Lorentz-"
    },
    {
        "id_": "443f346b-ca48-43c1-85ac-a7e4f885851b",
        "text": "# boosted that the jets emanating from them would be collimated. Consequently, the hadronically"
    },
    {
        "id_": "61a5bc1d-7fc8-4849-8cae-f9eecb3f7942",
        "text": "# decaying candidates (primarily, the top quarks in our case) are more likely to manifest as a single fat"
    },
    {
        "id_": "2e47e936-ede3-4ccb-bfb1-a440d878c2a4",
        "text": "# jet rather than multiple resolved jets. In this section, without pretending to provide an exhaustive"
    },
    {
        "id_": "78c54d83-5675-4acb-99c0-2267806b120f",
        "text": "# and self-sufficient description of the new physics-induced top searches, we briefly discuss various"
    },
    {
        "id_": "6dc089c3-1537-481b-a0d9-3da3dbcebaf3",
        "text": "# new physics scenarios contributing to the top-enriched final states at the LHC and summarise the"
    },
    {
        "id_": "e7c678f3-7e0c-4d0f-8e9b-067c3b8d2cbb",
        "text": "# relevant LHC searches, including those targeting boosted top quarks in the final state."
    },
    {
        "id_": "4f92429f-fb72-46b1-a3f7-ed49bcaf89a9",
        "text": "# 5.1 Extra Gauge Bosons"
    },
    {
        "id_": "4611aba2-aaf3-490d-809a-0cf0262a332b",
        "text": "# A wealth of BSM models, such as Grand Unification Theories (GUTs) [97–103], supersymmetry"
    },
    {
        "id_": "60d79364-4209-4962-a963-7d8e4d127fc2",
        "text": "# [104–109], wrapped extra-dimensions [110–113], technicolour models [115–117], little Higgs theories"
    },
    {
        "id_": "81c4e006-51c7-4293-aca1-bc3d3725f783",
        "text": "# [118–122] and theories featuring dynamical or elaborated spontaneous symmetry breaking [109, 123–133],"
    },
    {
        "id_": "5305d70c-15c5-4263-90fa-fc732e448cca",
        "text": "# envisage the presence of extra gauge bosons (W ↑ , Z ), with properties similar or different to"
    },
    {
        "id_": "00f0b4c4-52e4-4377-974c-f811c5009506",
        "text": "# those of the SM gauge bosons (W, Z). Specifically, Z ↑ bosons require the SM to be superseded with"
    },
    {
        "id_": "9be8b534-f9cc-443c-af2e-f55d9989e952",
        "text": "# at least an extra U (1) symmetry, while W↑ bosons require at least an extra SU (2) gauge group."
    },
    {
        "id_": "8b6a9571-05b3-4e43-bf42-14cbac97f4e8",
        "text": "# Some theories assume (or motivate) preferential couplings of W↑ , Z ↑ bosons to the top-quark (or"
    },
    {
        "id_": "66e2e130-ad54-47c4-8e5f-2a98a3bc8889",
        "text": "# the third-generation fermions). Examples of such theories include the top-colour [135–138] and"
    },
    {
        "id_": "ad50d276-06a0-462c-8923-8e3808a1953a",
        "text": "# top-flavour [116, 139, 140] models, and gauged-flavour symmetry models [141]. Some simplified"
    },
    {
        "id_": "9ab183df-7538-46b8-b840-735d233d2975",
        "text": "# models for dark matter (DM) also predict Z↑ bosons mediating the interactions between DM and"
    },
    {
        "id_": "016e3ad4-f438-4c19-9ce4-1cf59f370b34",
        "text": "# normal matter, see Ref. [142] and references therein. Moreover, as possible explanation for the"
    },
    {
        "id_": "07268bec-7d4e-4750-ac3e-4f018bae4e36",
        "text": "# recent flavour anomalies [143, 144], such models have been pursued with great interest, see for"
    },
    {
        "id_": "61da0470-0a48-4b44-873c-ed7e92906cfe",
        "text": "# example [145]."
    },
    {
        "id_": "c8e146d6-811a-4a76-9aeb-242caf0b9a9b",
        "text": "# Motivated by such theories, several dedicated searches for W↑ with right-handed or/and left-"
    },
    {
        "id_": "e0519434-e341-4bfa-8232-56a92a143c11",
        "text": "# handed charged current interactions and Z↑ bosons decaying to the third-generation quarks (a"
    },
    {
        "id_": "0b6c05f0-1f12-4db0-83fc-64bfcc073c9c",
        "text": "# top-quark and a bottom-quark or a pair of top-quarks, see Figure 11) have been performed by both"
    },
    {
        "id_": "ea87dee1-5e47-4c93-adeb-aa64fff9d202",
        "text": "# the CMS and ATLAS Collaborations; see for example [21, 147–151] for W↑ searches, and [17, 18,"
    },
    {
        "id_": "dc483788-30dd-4433-9319-04cbc58f2721",
        "text": "# 23, 29, 146, 152, 153] for Z ↑ searches. Considering leptonic final states, CMS has excluded left- and"
    },
    {
        "id_": "042902ff-ef8c-49dd-9011-973f93e6c257",
        "text": "# right-handed W ↑ bosons with mass below 3.9 and 4.3 TeV, respectively, at the 95% confidence level"
    },
    {
        "id_": "ecf5cda8-3d8d-4ddc-baed-de067272626b",
        "text": "# (CL)[151]. On the contrary, with the all-hadronic final state considered by CMS, the resulting"
    },
    {
        "id_": "d664a107-2300-4504-8f2e-87b404cf330c",
        "text": "# limits are 3.4 TeV for both left- and right-handed W ↑ bosons [150]. Considering both leptonic and"
    },
    {
        "id_": "633c2263-74f1-4472-a7b2-e3f6d2abcf06",
        "text": "# All the limits quoted here are valid under the assumption that the new particle has a narrow decay width, and"
    },
    {
        "id_": "d70e45fd-abfc-4e41-88c3-303fea901a3d",
        "text": "# they have SM-like couplings."
    },
    {
        "id_": "fc4055ac-5543-41d6-b20a-0d008bbf86c2",
        "text": "# 5.2 Kaluza-Klein excitations\n\nIn the Randall-Sundrum (RS) model of wrapped extra dimension, four-dimensional spacetime is embedded in a larger dimensional bulk with a fifth wrapped extra dimension [110]. The propagation of a field in the finite extra dimension manifests as the Kaluza-Klein (KK) excitations in the four-dimensional effective theory. The KK excitations appear as particles with the same quantum number as the original particle but with larger masses. The KK excitations of gluons, gauge bosons and graviton are likely to be localised close to the TeV brane, thereby leading to preferential couplings between these modes and the SM top quark [154–157, 157, 158]. At the LHC, the first KK excitation of gluon gKK, spin-1 color-octet boson, will have the largest production rate among the KK states and are therefore expected to be the first observed signal of the RS model [154, 155]. Several searches looking for the gKK have been performed by CMS and ATLAS [17, 18, 146, 152]. The gKK is primarily produced quark-antiquark annihilation and decays predominantly into tt¯ (see Fig. 12). On the contrary, the first KK excitation of the graviton GKK, spin-2 color-singlet boson, predicted in the bulk RS model [156, 157, 157, 158] is mainly produced in gluon-gluon fusion, and dominantly decays into tt¯ for (sub-)TeV masses (see Fig. 12). Several searches looking for the GKK have been performed by CMS and ATLAS [17, 18, 146].\n\nConsidering dilepton, single-lepton and fully hadronic final states and employing jet substructure techniques optimised for top quarks with high Lorentz boosts, CMS has excluded gKK excitations with mass below 4.55 TeV [17].5 While the ATLAS search [146] targeting heavy particles decaying into a tt¯ pair in the lepton-plus-jets events has excluded GKK in the 0.45–0.65 TeV mass.\n\n5 With the couplings described in Ref. [159], the relative decay width of the gKK resonance lies between 10% and 30% depending on its coupling to the top quark."
    },
    {
        "id_": "65763f8e-adab-45f7-a0ac-e40132307b4d",
        "text": "# 5.3 Extra Scalars\n\nThough the properties of the SM Higgs reported so far have been largely consistent with the SM prediction, the minimality of the SM Higgs sector—the presence of a single SU (2) L doublet scalar that simultaneously gives mass to the electroweak gauge bosons and all SM fermions—is not guaranteed by any guiding principle or symmetry. As such, a plethora of models with the extended scalar sector have been proposed in the literature, including the addition of SU (2) L singlets [160–162], doublets [163–167] and triplets [168–173]. Many of these models predict new pseudoscalar (A) and scalar (H) states and sometimes also a charged scalar (H ± ) coupling strongly to the third-generation quarks (see Fig. 13). The widely studied example of such a model is the two-Higgs-doublet models (2HDMs) [174]. In particular, the type-II variant of 2HDMs, akin to the Higgs sector of the minimal supersymmetric standard model (MSSM) [174–181], predict such states predominantly decaying into t¯ for t mA,H ↫ 500 GeV and small tan ς (where tan ς is the ratio of the vacuum expectation values of the two Higgs fields).\n\nFigure 13: Feynman diagrams for the production of a pseudoscalar or scalar Higgs, denoted by a common symbol # (left) [182], and in association with a pair of top quarks (middle) [183], and charged Higgs in association with a top quark and a bottom quark (right) [184] with the Higgses decaying into third-generation quarks.\n\nFor the mass range 0.4–3 TeV, GKK width varies from 3% to 6% when the characteristic dimensionless coupling is set to 1 (ω is the curvature of the warped extra dimension and M Pl = MPl/→8ε is the reduced constant ω/M Pl Planck mass). The branching ratio of GKK to tt¯ increases rapidly from 18% to 50% for masses between 400 and 600 GeV, plateauing at 68% for masses larger than 1 TeV."
    },
    {
        "id_": "0c4556fe-02a2-4613-8926-fcab708cca07",
        "text": "# 5.4 Leptoquarks\n\nSimilarities between quarks and leptons in the SM, such as their transformations under the SM gauge groups, the number of generations and the hierarchy across generations, motivate a fundamental symmetry connecting them. Such symmetries are embedded in many BSM models, including GUTs [97–99], technicolour models [191], or theories of quark and lepton compositeness [192]. Such models predict the existence of “leptoquarks” (LQs) with spin-0 (scalar LQs) or spin-1 (vector LQs) that couple to both leptons and quarks simultaneously. LQs transform as triplets under the SU (3) C and carry fractional electric charges. For vector LQs, in addition, the coupling strength depends on the anomalous magnetic moment (κ). The κ = 1 limit refers to the Yang-Mills-type coupling scenario, and the κ = 0 limit refers to the minimal vector coupling scenario, where the Yang-Mills-type couplings are turned off [193]. While in the minimal Buchmuller-Ruckl-Wyler (BRW) model [194], LQs are assumed to couple only to leptons and quarks from the same generation, cross-generational couplings with varying strengths are also possible. As such, LQs can generate lepton flavour universality-violating (LFUV) interactions and, therefore, have been pursued with great interest as a viable solution to the b-anomalies [195–198] and the muon’s anomalous magnetic dipole moment anomaly [199, 200].\n\nFigure 14: Leading order Feynman diagrams for the production of LQ pairs at the LHC [201].\n\nAt the LHC, LQs are produced singly or in pairs. Pair production proceeds via gluon–gluon fusion and quark-antiquark annihilation mediated by the strong interaction (see Fig. 14). Several searches have been performed to find LQs decaying to a top quark and a lepton [15, 19, 20, 201–]."
    },
    {
        "id_": "6930ef2c-eff1-462e-af58-47de4a11f78e",
        "text": "# 5.5 Vector-like Quarks\n\nAttempts to alleviate the fine-tuning or naturalness or hierarchy problem of the SM have led to the idea that the Higgs boson is a composite particle generated by a new strongly interacting sector at the compositeness scale much larger than the electroweak (EW) scale, and the gap between these two scales is explained by interpreting the Higgs boson as a pseudo-Nambu–Goldstone boson (pNGB) associated with spontaneous symmetry breaking at the compositeness scale [114, 117, 128, 211–215].\n\nOn this idea, based are the Composite Higgs models [114, 211, 212] and Little Higgs models [118–121]. Such models usually follow a sequential symmetry-breaking pattern: a large global symmetry group above the compositeness scale is spontaneously broken to a smaller group, which is explicitly broken to the SM EW group. Such models, therefore, naturally accommodate several new particles, in particular, new fermionic resonances called vector-like quarks (VLQs): colour-triplet spin-1/2 fermions with both left- and right-handed chiral components transforming identically under SU (2). Renormalisability and gauge completeness restrict the SU (2) representation of the VLQs to 1, 2 and"
    },
    {
        "id_": "06b6849b-df56-4571-9b9a-53ff6240bfdf",
        "text": "# 3. Like the SM quarks, VLQs carry fractional electric charges.\n\nThe VLQs with +2/3 electric charge (T) can, therefore, decay into tZ, th and bW; those with ↗1/3 charge (B) decay into bZ, bh and tW; those with +5/3 charge (X) into tW; and those with ↗4/3 charge (Y) into bW. The relative couplings for these interactions are determined by the gauge representation of the vector-like quarks.\n\nAt the LHC, VLQs are produced singly or in pairs. Pair production proceeds via gluon-gluon fusion, and single productions (both resonant and non-resonant) in association with quarks are mediated by a gauge boson (see Fig. 16). Note that VLQs also appear in many other BSM scenarios which are well-placed to address various theoretical as well as experimental issues of the SM. Such models, often, introduce additional matter states for the sake of ultraviolet completion, thereby opening up new production and decay channels. The rate for these new channels depend on the details of the UV-completion, for example, their coupling with additional states, or their effective mixings with the SM counterparts, either through direct Yukawa couplings or through a combination of couplings involving yet other new states. In the following, we briefly discuss the LHC searches targeting the VLQs coupling preferentially to the top quark.\n\n|q|q→|b, t, t|q|q→|b/t|\n|---|---|---|---|---|---|\n|g| |h/Z|W/Z| | |\n|g|T|W +, h, Z| |T| |\n|W →, h, Z|b| |t|h/Z| |\n|g|T¯| |t| | |\n|b, t, t|g| |b| |g|\n| | |t| | |¯|\n\nFigure 16: Leading order Feynman diagrams for the pair (left) [216] and single (middle [27] and right [217]) production of VLQs at the LHC.\n\nSeveral searches for pair-produced VLQs decaying to a W/Z/h-boson and a t/b-quark have been performed by CMS and ATLAS [26, 216, 218–224]. Considering events with at least one leptonically decaying Z-boson and a b-tagged jet, the ATLAS search [223] has put the most stringent limit of 1.6 TeV on T decaying into tZ. On the contrary, the CMS search [221] with single-lepton, same-sign charge dilepton, and multilepton final states has the most stringent limit of 1.56 TeV on B decaying into tW. The same search has also put the most stringent limit of 1.50 TeV on T decaying into th. Notably, equally competitive limits of 1.46(1.47) TeV have been put on T decaying into Zt (B/X decaying into tW) by another ATLAS search [222]. This analysis is based on a final-state signature with high missing transverse momentum, one lepton, and at least four jets, including a b-tagged jet.\n\nBefore going into the single production searches, we briefly mention the searches with fully hadronic final states [26, 219]. Of these, the CMS search [26], employing a multiclass classification algorithm 'boosted event shape tagger' to tag the jets emanating from W, Z, h-bosons or t-quarks, has put limits of 1.26(1.37)[1.23] TeV on T decaying into tZ (T decaying into th) [B decaying into tW]. Note that these limits are quite weaker than those from the leptonic and semileptonic searches.\n\nFor single VLQ production in association with quarks, the cross-section depends on the couplings of the VLQ to third-generation quarks kVLQ. In the rest of this section, we briefly summarise some of the most recent LHC searches. The CMS search [225] targeting single T production with T ↘ th has put an upper limit of 0.16–0.33 on kT for 600–1200 GeV mass. This search exploits the Higgs boson decay to a pair of photons and, in addition, requires at least one b-tagged jet. On the other hand, the ATLAS search [27] considers fully hadronic final states from T ↘ th and uses tagging algorithms to identify the jets emanating from the hadronically decaying h and t."
    },
    {
        "id_": "794ae10c-ee7a-42a6-a5c2-435297484160",
        "text": "# 5.6 Supersymmetry\n\nSupersymmetry (SUSY) theories have been accepted as one of the most motivated theoretical constructs going beyond the SM. For one, SUSY, an extension of the Poincaré space-time symmetry, relates bosons and fermions, thereby, as a consequence, solving the hierarchy problem of the SM. SUSY theories are promising candidates for a unified theory, as, within such theories, the measured gauge couplings extrapolated from the EW scale through the SUSY renormalization group equations unify at a GUT scale. Also, the lightest SUSY particle can serve as a DM candidate in the presence of an additional conserved quantum number, such as R-parity that distinguishes SM states from their SUSY partners. Moreover, being the only possible extension of Poincaré space-time symmetry group, SUSY is a very likely description of Nature if one considers the sheer elegance of SUSY as a theory.\n\nThe SUSY phenomenology, to a large extent, is driven by the presence of R-parity. In most of the SUSY models, as we do here, the conservation of R-parity is assumed. The R-parity-violating scenarios are discussed in a separate section (see Sec. 5.7). This has two significant implications for phenomenology. For one, SUSY particles are produced only in pairs. Second, it ensures the stability of the lightest SUSY particle (LSP), thus implying that each SUSY particle produced will entail the LSP at the end of its decay chain. The LSPs, much like the SM neutrinos, leave the detector undetected. Therefore, their presence is usually sought for as missing transverse momentum pTmiss. Note that, depending on the underlying mechanism of SUSY breaking, different models predict different SUSY particles as the LSP, thereby leading to different phenomenology. For example, the minimal super-gravity models predict the lightest neutralino as the LSP, while the gauge-mediated SUSY breaking models predict the nearly massless gravitino as the LSP. A detailed discussion of various models is beyond the scope of this paper. The phenomenological mentions hereinafter is based on the simplified models proposed by the LHC New Physics Working Group and the phenomenological minimal supersymmetric model (pMSSM). In particular, the scenarios leading to top-enriched final states are mentioned. And, unless otherwise stated, the lightest neutralino is assumed to be the LSP.\n\nAmong the SUSY productions, the pair production of gluinos dominates for a given mass scale, followed by squarks, sleptons, and sneutrinos. Both CMS and ATLAS have performed numerous dedicated searches looking for the SUSY particles. Of these, the ATLAS search targeting gluinos decaying into a pair of top-quarks and the lightest neutralino in final states with missing transverse momentum has been significant.\n\nR-parity is defined as R = (↑1)3(B→L)+2S, where S is the spin of the particle, and B and L are the baryon and lepton numbers. The SM particles have R-parity of +1, while their SUSY partners have R-parity of -1."
    },
    {
        "id_": "aa79cfcc-512e-44ab-884d-d47a684ba09d",
        "text": "# Figure 17: Representative decay topology for pair production of gluino and squarks [15, 16, 239].\n\nmomentum and three or more b-jets, has put the most stringent limit of 2.44 TeV. On the contrary, among the searches targeting third-generation squarks b˜1 and t˜1, the ATLAS searches [239] and [15] are the most constraining ones. The search in Ref. [239], considering final states with same-sign leptons and energetic jets, excludes b˜1 decaying into a top quark, a W boson and the LSP ◁1˜0 (see the second diagram in Fig. 17) up to 750 GeV. The same search also excludes t˜1 decaying into a top quark, a W boson and ◁1 ˜±, with ˜1 ◁± further decaying into the LSP ◁1 ˜0 and an off-shell W boson (see the third diagram in Fig. 17) up to 750 GeV; while the search in Ref. [15] has excluded t˜1 decaying into a top quark and the LSP ◁1 ˜0 up to 1.25 TeV, considering all hadronic tt plus missing transverse momentum final state. Notably, this search exploits fatjet properties to efficiently reconstruct top quarks that are Lorentz-boosted in the laboratory frame."
    },
    {
        "id_": "e01552d4-df85-4c36-b006-6c5715ed8ac8",
        "text": "# 5.7 R-parity-violating Supersymmetry\n\nThe previous section extensively discussed the existing collider constraints on R-parity-conserving SUSY scenarios. The primary motivation behind introducing R-parity was to prevent rapid proton decay. This ad hoc symmetry makes the lightest supersymmetric particle (LSP) stable, potentially qualifying it as a dark matter candidate [267, 268]. However, the rationale behind enforcing R-parity lacks a theoretical foundation, and SUSY theories contravening this symmetry are equally compelling. Supersymmetric theories violating R-parity exhibit more natural mass spectra and encounter fewer experimental constraints than their R-parity-conserving counterparts. This fact has motivated the ATLAS [269–279] and CMS collaboration [280–286] to conduct several searches of RPV SUSY scenarios. In the following, we briefly list a few such searches.\n\nLooking into the final state with a single isolated lepton (either an electron or a muon) accompanied by numerous jets, some of which may be b-tagged, the ATLAS collaboration [287] has effectively constrained various simplified RPV SUSY scenarios, as illustrated in Figure models where g ↘ tt◁0↘ tttbs (depicted in Fig. 18a), gluino masses of up to 2.38 TeV have been excluded at a confidence level of 95%. Similarly, for scenarios involving direct stop production (as depicted in Fig. 18c), stop masses up to 1.36 TeV have been ruled out at a 95% confidence level. In models where g ↘ t¯˜ and t ˜ ↘ t ¯¯ bs (illustrated in Fig. 18b), as well as (shown in Fig. 18f), gluino masses up to 1.83 TeV and 2.25 TeV, respectively, have been excluded at a confidence level of 95%. Additionally, for models involving direct electroweakinos production (as portrayed in Fig. 18d and 18e), Higgsino (Wino) masses ranging from 200 (197) GeV to 320 (365) GeV have been excluded. Pair production of stops with subsequent decay into the top quark and the lightest neutralino (which further decays into three light quarks) is also considered by the CMS analysis [283]. This search has led to the exclusion of top squark masses up to 670 GeV at a confidence level of 95%.\n\nThe ATLAS analysis [288] has considered scenarios with stops as the lightest colored SUSY particles where the LSP is assumed to be a triplet of two neutralinos ( ˜1◁0, ˜2◁0) and one chargino."
    },
    {
        "id_": "81becd3e-d6a3-41c7-b6e8-e4d9fa813569",
        "text": "# Figure 18: Examples of signal diagrams for the simplified RPV models considered in the ATLAS analysis [287].\n\nFor simplicity, particles and anti-particles are shown using the same symbols, omitting the anti-particle notation.\n\n( ˜ 1 ◁±) states that are mass-degenerate and carry dominantly higgsino components. Here, the strong production of stop pairs can give rise to a final state with a high jet multiplicity (See Figure 19). The absence of any significant deviations from the Standard Model predictions prompts the establishment of a 95% confidence level upper limit on the stop mass, capped at 950 GeV within the region where m ˜ t↗ m ˜ 1,2 , ˜ 1ϑ0 ϑ± ↔ mtop, an exclusive sensitivity zone for this analysis. Meanwhile, the CMS collaboration also investigates stop pair production [289], where each stop decays into four quarks via an intermediate Higgsino-like LSP. This analysis has effectively ruled out stop masses ranging from 100 to 720 GeV, with the Higgsino mass set to 75% of the stop mass.\n\nThe ATLAS analysis [239] explored a final state characterized by same-sign leptons, multiple jets, and significant missing transverse momentum (depicted in Figure 20a). This search excludes gluino masses below 1.6 TeV for t˜ masses up to 1.2 TeV. Subsequently, another ATLAS study [290] revisited a similar final state, further refining the analysis. Here, the previously established bound on gluino mass is extended to 1.65 TeV, with a Stop mass requirement below 1.45 TeV. The ATLAS analysis [291] has also looked into the same sign lepton final state but for the case of a higgsino-like ◁0/ ˜ 2 ˜ 1 ◁0 (See Figure 20b). The search excludes ◁ 1 ˜0/ ˜ 2 ◁0 masses up to 200 GeV. Furthermore, the CMS analysis [260] also investigated the same-sign lepton final state within two simplified RPV scenarios. Both scenarios entail strong production of gluino pairs. In the first scenario, each gluino is assumed to decay into four quarks and a lepton. The non-observation of any excess over the SM background allowed the analysis to exclude gluino mass up to 2.1 TeV. Conversely, the second scenario involves gluino decay in the ˜ ↘ tbs channel. This scenario was able to rule out gluino mass up to 1.7 TeV.\n\n– 27 –"
    },
    {
        "id_": "c073a62c-3a11-41eb-9c25-59c61700fb4b",
        "text": "Figure 19: Diagrams of the signal processes involving pair production of top squarks t˜ [288].\n\nFigure 20: Examples of signal diagrams for the simplified RPV models considered in the ATLAS analysis [287]. For simplicity, particles and anti-particles are shown using the same symbols, omitting the anti-particle notation."
    },
    {
        "id_": "1b34cdc1-c6c9-47df-a1ec-3713c47f5ccf",
        "text": "# 6 Summary and Outlook\n\nThe application of advanced machine learning techniques in the field of top tagging has seen significant progress in recent years. In this review, we have tried to summarise some of the recent developments in top tagging algorithms and their possible applications in the field of high-energy physics.\n\nThe paper begins with a discussion of various top taggers, focusing on high-level feature-based classifiers, Convolutional Neural Networks (CNNs), and Graph Neural Networks (GNNs). We have hand-picked some algorithms in each category and demonstrated their performance for top tagging. It’s important to note that all findings discussed herein are borrowed from their respective sources. Due to variations in datasets used by different algorithms, a direct comparison of their performance isn’t feasible. Nevertheless, our exploration offers valuable insights into their efficacy for the task at hand.\n\nThe second part of the paper discusses various BSM scenarios that can lead to a final state"
    },
    {
        "id_": "e9497f37-6131-4ce3-9cdb-49bd36f00bbc",
        "text": "# topology with boosted top quarks at the LHC\n\nWe have also discussed the bounds on these scenarios from previous collider searches. In all these cases, an efficient Identification of the final state top quark can help drastically reduce the SM background. It can lead to a possible discovery or even stronger constraints on the model parameter space. We must mention that numerous studies have already implemented different top tagging techniques for the study of BSM physics. However, a review of those studies is beyond the scope of our present discussion."
    }
]