[
    {
        "id_": "3b77e582-42f2-4ffc-9f62-7d49b93ddfce",
        "text": "# 1 Introduction\n\nPurely-forward and local algorithms propagate signals from inputs to outputs, without any backward pass, and employ a learning rule that is local in space, using signals only from pre- and post- neurons to update a weight. These algorithms, which include well-known Hebbian learning methods [1], have long been studied to model the brain, as their features align with biological processes, contrarily to error backpropagation [2]. They are also ideal for on-chip learning of low-power embedded hardware. The purely forward architecture addresses the challenge of implementing backpropagation on neuromorphic chips, a key obstacle that has been limiting the development of on-chip training for these systems [3]. The local learning rule allows synaptic devices to be directly updated by signals from neuron devices, eliminating the need to store neural activations for gradient computation, thus significantly reducing memory usage, power consumption as well as training times [4].\n\nThe practical application of these algorithms was historically limited by low accuracy on deep learning image recognition benchmarks, such as CIFAR-10 (labeled) and STL-10 (unlabeled). However, in recent years, accuracy has significantly improved thanks to their integration with automatic differentiation tools and activation functions, combined with the use of standardization and pruning techniques [5–14]. As a result, these algorithms have become relevant not only for brain modeling and energy-efficient training of neuromorphic chips, but also for distributed learning of large-scale models across multiple devices [15–18].\n\nThese advances have motivated the recent development of novel purely forward and local algorithms aimed at improving accuracy and simplifying implementation for various applications. The Forward-Forward (FF) algorithm is a key example of this new breed of algorithms [19]. Its main advantages are its simplicity and versatility, allowing it in principle to handle both unsupervised and supervised learning, and to process time-series as well as static inputs — while many other purely forward and local algorithms are limited to a single category. As a result, the FF algorithm has attracted significant interest since its introduction [7, 20–27], inspiring variants with improved accuracy and architectures and addressing tasks such as image recognition, temporal sequence classification, and reinforcement learning [14, 28–39]. Early demonstrations of FF in silico already cover a wide range of hardware platforms, including microcontrollers [40], in-memory computing systems utilizing Vertical NAND Flash Memory as synapses [41], and physical implementations where neurons are replaced by acoustic, optical, or microwave technologies [42, 43].\n\nFig 1 highlights recent advancements in the Forward-Forward algorithm (FF, shown in purple) [14, 19, 33, 38], comparing its accuracy on image classification tasks with algorithms that are also both purely forward and local (shown in black [5–14]). As illustrated, the accuracy of FF supervised training has significantly improved from below 60% [19] to 88.2% recently [44], progressively catching up with the best supervised, purely local, and forward algorithm [5]. In unsupervised learning, where FF’s greatest potential for on-chip training may lie, it has only achieved high accuracy on the MNIST task [19, 45], with limited [38] or no success on more challenging datasets such as CIFAR-10 [46] or STL-10 [47]."
    },
    {
        "id_": "b02ba276-4ab5-4678-8321-21258e4c363f",
        "text": "# Comparison of Algorithms\n\n|Supervised|Unsupervised|\n|---|---|\n|CIFAR-IU (CNM)|CIFAR-J (CMD)|\n|STL IO (CNN)|MNIST (MLF)|\n|SigProp|SCFF (us)|\n|DRTP|HardHebb?|\n|Act: Learning|SoftHebb|\n|FFI|Learning|\n|PEPITA|Hebb+WTA|"
    },
    {
        "id_": "27768f03-da66-4376-800e-fc924213e42b",
        "text": "# Accuracy (%)\n\nFig. 1: Comparison of algorithms that are both purely forward and local across different tasks. The blue frame displays the CIFAR-10 results achieved by various supervised local learning methods. The green frame presents the results of different local and unsupervised learning methods on CIFAR-10, STL-10, and MNIST datasets, respectively. Algorithms related to FF are highlighted in purple. FF1 refers to [19], FF2 to [33], FF3 to [14], FF4 to [44], and FF5 to [38]. Our method, SCFF, is indicated in red. SigProp refers to [5], DRTP to [6], Act.Learning to [7], PEPITA to [8], SoftHebb to [9], CSNNs to [10], HardHebb2 to [12], HardHebb1 to [11], Hebb+WTA to [9] and Hebb-CHU to [13]."
    },
    {
        "id_": "ed480ecb-9bff-49f1-8341-68d6fa282d4e",
        "text": "# Forward-Forward (FF) Algorithm\n\nInspired by the noise contrastive estimation (NCE) method [48], the Forward-Forward (FF) algorithm presents positive and negative examples sequentially to the network inputs. Once trained, the network is expected to produce distinctly different neural responses across all layers for these examples. The key challenge is generating “negative” examples that closely resemble the training data but still provide enough contrast for the network to learn meaningful representations.\n\nSupervised learning methods applicable to any dataset have been developed, solving tasks like MNIST and CIFAR-10 [19, 32]. However, for unsupervised FF learning, there is no universal method to generate positive and negative examples for all databases, hindering FF’s application to more complex unsupervised tasks beyond MNIST. This limitation is evident in the few FF points in the Unsupervised panel of Fig 1, where only Hebbian and activation learning algorithms have successfully solved CIFAR-10 and STL-10 by combining a local rule with a purely forward architecture.\n\nMoreover, the FF algorithm currently lacks the ability to handle time-varying sequential data, limiting its applicability in neuromorphic systems which often deal with dynamic inputs from the real world. While the original FF paper [19] includes a multi-layer recurrent neural network, its purpose is to model top-down effects, using a static MNIST image repeated over time frames as input. Another implementation demonstrates a limited form of sequence learning with a fully connected network."
    },
    {
        "id_": "cdbea3e6-6370-43da-b505-149a44dedaaa",
        "text": "# Self-Contrastive Forward-Forward (SCFF) Method\n\nBut this architecture could not handle real-time sequential data due to the absence of recurrence. As a result, FF has yet to be extended to effectively handle recurrent network scenarios for time-varying inputs.\n\nIn this work, we introduce the Self-Contrastive Forward-Forward (SCFF) method, where each data sample is contrasted within itself to enable efficient learning. This method is inspired by self-supervised learning to provide a simple and effective way to generate positive and negative examples for any dataset. In SCFF, a positive example is created by concatenating an input with itself, while a negative example is formed by concatenating the input with a randomly selected example from the training set. This simple method not only extends the capabilities of FF to unsupervised tasks but also surpasses the state-of-the-art accuracy of similar algorithms on the MNIST, CIFAR-10, and STL-10 image datasets. It also opens the path to solving sequential tasks by contrasting inputs with FF."
    },
    {
        "id_": "c9865b1f-e97c-4a98-bb1a-7f678af32aff",
        "text": "# Contributions\n\n- We propose an approach called SCFF that takes inspiration from self-supervised learning to generate positive and negative examples and train neural networks with the Forward-Forward algorithm in an unsupervised way, applicable to a wide range of databases.\n- We show that SCFF significantly outperforms existing unsupervised purely-Forward and local learning algorithms in image classification tasks. With a multilayer perceptron (MLP), we achieved a test accuracy of 98.7% on MNIST. With a convolutional neural network (CNN), we reached 80.75% on CIFAR-10 and 77.3% STL-10 (which includes a small number of labeled examples alongside a much larger set of unlabeled data).\n- We present the first demonstration of the FF approach being successfully applied to sequential data. Our findings show that the proposed SCFF method effectively learns representations from time-varying sequential data using a recurrent neural network. In the Free Spoken Digit Dataset (FSDD), SCFF training results in an accuracy improvement of over 10 percentage points compared to scenarios where hidden connections remain untrained (random).\n- We conduct a theoretical and illustrative analysis of the distribution of negative examples generated with our method in comparison to positive examples within the data space. The analysis reveals that negative data points consistently position themselves between distinct clusters of positive examples. This positioning suggests that negative examples play a crucial role in pushing apart and further separating adjacent positive clusters, thereby enhancing the efficiency of classification.\n\nOur results demonstrate the potential of Self-Contrastive Forward-Forward (SCFF) methods for efficient and flexible layer-wise learning of useful representations in a local and purely forward unsupervised manner. In section 2.1 and 2.2, we will introduce the two foundational methods that SCFF builds upon: the original Forward-Forward algorithm and Contrastive Self-Supervised learning, highlighting their differences and similarities. Next, in section 3, we will present our Contrastive Forward-Forward algorithm and discuss our findings. Finally, we will explore the relationship of SCFF to other purely forward and/or local learning methods in the discussion of section 4."
    },
    {
        "id_": "420d1eb7-d2ea-435b-8db0-fdc0d4081c93",
        "text": "# 2 Background\n\n|FF'$ Negative example|Forward Forward|Contrastive Learning|Self-Contrastive Forward-Forward|\n|---|---|---|---|\n|Mirimite coodrey|Minimite Eoodrey|Encocc|Encodnt|\n|Mjumlie coodnes|Maumlie coodness|hybrid|Positive pair|\n|Mulme roodness|Minimice goodnet)|Klnimlz aercement|Maumle Foooness|\n|Lncoder|Encodtr|Positive input|Negative input|\n|Nleative pair|Positive input|Negative input| |\n\nFig. 2: Comparative diagram illustrating three distinct unsupervised (self-supervised) learning paradigms. a. Generation of a negative example is implemented by hybridization of two different images in the original FF paper [19]. b. In Forward Forward (FF) Learning, the layer-wise loss function is defined so as to maximize the goodness for positive inputs (real images) and minimize the goodness for negative inputs, each of which is generated by corrupting the real image to form a fake image, as shown in a. c. In Contrastive Learning, the InfoNCE loss function determines the similarity between representations of two inputs (two different inputs or two same inputs but with different augmentations) in the end of the network [49]. d. Our proposed Contrastive Forward Forward Learning algorithm combines the principles of Forward Forward Learning and Contrastive Learning algorithms to maximize the goodness for concatenated similar pairs and minimize the goodness for dissimilar pairs with a layer-wise loss function."
    },
    {
        "id_": "d0cb473d-1a4c-42d1-b1c8-9289086e56f3",
        "text": "# 2.1 The original Forward-Forward algorithm\n\nThe original Forward-Forward algorithm is depicted in Fig 2b. For an input example x, each layer’s output is assigned a ‘goodness’ score Gi(l), where l is the layer index. This score is calculated as the mean of the squared activities of the output neurons at layer l: Gi(l) = M1(l) m y2(l), where M(l) is the number of neurons at layer l and m represents the neuron index.\n\nPredefined positive and negative examples are successively presented to the network’s input. The possibility of a positive example xi being recognized as positive and a negative example (l) and pneg(xj) = ω(#neg→ Gj xj) being recognized as negative by the network are defined as ppos(xi) = ω(Gi(l)→ #pos(l)(l)) respectively. The sigmoid function ω(x) = 1 + e1→x evaluates the effectiveness of the separation, where #pos(l) and #neg(l) are fixed values that serve as hyperparameters of the network."
    },
    {
        "id_": "db66aee1-1e2d-40c0-94af-6adc6c4fb465",
        "text": "# Inspired by Noise-Contrastive Estimation\n\nInspired by Noise-Contrastive Estimation [48], which aims to distinguish positive examples from noisy examples, the objective of FF learning is to increase the goodness score for positive input examples so that it significantly exceeds the threshold (ppos(x) ↑ 1) and to decrease the goodness score for negative input examples so that it falls well below the threshold (pneg(xj) ↑ 1). Weight updates are performed locally by combining the gradients derived from both positive and negative examples:\n\nLFF = Expos[log ppos(x)] - Exjneg[log pneg(xj)] + EGj,neg[log ω(#neg Gj,neg)]\n\n= EGi,pos[log ω(Gi,pos #pos)]\n\nwhere Gi,pos(l) and Gi,neg(l) respectively correspond to the goodness for the positive and negative examples input at layer l. The final loss is computed over all N examples in the batch.\n\nHinton [19] proposed that FF can be used for self-supervised representation learning by using real data vectors as the positive examples and engineered data vectors as the negative examples. The negative examples should be carefully designed, rather than relying on random corruption methods of the training data like noise injection or occlusion. The negative data needs to be sufficiently challenging to ensure the network learns useful information. To make FF focus on the long-range correlations in images that characterize shapes, the negative data should have very different long-range correlations while maintaining similar short-range correlations. For the MNIST dataset, this can be achieved by creating a mask with large regions of ones and zeros. As proposed in the original framework [19], negative data is then generated by combining one digit image multiplied by the mask with a different digit image multiplied by the inverse of the mask, as illustrated in Fig 2a. Although this method performs well for MNIST (as shown in the Unsupervised panel in Fig 1), it does not easily translate to more complex image databases like CIFAR-10 and STL-10, resulting in limited accuracy on these benchmarks."
    },
    {
        "id_": "2504140b-dd50-4180-8df4-7db1887c92cb",
        "text": "# 2.2 Contrastive self-supervised learning\n\nIn this article, we draw inspiration from contrastive self-supervised learning methods to construct positive and negative examples for FF, applicable to any image database and beyond. Contrastive learning, illustrated in Fig 2c, is a self-supervised technique designed to learn representations by contrasting positive pairs against negative pairs. This approach relies heavily on data augmentation, where a positive pair consists of two different augmented views of the same data sample, and negative pairs are constructed from different samples. While FF shares strong similarities with contrastive learning, it also has key differences.\n\nBoth methods utilize the concept of contrasting positive and negative examples to guide the learning process. They also both aim to learn meaningful data representations without labeled data in the first phase, allowing to extract information from the hidden layers in the second phase by training a linear classifier."
    },
    {
        "id_": "464a8e49-dbdb-49cc-9be1-5fd07d1a8a77",
        "text": "However, their loss functions are fundamentally different. FF focuses on the absolute goodness of positive versus negative examples, decoupling them in the loss. On the other hand, general contrastive losses employ relative comparisons through distances or similarities between positive and negative examples.\n\nAdditionally, the neural network optimization method differs. FF defines a local loss function at each layer and avoids backpropagation through hidden layers, whereas typical contrastive learning approaches train deep networks end-to-end. Lastly, self-supervised FF emphasizes data corruption to generate negative examples, focusing on specific characteristics of corruption to enhance learning, while contrastive learning relies heavily on varied data augmentation techniques."
    },
    {
        "id_": "6372b7b5-86a1-4edc-ad27-e40afd079f0a",
        "text": "# 3 Self-Contrastive Forward-Forward algorithm and results\n\nFig. 3: SCFF method for processing with Convolutional Neural Network Architecture. a. The original batch of images (top row) is processed to generate positive (middle row) and negative examples (bottom row). b. The generated positive and negative examples undergo a series of convolutional (Conv.) and pooling (AvgPool or Maxpool) operations to extract relevant features. The output neurons which are extracted from each hidden layer after an external average pooling layer are then fed together into a softmax layer for final classification.\n\nWe draw inspiration from contrastive self-supervised learning algorithms to propose the Self-Contrastive Forward-Forward (SCFF) algorithm, which addresses the drawbacks of FF, including the complexity of generating negative examples and its inability to generalize well to convolutional neural networks in a purely forward manner."
    },
    {
        "id_": "9a245393-8ec5-4bf5-840a-c4ba4a0b25b3",
        "text": "# 3.1 Creating the negative and positive examples\n\nInstead of contrasting the representations in the feature space as in contrastive self-supervised learning (Fig 2c), SCFF directly takes pairs of positive and negative images as inputs to the neural network (Fig 2d). More specifically, given a batch of N training examples, and for a randomly selected example xk (k ∈ {1, N}) in the batch, the positive example xi,pos(0) (the number 0 is the layer index) is the concatenation of two repeated xk, i.e., xi,pos = [xk, xk]T. The negative example xj,neg(0) is obtained by concatenating xk with another example xn (n ≠ k) in the batch, i.e., xj,neg(0) = [xk, xn]T (or [xn, xk]T). Fig 3a shows some instances of generated positive and negative examples from the original training batch for the STL-10 dataset.\n\nConsidering the case of a fully connected network, the concatenated pair of images (positive or negative examples) are sent as inputs to the network. The outputs for the positive and negative examples from the first layer can be written respectively as yi,pos = f(W1xk + W2xk), yj,neg = f(W1xk + W2xn), where f is the ReLU activation function. The weight matrices W1 and W2 correspond to the connections to the two images. In practice, we set W1 = W2 because the gradient of the loss function with respect to W1 and W2 converges to the same value. Setting W1 = W2 accelerates the training speed and improves the performance. Intuitively, this can be understood by recognizing that swapping the positions of xk and xn in the concatenated image should not affect the output neural activities. For a rigorous mathematical proof, see Appendix A."
    },
    {
        "id_": "d6fa7dd3-c30d-4044-9e0f-740f25246ac9",
        "text": "# Relative positions between positive and negative examples (IRIS dataset)\n\n|LDA for positive and negative examples|Mean positions|\n|---|---|\n|Setosa|1|\n|Versicolor|0.05|\n|Negative data| |\n\nFig. 4: Probability distributions of relative positions between positive and negative examples. a Theoretical distributions of positive examples from two different classes with distinct means (2μ1 = 0 and 2μ2 = 15) and identical variance (2σ2 = 4) are shown with blue and orange curves, respectively. The theoretical distribution of negative examples derived from the two classes using the formula 2 is depicted by the grey curve. b Continuous probability density of LDA applied to the IRIS dataset, displaying contours for positive examples in green, red, and blue, and for negative examples in grey."
    },
    {
        "id_": "cb83d656-0b8c-4f23-b551-f060225a82d0",
        "text": "# The concept of SCFF\n\nThe concept of SCFF can be understood through the lens of Noise Contrastive Estimation (NCE). In NCE, a key insight is that “the noise distribution should be close to the data distribution, because otherwise, the classification problem might be too easy and would not require the system to learn much about the structure of the data”[48]. Our method of generating the positive and negative examples aligns with this principle if we treat the negative examples as “noise data”. We assume that the data samples for each class follow a multivariate Gaussian distribution with a shared covariance matrix $ and that each class is statistically independent of the others—assumptions commonly employed in various statistical models [50]. Furthermore, noting that the input weight matrices W1 and W2 are identical, i.e. W = W1 = W2, the concatenated inputs processed by the network are simplified as follows: the positive(0) example becomes yi,pos = f(W1xk + W2xk) = f(W(2xk)), and the negative example is yj,neg = f(W1xk + W2xn) = f(W(xk + xn)). This is equivalent to treating the positive examples as xi,pos = 2xk and negative examples as xj,neg = xk + xn. Therefore, the distributions of positive examples xi,pos and negative examples xj,neg follow:\n\nxi,pos ↗ N(2μ1, 2!)\n\nxj,neg ↗ N(μ1 + μ2, 2!)(0)\n\nwhere μ1 and μ2 are means of two different classes respectively. Theoretically, the negative examples always lie somewhere between two different clusters of positive examples in the sample space, as illustrated in Fig. 4a for the one-dimensional case. For practical analysis with a real-world dataset, we visualized the distributions of positive and negative examples from the IRIS dataset [51] using 2D linear discriminant analysis (LDA), which distinguishes between three different types of irises, as shown in Fig. 4b. This visualization shows that the negative examples are positioned between different clusters of positive examples, suggesting that they contribute to pushing apart and further separating adjacent positive examples as they are mapped into higher-dimensional space during training. Additionally, negative examples are formed by combining two examples from different classes, enriching the diversity of negative examples and leading to more robust training. For a detailed analysis of how the LDA components evolve during training as the input data is mapped into the feature space and more theoretical results, please refer to Appendix B."
    },
    {
        "id_": "8bca857f-a95e-46ce-a3f7-f44efff57e82",
        "text": "# 3.2 Training procedure\n\nWe evaluate SCFF on different image datasets including MNIST [45], CIFAR-10 [46] and STL-10 [47] (results in Fig. 1 and Fig. 5), as well as an audio dataset Free Spoken Digit Dataset (FSDD) [52] (results in Fig. 6).\n\nEach layer of the network was fully trained and frozen before training the next one. After unsupervised training with SCFF, we froze the network and trained a linear downstream classifier [9, 53] with the back-propagation method on representations created by the network using the labeled data. The linear classifier was optimized using cross-entropy loss. The accuracy of this classification serves as a measure of the quality."
    },
    {
        "id_": "13a85ef0-0c5c-4f7a-87b7-c58f0655a710",
        "text": "# 3.3 Fully connected network: MNIST\n\nOn the MNIST dataset, when trained on a two-layer fully connected network with 2000 hidden neurons each, SCFF achieves a test precision of 98.7%, which is comparable to the performance achieved by backpropagation [19]. This surpasses previously published benchmarks on other biologically inspired algorithms, such as 97.9% in [9], 98.42% in [8] (supervised training), and 96.6% in [7]. The full comparison is shown in the right column of the green frame in Fig 1."
    },
    {
        "id_": "d4c7ca79-d9da-464c-947a-df63bad8f9ed",
        "text": "# 3.4 Convolutional neural networks: CIFAR-10 and STL-10\n\n| |SCFF|Backprop|\n|---|---|---|\n|1 layer|90|50|\n|2 layers|83.8|66.2|\n|3 layers|80.8|72.4|\n|4 layers| |77.3|\n\nFig. 5: Comparison of test accuracy at different layers by using SCFF and Back-propagation methods on CIFAR-10 in a and on STL-10 dataset in b.\n\nThe convolutional neural network processes three-dimensional color images. The original images are concatenated along the channel dimension to form positive or negative inputs (see Fig 3). The output of each convolutional layer is represented as a three-dimensional vector yi,pos (or yi,neg) ∈ RC↑H↑W. The Loss function at layer (l) is then defined as:"
    },
    {
        "id_": "22a166db-0629-4e4f-8514-e9d120361331",
        "text": "# Current Page"
    },
    {
        "id_": "52e48707-4390-4533-8053-f98f65b52d0a",
        "text": "# Equations\n\nLSCFF = → E Gi,pos(l) 1 (l) (l)\n\n[ ∑∑logω(Gi,h,w,pos → # pos)]\n\n[ ∑∑logω(# neg → Gj,h,w,neg)]\n\n→ E Gj,neg(l) 1 (l) (l) (3)\n\nwhere the goodness of neural activities is calculated over the channels as Gi,h,w,pos =(l)\n1 ∑c y2(l) (or Gi,h,w,neg(l)= C∑c y2(l)1\ni,c,h,w,neg)."
    },
    {
        "id_": "4a802cf3-c949-4174-9e88-759a4a766121",
        "text": "# Results\n\nFor the CIFAR-10 and STL-10 datasets, we employed convolutional neural networks with architectures identical to those in [9] to extract features. SCFF achieves a test accuracy of 80.75% with a three-layer convolutional architecture (number of filters each layer: 96-384-1536) on CIFAR-10 and 77.3% with a four-layer convolutional architecture (number of filters each layer: 96-384-1536-6144) on STL-10. These results surpass the previous state-of-the-art accuracies for purely-forward unsupervised learning, of 80.3% on CIFAR-10 and 76.2% on STL-10 achieved using the SoftHebb algorithm [9]. This demonstrates the significant potential of the SCFF method to scale effectively to more complex datasets and architectures. The full comparison is shown in the left column of the green frame in Fig. 1.\n\nWe also compared the test accuracies at each layer using SCFF and Backpropagation (BP) methods on CIFAR-10 and STL-10, as shown in 5. Notably, for STL-10, SCFF achieved a final layer performance of 77.3% higher than the one of BP: 77.02% (Fig. 5b). This is because the STL-10 dataset contains a large amount of unlabeled images, which limits the effectiveness of supervised BP training. By fine-tuning SCFF with end-to-end BP on the few labelled STL-10 examples, SCFF’s accuracy further improves to 80.13%. This demonstrates that SCFF is highly suitable for unsupervised pretraining followed by supervised BP training, making it ideal for semi-supervised learning approaches.\n\nUnlike other unsupervised learning methods, where the result is obtained solely from the final layer’s output, SCFF integrates neuron information with the linear classifier from intermediate layers, leading to more comprehensive feature extraction [19]. For CIFAR-10 (Fig. 5 a), the test accuracy for the two-layer and three-layer models was obtained by combining the outputs of all previous layers (pooled information for dimensionality reduction; see Methods section) before feeding them into the final linear classifier. However, because the STL-10 dataset consists of high-quality images, the number of output neurons in each layer is much larger than that in CIFAR-10. Therefore, for the STL-10 dataset, we did not combine outputs from previous layers for training the linear classifier, with the exception of the fourth layer. In this case, we combined the outputs from both the third and fourth layers for the final classification, resulting in a 1% improvement in accuracy compared to using only the fourth layer’s outputs as input to the linear classifier.\n\nBy visualizing and investigating the class activation map, which highlights the importance of each region of a feature map in relation to the model’s output, we\n\n11"
    },
    {
        "id_": "dbab609b-2909-4a40-aa16-ec5d1730a9f3",
        "text": "# 3.5 Recurrent neural network: FSDD audio dataset\n\nThe original FF paper [19] introduces a specialized recurrent neural network (RNN) that models top-down effects using repeated static images as input for each time frame. In contrast, our work focuses on training an RNN that processes time-varying inputs.\n\nWe employ the Free Spoken Digit Dataset (FSDD), a standard benchmark task for evaluating RNN training performance [55–57]. The FSDD is a collection of audio recordings where speakers pronounce digits (0-9) in English. We follow the standard procedure consisting in extracting frequency domain information at different time intervals, here through Mel-Frequency Cepstral Coefficient (MFCC) features (39 channels) [58]. Plots of the evolution of MFCC feature with time are shown in Fig. 6 for the digits 3 and 8. The SCFF method forms positive and negative examples by concatenating the same input for positive examples, and different ones for negative examples. Fig. 6a presents a negative example which is generated by concatenating MFCC features from two different digits. The goal of the task is to recognize the digit after feeding in the full sequence, from the output of the network at the last time step.\n\nWe train a Bi-directional Recurrent Neural Network (Bi-RNN) in an unsupervised way using the SCFF method to classify the digits. The procedure that we use for this purpose is illustrated in Fig. 6a. Unlike conventional uni-directional RNNs, where sequential input is processed step by step in a single direction, resulting in a sequence of hidden states from H0 to HT (as depicted in the bottom RNN in Fig. 6a), the Bi-RNN comprises two RNNs that process the input in parallel in both forward and backward directions. This results in hidden states evolving from H0 to HT in the forward RNN and from HT to H0 in the backward RNN, as shown in the top portion of the figure. The red regions in the figure highlight the features at different time steps. This bidirectional structure is particularly advantageous for tasks where context from both preceding and succeeding audio frames is critical, such as speech recognition, enhancing model performance compared to conventional uni-directional RNNs [59].\n\nThe output of each directional RNN for a positive or negative input example is a two-dimensional vector hi ∈ RM×T, where T represents the number of time steps and M denotes the number of hidden neurons. The loss function at layer l is then defined as:\n\nLSCFF = EGi,pos(l) [1(l) ∑t logω(Gi,t,pos → #pos)]\n- EGj,neg(l) [1(l) ∑t logω(#neg → Gj,t,neg)]"
    },
    {
        "id_": "8ec97bf9-c3ed-42dd-a2fa-d624064c6d94",
        "text": "# Bi-directional RNN results on FSDD dataset"
    },
    {
        "id_": "473441ea-5cb5-4522-8f0b-e36a9d94fd31",
        "text": "# 1. Training procedure of SCFF on a Bi-RNN\n\nwhere the goodness of neural activities is calculated at each time step as\n\nG(l)posi,t = M1 ∑m h2(l)i,t,m,pos (or Gi,t,neg = M1(l) ∑m h2(l)i,t,m,neg).\n\nUnsupervised training on hidden connections (grey arrows)"
    },
    {
        "id_": "f657ca07-70c9-4610-9c24-6889c16f59ef",
        "text": "# 2. Comparison of test Accuracy\n\n|Digit|Trained with SCFF|Untrained|Trained with Backprop|\n|---|---|---|---|\n|3| | | |\n|8|1| | |\n\nAt each time step, the features are sequentially fed into the Bi-RNN (RNN and RNN↓). The red regions indicate features at different time steps. In the second stage, a linear classifier is trained using the final hidden states from both RNNs, i.e., HT and H0↓ as inputs for classification task."
    },
    {
        "id_": "aa3be594-2ea9-488a-8154-4904c4c90795",
        "text": "# 3. Test accuracy comparison\n\nThe blue, orange and green curves in Fig. 6b depict the test accuracy of the linear output classifier with hidden connections trained using SCFF, with random (untrained) hidden connections, and with Backpropagation methods, respectively. SCFF achieves a test accuracy of 90.3% when trained with a one-layer Bi-RNN containing 500 hidden neurons in each direction (refer to Appendix E for further architectural details). It is below the performance of BackPropagation Through Time, reaching 96% accuracy on this small task, but well above the model with untrained (random) hidden connections which plateaus at 80.7%.\n\nThis result constitutes the first successful application of the FF approach to sequential data in an unsupervised manner. Unlike the BPTT method for training RNNs, SCFF avoids the issues of vanishing and exploding gradients, as the gradients at each\n\n13"
    },
    {
        "id_": "65b09492-6085-4eeb-a6fb-0594049b893b",
        "text": "# Table 1: Comparisons of the learning capabilities of different local learning methods and their test accuracy [%] on CIFAR-10 and STL-10 dataset.\n\n|Method|(Self) Unsupervised1|Local|Sequential2|CIFAR-10|STL-10|\n|---|---|---|---|---|---|\n|SimCLR (Chen et. al. 2020 [49])|✁|✂|✁|94.0|89.7|\n|PNN-SSL (Laydevant et. al. 2023 [62])|✁|✂|✂|77.0|-|\n|Bio-SSL (Tang et. al. 2022 [63])|✁|✁|✂|72.7|68.8|\n|CLAPP (Illing et. al. 2021 [64])|✁|✁|✂|-|73.6|\n|SigProp (Kohan et. al. 2023 [5])|✂|✁|✂|91.6|-|\n|EqProp (Laborieux et. al. 2021 [65])|✂|✁|✂|88.6|-|\n|DualProp (Høier et. al. 2023 [66])|✂|✁|✂|92.3|-|\n|FF (Hinton et. al. 2023 [19])|✁|✁|✂|59.0|-|\n|FF (Papachristodoulou et. al. 2024 [33])|✂|✁|✂|78.1|-|\n|FF (Wu et. al. 2024 [44])|✂|✁|✂|84.7|-|\n|PEPITA (Srinivasan et. al. 2023 [8])|✂|✁|✂|53.8|-|\n|Act. Learning (Zhou et. al. 2022 [7])|✁|✁|✂|58.7|-|\n|HardHebb (Miconi et. al. 2021 [11])|✁|✁|✂|64.8|-|\n|HardHebb (Lagani et. al. 2022 [12])|✁|✁|✂|64.6|-|\n|Hebb-CHU (Krotov et. al. 2019 [13])|✁|✁|✂|50.8|-|\n|Hebb-PNorm (Grinberg et. al. 2019 [67])|✁|✁|✂|72.2|-|\n|SoftHebb (Journé et. al. 2022 [9])|✁|✁|✂|80.3|76.2|\n|SCFF (ours)|✁|✁|✁|80.8|77.3|\n\n1 Self-supervised or unsupervised\n\n2 Can handle sequential data or not\n\nTime steps are calculated independently. This eliminates the dependency between time steps, providing a more stable training process.\n\nIt is also interesting to note that the network with untrained hidden and input connections is akin to Reservoir Computing, a method that is often used to leverage physical systems on sequential data for neuromorphic applications [60]. SCFF provides a way to train these input and hidden layer connections in a simple, hardware-compatible way, and opens the path to a considerable gain of accuracy. This achievement opens the door for its extension to more complex tasks involving temporal sequences and its potential use in neuromorphic computing domains, such as dynamic vision sensors [61]."
    },
    {
        "id_": "ac97545d-682c-4e7d-9966-0b8f58a80427",
        "text": "# 4 Discussion"
    },
    {
        "id_": "b2665e02-3ad8-4b40-85b8-0416182e5289",
        "text": "# 4.1 Comparison to the original FF algorithm\n\nIn addressing the limitations of the original FF algorithm, our method introduces several key improvements. Firstly, we have developed an approach for generating negative examples that can be applied to any database. This approach is also biologically plausible, since it operates by aggregating two similar or different images at the input, very much like our eyes do. This innovation directly addresses the criticisms highlighted in [8], which pointed out the biological implausibility of the negative examples used in the original FF algorithm.\n\nFurthermore, we have expanded the applicability of FF to complex unsupervised tasks beyond the MNIST dataset. The SCFF method achieves state-of-the-art (SOTA) accuracy for local methods on challenging datasets such as CIFAR-10 and STL-10."
    },
    {
        "id_": "e5f22042-9a20-4d47-81ea-72f83f4a42f7",
        "text": "# 4.2 Comparison to SOTA self-supervised learning (SSL)\n\nOur SCFF method draws significant inspiration from self-supervised contrastive learning techniques[49, 68]. While the accuracy of SCFF may be lower compared to these methods, primarily due to its layer-wise learning in a purely local manner, it offers unique advantages. Unlike global self-supervised methods, SCFF operates without requiring auxiliary heads (multi-layer nonlinear projector) or complex regularization techniques, which simplifies its implementation and makes it more suitable for neuromorphic computing applications.\n\nRecent developments in local versions of contrastive self-supervised learning have shown promising results[62, 63]. For instance, Laydevant et al.[62] empirically demonstrated that layer-wise SSL objectives can be optimized rather than a single global one, achieving performance comparable to global optimization on datasets such as MNIST and CIFAR-10 (see Table 1). However, the layer-wise training methods involving multi-layer MLP as projector heads might offer better performance in certain tasks, but at the cost of increased computational complexity. Illing et al.[64] have shown that local plasticity rules, when applied through the CLAPP model, can successfully build deep hierarchical representations without the need for backpropagation. However, this method introduces additional processing along the time axis, which may add complexity when dealing with data that lacks temporal dynamics."
    },
    {
        "id_": "ff63a293-b180-4328-bc6c-080830731dca",
        "text": "# 4.3 Comparison to other forward-only methods including non-Hebbian and Hebbian based\n\nRecently, other purely forward learning techniques have been developed, driven by their potential for biologically plausible and neuromorphic computing applications[7, 8]. Similar to Forward-Forward (FF), Pepita[8] processes data samples in two forward passes. The first pass is identical to FF, while the input of the second pass is modulated by incorporating information about the error from the first forward pass through top-down feedback. Activation Learning[7] builds on Hebb’s well-known proposal, discovering unsupervised features through local competitions among neurons. However, these methods do not yet scale to more complex tasks, limiting their potential applications.\n\nRecent advances in Hebbian deep learning have also shown remarkable progress[9, 11, 12, 69]. These methods are purely local in space and can be applied purely."
    },
    {
        "id_": "c8a590ef-9e9b-486c-a4f0-250ef48e328a",
        "text": "# 4.4 Comparison to energy-based learning methods\n\nEnergy-based learning methods, such as Equilibrium Propagation (EP), Dual Propagation (DP) and Latent Equilibrium (LE) [66, 70, 71], also offer locality in space and time. These methods have a significant advantage over SCFF due to their strong mathematical foundations, closely approximating gradients from BP and backpropagation through time (BPTT). This theoretical rigor allows them to be applied to a wide range of physical systems, making them particularly appealing for neuromorphic computing applications. EP, for instance, can operate in an unsupervised manner, while recent advancements in Genralized Latent Equilibrium (GLE) [72] have extended these models to handle sequential data effectively.\n\nHowever, the implementation of energy-based methods poses certain challenges. Specifically, the backward pass in these methods requires either bidirectional neural networks or dedicated backward circuits [73, 74]. These requirements can be complex to design and build in a manner that is both energy-efficient and compact. In contrast, the simplicity and versatility of SCFF in supporting both supervised and unsupervised learning, without the need for complex backward circuitry, make it a practical alternative for many applications [3]. This balance of accuracy, ease of implementation, and versatility underscores the potential of SCFF in advancing neuromorphic computing and biologically inspired learning systems."
    },
    {
        "id_": "ea281ed6-62dc-4ed0-b6d1-696759d02278",
        "text": "# 5 Conclusion\n\nIn conclusion, the Forward-Forward (FF) algorithm has sparked significant advancements in both biologically-inspired deep learning and hardware-efficient computation. However, its original form faced challenges in handling complex datasets and time-varying sequential data. Our method, Self Contrastive Forward-Forward (SCFF),"
    },
    {
        "id_": "49d39e22-1957-487f-91e0-cd984b1ab631",
        "text": "# 6 Methods\n\nSCFF learns representations by maximizing agreement (increasing activations/goodness) between concatenated pairs of identical data examples while minimizing agreement (reducing activations/goodness) between concatenated pairs of different data examples using a cross-entropy-like loss function at each layer. The network is trained layer by layer, with each layer’s weights being frozen before moving on to the next. Unlike the original FF framework, this approach incorporates several key components that contribute to achieving high accuracy across various tasks."
    },
    {
        "id_": "140c69aa-7ffc-4114-8258-80cdfdaf9887",
        "text": "# Normalization and Standardization\n\nFor vision tasks, the data is typically normalized by subtracting the mean and dividing by the standard deviation for each channel. These mean and standard deviation values are computed across the entire training dataset, separately for each of the three color channels. This dataset-wide normalization centers the data, ensuring that each channel has a mean of 0 and is on a comparable scale.\n\nIn addition to dataset-wide normalization, we also applied per-image standardization, which plays an important role in unsupervised feature learning [75]. Standardizing the images involves scaling the pixel values of each image such that the resultant pixel values of the image have a mean of 0 and a standard deviation of 1. This is done before each layer during processing [11, 47], ensuring that each sample is centered, which improves learning stability and helps the network handle varying illumination or contrast between images."
    },
    {
        "id_": "c77e78e9-95b0-41a0-a382-c8fbf8f79e64",
        "text": "# Concatenation\n\nThe positive and negative examples (e.g. xi,pos and xj,neg) are generated by concatenating two identical images for the positive examples and two different images for the negative examples. After being processed by the first layer, the output vectors yi,pos(0) and yj,neg(0) are obtained. There are two approaches for generating the inputs to the next layer. The first approach is to directly use the first layer’s output of the positive example yi,pos(0) as the positive input xi,pos, and the first layer output of the negative example yj,neg(0) as the negative input xj,neg for the next layer (refer to the highlighted blue section in Algorithm 1 in Appendix C). The second approach involves re-concatenating to form new positive and negative inputs for the next layer. This is done by treating the first layer’s positive outputs as a new dataset and recreating."
    },
    {
        "id_": "4cc1c233-f4f9-411c-8e90-4d12aff5e0ca",
        "text": "# Appendix C\n\nthe corresponding positive and negative examples, similar to how the original dataset was processed to generate the initial positive and negative examples (refer to the highlighted blue section in Algorithm 2 in Appendix C).\n\nAppendix C details the workflows of Algorithm 1 and Algorithm 2, focusing on their different approaches to generating positive and negative examples after the first layer. In practice, Algorithm 1 tends to be more effective for training the lower layers immediately following the first layer, while Algorithm 2 shows better performance in training deeper layers. Specifically, for the CIFAR-10 dataset, Algorithm 1 is utilized to train the second layer, while Algorithm 2 is applied to train the third layer. Similarly, for the STL-10 dataset, Algorithm 1 is employed for training the second and third layers, and Algorithm 2 is used for the fourth layer."
    },
    {
        "id_": "c884cdac-7660-4d6c-8e46-4fb5db12804c",
        "text": "# Triangle method of transmitting the information\n\n“Triangle” method was firstly introduced by Coates et al. [47] to compute the activations of the learned features by K-means clustering. This method was later found to be effective in other Hebbian-based algorithms [9, 11] for transmitting the information from one layer to the next. The method involves subtracting the mean activation (computed across all channels at a given position) from each channel, and then rectifying any negative values to zero before the pooling layer. This approach to feature mapping can be viewed as a simple form of ”competition” between features while also promoting sparsity.\n\nImportantly, the ”Triangle” activation only determines the responses passed to the next layer; it does not influence the plasticity. The output used for plasticity at each position is given by yi,pos = f(l)(xi,pos) and y(l) = f(l)(xj,neg), where f(l) refers to the convolutional operations followed by ReLU activation at layer l."
    },
    {
        "id_": "23f59b77-100c-4ba6-affc-dff0f225062b",
        "text": "# Penalty term\n\nTraining with the FF loss can lead to excessively high output activations for positive examples, which significantly drives positive gradients and encourages unchecked growth in their activation. To mitigate this, we introduce a small penalty term—the Frobenius Norm of the Goodness vector—into the training loss function. For outputs from a CNN layer, the goodness vector Gi,h,w,pos(l) is a two-dimensional matrix where each element represents the goodness calculated over the channel outputs processed by all filters under the same receptive field. In the case of Bi-RNN outputs, the goodness vector G(l)i,t is a one-dimensional matrix, with each element representing the goodness at each time step. When a large goodness value is computed for a positive example, it generates a negative gradient that reduces the activation, thereby preventing excessive growth. The impact of this penalty term on training performance is further analyzed in Appendix F."
    },
    {
        "id_": "b182fda4-942a-42d9-8efd-c247201144e9",
        "text": "# Additional pooling operation to retrieve the features\n\nTo assess the performance of the intermediate layers in image classification tasks, we apply an additional pooling operation (average or max pooling) to the output of the"
    },
    {
        "id_": "ec52799e-26c0-4a7a-be5d-81ebff46b56f",
        "text": "# Pooling Layer\n\nThis reduces the dimensionality of the features and helps in selecting relevant neuron activities. This approach is inspired by the \"four quadrant\" method used in previous work [11, 47], where local regions extracted from the convolutional layer are divided into four equal-sized quadrants, and the sum of neuron activations in each quadrant is computed for downstream linear classification tasks.\n\nAppendix D provides detailed information on the specific architecture of this additional pooling layer for various tasks."
    }
]