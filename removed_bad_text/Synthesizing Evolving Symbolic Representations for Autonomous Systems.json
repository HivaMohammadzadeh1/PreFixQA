[
    {
        "id_": "65abe763-15a1-4033-b808-b6b00082c28f",
        "text": "# 1 Introduction\n\nIn the last few years, new AI systems have solved incredible tasks. These tasks include real-world games, such as chess [1] and Go [2–4], videogames such as Atari [5], Dota [6], and different robotics tasks [7–10]. These results have been mostly achieved through the intensive use of Reinforcement Learning (RL, [11]) with the rediscovered technology of neural networks and deep learning [12]. Usually, “standard” RL focuses on acquiring policies that maximise the achievement of fixed assigned tasks (through reward maximisation) with a predefined collection of skills. New approaches have been proposed to enrich RL, allowing the agent to extend its initial capabilities over time inspired by neuroscience and psychology. Indeed, studies on animals [13–15] and humans [16–18] have explored the inherent inclination towards novelty, which is further supported by neuroscience experiments [19–21]. The field of intrinsically motivated open-ended learning (IMOL [22]) tackles the problem of developing agents that aim at improving their capabilities to interact with the environment without any specific assigned task. More precisely, Intrinsic Motivations (IMs [23, 24]) are a class of self-generated signals that have been used to provide robots with autonomous guidance for several different processes, from state-and-action space exploration [25, 26], to the autonomous discovery, selection and learning of multiple goals [27–29]. In general, IMs guide the agent in acquiring new knowledge independently (or even in the absence) of any assigned task to support open-ended learning processes [30]. This knowledge will then be available to the system to solve user-assigned tasks [31] or as a scaffolding to acquire new knowledge cumulatively [32–34] (similarly to what has been called curriculum learning [35]).\n\nThe option framework has been combined with IMs and “curiosity-driven” approaches to drive option learning [32] and option discovery [36–39]. In the hierarchical RL setting [40], where agents must chunk together different options to properly achieve complex tasks, IMs have been used to foster sub-task discovery and learning [41–43], and exploration [26]. Autonomously learning and combining different skills is a crucial problem for agents acting in complex environments, where task solving consists of achieving several (possibly unknown) intermediate sub-tasks that are dependent on each other. An increasing number of works are tackling this problem [29, 44, 45], most focused on low-level, sub-symbolic policy learning [46], in turn combined in a hierarchical manner using some sort of meta-policy [47]. While promising, these approaches necessarily face the problem of exploration, which becomes slower and less efficient as the space of states and actions increases."
    },
    {
        "id_": "b0126a7a-26b1-4aa2-b276-99d6e71c4123",
        "text": "# In contrast to sub-symbolic methods, symbolic approaches like Automated Planning\n\n[48, 49] enable the use of higher-level objects (referred to as symbols), resulting in quicker execution, facilitating the composition of complex sub-task sequences, and making the agent’s internal knowledge interpretable. However, Automated Planning approaches require that the high-level representation of the planning domain is appropriately defined in advance. Generally, planning requires prior knowledge of the world in which the agent operates expressed in terms of both the preconditions necessary for the execution of the actions as well as the effects that follow from executing them. The need to be provided with an ad-hoc symbolic representation of the environment limits the utilization of high-level planning for artificial agents in unknown or highly unstructured settings, where the acquisition of new knowledge and new skills is the progressive result of the agent’s autonomous exploration of the environment. However, some works tried to improve classical planning with autonomous model learning [50], suggesting to add new symbols to the symbolic representation supported by the human [51] and using a cognitive layer to manage an intermediate representation [52]."
    },
    {
        "id_": "0360c4f2-b3de-48a3-9183-dae88372b803",
        "text": "# Recently, some ideas have appeared in the literature proposing methodologies for integrating sub-symbolic and symbolic approaches\n\nor more generally, low-level and high-level modules [53]. On the one hand, some works tried to reconcile deep learning with planning [54, 55], goal recognition [56] and the synthesis of a symbolic representation of the domain [57, 58]. On the other hand, the integration has also been performed through a specific algorithm designed to produce an automated symbolic abstraction of the low-level information acquired by an exploring agent [59] in terms of a high-level planning representation such as the PDDL formalism [60], which explicitly describes the context necessary to execute an action on the current state (i.e., the preconditions and the effects) making use of symbols. This algorithm has been used as a module in architectures that integrate abstraction, planning and intrinsic motivations, such as IMPACT [37, 61]."
    },
    {
        "id_": "04318af5-62a5-4993-bcf5-c3c7edf9ca2f",
        "text": "# This work presents an approach to the integration of low-level skills and high-level representations\n\nthat allows to continuously update the set of low-level capabilities and their corresponding abstract representations. Both the creation and the extension of the agent’s knowledge is based on the implementation of two forms of intrinsic motivation (IM) which, respectively, (i) drive the agent to learn new policies while exploring the environment and (ii) encourage it to use its skills to reach less explored states, the rationale being that exploring unknown states increases the likelihood to learn new skills. Then, the data collected by the agent’s sensors before and after the execution of its skills are used by a specific algorithm to synthesize an updated abstract representation which can be used to plan the execution of sequences of low-level skills to reach more complex goals. The main contribution of this study is to create a framework that, virtually starting from zero symbolic knowledge, produces an abstraction of the low-level data acquired from the agent’s sensors, whose enhanced expressiveness can be exploited to plan sequences of actions that reach more and more complex goals."
    },
    {
        "id_": "9af5083e-095b-41bb-8e27-5a9549a53f2d",
        "text": "# 2 Background\n\nTo reach a high level of autonomy, an agent acting in the low-level space, sensing the environment with its sensors and modifying it through its actuators must implement a series of layers of abstraction over its state and action spaces. As human beings reason over both simple and complex concepts to perform their activities, so robots should be able to build their own abstract representation of the world to deal with the increased complexity, using labels to refer to actions and events to be recognized and reasoned upon. In this paper, two levels of abstractions are applied: the first one, from primitive actions to options [11, 62] and the second one, from options to classical planning [49]."
    },
    {
        "id_": "e33def62-ce46-48b8-aefc-0fc0e2452524",
        "text": "# 2.1 From primitives to options\n\nAs discussed before, at the lowest level, the agent sees the world with its sensor’s values and changes it through the movement of its actuators. The most common formalism at this stage to deal with this type of representation is the Markov Decision Process (MDP), which models the environment as the tuple:\n\n(S, A, R, T, ω), (1)\n\nin which S represents the set of possible high-dimensional states where each s → S is described by a vector of real values returned by the agent’s sensors, A describes the set of low-level actions a → A in some cases also called primitives, R the reward function where R(s, a, s→) is a real value returned executing a from state s achieving s→, T the transition function describing for T (s→ |s, a) the probability of reaching the state s→ executing a from s, and the discount factor ω → (0, 1] describing the agent’s preference for immediate over future rewards. Usually, in this setting, the goal is to maximize return, defined as\n\nR = ∑ωi R(si, ai, si+1).↑ (2)\n\nHowever, dealing with the state and action spaces of the formulation (1) is, in certain cases, impractical due to the high dimensional spaces considered. An effective formalism introduced to reduce the complexity of the problems is the option framework [62]. The option is a temporally-extended action definition which employs the following abstracted representation:\n\no = (Io, εo, ϑo), (3)\n\nwhere the option o is defined by an initiation set Io = {s|o → O(s)} representing the set of states in which o can be executed, a termination condition ϑo(s) ↑ [0, 1] returning the probability of termination upon reaching s by o, and a policy εo which can be run from a state s → Io and terminated reaching s→ such that the probability ϑo(s→) is sufficiently high. A policy is a function defining the behavior of the agent, mapping the perceived state of the environment to the action to be taken [11]. It is worth noting that options create a temporally-extended definition of the actions [62]."
    },
    {
        "id_": "189084ba-7cee-4ce0-ba14-3de5599ab839",
        "text": "Fig. 1: Representation of the option reach the ball.\n\nIndeed, the option is an abstraction defining an action as a repeated execution of policy ε from a state s → I o to another s→ in a maximum amount of time steps ϖ.\n\nFor instance, Figure 1 depicts an environment before and after executing the option “reach the ball”, respectively Figure 1a and 1b. When the agent wants to execute this option, it checks whether its current state belongs to the initiation set of the option. In our case, and assuming that the option’s policy simply consists in moving towards the ball, the agent can execute the option and runs the policy ε o until the termination condition returns a sufficiently high probability of success or ϖ time steps are reached. In the Figure 1b the options successfully terminates getting close to the ball.\n\nPassing from low-level actions to options reduces the agent’s action space. Adopting options in the MDP formalism implies moving to the semi-Markov Decision Process (SMDP):\n\n(S, O, R, P, ω), (4)\n\nwhere S is the original state space, O(s) is the set of options executable from state s, R(s, ϖ |s, o) describes the reward expected executing o → O(s) from state s reaching s → after ϖ time steps, P (s→ , ϖ |s, o) returns the probability of reaching state s→ after ϖ time steps executing o → O(s) from state s, and the discount factor ω → (0, 1]. Using options entails moving in the low-level state space S with abstracted actions permitting the agent to perform extended and more complex behaviors, reducing the number of actions to achieve a certain task and simplifying the problem."
    },
    {
        "id_": "9f624c9b-5542-4363-9f0e-c3cf710d75f4",
        "text": "# 2.2 Options and Classical Planning\n\nThe option formalism and its way of abstracting the dynamics of an environment share common characteristics with classical planning, in which the world is described in a simplified formal description considering only the aspects necessary to solve the agent’s task [49]. Planning is the field of research studying formal methods to automatically find solutions, also called plans, to tasks requiring a sequence of actions [ϱ 1, . . . , ϱ n] to reach a goal state s g from an initial state s init. The plan ς is obtained by giving in input a model of the environment dynamics and the problem definition to a planner, returning a solution applying general optimization algorithms.\n\n5"
    },
    {
        "id_": "c4426aad-6ad8-49ac-9669-9d28f66788dc",
        "text": "# Classical Planning\n\nClassical planning is a particular instance of this methodology exploiting a logical language (i.e. propositional logic, first-order logic, etc.) to capture an abstract symbolic description of the world [49]. The symbol, which is the core of classical planning, is a name given to a certain set of states s → S satisfying a specific condition in the sensorimotor space. This mapping from the symbol to the real world states is called grounding. Specifically, a symbol φZ → !, with ! set of the available symbols, is the name given to a test ϖZ, and the corresponding set of low-level states where the test is satisfied Z = {s → S | ϖZ(s) = True}, with the high-dimensional low-level state space S [59]. To determine whether or not a low-level state si belongs to the state set Z, the φZ(s) test is run, which will return either True or False. Again, both iZ and φZ provide the semantics of the symbol; Z is the symbol’s grounding set, while φZ is its grounding classifier. Symbols can be combined using operators having the following meaning:\n\n- ¬φZ corresponds to the negation of symbol φZ;\n- φX ↓ φY corresponds to the union of symbols φX and φY (i.e., the union of their respective grounding sets);\n- φX ↔ φY corresponds to the intersection of symbols φX and φY (i.e., of their respective grounding sets).\n\nIn classical planning, operators and symbols are used to describe actions in the following form:\n\nϱi = (prei, eff+, effi ↓), meaning that the action ϱi → A can be executed when all the symbols {φ|φ → pre}i also called preconditions, are True, and executing ϱi produces the changes of the value of some symbols, also called effects, implying that all symbols {φ|φ → effi+} assume the value True and symbols {φ|φ → effi↓} become False.\n\nFinally, using symbols ! and high-level actions A as building blocks, it is possible to define the model of environment D, also called domain, and the problem P to solve. A classical planning domain can be defined as:\n\nD = (!, A, ”), using a set of symbols !, actions A and a state-transition function ”: ˆ ↗ A ↑ ! !ˆ, where !ˆ is the set of possible subsets of !. The state-transition function ”(!ˆs, ϱ) = (ˆs ↘ effi↓) ≃ effi+, if ϱi is applicable to !ˆs, where !ˆs is the set of symbols whose grounding set intersection defines the state s → S. These elements are sufficient to describe the dynamics of the environment, over which the planner can reason and create chains of actions to reach a final goal. The function ” encapsulates the transition model of the environment in each action model described by (5), defining the possible way to build sequences of them. Instead, the problem can be formalized as:\n\nP = (ˆsinit, !!ˆsg), where !ˆsinit is the set of symbols whose grounding set intersection describes sinit and !ˆsg the set of symbols whose grounding set intersection characterizes sg."
    },
    {
        "id_": "1798167d-da77-41cc-9883-02c0cd306521",
        "text": "# 2.3 Intrinsic Motivation\n\nThe impulse to drive the agent away from the monotony of its usual activities, which psychologists and cognitive scientists have studied under the name of intrinsic motivation, is one of the most important elements enabling Open Ended Learning (OEL). The research in the field of Intrinsic Motivation (IM) concerns the study of human behaviors not influenced by external factors but characterized by internal stimuli (i.e. curiosity, exploration, novelty, surprise). In the case of artificial agents, we can summarize such aspect as anything that can drive the agent’s behavior which is not directly dependent on its assigned task.\n\nThe insights provided by the IMs gave the researchers new ideas to model the stimuli of the agent (e.g. curiosity). Indeed, some models have been implemented using the prediction error (PE) in anticipating the effect of agent’s actions (and more precisely the improvement of the prediction error [64, 65]) as an IM signal. A formal definition of agent driven by its curiosity has been formulated by Schmidhuber [64] as"
    },
    {
        "id_": "5774e423-1a36-43f6-bb4c-7df2e377fb25",
        "text": "simply maximizing its future success or utility, which is the conditional expectation\n\n[  ∑T          ∣h(⇐ t)],\n\nu(t) = E μ     ω =t+1r(ϖ )∣∣                                            (10)\n\nover   t = 1, 2, . . . , T time steps, receiving in input a vector x(t), executing the action y(t), returning the reward r(t) at time t, taking into consideration the triplets h(t) = [x(t), y(t), r(t)] as the previous data experienced until time step t (also called history).\n\nThe conditional expectation E μ (·|·) assumes an unknown probability distribution μ from M representing possible probabilistic reactions of the environment. To maximize (10), the agent also has to build a predictor p(t) of the environment to anticipate the effects of its actions. The reward signal is defined as follows\n\nr(t) = g(r ext (t), rint (t)),                                     (11)\n\nwhich is a certain combination g of an external reward r ext (t) and an intrinsic reward r int (t). In particular, r int (t + 1) is seen as surprise or novelty in assessing the improvements in the results of p at time t + 1\n\nr int (t + 1) = f |C(p(t), h(⇐ t + 1)), C(p(t + 1), h(⇐ t + 1))|,                         (12)\n\nwhere C(p, h) is a function evaluating the performance of p on a history h and f is a function combining its two parameters (e.g. in this case, it could be simply the improvement f (a, b) = a ↘ b). It is important to notice that, as a baby does, an intrinsically motivated agent needs to find regularities in the environment to learn something. Consequently, if something does not present a pattern, there is nothing to learn and this becomes boring for both an agent and a human being.\n\nIn literature, IMs have also been categorized into different typologies [66–68]. An important discriminant aspect is the kind of signal received by the agent which can be of two types: knowledge-based (KB-IMs), which depends on the prediction model of the world (e.g. [64]), and competence-based (CB-IMs), which depends on the improvement of the agent’s skills (e.g. [27]). In the framework presented in the next section, both these typologies are employed. CB-IM is used at a lower level to learn new skills and KB-IM at a higher level to push the system to focus on the frontier of the visited states, from which it is more likely to discover novel information (e.g., find new states and learn new actions)."
    },
    {
        "id_": "cc54391c-aa25-4d81-8487-dd7cf0eb8750",
        "text": "# 3 System Overview\n\nThis section presents a new framework of an open-ended learning agent which, starting from a set of action primitives, is able to (i) discover options, (ii) explore the environment using them, (iii) create a PPDDL representation of the collected knowledge and (iv) plan to improve its exploration while reaching a high-level objective set by the game."
    },
    {
        "id_": "9db843de-0cc1-4cc9-b20c-80f1c370c998",
        "text": "# The aim of this study is to assess the potential of abstraction in autonomous systems\n\nThe aim of this study is to assess the potential of abstraction in autonomous systems and propose a new approach for planning systems, extending them with learning capabilities and behaviors driven by IMs. IMs are employed for discovering new options in a surprise-based manner at low-level and continuously exploring new states driven by curiosity at high-level. By the term abstraction, we simply mean mapping a certain problem space into a simpler one (e.g. converting a continuous domain into a discrete domain). In the proposed architecture, the abstraction is applied at two levels: a) passing from the primitive action space to the options action space and b) converting low-level data collected during the exploration into a high-level domain representation suitable for high-level planning, thus from raw sensors’ data to a PPDDL representation.\n\n|LTA|Planner|KB|\n|---|---|---|\n|High-level system|Abstraction/dispatching|EX|\n|Dispatcher|Abstraction| |\n|Low-level system|Option|Controller|\n|Exploration|ID, TD| |\n| |ID; ID'|KBo|\n\nFig. 2: The architecture of the system. The system, equipped with five primitive actions, iteratively (1) learns some options from scratch, (2) uses the options to explore the environment and gather low-level states data, (3) creates a PPDDL representation of the collected experience, (4) plan to solve the game and to improve its exploration.\n\nPerforming the pipeline depicted in Figure 2, the system creates different layers of abstraction, enriching the agent’s knowledge with causal correlations between options and enabling more efficient reasoning (i.e. using classical planning). Symbols can be seen as knowledge building blocks that can be used to search for interesting states and find new knowledge in a virtuous loop."
    },
    {
        "id_": "c721c44f-af71-479b-be40-94d6263d30e8",
        "text": "# 3.1 Architecture description\n\nAs depicted in Figure 2, the system can be seen as a three-layered architecture: (i) the higher level contains the explicit agent’s knowledge, (ii) the middle layer maps the high-level actions to their controllers and convert the raw sensors data into an explicit representation, and (iii) the lower level containing the components to sense the environment and interact with it. Mainly, the system executes the following pipeline:\n\n9"
    },
    {
        "id_": "e42a64f6-29b3-44ed-b319-580ccdc0f3e7",
        "text": "# 1. Option Discovery\n\nusing a set of primitives A = {a1, a2, ...} belonging to the agent, the system combines them to create higher level actions, in this case, the options;"
    },
    {
        "id_": "ae9b0e91-db43-4ee1-aae3-4c572a77eae5",
        "text": "# 2. Exploration\n\nthe set of options O = {o1, o2, ...} discovered in the previous step is used to explore the environment. In the meantime, the visited low-level states data are collected into two datasets: the initiation data ID and transition data TD containing, respectively, the samples of states in which an option can be run and the transition between states executing it;"
    },
    {
        "id_": "46229e72-4e24-4da6-88fc-5edaac3c5e95",
        "text": "# 3. Abstraction\n\ndatasets ID and TD of the visited low-level states until that moment are processed by the algorithm of abstraction, generating a PPDDL domain D of the knowledge collected;"
    },
    {
        "id_": "ac60f533-5825-4c91-bed3-57793755a008",
        "text": "# 4. Planning\n\nthe PPDDL representation D is used to assess whether the final goal of the task sg can be reached with the currently synthesized knowledge, and to generate a plan ςEX to explore interesting areas of the domain. This plan is suggested by the Goal Selector, function included in the Long-Term Autonomy (LTA) module."
    },
    {
        "id_": "c7adb32a-429d-474e-a91c-ca887603dad0",
        "text": "# 5. Execution Loop\n\nThe system, coordinated by the LTA, will execute again the loop from step 1, exploiting ςEX to improve the Option Discovery and Exploration phases."
    },
    {
        "id_": "bc1e0629-e191-4a3e-86b5-f8a16b90d993",
        "text": "# Algorithm 1: Discover-Plan-Act algorithm\n\n1:  procedure DISCOVER_PLAN_ACT(cycles, dpa_eps, dpa_steps, d_eps, d_steps)\n2:       c → 0 //Cycle initialization\n3:       O → {} //Option set initialization\n4:       ID → {} //Initiation Data initialization\n5:       TD → {} //Transition Data initialization\n6:       ωEX → {} //Initially, the high-level plan is empty\n7:       while c < cycles do //For each cycle\n8:            Onew → DISCOVER(d_eps, d_steps, ωEX) //Learning the available options\n9:            O → O ↑ Onew\n10:             IDnew ID ↑ IDnew, TDnew → Collect_Data(dpa_eps, dpa_steps, O, ωEX)\n11:             ID → TD ↑ TDnew\n12:             TD →\n13:             D → Create_PPDDL(ID, TD)\n14:             starget → Get_Target_State()\n15:             Ptarget → Generate_PPDDL_Problem(starget)\n16:             ωEX → Plan(D, Ptarget)\n17:             Check_PPDDL_Validity(D)\n18:             c → c + 1\n19:        end while\n20:   end procedure\n\nIn this setting, the agent is initially only endowed with a set of primitive movements A = {a0, ..., am}, and the world s → S is represented in terms of a vector (v0, ..., vn) of low-level variables vi → R, whose values as retrieved by the agent’s sensors.\n\nThe iterative utilization of this framework allows the synthesis of an emerging abstract representation of the world from the raw data collected by the agent, which continuously undergoes a refinement process over time, as it gets enriched with new."
    },
    {
        "id_": "0c5a4fea-8b11-40dc-8450-2805dd08a215",
        "text": "# 3.1.1 Option Discovery\n\nIn this section, we will analyze the Option Discovery module (Algorithm 1, line 8) in greater detail. As previously anticipated, the discovery of new options is considered to be driven by the agent’s surprise in finding out that new primitives are available for execution, during the agent’s operations. When the agent encounters a change in the availability of its primitive abilities, it stores this event as a low-level skill that can be re-used later to explore the surrounding environment.\n\nBy executing the algorithm, the agent can discover a set of options O from scratch; such options are generated by repeatedly executing a certain primitive a → A among the available ones and collecting the produced changes in the environment. This procedure is intentionally implemented in a simplified way, given that the focus of this work is on the architecture for extensible symbolic knowledge to be reusable to reach intrinsic and extrinsic goals autonomously; more sophisticated strategies to discover new policies are left to future works.\n\nIn more detail, the agent creates new options considering the following modified definition of option: o(ap, at, I, ε, ϑ), where ap and at are primitive actions such that: (i) ap ⇒= at, ap is used by the execution of ε, (ii) at stops the execution of ε when it becomes available, (iii) ε is the policy applied by the option, consisting in repeatedly executing ap until it can no longer be executed or at becomes available, (iv) I is the set of states from which ap can run; and (v) ϑ is the termination condition of the option, corresponding to the availability of the primitive at or to the impossibility of further executing ap. For the sake of simplicity, in the remainder of the paper the option’s definition will follow the more compact syntax I o(ap, at) meaning that I checks the following two conditions: at becomes available or ap is no longer available, and ϑ verifies.\n\nAlgorithm 2 describes in further details the option discovery procedure previously described. At the beginning of each discovery episode, the plan ςEX is executed to reach a new area where to start learning new options (line 7-9). Then, for a maximum number of episodes and steps, the agent saves the current state s and randomly selects a primitive ap which can be executed in s (line 10-12). ap is repeatedly executed towards reaching the state s → t until either ap is no longer available or new primitives beyond ap become available. If s ⇒= t s →, the procedure creates a new option o(ap, {}) in the case where o = o(ap, a) if a new primitive a has become available, or o = in the opposite case. In either way, in case o has not been discovered before, it is added to the other collected options. It is important to note that the options are independent on the state where the agent is, and are defined by the primitives’ availability. This definition makes options reusable on different floors and with different objects, just."
    },
    {
        "id_": "a771a3e9-d2e2-4afb-aecf-4ab0bffb92ea",
        "text": "# 3.1.2 Exploration\n\nAfter discovering a set of valid options O as explained in the previous section, the system exploits them to explore the environment, collecting data about the reached low-level states (Algorithm 1, line 10). Considering that the abstracted representation of the world does not change significantly with a small amount of new data, the function Collect_Data() is in charge of executing d_steps options for d_eps episodes, in which the agent starts its exploration from the initial configuration of the environment.\n\nAt each timestep, the agent attempts to perform an option o → O from a certain low-level state s → S. The selection of the action o during the exploration can follow different strategies, which are described in the subsection 3.1.4. In case the execution of\n\n1 Link to the original implementation of [59]: https://github.com/sd-james/skills-to-symbols"
    },
    {
        "id_": "5696f758-86f1-4eb9-8a57-bf09356030db",
        "text": "# Algorithm 2 Option Discovery\n\n1:  procedure OPTION_DISCOVERY(d_eps, d_steps, ω EX )\n2:      O new  0ep < d_eps do //For each episode→ {}\n3:      ep →\n4:      while → 0\n5:          T\n6:          Reset_Game()EX\n7:          for option in ω        do // Execute IM plan\n8:              Execute(option)\n9:          end for\n10:         while T < d_steps do //For each step\n11:             s → Get_State()\n12:             ap → Get_Available_Primitive()\n13:             while Is_Available(a p ) and not (New_Available_Prim()) do\n14:                 Execute(a p )\n15:                 s→ → Get_State()\n16:             end whiles ↓ = s → then\n17:             if\n18:                 if New_Available_Prim() then\n19:                     at → Get_New_Available_Prim()\n20:                 elseo → Create_N ew_Option(a p , {})p , a t )\n21:\n22:                     o → Create_N ew_Option(a\n23:                 end if→ O new ↑ o\n24:                 O new\n25:             end if + 1 //End For each step\n26:             T → T\n27:         end while\n28:         ep → ep + 1 //End For each episode\n29:     end while\n30:     return O new\n31: end procedure"
    },
    {
        "id_": "83fa39a4-5cae-4b86-bd8a-3d1dae8bc281",
        "text": "the option changes the low-level variables of the state s and, consequently, the mask 2 m is not null, the system registers two types of data tuple (Algorithm 1, line 10): the initiation data tuple id and the transition data tuple td. The multiple instances of these tuples are stored, respectively, in the datasets ID, for the initiation data, and T D, for transition data. A single initiation data tuple idi has the following structure\n\nidi = (s, o, f (s, o)), (15)\n\nwhere the function f (s, o) returns the feasibility of executing o from s (True if s → I o and False otherwise). The transition data tuple tdj takes the following structure\n\ntdi = (s, o, r, s→, g, m, O), (16)\n\nwhere s→ is the state reached after executing option o from the state s, g is a flag stating whether the final objective of the task has been reached, m is the mask of the option and O→ is a list defining the options that can be executed from s→. When all the steps of the episode have been executed, the environment is reset and the next episode is started until reaching the maximum number of allowed episodes deps, where the Collect_Data() procedure terminates and the stored data are added to the existing datasets ID and T D (line 11-12)."
    },
    {
        "id_": "e62f2531-1018-48ee-9839-766335de44b9",
        "text": "# 3.1.3 Abstraction\n\nThe datasets collected in the previous step are then used as input for the function Create_P P DDL() (Algorithm 1, line 13), returning a symbolic representation D of the agent’s current knowledge expressed in PPDDL formalism (PPDDL domain). The main advantage of the obtained PPDDL representation is that it makes explicit the causal correlations between operators that would have remained implicit at the option level. In the following, we provide a summary description of the abstraction procedure; for further details, the reader is referred to the original article [59].\n\nThe abstraction procedure executes the following five steps:\n\n1. Options partition: this step is dedicated to partitioning the learned options into abstract subgoal options, a necessary assumption of the abstraction procedure. Abstract subgoal options are characterized by a single precondition and effect set. However, given the uncertainty of the actions’ effects in the environment, the operators’ effects will be modeled as mixture distributions over states. This phase utilizes the transition dataset T D collected before, as it captures the information about the domain segment the option modifies. Basically, the transition dataset is divided into sets of transition tuples presenting the same option o and mask m. Then, for each set, the partitions are ultimately obtained by performing clustering on the final states s→ through the DBSCAN algorithm [69]. If some clusters overlap in their\n\n2 The mask is the list of all the state variables that are changed by the execution of a specific option. See details in [59]. 3 An abstract subgoal option o is characterized by a list of indices of the low-level variables, called mask, which are changed with the execution of o, without modifying other variables. In addition, the changing variables’ values do not depend on their initial value.\n\n13"
    },
    {
        "id_": "e2dadf16-6222-48d9-8a4f-1bc3d6982ef4",
        "text": "# 2. Precondition estimation\n\nThis step is dedicated to learning the classifiers that will identify the preconditions of the PPDDL operators. In order to have negative examples of the initiation set classifier for each operator, this operation utilizes the initiation dataset ID considering all the samples with option o and f(s, o) = False. The positive examples comprise instead the initial states s taken from TD tuples belonging to the same partition. The initiation set classifier of the option is computed using the Support Vector Machines (SVM) [70]. The output of this phase is the set of all the initiation set classifiers of all the operators."
    },
    {
        "id_": "7db169fc-5f4f-4f6e-a731-0da67968b7bb",
        "text": "# 3. Effect estimation\n\nAnalogously, this step is dedicated to learning the symbols that will constitute the effects of the PPDDL operators. The effects distribution is modelled through the Kernel density estimation [71, 72], taking in input the final states s→ of each partition."
    },
    {
        "id_": "485a0ffc-ea2f-4637-9b77-c45592a877bc",
        "text": "# 4. PPDDL Domain synthesis\n\nFinally, this step is dedicated to synthesising the PPDDL domain, characterized by the complete definition of all the operators associated with the learned options in terms of preconditions and effect symbols. This step entails the simple mapping of all the data structures generated during the previous steps in terms of symbolic predicates to be used as preconditions and/or effects for every operator.\n\nThe produced PPDDL domain can be potentially used to reach any subgoal that can be expressed in terms of the available generated symbols at any point during the Discovery-Plan-Act (DPA) loop. One interesting aspect of the proposed DPA framework is that the semantic precision of the abstract representation and its expressiveness increase as the DPA cycles proceed, as will be described in the experimental section."
    },
    {
        "id_": "7b3b7c31-fa53-4e35-958a-0fbb4f97a05b",
        "text": "# 3.1.4 Goal Selector\n\nThis module aims at simulating the intrinsic motivations driving the agent towards interesting areas to satisfy its curiosity and optimize its exploration. In particular, one of the most fascinating aspects of this system is the capability of setting its high-level goals, which potentially could be a combination of symbols defining a state that the agent has never experienced before. In other words, abstract reasoning could be the driving criterion for using the imagination to explore an unknown environment.\n\nDespite the previous goal is rather ambitious and still the object of future work, we will demonstrate in this work that the abstract reasoning can indeed be used for the more “down-to-earth” task of devising rational criteria to make more efficient the exploration of unexplored parts of the environment.\n\nThe selection of the target state starget → S to be reached in the next exploration cycle is performed at line 14 of Algorithm 1, calling the procedure Get_Target_State(). The Goal Selector suggests such state to the system, following an internal strategy which can be, in this case, Action Babbling, Goal Babbling and Distance-based Goal Babbling.\n\nAction Babbling is the simplest strategy of the system for the exploration, consisting of a pure random walking of the agent. This strategy returns starget = NULL, so"
    },
    {
        "id_": "4c761328-1033-4115-9ab4-a937bdd50bd8",
        "text": "# Neighborhoods"
    },
    {
        "id_": "9fb4d5d0-651e-49c9-ae18-17ee7b8c8d2e",
        "text": "# Symbol classifiers"
    },
    {
        "id_": "736118eb-9dbe-462a-b730-3c7ffe143c06",
        "text": "# Evaluating all the candidates\n\nFig. 3: The conceptual depiction of the selection of the symbols necessary to represent the goal's target. The set of symbols having the distribution most similar to the goal will define the goal of the PPDDL problem file.\n\nNo plan * s *target is generated and executed in the exploration phase on the subsequent cycle. Goal Babbling and Distance-based Goal Babbling differ in the way they implement IMs (such as curiosity). More specifically, in our system curiosity is formalised as an interest to reach (i) a random goal among those already achieved, and (ii) the border of the already acquired knowledge.\n\nThe first strategy is represented by Goal Babbling, consisting in randomly selecting a low-level state in the environment and trying to reach it [73]. Usually, the assumption of Goal Babbling is that all the goals which can be set belong to the world’s low-level states that are reachable; each goal is formalised as the configuration of joints or position to be reached with the robot’s actuators [27]. Since in general not all the states *s → S are valid (e.g. the agent can’t move inside the wall), this strategy selects a random state *s*target among the visited ones (line 14). Subsequently, s*target is translated into a set of propositional symbols {φ1, ..., φk}, as described in the following subsection 3.1.5, which represent the high-level goal to be reached using an off-the-shelf PPDDL planner. The capability of translating low-level states into symbols gives the agent a chance to reason on causal dependencies and, consequently, plan. It is important to notice that a pure Goal Propositional Babbling, consisting of the selection of a random subset of high-level symbols, would not be an effective strategy because only a limited number of combinations of symbols conjunctions are valid goals. Consequently, Goal Propositional Babbling is not taken into consideration.\n\nThe second strategy (Distance-based Goal Babbling) is implemented as a modified version of the Goal Babbling, and models the curiosity towards the less explored states as being influenced by the goal’s distance from its starting location sinit."
    },
    {
        "id_": "d72f21f5-cc62-4c0f-928c-f7e8141748f8",
        "text": "The curiosity level for a state is defined as\n\n↼(s) = ⇑sinit ↘ s + Z⇑, (17)\n\nconsisting of the norm of the difference between the state vector of the agent at the beginning of each episode sinit and the visited state s. The low-level state starget is selected between the farthest visited states, as follows:\n\nstarget = max↼(s), ⇓s → Svisited. s (18)\n\nMore precisely, the low-level state starget of the exploration, is the state that maximizes the distance ↼(s). In the computation of starget, a Gaussian noise Z ⇔ N(0, 1) is added, to facilitate the reaching of different states. The Goal Selector driven by ↼ continuously pushes the agent towards the border of the already acquired knowledge (similarly to the idea presented in [74, 75], but with a different implementation) and is thus the main responsible for the knowledge increase of the agent. Moving towards the frontier states, the agent is more likely to encounter novel states thus maximizing the probability to acquire new symbolic knowledge, which can in its turn be used to synthesize new (e.g., more expressive) high-level goals to be eventually reached through planning, ultimately creating a virtuous loop in which the agent’s capabilities of increasing its knowledge through environment exploration are iteratively enhanced."
    },
    {
        "id_": "1dcba383-385c-4eeb-8309-5d7af9ff5310",
        "text": "# 3.1.5 Translating low-level states into symbols\n\nThe peculiarity and, simultaneously, the biggest challenge of this software architecture is thus to use an abstract symbolic representation to manage the evolution of the agent’s knowledge. Using a symbolic representation to describe a desired environment configuration is a powerful tool for an efficient exploration of the world.\n\nAfter selecting starget (line 14), the purpose of the planning module is twofold. On the one hand, it can be employed as usual to verify the reachability of the goal sg of the environment (i.e. get the treasure and bring it \"home\" in the Treasure Game, in line 17) and, on the other hand, it can be exploited to generate a plan driving the agent towards states, in our case starget, relevant to extend the knowledge of the system (line 16). In both cases, to take advantage of planning it is necessary to transform a low-level state into a high-level one. This operation requires finding the combination of propositional symbols\n\n!ˆtarget = {φ1, ..., φk} (19)\n\nthat best represent the portion of state space including starget. In other words, we look for a subset of symbols !ˆtarget whose grounding is starget. These symbols make it possible to generate the definition of a planning problem (line 15) which, together with the domain definition, can be used to perform planning and solve the problem (line 16).\n\nIn order to select the right symbols conjunction, the system creates the classifier Cltarget ⇔ p(starget) approximating a distribution over starget. Cltarget is a SVM"
    },
    {
        "id_": "4344a7e4-f4ca-4b98-92db-695b1f67aa95",
        "text": "# Classifier Trained on the States\n\nrepresenting, as positive samples, all the neighbours of s target within a maximal distance and, as negative samples, all the remaining states encountered in the agent’s experience. Once the classifier is generated, all the propositional symbols whose mask is equal to a factor4 contained by Cltarget are collected as candidates !ˆ candidates. Then, all the subsets of symbols !ˆ i ↖ !ˆ candidates, whose respective masks do not overlap are evaluated as representations of s target. From each !ˆ i, m state samples S ! i = {s1, ..., sm} are generated and the score of the subset !ˆ i is calculated as\n\nscore(! ) = 1/m ∑s↔S !Cltarget(s) (21) where Cltarget(s). Then, the subset of symbols s ˆ returns the probability that !  belongs to the positive class of the classifier Cltarget maximizing the score function is used as a goal in the problem definition Ptarget."
    },
    {
        "id_": "13436b4e-fff1-42bf-80f6-d3b0079417df",
        "text": "# 3.1.6 Planning\n\nAt the end of each cycle, the planning process generates a plan to reach either s target and s g. In both cases, in order to create a PDDL problem P, it is necessary to find the set of symbols !ˆ init, ˆ g ! and !ˆ target, describing the most suitable high-level state representation for s init, s g and s target respectively, as described in the previous sub-section 3.1.5. Indeed, the couples (!ˆ init, ˆ g) and ( ˆ init, !! ˆ target) define the problems P g and P target. At line 15 of Algorithm 1, P target is generated as previously discussed and the plan to solve it, ςEX, is generated by the planner (line 16).\n\nBefore moving to the next cycle, the system tries to solve also the problem (line 17). The resulting plan P g g, performing the function Check_P P DDL_V alidity ς is only used internally by the system to keep track of the success ratio of the planner with the evolution of the synthesized knowledge of the agent."
    },
    {
        "id_": "7dac2a92-b61d-49fe-87e3-3854223579c2",
        "text": "# 4 Experiment and Results\n\nThis section, dedicated to the experimental analysis, will first describe the dynamics of the environment, followed by an example of the system’s cycle execution with its outputs and, finally, the overall results collected over different environment configurations."
    },
    {
        "id_": "5d4fbab5-7a4c-4cfd-9651-5dff61162a3e",
        "text": "# 4.1 Environment Setup\n\nThe implemented system has been tested in the so-called Treasure Game domain [59]. In such an environment, an agent can explore the maze-like space by moving through\n\n4 \"Sets of low-level state variables that, if changed by an option execution, are always changed simultaneously\" [59]."
    },
    {
        "id_": "ae897ac6-08ff-4c7d-accc-38aabfb96f2c",
        "text": "# Fig. 4: All the domain configurations used in the experimental analysis.\n\nThe game’s purpose is to get the treasure at the bottom of the maze and bring it back to the top ladder.\n\nThe agent interacts with corridors and doors, climbing stairs, interacting with handles (necessary to open/close the doors), bolts, keys (necessary to unlock the bolts) and a treasure. The agent starts its activity from the ladder on top of the maze (home location) and its overall task is to find the treasure and bring it back to the starting location. In our experimentation, the agent starts endowed with no previous knowledge about the possible actions that can be executed in the environment; the agent is only aware of the basic motion primitives at its disposal A = {go_up, go_down, go_left, go_right, interact}, respectively used to move the agent up, down, left or right by 2-4 pixels (the exact value is randomly selected with a uniform distribution) and to interact with the closest object.\n\nThe interaction with a lever changes the state (open/close) of the doors associated with that lever (both on the same floor or on different floors) while the interaction with the key and/or the treasure simply collects the key and/or the treasure inside the agent’s bag (in the bottom-right corner of the screen). The interaction with the bolt opens the door next to the treasure and it is feasible only when the agent has the key in its bag.\n\nThe state s → S is defined in terms of the following low-level variables:\n\ns = (xagent, yagent, ⇀ 1, ⇀ 2, ..., xkey, ykey, xbolt, xtreasure, ytreasure) (22)\n\nin which xagent, yagent is the (x,y) position of the agent, ⇀ i is the angle of the lever i, xkey, ykey is the (x,y) location of the key, xbolt is the state of the bolt (1 if open and 0 if locked) and xtreasure, ytreasure is the (x,y) location of the treasure."
    },
    {
        "id_": "9c9cd8af-ec90-4429-8732-9f94fbcccd20",
        "text": "The system is tested in five different configurations of the environment, of increasing complexity: two small-sized (Figure 4a and 4b), two medium-sized (Figure 4c and 4d) and one complete instance (Figure 4e). For example, in the setting depicted in Figure 4e the agent has to: (i) pull the two levers on the top of the maze to open the doors, (ii) get the key which is used to (iii) unlock the bolt, (iv) get the treasure and (v) bring it back on top of the environment. The obstacles that pertain to the other configurations are shown in Table 1.\n\n|Domain|Levers|Keys|Bolts|Notes|\n|---|---|---|---|---|\n|domain1|1|0|0|None.|\n|domain2|0|1|1|None.|\n|domain3|1|1|1|None.|\n|domain4|1|1|1|Shortcut available.|\n|domain5|3|1|1|2 levers going to the treasure, 1 going home.|\n\nTable 1: Obstacles to be solved to end the game."
    },
    {
        "id_": "6da85dfc-c5b4-46d0-8014-dd48fbec6705",
        "text": "# 4.2 The cycle\n\nFor exemplificatory purposes, a complete execution cycle of the system is briefly described in the following, showing the output of each phase of an intermediate cycle (cycle n. 10) performed on domain3 (see Figure 4c) in the Goal Babbling strategy case."
    },
    {
        "id_": "e141eb2b-e75e-494d-85cb-84776325b85f",
        "text": "# Option Generation\n\nAt the beginning of each cycle, the agent executes Algorithm 2 to collect some options exploiting the agent’s primitives, before the exploration can commence. In the Treasure Game environment selected for this work, the agent executes d_eps = 1 episodes, composed by d_steps = 200 primitive actions. After the execution of the plan ς EX, the random exploration is reprised using the primitives contained in A (see Section 2.1). The result is the following set of learned options (11 in total) following the formalization (14):\n\nO = { (go_up,{}), (go_down,{}), (go_left,{}), (go_left,go_up), (go_left, go_down), (go_left,interact), (go_right,{}), (go_right,go_up), (go_right,go_down), (go_right,interact), (interact,{}) }.\n\nIt is important to note that, in general, the discovered options are not all the options that may be possibly discovered in the environment, but only those experienced by the agent during the exploration. This procedure is incremental, adding options to the set O each iteration of the Algorithm 1. As described in section 3.1.1, this procedure leverages IMs at low level, capturing the curiosity of the agent when it discovers to have new available primitives to exploit."
    },
    {
        "id_": "4494045b-ddb8-4833-b4f1-ede05a813327",
        "text": "# Treasure Game Domain Definition\n\n(define (domain TreasureGame)(:requirements :strips :probabilistic-effects :rewards)\n(:predicates(notfailed)\n(symbol_0)\n(symbol_1)\n(symbol_2)\n...\n(symbol_25)\n)\n(:action option-0-partition-0-0:parameters ()\n:precondition (and (notfailed) (symbol_5) (symbol_22)(symbol_14))\n:effect (and (symbol_6) (not (symbol_5)) (decrease(reward) 36.00))\n)\n...\n(:action option-10-partition-3-715:parameters ()\n:precondition (and (notfailed) (symbol_20) (symbol_22)(symbol_14) (symbol_15) (symbol_0))\n:effect (and (symbol_13) (not (symbol_20)) (decrease(reward) 90.00))\n)\n)\n\nFig. 5: An extract of the PPDDL generated by the abstraction procedure. Notice that option-0-partition-0-0 can be explained by looking at the symbols of Figure 6. In fact, the effect is to change the x position replacing symbol_5 with symbol_6, maintaining invariant the presence of symbol_14 and symbol_22. Consequently, it is the option moving the agent on the right on the last floor."
    },
    {
        "id_": "b642bb81-a474-4855-8e1f-f9cbc51b76b3",
        "text": "# Exploration\n\nIn this step, the plan ς EX is again executed before starting the random walk over the available options O. In this particular example, 600 data entries have been collected in the ID dataset and 6600 in the TD dataset. It is important to remember that the number of collected data over the cycles is not constant because when an option does not produce effects on the environment, no data is collected."
    },
    {
        "id_": "e9580ae2-8ad4-4475-adba-ae6e207a170d",
        "text": "# Abstraction\n\nFigure 5 shows a part of the output of the PPDDL domain obtained from the abstraction procedure. As visible, the figure does present a valid PPDDL domain description, upon which the planner may reason. Moreover, any subset of the produced domain predicates (symbols) can be used to define high-level goals the planner may try to plan for. In this specific case, the system generated 26 symbols and 716 operators.\n\n20"
    },
    {
        "id_": "b63be676-d1ae-40e6-a503-6aef9560bd8b",
        "text": "# Planning\n\nThe produced PPDDL domain is used by the system to (1) solve the entire game, or task, and (2) improve the exploration. Figure 7 illustrates two PDDL problems: the problem on top refers to the general goal of solving the game (Pg) while the problem at the bottom refers to the goal dynamically synthesized by the Goal Selector module, which in this example relates to reaching the left corner of the middle floor (Ptarget). The solution for both problems is presented in Figure 8. The first solution, ςg, solves the game problem in 21 moves and the second one, ςEX, reaches the Goal Selector goal in 6 moves."
    },
    {
        "id_": "6d13ef09-57c9-4711-928d-e0ef856f80f7",
        "text": "# 4.3 Results\n\nIn this section, the overall results of the system over different domain instances are described. First, the setting of the environment is discussed, then the adopted baseline, as well as the other strategies enabling the planning exploration. Subsequently, some charts are presented, that highlight the system’s performance in terms of success ratio on the planning task. Finally, some issues worth being underscored are commented, and the limitations of the employed technologies are discussed.\n\nFor our purposes, the Treasure Game5 has been used with five different mazes (see Figure 4), to focus on the performances of the system on smaller domains, highlighting pros and cons of the symbolic approach proposed. The system has been executed, following the workflow described in Algorithm 1, in the cited five mazes configurations using parameters suitable to solve the tasks, which are described later. To summarize,\n\n5 Github repository: https://github.com/sd-james/gym-treasure-game"
    },
    {
        "id_": "fc8b6fbe-dec7-4b5b-9139-b218d326e25b",
        "text": "# Problem Definitions\n\n(define (problem task_goal)(:domain TreasureGame)\n(:init (notfailed) (symbol_0) (symbol_1)(symbol_2) (symbol_3)\n(symbol_4) (symbol_5) )\n(:goal (and (notfailed) (symbol_4)(symbol_17) (symbol_18)) )\n)\n\n(define (problem im_goal)(:domain TreasureGame)\n(:init (notfailed) (symbol_0)(symbol_1) (symbol_2)\n(symbol_3) (symbol_4)\n(symbol_5) )\n(:goal (and (symbol_13) (symbol_24)(symbol_14) (symbol_2)\n(notfailed)) )\n)"
    },
    {
        "id_": "61cd8e03-473e-4600-af66-01d8a7316d2c",
        "text": "# Figure 7\n\nOn the top, the problem of solving the task is synthesized by the system Pg and, on the bottom, the problem generated by the Goal Selector module Ptarget.\n\nThe system iteratively (i) looks for new options Onew performing dsteps primitives for deps episodes, (ii) explores for dpa_eps episodes the maze executing dpa_steps, (iii) creates a symbolic abstraction of the domain D, (iv) creates a plan ςEX to optimize the exploration of the successive cycle c + 1. It is important to note that the autonomously discovered options successfully produced a valid high-level model, usable to build correct plans. To assess the accuracy of the current abstraction D, we evaluate its ability to reach the goal sg at the end of each cycle. This is done by requesting the planner to solve the problem Pg using the currently produced symbolic domain description D.\n\nThe first strategy used in the experiments is the random walk, which is called Action Babbling [50] (see Section 3.1.4). This strategy simply executes the Algorithm 1 without using planning because the Goal Selector returns starget = N U LL and no plan ςEX is generated. This simple strategy is used as a baseline for the experimental analysis, and it is necessary to fully appreciate the advantages of using the symbolic approach. The exploration is equivalent to the one used by Konidaris [59], and we use it to observe the development of the symbolic model over time.\n\nTo support the use of symbolic planning, two other strategies have been considered: Goal Babbling and Distance-based Goal Babbling (see Section 3.1.4). These three strategies could be seen as having an increasing complexity and effectiveness in the exploration:\n\n- Action Babbling selects completely random actions;\n- Goal Babbling selects random goals, reaching them with a planned sequence of actions;"
    },
    {
        "id_": "8550eb51-481a-49c6-ac93-cf1d62684c1d",
        "text": "# PLAN TASK GOAL:\n\n|1|(go_down,{})|; climb down the stairs|\n|---|---|---|\n|2|(go_left,go_down)|; go left until it can go down|\n|3|(interact,{})|; pull the lever|\n|4|(go_right,go_down)|; go right up to the stairs to go down|\n|5|(go_down,{})|; climb down the stairs|\n|6|(go_right,{})|; go right up to the end of the corridor|\n|7|(go_left,{})|; go left up to the end of the corridor|\n|8|(interact,{})|; take the key|\n|9|(go_right,go_down)|; go right up to the stairs|\n|10|(go_down,{})|; climb down the stairs|\n|11|(go_right,interact)|; go right up to the bolt|\n|12|(interact,{})|; unlock the bolt|\n|13|(go_left,go_down)|; go left until it is possible|\n|14|(interact,{})|; get the treasure|\n|15|(go_right,go_up)|; go right up to the stairs|\n|16|(go_up,{})|; go upstairs|\n|17|(go_right,{})|; go right up to the end of the corridor|\n|18|(go_left,go_up)|; go left up to the stairs|\n|19|(go_up,{})|; go upstairs|\n|20|(go_left,go_up)|; go left up to the stairs|\n|21|(go_up,{})|; go up to home|"
    },
    {
        "id_": "9a546f00-ae06-4762-85f7-503062ef5e78",
        "text": "# PLAN IM GOAL:\n\n|1|(go_down,{})|; climb down the stairs|\n|---|---|---|\n|2|(go_left,go_down)|; go left up to the lever|\n|3|(interact,{})|; pull the lever|\n|4|(go_right,go_down)|; go right until it can go down|\n|5|(go_down,{})|; climb down the stairs|\n|6|(go_left,interact)|; go left up to the key|\n\nFig. 8: The plans generated ς g and ςEX.\n\n• Distance-based Goal Babbling selects the farthest goals and reaches them by using planning (see subsection 3.1.4).\n\nIt is reasonable to conjecture that in smaller domains where the reasoning is less necessary and purely random exploration may suffice, we do not expect to observe much difference among the previous strategies; while in the bigger domains the time to reach the goal may significantly change, depending on the strategy employed.\n\nThe five configurations considered are depicted in Figure 4, and the parameter values used in the exploration function Collect_Data for each configuration are shown in Table 2. Smaller domains domain1 and domain2 required the execution of 50 options per episode, domain3 required 150, domain4 required 200, and finally 800 options were executed in domain5.\n\nThe main results of the system are depicted in Figure 9. Precisely, in the graphs, it is shown the probability of success in solving the game over time using different strategies. Such success entails the generation of a sufficiently mature domain D and a correct problem formalization of P g, resulting in a correct plan ς g.\n\nThe system has been run with cycles = 15, assessing at the end of each cycle whether ς g was able to solve the game using the current synthesized PPDDL domain representation D. This mechanism has been performed for ten trials. Consequently, in\n\n23"
    },
    {
        "id_": "2627898d-3502-4f45-a623-51f6bc49db88",
        "text": "# Cycle"
    },
    {
        "id_": "4b9d79b7-b466-4109-ae7a-de54bfe67e05",
        "text": "# (a) domain 1"
    },
    {
        "id_": "b53491e5-28bc-426b-aa06-b8d0b0ce9af1",
        "text": "# (b) domain 2"
    },
    {
        "id_": "02fc7ecc-b656-4117-934d-9b5f7540513a",
        "text": "# Cycle"
    },
    {
        "id_": "08535aa0-973e-42be-a037-e11575298da6",
        "text": "# (c) domain 3"
    },
    {
        "id_": "9fe74eee-b4e2-4e92-a04f-9a083cd2f4c9",
        "text": "# (d) domain 4"
    },
    {
        "id_": "08892705-132f-4715-878b-8284cc73165c",
        "text": "# Cycle"
    },
    {
        "id_": "7d22ff75-fc9f-43b2-af19-c1d50f830353",
        "text": "# (e) domain 5\n\nFig. 9: The figure depicts the success rate of ς g using different exploration strategies over time. In figure 9a, 9b, 9c, 9d and 9e, we have respectively the results of the domain of figures 4a, 4b, 4c, 4d, 4e over 15 cycles of exploration.\n\n24"
    },
    {
        "id_": "a6e8ecc6-2ccd-4078-9160-c4bffc78b22c",
        "text": "# Table 2: Main parameters used in the different environment configurations.\n\n|Domain|dpa_eps|dpa_steps|minimal solution steps|\n|---|---|---|---|\n|domain1|4|50|11|\n|domain2|4|50|13|\n|domain3|4|150|19|\n|domain4|4|200|15|\n|domain5|4|800|31|\n\nThe charts of Figure 9 are depicted cycles on the x-axis, and the percentage of trials that have succeeded in solving the game in the specific cycle on the y-axis.\n\nIn the smaller domains domain1, domain2, domain3 and domain4, the planning contribution is limited and sometimes not visible at all. Especially in the simplest domain domain1 the three strategies present similar performances, solving in the last cycles 80-90% of the trials (Figure 9a). In domains domain2 and domain3, presenting slightly higher complexities, the advantages of executing plans start being visible (Figure 9b and 9c). It is evident that Goal Babbling behaves almost perfectly in the last cycles, demonstrating the usefulness of collecting transition data with reasonable sequences of actions. The combination of randomness in selecting the goal to be reached and the reasoned transitions speed up the exploration and the consequent maturity of the PPDDL representation of the environment. Instead, the Distance-based Goal Babbling seems better than a completely random search but less effective than the precedent. This result entails that applying an exploration focused on the frontier’s goals is not convenient in smaller domains. In fact, in the conclusive cycles of domain2 and domain3, Goal Babbling maintains 90-100% of success ratio, Distance-based Goal Babbling around 80% and Action Babbling between 60-70%.\n\nAlthough the problem faced by domain3 seems easier than domain4, presenting respectively minimal solutions of 19 and 15 steps (see Table 2), the system needs more steps per episode to solve domain4 using the same amount of episodes used by domain3. The reason of this behaviour is due to the higher complexity of synthesizing a PPDDL domain D for this scenario. In fact, the scenario domain4 presents a sort of \"shortcut\" to reach the treasure, which does not require collecting the key and opening the door. From the point of view of the abstraction procedure, the shortcut represents a branch in the possible choices of the agent, thus presenting the agent with additional concepts to be abstracted in order to synthesize a complete representation. Consequently, the system generates additional symbols and operators, requiring more experience to strengthen the transition model captured by the PPDDL and provide satisfying plans.\n\nThe significance of the frontier exploration emerges in the domain5 configuration, where planning evidently boosts the results of the agent. Indeed, being bigger than other domains, domain5 is more difficult to be solved and planning results to be efficient in driving the exploration of the agent immediately towards the borders of its knowledge. The main result highlighted by the charts is that in bigger domains (Figure 9e), the impact of using planning is evident. Planning is able to easily drive the agent towards interesting visited states, where it can continue exploring. Statistically, after collecting the transition data for 15 cycles, the system struggles to solve the problem."
    },
    {
        "id_": "75be6ab6-c7f4-47cf-9789-27ffd409bf1e",
        "text": "# 4.4 Discussion and Future Work\n\nAs shown in Figure 9, the results are not deterministic and change over time. On the one hand, the system is continuously evolving in terms of knowledge and, on the other hand, ML techniques introduce further stochasticity to the final representation generated. During the abstraction procedure, described in section 3.1.3, some statistical tools are employed to create data structures to represent preconditions, effects and symbols. For instance, an erroneous 6 clustering phase could generate unexpected effects on symbols and operators. An example highlighting this fact is the noisiness of some symbols, interfering with the generation of a correct formalization of D, P and, consequently, the resulting plan ς. In Figure 10 it can be seen the graphical representation of the symbol \"on the highest y-axis position\", meaning \"on the top ladder\" because it is the only possible state with such y-axis value (it is not allowed to move inside the walls). In some cases, it could happen that the initial state of the game is interpreted as being at the top floor under the ladder and, consequently, it is not necessary to climb down the ladder to execute the agent’s task. Then, although almost the whole plan ς is correct to complete the game, without the option of climbing down the ladder as the first action, the plan is incomplete, and the trial is considered unsuccessful. On the one hand, this is a drawback of using ML tools, which can introduce noise in.\n\n6 Statistical methods are not mistaken because they just create a representation based on the data provided. However, for our purposes, some models’ instances could be obstacle on the reaching of our goal.\n\nFigure 10: The graphical representation of the symbol \"on the higher y-axis position\". It can be seen that it is significantly noisy because some samples of the agent are depicted on the top y-axis and other samples almost on the lower floor."
    },
    {
        "id_": "9f7cc901-b470-44a1-a33f-66dad131e085",
        "text": "# 5 Conclusions\n\nIn this paper, a novel approach for open-ended learning in autonomous agents based on intrinsically motivated planning is presented. This approach integrates two powerful paradigms, intrinsic motivation and classical planning, to enable agents to continuously learn and improve their knowledge and skills without relying on external supervision or rewards.\n\nThis work suggests an alternative or complementary approach to the advanced and popular sub-symbolic methods, demonstrating interesting features. First, it allows agents to explore and learn in a self-directed and open-ended manner without being limited to a predefined set of goals or tasks. Second, it enables agents to represent and reason about their knowledge and skills in a structured and formal way, which can facilitate planning and generalization to new situations. Third, it can incorporate intrinsic motivations that drive the agent to explore and learn beyond extrinsic goals."
    },
    {
        "id_": "08ef7aab-d90a-491a-bf49-95d75e847dfc",
        "text": "which can enhance the agent’s adaptability, robustness, and creativity. However, there are still several challenges and opportunities for future research in this area to enable the systems to perform complex activities in a relevant operational environment.\n\nOverall, we believe that our approach represents a promising step towards more autonomous and intelligent agents that can continuously learn and improve in an open-ended and self-directed manner."
    }
]