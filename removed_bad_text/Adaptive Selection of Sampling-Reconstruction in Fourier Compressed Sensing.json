[
  {
    "id_": "2fa1b4fc-7b84-407f-b898-4b61b3cf1a0d",
    "text": "# Adaptive Selection of Sampling-Reconstruction in Fourier Compressed Sensing"
  },
  {
    "id_": "35e229b9-3926-41d0-9fd1-d118ce3eadef",
    "text": "# 1 Introduction\n\nCompressed sensing (CS) has revolutionized the field of image acquisition, enabling the reconstruction of high-quality images from a reduced number of measurements. This remarkable feat is achieved by exploiting the sparsity of natural images (or medical images) in certain transform domains. The CS theory [7, 13]\n\n⋆ Corresponding authors."
  },
  {
    "id_": "d42de50e-4439-4527-a629-610d1412f81f",
    "text": "# Data\n\n|Mask M|Mask Backpropagation|\n|---|---|\n|Input k|Output I′|\n|⨀|Recon. Network θ|"
  },
  {
    "id_": "fabe7195-2947-4628-8fcf-5e86cca95c75",
    "text": "# (a) Joint optimization of sampling-reconstruction (H1 [3, 8, 9, 19, 38])\n\n|Mask M0| | |\n|---|---|---|\n|⨀|πϕ : M0K → M| |\n|Input k| |Output I′|\n|⨀|Recon. Network θ| |"
  },
  {
    "id_": "8cda7a47-1fc4-4d64-bd4f-ad024678ab07",
    "text": "# (b) Adaptive sampling (H2 [4, 5, 35, 36])\n\n|Mask M0|Output I′|\n|---|---|\n|⨀|eψ|\n|M1|⨀|\n|θ1|MJ|\n|⋮|⨀|\n|θJ| |"
  },
  {
    "id_": "b00b9913-615d-4c23-bb80-ece733fa07b8",
    "text": "# (c) Adaptive selection of sampling-reconstruction (H1.5, ours)\n\nFig. 1: We propose the adaptive selection of sampling-reconstruction (H1.5, c). In Fourier compressed sensing, there were two classes of methods for finding the optimal sampling: joint optimization of sampling-reconstruction (H1, a) and adaptive sampling (H2, b). H1 has low potential as its mask M is not adaptive to each data point. H2 poses a challenge in optimizing the mask generator (πϕ) and then exhibits Pareto suboptimality, where a single θ is not optimal for multiple masks M ∈ R(πϕ). In contrast, H1.5 is adaptive for input k (eψ in c selects the best M -θ pair), avoids the challenge of backpropagation to discrete space (red lines in a, b, c), and achieves Pareto optimality by dedicating each network θj exclusively to Mj. ⊙ denotes the componentwise multiplication.\n\nguarantees that an image can be accurately recovered from a non-adaptive random sampling pattern, with much fewer samples than the Nyquist-Shannon sampling theorem requires, if the image has a sparse representation in that domain. Before the era of deep learning, CS used to refer to obtaining the final image through l1-regularized reconstruction, i.e., solving Lasso [7]. Recently, reconstruction has"
  },
  {
    "id_": "f8b746da-1ab4-495c-ac36-5d1d34f75a3b",
    "text": "# Adaptive Selection of Sampling-Reconstruction in FCS"
  },
  {
    "id_": "22250a1d-9b14-403d-aae4-a475c9fda15d",
    "text": "# 3\n\n|Methods|Adaptive to input k|Backprop to a continuous space|Pareto optimal θ|\n|---|---|---|---|\n|H1 [3, 8, 9, 19, 33, 38]|%|%(! [33])|!|\n|H2 [4, 5, 29, 35, 36]|!|%(! [29])|%|\n|H1.5 (ours)|!|!|!|\n\nOften been performed using deep neural networks trained on the data. In this paper, we focus on deep learning-based reconstructions.\n\nFourier compressed sensing (Fourier CS) refers to CS where the measurement is in the discrete Fourier transform (DFT) of an image. As electromagnetic waves are inherently wave-like, obtaining spatial information such as pixel values directly is not feasible. Instead, spatial information is acquired through DFT. Obtaining samples for every Fourier-transformed element can be costly. Unlike the CS theory that suggests random sampling is sufficient, the sampling results of Fourier CS, similar to the prior in natural images, concentrate a significant amount of energy in the low-frequency (LF) components [25,34]. Despite deviating from the random sampling principle of CS theory, Fourier CS achieves excellent image quality in many domains by extensively sampling LF components; hence it has been successfully applied to various electromagnetic imaging applications, including magnetic resonance imaging (MRI) [25, 26] or radar [10, 14].\n\nHowever, finding an optimal sampling method that is both efficient and effective remains a challenge in Fourier CS. One approach is the joint optimization of sampling-reconstruction (denoted by H1) [3,8,9,19,33,38], where the parameters of both the sampling and reconstruction networks are jointly trained using the dataset, as depicted in Fig. 1a. But this approach has two drawbacks, as described in Tab. 1. Firstly, it is not adaptive to each data point, i.e., the optimized sampling and the reconstruction parameter would not be the best pair for a specific input. Moreover, the reconstruction network is usually trained by backpropagation. To employ backpropagation, the parameters are expected to be defined within continuous spaces. This makes training the sampling mask not trivial, as it is defined in a discrete space. Most methods [3, 8, 9, 19, 38] just perform discrete optimization anyway using the straight-through estimator [6, 38].\n\nThe other approach is adaptive sampling (denoted by H2) [4,5,27,32,35,36,41], which aims to generate the best sampling mask for each data point (or each image) based on the fact that a predetermined sampling mask may not be optimal for every situation. Most adaptive sampling studies generate the optimal sampling mask based on the information from the initially measured LF components of each data point, which has the potential to achieve excellent results. Then, they usually have a single reconstruction network that is responsible for many optimal masks for all data points. Unfortunately, there are a couple of major issues in"
  },
  {
    "id_": "4e7f1f1f-e732-4306-b5de-cc2e13e784eb",
    "text": "# 4 S. Hong et al.\n\ncurrent adaptive sampling, as in Tab. 1. Similar to the joint optimization of sampling-reconstruction models, optimizing the mask generator is challenging due to the broad and discrete mask space, as depicted in Fig. 1b. Secondly, the single reconstruction network for diverse sampling masks may not be Pareto optimal, which we called Pareto suboptimal reconstruction network. Note that a similar issue can arise in the task of restoring various degradations (e.g., the performance of a blind denoising network trained on multiple noise levels is usually lower than that of an identical network trained only on the specific noise level used as the actual input [39]).\n\nIn this paper, we propose a novel adaptive selection of the sampling-reconstruction framework for Fourier CS that alleviates the drawbacks of joint optimization of sampling-reconstruction and adaptive sampling. It is adaptive to each data point, avoids backpropagation to discrete spaces, and its reconstruction network is Pareto optimal. In the adaptive selection, we first sample LF components quickly and then leverage a super-resolution (SR) space generation model, to quantify the high-frequency (HF) Bayesian uncertainty. This approach ensures that HF components, which contain crucial details, are sampled more effectively, leading to improved reconstruction quality. The main contributions of our paper are as follows:\n\n- Proposing a novel adaptive selection of sampling-reconstruction framework for Fourier CS that alleviates the drawbacks of joint optimization of sampling-reconstruction and adaptive sampling with theoretical justification.\n- Designing the adaptive selection to efficiently quantify HF Bayesian uncertainty by leveraging an SR space generation model for determining sampling masks.\n- Demonstrating that our adaptive selection improves performance in multiple Fourier CS problems such as facial image restoration (up to 0.04 average gain in SSIM) and multi-coil MR reconstruction (up to 0.004 average gain in SSIM)."
  },
  {
    "id_": "9bd06c2d-2b6b-4d7a-9dcf-ae70ebc8ceff",
    "text": "# 2 Related Works"
  },
  {
    "id_": "fbf38b09-c2c7-4d9a-a616-44c0ae2cbfe5",
    "text": "# 2.1 Fourier compressed sensing\n\nFourier CS can be defined as the following regression problem. Let us define the dataset D = {(ki, Ii)}i=1 , . . . , IN ∈ I ⊆ RL are the corresponding images, sampled k-space data and I1, I2 such that k1, k2, . . . , kN ∈ K ⊆ CL are fully-N respectively. Let us define the mask space by M ⊆ {0, 1}L×L whose element is a diagonal binary matrix (indicating acquired (1) and unacquired (0) grid points). Let h(k; M, θ) : K × M × Θ → I be a reconstruction function of k for a sampling mask M ∈ M and a reconstruction network (e.g., U-Net [28] or E2E-VarNet [31]) parameterized by θ ∈ Θ. Then, this function h can be used as a joint sampling-reconstruction model that optimizes both the sampling mask M and the reconstruction network θ. Specifically, for the given dataset D, the model is optimized to minimize the following empirical risk:\n\nLˆ[h(K; M, θ)] = N-1 ∑i=1N l(Ii, h(k; M, θ))"
  },
  {
    "id_": "62638af0-1a0e-4a52-8209-08732456f5da",
    "text": "# Adaptive Selection of Sampling-Reconstruction in FCS\n\nwhere l is the loss function (e.g., l(I, Iˆ) = 1 − SSIM(I, I))."
  },
  {
    "id_": "72fb98d7-a642-4ef5-8d39-e79f156783bc",
    "text": "# 2.2 Joint optimization of sampling-reconstruction\n\nOne of the recent approaches to finding a good sampling mask is joint optimization of sampling-reconstruction [3, 8, 9, 19, 33, 38], which reconstructs the image with a non-adaptive mask M ∈ M. They are defined as follows:\n\nH1 = {h(·; M, θ)|M ∈ M, θ ∈ Θ}. (1)\n\nThey jointly optimize M and θ for a dataset; however, M is not adaptive to each data point. Whether using a tailored M or not, the fundamental limitation of H1 is that the sampling mask is not optimal for each data point. Moreover, they exhibit highly varying results across different settings, as they require discrete optimization [3, 8, 9, 19, 38] or virtual data [33], as shown in Fig. 1a and Tab. 1."
  },
  {
    "id_": "aa8b020d-b55d-4f8c-8df6-4a2420088447",
    "text": "# 2.3 Adaptive sampling\n\nSome recent works [4, 5, 27, 35, 35, 36] employ adaptive mask, using a mask generator πϕ : M0K → M, parameterized by ϕ ∈ Φ, as shown in Fig. 1b. Here, M0 ∈ M denotes a mask that samples only LF components. Adaptive sampling approaches minimize Lˆ[h] on\n\nH2 = {h(·; πϕ(·), θ)|πϕ : M0K → M, θ ∈ Θ, ϕ ∈ Φ}. (2)\n\nObviously, H1 ⊆ H2. That is, H2 has the greatest potential but is hard to train because of its complexity. Specifically, H2 faces two main issues: the difficulty of the mask generator (πϕ) optimization, and Pareto suboptimality of θ, due to the fact that a single reconstruction network is responsible for multiple masks.\n\nPrevious studies on H2 have used reinforcement learning [4, 27, 35] or back-propagation [4, 5, 35, 36] using straight-through estimator [6, 38] to optimize mask generator πϕ, but this is a complicated problem because the action space M is too broad and discrete. θ is Pareto suboptimal in adaptive sampling studies since there are multiple sampling masks M while only one θ exists at inference time. Due to these difficulties, most adaptive sampling studies in CS-MRI have been conducted in a clinically less relevant simple setting of single-coil [4, 27, 32, 35, 36, 41]. There was only one study conducted on realistic multi-coil setting [5], but the final models of [5] turned out to be non-adaptive, which is an unintended consequence.\n\nTo avoid optimization in discrete space, [29] proposed adaptive sampling using a conditional Generative Adversarial Network (cGAN). During sampling, cGAN assesses the uncertainty of samples yet to be acquired. Subsequently, the user selects a sample with the highest uncertainty for acquisition (i.e., greedy algorithm). This process of quantifying and sampling is iteratively repeated. While this method benefits from backpropagation occurring only in continuous space (Tab. 1) it still faces the challenge of a single reconstruction network, having to perform reconstruction for all masks. Consequently, CS-MRI experiments were conducted using a simple single-coil setup."
  },
  {
    "id_": "9c432b0e-c8c4-4a74-93ab-b7b975355792",
    "text": "# 2.4 Super-resolution space generation\n\nSuper-resolution (SR) space generation [23, 24] aims to create diverse high-resolution (HR) images that can be downsampled to the same low-resolution (LR) image (i.e., qψ (IHR|ILR) ∈ {δI |I ∈ I}). For this purpose, a stochastic approach is used rather than a deterministic one. Conditional normalizing flow-based SR space generation methods [15, 22, 30] explicitly obtain qψ (IHR|ILR) using a diffeomorphic mapping fψ : I → Z and a simple base distribution qz (e.g., standard Gaussian), as qψ (IHR|ILR) = qz (fψ (IHR; ILR))|det ∂fψ (IHR; ILR)|.\n\nSince fψ is invertible, qz and IHR can be used to directly sample IHR from qψ (i.e., z ∼ qz =⇒ fψ−1(z; ILR) ∼ qψ (·|ILR)). In this work, we trained and exploited a recent robust flow-based SR space generation method [30], with tuned hyperparameters [15] for stability, to generate HR images from the corresponding LR image that is reconstructed from undersampled k-space data with mask M0."
  },
  {
    "id_": "9d607749-8bfd-427b-b654-f1798547e193",
    "text": "# 3 Proposed methods\n\nIn Sec. 2.2 and 2.3, we investigated the difficulty of optimizing the mask generator πϕ, and Pareto optimal θ (for all masks). This section proposes a novel scheme, adaptive selection of sampling-reconstruction, which does not encounter these problems. Using two Theorems 1 and 2, we explain our adaptive selection (H1.5) is better than the joint optimization (H1) and the adaptive sampling (H2)."
  },
  {
    "id_": "e5f01b22-0907-47c7-b57b-58f5ec9baf76",
    "text": "# 3.1 Adaptive selection of sampling-reconstruction\n\nOur adaptive selection model H1.5 is defined as follows:\n\nH1.5 =\n\nXeψ (·)j h(·; M, θj )\n\neψ : M0K → {ej }J=1, Mj ∈ M, θj ∈ Θ, ∀j\n\nwhere ej is the j-th standard unit vector (i.e., one-hot vector). Each submodel h(·; Mj , θj ) contains mask Mj and reconstruction network θj as a pair, which is Pareto optimal. At inference time, each data selects an appropriate submodel through the mask selector eψ (·)j, which takes input M0k. This scheme is similar to a segmented regression problem that ensembles multiple submodels using one-hot encoding.\n\nRemark 1. If H1 is a linear regression, then H1.5 is a segmented linear regression.\n\nWe propose the following Theorems 1 and 2. Theorem 1 shows that H1.5 is better than H1 due to its adaptivity, and Theorem 2 demonstrates that H1.5 is superior to H2 because H2 has poor Pareto optimality."
  },
  {
    "id_": "62c241a5-f3c6-427f-873d-a728289abc4c",
    "text": "# Theorem 1 (Adaptive selection is better than non-adaptive).\n\nFor a true risk L,   h∈Hinf1.5L[h] ≤ inf1L[h].h∈H"
  },
  {
    "id_": "ff7a3276-10d2-4842-aa40-0fa52663ef24",
    "text": "# Adaptive Selection of Sampling-Reconstruction in FCS"
  },
  {
    "id_": "ac8dc4d2-355b-4933-a90c-17dafd646270",
    "text": "# Theorem 2 (Adaptive selection is Pareto optimal).\n\nFor a true risk L, |πϕ(M0K)| ≤ J ⇒ h∈Hinf1.5L[h] ≤ inf2L[h].h∈H\n\nPlease see the supplementary material for the proofs. Theorem 2 requires an assumption that optimizing πϕ is difficult (i.e., |πϕ(M0K)| ≤ J), which is justified in Section 2.2 (e.g., The final model in [5] converged to |πϕ(M0K)| → 1, which means non-adaptive). Theorems 1 and 2 suggest that the proposed adaptive selection scheme (H1.5) may outperform both non-adaptive methods (H1) and adaptive sampling (H2). In Section 3.2, we describe the implementation of the scheme using the HF Bayesian uncertainty quantified by an SR space generation method [30]."
  },
  {
    "id_": "5090724b-e7d5-479e-aed2-5719cbbdb862",
    "text": "# 3.2 How to and what to adaptively select?"
  },
  {
    "id_": "6d4872d5-4a5c-4a36-8878-73bcd9f6de8e",
    "text": "# (How to) proposed mask selector eψ:\n\nThe sample variance of a generative model to produce diverse samples can be utilized to quantify uncertainty for adaptive sampling [29]. Specifically in Fourier CS, inspired by the idea of initially sampling LF components, we employ an SR space generation model [15, 30] as a HF uncertainty quantifier.\n\nThe sample variance v(M0k) := (Vardqψ [ks′])s=1 is an estimator of the mean square error in k-space domain, where S is the number of the SR samples and k′s is the Fourier transform of the s-th sample. We make up the mask selector eψ using v(M0k). Specifically, at train time, we normalize v(M0k) so that u(v) := v/∥v∥2 and then use the k-means++ clustering algorithm [2] to {u(v(M0ki))}i=1 to N create centroids (cj )J=1. At inference time, we select adaptive mask index j by calculating the distance u(v(M0k)) and (cj )J=1."
  },
  {
    "id_": "0bdc3ca4-68b3-45eb-aa50-95d1781677bb",
    "text": "# We also need to determine the number of the sampling-reconstruction pairs J.\n\nThinking of Remark 1, increasing J doesn’t always mean better average performance; while increasing J can help in robustly handling outliers. This trade-off can be organized as Remark 2:"
  },
  {
    "id_": "e55d78dc-aec3-4f92-a226-1054c91075e9",
    "text": "# Remark 2 (Trade-off with the number of segments J).\n\nAs J increases, despite more training resources, the average performance reaches a plateau at some point, but it becomes more robust against outliers. The choice of J depends on the user’s needs; we defaulted to J = 3. We delve into and validate Remark 2 in Sec. 5."
  },
  {
    "id_": "13dbd333-a535-42e2-8059-e90c5eda9e8c",
    "text": "# (What to) constructed sampling-reconstruction pairs (Mj , θj )J=1:\n\nOne might try to create Mj from cj using just sorting, i.e.,\n\nMj = arg max∥M cj ∥,\n\nbased on the following proposition."
  },
  {
    "id_": "0beeee7f-6629-496a-9d97-905a65ec22dd",
    "text": "# Proposition 1 (simplified version).\n\nWith mild assumptions, the sorted sample variance (4) is the PSNR-maximizing mask."
  },
  {
    "id_": "4756a8f3-8e83-4a7f-8fde-56fc85f11186",
    "text": "# 8 S. Hong et al."
  },
  {
    "id_": "b00fda85-6b78-46f5-9c97-4d63dc75bc98",
    "text": "# SR image samples\n\n|LF k-space data|Mask M1|Recon θ1|\n|---|---|---|\n|Sample variance in k-space|Mask MJ|Recon θJ|"
  },
  {
    "id_": "7fa02eed-f6ff-4a49-9b1c-a6298213cb84",
    "text": "# (2) Scan LF k-space region\n\nMRI scanner"
  },
  {
    "id_": "be68c0b3-9f60-40a2-85e9-d8677217ee0e",
    "text": "# (3) HF Bayesian uncertainty quantification (eψ)\n\n(6.25% samples)"
  },
  {
    "id_": "f09a854f-1131-4f13-9dcf-64e1dfca11ff",
    "text": "# (1) LF mask M0\n\n(6.25% samples)"
  },
  {
    "id_": "19864b9a-1bb1-4be3-accf-973fec10801a",
    "text": "# (4) Additionally scan masked k-space region\n\nFig. 2: Using the sample variance of SR space generation [15,30] results, we can quantify HF Bayesian uncertainty (highlighted in magenta dotted box). Then, we can adaptively select a sampling-reconstruction (M − θ) pair (highlighted in cyan dotted box). Here, we illustrate how our adaptive selection of sampling-reconstruction (Algorithm 2) is employed in CS-MRI. In (1)&(2) in the figure, MRI scanner scans LF k-space region and adaptively selects mask (Mj⋆ , θj⋆ ) pair using HF Bayesian uncertainty quantification. (3)&(4) After additionally scanned masked k-space region from Mj⋆ , reconstructed images are generated from masked k-space data using θj⋆ . See Algorithm 2 for details."
  },
  {
    "id_": "e6f14a3e-20c1-4f3c-9c5d-ea9b08a6dd30",
    "text": "# Algorithm 1 Training\n\nInput: Training set {ki}i=1, initial sampling mask M0 ∈N M, trained SR space generation model fψ : I → Z, the number of segments J, the number of SR generated images S, the number of total sampling points NM, and Output: Masks (Mj )j the empirical risk Lˆ.J=1, reconstruction parameters (θj )J=1, and centroids of uncertainty (cj )J=1"
  },
  {
    "id_": "a3fd54b9-9973-4b9f-a9f8-cd15f0e9de31",
    "text": "# for i = 1 to N do\n\nfor s = 1 to S do\n\ni ← SPS′=1 (ψ 1(z′2)M0ki) − m)◦2\n\nmi ← −1Ps=1f−1(zss; M0ki)\n\nv S11 sS f− ψ ; i\n\n(cj )uJ=1 ← k-means++({ui}i=1, J)i ← vi/∥vi∥2"
  },
  {
    "id_": "ed8a2ee0-2227-4022-a830-84d2cace762f",
    "text": "# for j = 1 to J do\n\nMj ← M0 + RejectionSampling(cj , NM − Tr(M0))\n\nTrain θj to minimize Lˆ[h(·; Mj , θj )]"
  },
  {
    "id_": "2c62c9d1-5864-4df6-b6c8-1592fffae82e",
    "text": "# Algorithm 2 Inference\n\nInput: k-space input k, initial sampling mask M0 ∈ M, trained SR space generation model fψ : I → Z, the number of segments J, masks (Mj )J=1, reconstruction network parameters (θj )J=1, and the centroids of uncertainty (cj )J=1"
  },
  {
    "id_": "458584d9-1ecb-49ad-8d5a-e66b3be131f1",
    "text": "# Output: Reconstructed image I′\n\nfor s = 1 to S do\n\nSample zs ∼ N (0, σs s′2)0k)\n\nm ← SPS′=1 f ψ 1 11PS=1(f ψ ; Ms s−1(z−1(zs; M0k) − m)◦2\n\nu ← v/∥v∥2\n\nj′ ← h(k; Mj⋆ , θj⋆ )\n\nI"
  },
  {
    "id_": "d2227848-1e42-4f58-a22d-26c7c5fad23e",
    "text": "# Adaptive Selection of Sampling-Reconstruction in FCS\n\nHowever, this method is not the optimal approach for maximizing SSIM. In general, it is known that introducing randomness to the mask is effective in maximizing SSIM [34]. Therefore, we generate Mj using rejection sampling proportional to cj. Then, we train the dedicated θj for the corresponding Mj. Figure 2 shows the overview of adaptive selection, clearly showing why the proposed adaptive selection is adaptive. Algorithms 1 and 2 provide detailed descriptions of the training and inference processes of our adaptive selection method, respectively."
  },
  {
    "id_": "99c756ba-c9df-46f1-afe9-7668032fd100",
    "text": "# 4 Experiments\n\nWe proposed the adaptive selection of sampling-reconstruction scheme in Sec. 3 to address the issues of H1 and H2. This section experimentally demonstrates that the proposed method performs well in various settings of Fourier CS."
  },
  {
    "id_": "a25da088-c8d6-489c-83ff-d18f7770a7a2",
    "text": "# 4.1 Fourier CS face reconstruction\n\nWe performed Fourier compressed sensing on the CelebA dataset [21], which consists of 160 × 160 RGB human face images. Similar to LOUPE [3], the reconstruction network used a U-Net [28] architecture, with 6 input channels and 3 output channels, because the input, zero-filling reconstruction, is complex.\n\nFigure 3 shows a qualitative comparison of our method (H1.5) and other H1, H2 methods [3, 5, 34] at the acceleration rate 16× with the metrics of SSIM and PSNR. Our final reconstruction results are superior to those of other methods, which supports Theorems 1 and 2. In detail, our method adeptly selects sampling-reconstruction pairs using HF Bayesian uncertainty. Looking at the first column of Fig. 3, in the case of A, the presence of horizontal stripes in the background results in a high uncertainty in the vertical direction, whereas in B, the elongated blonde hair leads to a high uncertainty in the horizontal HF components. In Fig. 4, which shows the sampling masks (with the selection) and the corresponding reconstruction results of H1.5, our eψ selected M2, which has a shape similar to the uncertainty of A in the second column, obtained the highest SSIM for A. M2 emphasizes in red in the error map, indicating effective suppression of artifacts caused by horizontal high-frequency components in the background, achieved by sampling more in the vertical direction. Similarly, M3 in the third column, which has a shape similar to the uncertainty of B, achieved the highest SSIM for B. M3 is highlighted in the error map, revealing reduced errors in the hair region of the subject due to increased sampling in the horizontal direction."
  },
  {
    "id_": "c9300e5e-b626-4cc4-bf1e-c1db8120a4b5",
    "text": "# 4.2 Multi-coil CS-MRI reconstruction\n\nWe also performed Fourier compressed sensing on the fastMRI multi-coil brain dataset [37]. We resized all slices to a size of 320 × 320. The number of coils was 16. Most implementations of H1 and H2 methods were based on the official."
  },
  {
    "id_": "a0a9760b-e5fd-44a5-839d-2d6b407bcc24",
    "text": "# 10 S. Hong et al."
  },
  {
    "id_": "ada5410e-537d-4653-9b88-dde0ddd7b884",
    "text": "# Ground truth\n\nH1: VD [34]\nH1: LOUPE [3]\nH2: Policy [5]\nH1.5 (ours)\n\nFig. 3: Our adaptive selection of sampling-reconstruction (H1.5) shows the strongest reconstruction performance (emphasized in red). Here, we show a qualitative comparison of reconstruction and error map at acceleration rate 16× in the CelebA dataset [21]. For comparison, we also show the results of the variable density (VD) [34], LOUPE [3], and policy-based adaptive sampling [5]. SSIMs and PSNRs are included in the reconstructions and the error maps, respectively.\n\nfastMRI repository1. We experimented not only with 2D undersampling patterns, as described in Sec. 5, but also with 1D line subsampling used in actual MRI. For the latter, we modified the SR space generation model [30] to achieve a 16× SR only in the horizontal direction.\n\nFigure 5 shows a qualitative comparison of our method (H1.5) and other H1, H2 methods [3, 5, 34] at 4× 1D undersampling with the metrics of SSIM and PSNR. Our final reconstructions outperform other methods, supporting Theorems 1 and 2. In the right half of Fig. 4, our eψ selected (M1, θ1), which samples more of the low-frequency components, obtained the best reconstruction result for A (second row). Besides, for input B (fourth row), (M3, θ3) generated the best reconstruction result (highlighted in red). Since M3 samples the high-frequency components more, it made the clearest imaging of the longitudinal.\n\n1 https://github.com/facebookresearch/fastMRI"
  },
  {
    "id_": "81a1811e-2e71-4776-ad95-302756d3c8bc",
    "text": "# Adaptive Selection of Sampling-Reconstruction in FCS\n\n|0.942|0.928|0.940|\n|---|---|---|\n|34.19|32.55|33.97|\n|0.962|0.961|0.963|\n|39.25|38.59|39.31|\n\nFig. 4: Our adaptive selection of sampling-reconstruction (H1.5) adaptively selects the best sampling-reconstruction pair based on the HF uncertainty of the input, leading to strong reconstruction performance. Here, we show a qualitative comparison of reconstruction and error map obtained using the mask-reconstruction pairs ((Mj , θj )3=1)j generated from Algorithm 1 at acceleration rate 16× in the CelebA dataset [21] and at acceleration rate 4× in the fastMRI dataset [37]. Our Algorithm 2 estimated the uncertainty of each image as in Figs. 3 and 5, and then selected the appropriate mask Mj (with θj) as emphasized in red. For all images in this case, the selected (M, θj)j resulted in the best reconstruction outcomes. SSIMs and PSNRs are included in the reconstructions and the error maps, respectively.\n\nfissure, indicating the effectiveness of the proposed method (i.e., Algorithms 1 and 2).\n\nIn Tab. 2, we present the average SSIM of the proposed scheme in various accelerations and datasets. For comparison, LOUPE [3] and policy-based method [5] are evaluated. Two non-adaptive methods, uniformly random mask and sampling from VD [34], are also evaluated. For a 1D line sampling CS-MRI, equispaced masks are additionally evaluated. As shown in Tab. 2, our adaptive selection approach consistently achieves higher SSIM compared to other methods in all scenarios. For example in CelebA dataset at acceleration rate 8×, SSIM of our method (0.9405) is about 0.04 higher than the best of H1 (0.9073) and"
  },
  {
    "id_": "27f28549-a6f3-41c2-aa68-dc78dbf00d4b",
    "text": "# 12 S. Hong et al.\n\n|Ground truth|H1: VD [34]|H1: LOUPE [3]|H2: Policy [5]|H1.5 (ours)|\n|---|---|---|---|---|\n|A|0.933|0.923|0.923|0.942|\n|Uncertainty of A|32.89|32.44|32.44|34.19|\n|B|0.960|0.956|0.956|0.963|\n|Uncertainty of B|38.59| |37.90|39.31|\n\nFig. 5: In a practical multi-coil CS-MRI 1D line sampling scenario, our adaptive selection of sampling-reconstruction shows the highest SSIM (highlighted in red). Here, we show a qualitative comparison of reconstruction and error map at acceleration rate 4× in the fastMRI dataset [37]. For comparison, we also show the results of the variable density (VD) [34], LOUPE [3], and policy-based adaptive sampling [5]. SSIMs and PSNRs are included in the reconstructions and the error maps, respectively.\n\nH2 (0.8501). In addition in a realistic setting, CS-MRI 1D at acceleration rate 8×, SSIM of our method (0.9407) is about 0.004 higher than the best of H1 (0.9367) and H2 (0.9240), which is a significant difference in MRI reconstruction problem [40]."
  },
  {
    "id_": "e0ba5fb8-550a-4f17-9b07-bbf10eea59c6",
    "text": "# 5 Discussion\n\nDoes the SR space generation model quantify the HF uncertainty well? Since sample variance estimates MSE, evaluating SR space generation can be done by sorting the sample variance, as in Proposition 2. After adaptive sampling and zero-filling for reconstruction, PSNR can be used as a metric for assessment. We qualitatively (in the supplementary material) and quantitatively (in Tab. 3) compare the adaptive sampling results in MRI [37] at acceleration."
  },
  {
    "id_": "f8deb538-f761-4131-891a-de1f0ef39a3d",
    "text": "# Adaptive Selection of Sampling-Reconstruction in FCS"
  },
  {
    "id_": "d29b4fdc-3206-4faf-8a65-76d24d180aad",
    "text": "# Table 2: Our adaptive selection of sampling-reconstruction (H1.5, Algorithms 1 and 2)\n\nshows the highest SSIM in Fourier CS in various settings (CelebA dataset [21] w/ 2D sampling, fastMRI multi-coil dataset [37] w/ 1D, 2D sampling).\n\n|SSIM↑|CelebA|CelebA|CelebA|CS-MRI|CS-MRI|CS-MRI|\n|---|---|---|\n|Method \\ Accel.|8×|16×|4×|8×|4×|8×|\n|Random|0.8378|0.8684|0.9663|0.9506|0.9533|0.9255|\n|H1 VD [34]|0.9073|0.8734|0.9698|0.9578|0.9603|0.9367|\n|LOUPE [3]|0.8742|0.8673|0.9671|0.9525|0.9541|0.9218|\n|Equispace (1D)|-|-|-|-|0.9603|0.9258|\n|H2 Policy [5]|0.8501|0.8394|0.9698|0.9572|0.9569|0.9240|\n|H1.5 Adaptive selection (ours)|0.9405|0.8952|0.9704|0.9585|0.9624|0.9407|"
  },
  {
    "id_": "cda90428-154a-49c5-a164-946232ebfb36",
    "text": "# Table 3: Quantitative comparison of PSNR and SSIM with zero-filling\n\n‘Sorted-Self’ achieved the highest PSNR, which supports our Proposition 2. Thus, we can assert that the SR space generation model effectively quantifies the HF uncertainty.\n\n|PSNR / SSIM|Sorted-Self|Sorted-Another|VD [34]|\n|---|---|---|---|\n|4×|37.15 / 0.939|36.36 / 0.922|33.33 / 0.854|\n|8×|34.79 / 0.910|34.24 / 0.894|32.16 / 0.834|\n\nrate 8×. Table 3 presents the average PSNR and SSIM of the proposed methods on the validation dataset. ‘Sorted-Self’ refers to the zero-filling reconstruction results obtained when sorting its own HF Bayesian uncertainty to create a mask, while ‘Sorted-Another’ randomly shuffles the masks among the data points. We additionally generate a mask sampled from VD [34] for comparison. As a result, the ‘Sorted-Self’ approach consistently achieves the highest PSNR and SSIM. These results show that the SR space generation effectively quantifies HF Bayesian uncertainty."
  },
  {
    "id_": "3c373871-cc08-4906-8945-451fdf0c12b3",
    "text": "# Effect of the number of segments J\n\nHere, we validate Remark 2 by conducting an ablation study using the dataset employed in our experiments. Figure 6low, shows the average of the lowest 5%, 10%, and 100% of SSIM values (SSIM 5% SSIM 10%low, and SSIM) for all J = 1, . . . , 4 in the CS-MRI 8× experiments (2D and 1D). The fact that the increase in SSIM is less noticeable when transitioning from J = 2 to J = 3 or J = 4 compared to the transition from J = 1 to J = 2 supports the first part of Remark 2, “As J increases, despite more training resources, the average performance reaches a plateau at some point.” Additionally, the relatively low SSIM 5% low when transitioning from J = 2 to J = 3 or from J = 3 to J = 4 supports the latter part of Remark 2, “As J increases, it becomes more robust against outliers.” Therefore, users can choose J considering this trade-off."
  },
  {
    "id_": "e42845f3-e33f-4ea8-abec-edd8779bae7d",
    "text": "# 4 ·10−3"
  },
  {
    "id_": "80657b88-5676-465f-ab83-5d8a4f38c868",
    "text": "# SSIM margin w.r.t. J = 13.5\n\n| |J, number of segments| |J, number of segments|\n|---|---|---|---|\n|SSIMlow|0.5|1|1.5|\n|2|2.5|3|4|"
  },
  {
    "id_": "592bc186-1163-4017-b8a0-dc90cf5533c6",
    "text": "# (a) CS-MRI 2D 8×"
  },
  {
    "id_": "b78aad11-fc68-4aa2-95ac-451dc2fc4991",
    "text": "# (b) CS-MRI 1D 8×\n\nFig. 6: Trade-off with the number of segments J. The average of the lowest {5%, 10%, 100%} of SSIM values according to the number of segments J in CS-MRI (a) 2D 8× and (b) 1D 8× experiment, respectively. All SSIM values were shown as margin with respect to J = 1. SSIM reaches a plateau after J = 2, but SSIM 10% more higher in J = 2, 3 or 4. These results support our Remark 2."
  },
  {
    "id_": "e5f646a9-98a1-445a-99f9-5ac83a3da6ae",
    "text": "# Table 4: Comparison of rejection sampling and ‘kmeans-Sorted’.\n\n|SSIM in CS-MRI J = 3|2D 4×|2D 8×|1D 4×|1D 8×|\n|---|---|---|---|---|\n|Rejection sampling (ours)|0.9704|0.9585|0.9624|0.9407|\n|kmeans-Sorted|0.9612|0.9478|0.9493|0.9167|\n\nOther sampling methods We employed the rejection sampling to introduce randomness to the mask [34]. To check the effectiveness of the rejection sampling, we compare it with ‘kmeans-Sorted’ method (i.e., applying ‘Self-Sorted’ in Tab. 3 to the k-means centroids). In Tab. 4, SSIM in rejection sampling is much higher than ‘kmeans-Sorted’ in MRI datasets, supporting the effectiveness of rejection sampling. For more discussions such as runtimes, see the supplementary material."
  },
  {
    "id_": "e1231f6f-8edb-41df-9d3d-203d3b3177d8",
    "text": "# 6 Conclusion\n\nWe have presented an adaptive selection of sampling-reconstruction framework for Fourier CS. Our method uses an SR space generation model to quantify the high-frequency Bayesian uncertainty of each input; hence is adaptive compared to the existing joint optimization of sampling-reconstruction (Theorem 1). Since our method has a dedicated reconstruction network for each sampling mask, unlike adaptive sampling, our method does not suffer from the Pareto suboptimality (Theorem 2). The proposed method improved SSIM in various Fourier CS experiments, such as CS of facial images and CS-MRI in a practical multi-coil setting."
  }
]
