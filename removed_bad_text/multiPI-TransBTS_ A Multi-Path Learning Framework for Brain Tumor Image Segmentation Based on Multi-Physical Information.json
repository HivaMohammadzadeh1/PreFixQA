[
    {
        "id_": "21b555e8-324d-4232-b22f-9fd0ffbeb01a",
        "text": "# 1 INTRODUCTION\n\nBrain tumors, although relatively rare compared to other cancers, pose significant clinical challenges due to their complex and often aggressive nature. Magnetic resonance imaging (MRI) is the only test needed to diagnose a brain tumor, as certain low-grade tumors, such as astrocytomas, not visible on computer tomography (CT) scans, may be detected by MRI [1]. MRI allows for tumor volume measurement and assessment of the tumorâ€™s relationship with critical brain structures, including blood vessels [2]. Consequently, MRI is regarded as the gold standard due to its high spatial resolution and contrast differentiation [3].\n\nThese MRI scans include pre- and post-contrast T1-weighted (T1 and T1Gd), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (T2-FLAIR) volumes [4]. T1Gd is acquired with intravenous gadolinium contrast, and enhancing regions indicate disruption (or absence) of the blood-brain barrier, which is consistent with viable tumor tissue and infiltrated brain [5]. Gadolinium enhancement in post-contrast T1-weighted images reveals focal areas where the blood-brain barrier is compromised, although it may not reveal larger areas of infiltrating tumor [6, 7]. Furthermore, some high-grade gliomas show no gadolinium enhancement [7, 8]. Since different MRI modalities capture distinct characteristics of the underlying anatomy, clinical analysis typically combines multiple MRI modalities for diagnosis and treatment planning [9, 10].\n\nIt is widely accepted that a higher intensity of enhancement, larger areas of necrosis, and edema are associated with higher-grade gliomas and poorer prognoses [5]. Therefore, Brain Tumor Segmentation (BraTS) of MRI scans is essential for clinical diagnosis, treatment planning, and disease monitoring [11, 12]. Accurate tumor delineation aids in planning surgical resection, radiotherapy, and other treatments, enabling surgeons to maximize tumor removal while minimizing damage to healthy brain tissue. Additionally, segmentation allows for the assessment of tumor progression or regression over time, facilitating treatment evaluation and adjustment of therapeutic strategies.\n\nDespite its importance, BraTS is a challenging task. Brain tumors vary widely, with over 20 different types [2]. The tumor variability in appearance, size, location, and intensity, coupled with the similarity between tumor and non-tumor tissue in imaging, presents significant challenges [11]. Furthermore, the difficulty in estimating precise tumor boundaries during surgery is reflected in segmentation labels, resulting in high uncertainty among experts in delineating these boundaries [13]. Manual segmentation is often time-consuming and subjective, emphasizing the need for automated brain tumor segmentation methods [14].\n\nTo address this, researchers have approached BraTS from various angles. Existing methods focus on aspects such as intensity, gradient, shape, contour, texture, and symmetry [3]. These methods range from threshold-based, feature-based, contour-based, and region-based to learning-based approaches, with learning-based methods proving the most effective for BraTS [3].\n\nHowever, current learning-based methods do not fully leverage available information. Specifically: (1) different brain tumor regions exhibit distinct characteristics across different MRI sequences, and (2) different MRI modalities vary in their performance across tumor region segmentation tasks. This limitation impedes further improvements in segmentation accuracy, particularly for boundary voxels that lack contextual information [14].\n\nIn response to these challenges, we propose multiPI-TransBTS, an integrative framework tailored for the BraTS task. This model incorporates multi-physical MRI information within a multi-"
    },
    {
        "id_": "b2ba79ea-4230-49b1-b02f-d399adfc1632",
        "text": "# Task Learning Framework\n\nOur main contributions include:\n\n1. We propose a Transformer-based framework that integrates multi-physical information for BraTS, reducing uncertainty in model representation and thereby improving segmentation accuracy.\n2. We construct a multi-branch network architecture to extract modality-specific features separately, avoiding interference from irrelevant modalities in specific BraTS tasks.\n3. We design an Adaptive Feature Fusion (AFF) module to fuse information from different MRI modalities, forming multi-scale features shared across tasks.\n4. We develop a multi-source and multi-scale feature decoder, which respects the differences between segmentation tasks and fully utilizes both common and individual features.\n5. We conduct comprehensive evaluations using real-world datasets. Our experiments on the BraTS2019 and BraTS2020 datasets demonstrate multiPI-TransBTS's superior performance over existing methods in terms of Dice coefficient, Hausdorff distance, and Sensitivity. To facilitate further research, the source code for the multiPI-TransBTS framework is available at https://github.com/JoetheReindeer/multiPI_TransBTS."
    },
    {
        "id_": "6af4c2fa-8f3f-41e1-9217-6da0e6f8d6ae",
        "text": "# 2 RELATED WORK\n\nMRI is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in clinical practice [15]. Manual segmentation of brain tumor extent from 3D MRI volumes is a very time-consuming task and the performance is highly reliant on the operatorâ€™s experience [16]. In this context, a reliable fully automatic segmentation method for brain tumor segmentation is necessary for an efficient measurement of the tumor extent [16]. For this reason, many research works have been undertaken to apply deep learning techniques for the BraTS task. These techniques can be classified roughly into four categories: CNN-based methods, RNN-based methods, GAN-based methods, and Transformer-based methods."
    },
    {
        "id_": "d1f50647-dd4b-436b-b44a-b72314bab2d3",
        "text": "# 2.1 CNN-based methods\n\nConvolutional Neural Networks (CNNs) inherently incorporate the spatial hierarchy of features within an image. Utilizing local receptive fields and weight sharing, CNNs embed the prior knowledge that similar patterns, such as edges or textures, are likely to recur across various parts of a brain tumor. This underpinning principle has facilitated the expansion of CNN applications within the BraTS domain, leading to the development of numerous models tailored to address the intricacies of these tasks."
    },
    {
        "id_": "8d30d199-ee4c-43a9-ad41-ffc51bcc9dba",
        "text": "# 2.1.1 2D CNN\n\nTraditional CNNs generally consist of several convolutional layers, followed by fully connected layers at the end to output a single label. To adapt this architecture for direct image-image mapping, Fully Convolutional Neural Networks (FCNNs) replace fully connected layers with additional convolutional layers, enhancing their utility in the BraTS challenges. For example,"
    },
    {
        "id_": "a043f6ed-6f84-4da0-ba99-342d3c7b8703",
        "text": "# Pereira et al. [15]\n\nproposed an automatic segmentation method using CNNs that leverage small 3 Ã— 3 kernels, enabling deeper network architectures while mitigating overfitting due to the reduced number of weights. Kamnitsas et al. [17] introduced a dual pathway architecture that processes images at multiple scales to incorporate both local and contextual information more effectively. Zhao et al. [18] combined FCNNs with Conditional Random Fields (CRFs) to achieve segmentation results with appearance and spatial consistency.\n\nLi et al. [19] developed an FCNN based on the U-Net architecture augmented with inception modules, optimizing the model for asymmetrical tumor regions. To further enhance model accuracy, Chen et al. [20] incorporated a Left-Right Similarity Mask (LRSM) into their FCNNs, addressing the inherent asymmetry in tumor imaging. Zhou et al. [14] utilized multi-task networks to distribute common information effectively and address class imbalance within the data. More recent developments include Cinar et al.'s DenseNet-UNet hybrid model [21] and Ullah et al.'s Multiscale Residual Attention-UNet (MRA-UNet) [22], both designed to refine BraTS segmentation accuracy. In addition, Allah et al. [23] introduced the U-Net model for enhanced localization of tumors, and Rehman et al. [24] proposed the RAAGR2-Net with a Residual Spatial Pyramid Pooling (RASPP) module to preserve location information across network layers."
    },
    {
        "id_": "99e0e9fb-d8c8-453e-af6c-8c58a287e3ff",
        "text": "# 2.1.2 3D CNN\n\n3D CNNs offer a solution to the slice-level inconsistencies resulting from 2D CNNs by harnessing the three-dimensional continuity of MRI data. Chen et al. [25] proposed the Multi-Level DeepMedic model that utilizes multi-level information to achieve more precise segmentation. Isensee et al. [26] developed nnU-Net, an adaptable and self-configuring system designed to automatically adjust to various medical imaging tasks without manual intervention. This model effectively addresses the diverse challenges presented by different medical imaging datasets.\n\nAdditionally, Li et al. [27] suggested the use of cascaded 3D U-Nets for enhanced performance in BraTS tasks, while Chang et al. [28] designed a residual dual-path attention-fusion 3D CNN to amalgamate global and local channel information. Raza et al. [29] introduced the dResU-Net, combining features of residual networks and U-Net for robust segmentation capabilities.\n\nDespite their potential, 3D CNNs are often constrained by their substantial computational demands and the network size required, which can become prohibitive, particularly with anisotropic datasets [26]. Therefore, 2D CNNs continue to be a popular choice due to their reduced computational requirements and robust performance across varying imaging conditions [18].\n\nIn summary, CNNs, through their architectural design, introduce general priors concerning spatial hierarchies, translation invariance, and local feature consistency. U-Net, in particular, brings additional specific priors about the importance of multi-scale features and the integration of detailed and contextual information within its unique architecture, proving essential for complex segmentation tasks like those found in BraTS."
    },
    {
        "id_": "44765d57-0d6e-4929-89de-d8d9e374e212",
        "text": "# 2.2 GAN-based methods\n\nGenerative Adversarial Networks (GANs) have significantly enhanced the BraTS performance by generating synthetic images that closely resemble authentic ones, thereby expanding the training."
    },
    {
        "id_": "f8de1a64-b583-42c6-b868-d03b110e6e68",
        "text": "# 2.3 RNN-based methods\n\nRecurrent Neural Networks (RNNs) are initially designed to handle sequential data, which have the same properties as MRI slices. RNN can introduce the prior knowledge that the input data has a temporal or sequential relationship. This is particularly relevant for the BraTS task, where consecutive slices of the brain may show the gradual growth or movement of a tumor.\n\nTo harness this sequential data effectively, Deng et al. [37] integrated a Conditional Random Field with a Recurrent Neural Network (CRF-RNN). This combination leverages the sequential dependencies across slices to improve the consistency and accuracy of segmentation.\n\nLong Short-Term Memory (LSTM) networks, an advanced form of RNNs, are specifically designed to handle long-term dependencies within sequential data. This ability is crucial in brain tumor segmentation, where characteristics of distant slices may influence the interpretation and segmentation of subsequent slices.\n\nBuilding on these capabilities, Hu et al. [38] proposed the UNET-LSTM algorithm, which aims to address the challenge of sample imbalance in the dataset. By integrating LSTM with the U-Net architecture, this approach enhances the model's ability to predict more balanced and accurate segmentations across the dataset."
    },
    {
        "id_": "a63f8995-b358-4373-a20e-c7219540d580",
        "text": "# 2.4 Transformer-based methods\n\nUnlike CNNs, which primarily focus on local correlations through convolutions, Transformers excel in capturing global context due to their self-attention mechanism. This mechanism can model relationships between distant regions in an input image, providing a comprehensive understanding of spatial contexts. Unlike RNNs, which process data sequentially, Transformers can handle different parts of the image in parallel, effectively managing long-range dependencies.\n\nTo fully exploit the merits of both Transformers and CNNs, numerous Transformer-CNN hybrid models have been developed. These models combine the global contextual capabilities of Transformers with the robust local feature extraction of CNNs, particularly leveraging the U-shaped architecture.\n\nFor instance, TransUNet [39] integrates the self-attention mechanism of Transformers with the encoding-decoding structure of UNet. This was one of the first models to effectively blend local and global information for segmentation accuracy increase. UNETR [40] employs a stack of transformers as the encoder, connected to a decoder via skip connections. This design allows for an effective synthesis of multi-level feature information. CKD-TransBTS [5] features a dual-branch hybrid encoder and a feature calibration decoder within a U-Net-like structure, integrating features at various scales.\n\nIn addition, TranSiam [41] consists of two identical sub-networks where convolutions extract detailed information at lower levels, and Transformers handle global information processing at higher levels. SDV-TUNet [42] utilizes multi-head self-attention and sparse dynamic adaptive fusion to meticulously extract global spatial semantic features, crucial for precise BraTS.\n\nTraditional Transformers operate on fixed-size patches, which can somewhat restrict their ability to process multi-scale information. To overcome this limitation, Swin Transformer utilizes a hierarchical feature representation strategy to capture both local and global information adeptly [43]. Similarly, IMS2Trans [44] employs Swin Transformer technology to enable efficient information sharing and fusion among different modalities.\n\nIn summary, the application of Transformers in brain tumor segmentation leverages their ability to capture global context and handle multi-scale information while maintaining minimal inductive biases. The self-attention mechanism allows these models to dynamically focus on crucial regions of the image, making them exceptionally suitable for the complex BraTS task. However, current methods predominantly focus on exploiting the generic capabilities of models and overlook the integration of domain-specific knowledge related to brain tumor imaging. This oversight limits further enhancements in the BraTS performance, suggesting a need for more specialized approaches that incorporate specific clinical and imaging insights into the model architecture."
    },
    {
        "id_": "9488a87d-7034-4595-9199-521b288c27f1",
        "text": "# 3 MRI PRINCIPLES OF BRAIN TUMORS"
    },
    {
        "id_": "dac00e8c-00a7-43df-8a7f-add05f18fe75",
        "text": "# 3.1 Physical principles of MRI\n\nMRI exploits the principles of nuclear magnetic resonance to generate detailed images of the body. In MRI, atomic nuclei with odd numbers of protons or neutrons, such as hydrogen, possess nuclear spin with a quantum mechanical property. By applying a radiofrequency pulse at a specific frequency, which corresponds to the energy difference between two states, these nuclei can be excited from their lower energy state to a higher one. This phenomenon is known as nuclear magnetic resonance [45].\n\nUpon removal of the RF pulse, the nuclei return to their lower energy state through a process called relaxation, which occurs in two primary forms: spin-lattice relaxation and spin-spin relaxation. Spin-lattice relaxation releases absorbed energy to the surrounding molecular lattice, returning the nuclei to thermal equilibrium. The rate of this relaxation is measured by the T1 relaxation time. Spin-spin relaxation involves the dephasing of spins in the transverse plane due to interactions among the spins themselves, without energy transfer to the lattice. The rate of spin-spin relaxation is characterized by the T2 relaxation time [45].\n\nT1-weighted imaging and T2-weighted imaging are techniques used to highlight different tissue characteristics based on their T1 and T2 relaxation times. T1-weighted imaging employs short repetition time (TR) and short echo time (TE) to emphasize differences in T1 properties between different types of tissue. T2-weighted imaging utilizes long TR and long TE to accentuate variations in T2 relaxation time [45].\n\nTo enhance the diagnostic capabilities of MRI, other two techniques tend to be used at the same time. One is gadolinium contrast-enhanced T1-weighted imaging (T1Gd), in which Gadolinium enhances the contrast by shortening the T1 relaxation time of nearby water protons, making the affected areas appear brighter in the images [46]. The other one is T2 Fluid Attenuated Inversion Recovery (FLAIR). This technique involves an inversion recovery pulse that nulls the signal from fluids, particularly cerebrospinal fluid, to suppress the background fluid signal and enhance the detection of lesions [47].\n\nEach of these MRI techniques provides a unique perspective on tissue properties and pathological changes. Moreover, tumorsâ€™ borders are often fuzzy and hard to distinguish from healthy tissue [48]. For these reasons, medical analysis and diagnosis are usually carried out in combination with multiple MRI modalities [9, 10]."
    },
    {
        "id_": "6bdc0c12-c035-4fa5-8ccf-ef0d668893d0",
        "text": "# 3.2 Structure of brain tumors\n\nBrain tumors refer to a diverse collection of intracranial neoplasms, comprising over 20 distinct types, each with its unique biology [1]. The MRI images of brain tumors, illustrated in Fig. 1, are composed of four primary sub-regions: necrotic core (NCR, green), edema (ED, yellow), non-enhancing tumor core (NET, red), and GD-enhancing tumor core (ET, blue) [4, 49]. Each of these sub-regions plays a crucial role in the clinical diagnosis and treatment of brain tumors [50]. It is important to note that these MRI sub-regions do not strictly represent biological entities, but are"
    },
    {
        "id_": "3f1d47b5-5be6-41d3-b39f-270a1c20699b",
        "text": "rather image-based constructs. Often, there is limited evidence in the imaging data for the presence of the non-enhancing solid core [13]. For this reason, the non-enhancing solid core has been excluded from the 2017-present BraTS dataset."
    },
    {
        "id_": "49e053da-d290-4a94-beca-1c3636e6058e",
        "text": "# Fig. 1. The structure of brain tumor.\n\nThe MRI images of a brain tumor consist of four distinct sub-regions: (1) necrotic core (NCR, green), (2) edema (ED, yellow), (3) non-enhancing tumor core (NET, red), and (4) GD-enhancing tumor core (ET, blue), also referred to as the active tumor core. Figure adapted from [4].\n\nDriven by the need for clinical relevance, the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS), a well-established community benchmark, focuses on the segmentation of three tumor regions: (1) the whole tumor (WT), which includes all four sub-regions; (2) the tumor core (TC), which excludes edema but includes the other three sub-regions; and (3) the GD-enhancing tumor core (ET) [13]. The relationship between the segmented regions and the annotated sub-regions in the BRATS datasets is summarized in Table 1."
    },
    {
        "id_": "f9be03de-0130-4523-a1bc-2c9e51219029",
        "text": "# Table 1. The relationship between the segmented regions and the annotated sub-regions in the BRATS datasets\n\n| |NCR/NET (label 1)|ED (label 2)|ET (label 4)|\n|---|---|---|---|\n|WT|1|1|1|\n|TC|1|0|1|\n|ET|0|0|1|\n\nDifferent MRI modalities exhibit varying sensitivities to different types of tissue, suggesting that their contributions may differ in segmenting various tumor regions. To investigate this, we evaluated the impact of different MRI modalities on the segmentation performance for the three tumor-region categories using the classic U-Net model [51]. The results obtained from the BRATS dataset, shown in Fig. 2, are consistent with findings from Yang [52], indicating that different modalities perform differently across the tumor region segmentation tasks."
    },
    {
        "id_": "d70db481-39f3-45c5-b20d-511b1c1c8f87",
        "text": "# 4 METHODOLOGY\n\nBuilding on the observation that different modalities exhibit variable performance across tumor region segmentation tasks, we propose multiPI-TransBTS, a Transformer-based framework that integrates multi-physical information for the BraTS task. The framework leverages common image data attributes such as spatial and semantic information, alongside specific information relevant to"
    },
    {
        "id_": "3fe82350-b5af-4e4d-9a86-43a502a60180",
        "text": "# 4.1 Overall framework\n\nThe overall model design is based on several key considerations: (1) CNNs are effective at capturing spatial information and local patterns, so multiPI-TransBTS predominantly utilizes CNN architectures; (2) As image data are high-dimensional and often contain redundant information, such as similar pixel values in smooth regions, multiPI-TransBTS adopts an encoding-decoding structure; (3) Given the heterogeneity in tumor size and shape, multiPI-TransBTS incorporates skip connections from the U-Net architecture to handle the significant variation in tumor characteristics; (4) Since different brain tumor regions exhibit distinct characteristics across different MRI scan modalities, multiPI-TransBTS employs a multi-branch network architecture to extract modality-specific features separately; and (5) recognizing that different modalities exhibit varying performance across tumor region segmentation tasks, multiPI-TransBTS implements a Task-Specific Feature Introduction (TSFI) strategy.\n\nThe overall framework of multiPI-TransBTS, shown in Fig. 3, is divided into three main components: the encoder, the fusion module, and the decoder."
    },
    {
        "id_": "c4b58a4d-b789-4d09-8a86-04f9c19679d4",
        "text": "# I. Transformer-based encoder\n\nThe encoder is responsible for capturing context and extracting features from the input image. Early encoder layers focus on fine details, while deeper layers capture more global context. To simplify feature expression, features are divided into common features (shared across tasks) and individual features (specific to each task), represented by the red and blue F-marked blocks in Fig. 3.\n\n| |T1|T1Gd|T2|FLAIR|\n|---|---|---|---|---|\n|WT| |85.4|86.3| |\n| |64.8|70.6| | |\n|TC| |79.6| | |\n| |18.1|31.9|29.2| |\n|ET| |62.5| | |\n| |42.5| |42.1| |\n\nFig. 2. Comparison of segmentation performance of three tumor-region categories using different modalities. WT, TC, and ET represent the whole tumor, tumor core, and active tumor, respectively. In the segmentation maps, green corresponds to the edema, yellow represents the enhancing tumor, and red indicates the non-enhancing tumor and necrosis."
    },
    {
        "id_": "dd5b160c-70f1-4c45-92a6-f0946d18cf35",
        "text": "# Framework of multiPI-TransBTS\n\n|WT|Conv|Dec3|Dec2|Dec1|\n|---|---|---|---|---|\n|TC|Conv|DecE3|DecE2|DecE1|\n|ET|Conv|F1|FE|FE|\n| |F2|F3|WT|Conv|\n| | |TCF1|Conv|ETF1|\n| |AFF1|AFF2|AFF3|F4|\n|FLAIR|1|FLAIR|2|FLAIR|\n|3|FLAIR|T2|Emb+Enc1|T2F1|\n|T2|Emb+Enc2|T2F1|Emb+Enc3|T2F1|\n| |4|T1Gd|Emb+Enc1|T1GdF1|\n| |Emb+Enc2|T1GdF1|Emb+Enc3|T1GdF1|\n| |Emb+Enc4|T1|Emb+Enc1|T1F1|\n| |Emb+Enc2|T1F1|Emb+Enc3|T1F1|\n| |Emb+Enc4|SRSA|F1|SRSA|\n| |F2|SRSA|F3|SRSA|\n\nFig. 3. Framework of multiPI-TransBTS. This framework consists of three main modules: an encoder, a fusion module, and a decoder. To simplify feature representation, features are divided into common features shared across tasks and individual features specific to each task, denoted by red and blue F-marked blocks, respectively. \"SRSA\" indicates the Spatial-Reduction Self-Attention module. \"AFF\" and \"FE\" stand for adaptive feature fusion and feature enhancement. \"Conv\" and \"Dec\" are abbreviations for convolution and decoder, respectively."
    },
    {
        "id_": "13d74af7-8f63-45d0-9633-e2d6fecddef5",
        "text": "# I. Introduction\n\nThe traditional U-Net architecture consists of convolutional layers followed by pooling layers, which capture features at multiple scales [51]. To allow the model to focus more on relevant parts of the input, multiPI-TransBTS incorporates self-attention mechanisms. Though the Vision Transformer (ViT) [53] has successfully introduced self-attention mechanisms into image classification tasks, its tendency to produce low-resolution outputs makes it unsuitable for dense prediction tasks.\n\nTo overcome this, we use the Spatial-Reduction Self-Attention (SRSA) modules from the Pyramid Vision Transformer (PVT) [54], which can process dense partitions of an image and achieve high output resolution through a progressively shrinking pyramid structure. However, applying PVT directly to the BraTS task is impractical because PVT was initially designed for single-image segmentation and lacks the capability to fuse multimodal information required in BraTS. Therefore, multiPI-TransBTS leverages SRSA modules to reconstruct the encoder architecture, ensuring effective multimodal information integration."
    },
    {
        "id_": "760740bc-80b4-4e45-8093-474919e302a2",
        "text": "# II. Adaptive feature fusion module\n\nThe fusion module in multiPI-TransBTS integrates complementary information from multiple sources or modalities into a unified representation. It performs multi-modality fusion across multi-scale features extracted by the backbone network, allowing fusion at different resolutions or layers. This strategy effectively combines fine-grained and coarse information, enabling the model to capture a broad range of contextual details.\n\nTo enhance feature representation, the fusion module uses Squeeze-and-Excitation (SE) mechanisms [55], allowing the network to recalibrate channel-wise importance and emphasize key features while suppressing less informative ones. Additionally, it incorporates an element-wise attention mechanism to amplify crucial spatial information, leading to a more accurate representation of important details in the input data.\n\nThis combination of channel-wise and element-wise attention mechanisms strengthens the model's ability to focus on critical features across both spatial and channel dimensions, thereby improving performance in segmentation tasks."
    },
    {
        "id_": "f408309c-34a5-4ab6-b43f-147f9f783677",
        "text": "# III. Multi-feature decoder\n\nThe decoder integrates both common and individual features from various tasks to reconstruct."
    },
    {
        "id_": "857c9b7a-5057-474f-848d-fec0fd7e77af",
        "text": "# 4.2 Transformer-based encoder\n\nThe encoder in multiPI-TransBTS consists of an initial convolutional layer followed by a PVT-like architecture. This architecture is organized into four stages, each responsible for generating feature maps at different scales, as illustrated in Fig. 3. To effectively capture the features from each modality separately, the first three stages include four branches, while the final stage is an exception. Each branch follows a similar structure, represented by the \"SRSA\" module in Fig. 3, which includes an Embedding Layer and Transformer Encoder. The internal details of the \"SRSA\" module are depicted in Fig. 4."
    },
    {
        "id_": "693a01e3-1fd6-4bd7-8347-110795df8bf5",
        "text": "# Fig. 4. \"SRSA\" module in Pyramid Vision Transformer (PVT)\n\nThis module represents the Spatial-Reduction Self-Attention (SRSA) mechanism, which is responsible for reducing the spatial dimension of input sequences while capturing relevant attention."
    },
    {
        "id_": "2e0147df-0c75-401a-8762-9b648492e1ed",
        "text": "# I. Embedding Layer\n\nGiven input scans X with size HÃ—WÃ—C, where H, W, and C represent the height, width, and number of channels, respectively, the image is first divided into HW patches, each of size pÃ—pÃ—C.\n\nThen, a patch embedding operation is performed, which can be formalized as:\n\nğ›‚ = Norá¹ƒ(ğ—ğ–T + ğ›), (1)\n\nwhere Norm denotes Layer Normalization, and ğ–, ğ› are learnable parameters. The resulting ğ›‚ is reshaped, and positional embeddings are added before being input to the Transformer encoder. These operations can be formulated as:\n\nğ›ƒ = Reshape(ğ›‚) + ğ. (2)\n\nHere, Reshape(.) is the reshaping operation, and ğ represents the positional embeddings, which are element-wise added to the input features to inject positional information into the model. These embeddings are initialized as trainable parameters and learned during the training process."
    },
    {
        "id_": "f16cfbfa-7db7-4a05-965d-ab912d55b83d",
        "text": "# II. Transformer Encoder"
    },
    {
        "id_": "5f99e8e1-bb43-4cb9-b640-8f45948f8b64",
        "text": "# The Transformer Encoder\n\nThe Transformer encoder consists of two residual networks. The first residual network can be expressed as:\n\nğœ¸ = SRA(Norá¹ƒ(ğ›ƒ)) + ğ›ƒ, (3)\n\nwhere SRA indicates spatial-reduction attention, formulated as:\n\nSRA(Norá¹ƒ(ğ›ƒ)) = Concat(ğ¡ğŸ, ğ¡ğŸ, â‹¯ , ğ¡ğ’Œ)ğ–ğ (4)\n\nwith ğ¡ğ’Š given by:\n\nğ¡ğ’Š = Softá¹ƒax (((Norá¹ƒ(ğ›ƒ))ğ–ğ’Šğ) (SR(Norá¹ƒ(ğ›ƒ))ğ–ğ’ŠğŠ)ğ‘‡) (SR(Norá¹ƒ(ğ›ƒ))ğ–ğ’Šğ•). (5)\n\nHere, Concat(Â·) denotes the concatenation operation, and SR(Â·) is the operation for reducing the spatial dimension of the input sequence, expressed as:\n\nSR(Norá¹ƒ(ğ›ƒ)) = Norá¹ƒ(Reshape(Norá¹ƒ(ğ›ƒ))ğ–ğ’), (6)\n\nwhere ğ–ğ, ğ–ğ, ğ’Šğ–ğŠ, ğ’Šğ–ğ•, and ğ’Š ğ–ğ’ are learnable weight matrices.\n\nThe second residual network is defined as:\n\nğ›’ = FFN(Norá¹ƒ(ğ›„)) + ğ›„, (7)\n\nwhere FFN(. ) denotes the Feed-Forward Network, given by:\n\nFFN(Norá¹ƒ(ğ›„)) = Max(0; (Norá¹ƒ(ğ›„))ğ–A + ğ›A)ğ–B + ğ›B + ğ›„. (8)\n\nHere, the parameters ğ–A, ğ–B, ğ›A, and ğ›B are learnable weight matrices and bias terms.\n\nFinally, the output Y of the Transformer Encoder is given by:\n\nğ˜ = Reshape(ğ›’) (9)\n\nThis patch embedding operation is applied across multiple stages, progressively shrinking the scale of feature maps. By adjusting the scale of the feature map in each stage, the encoder constructs a feature pyramid, enhancing its ability to handle features at different resolutions."
    },
    {
        "id_": "636b8f52-24c6-4bce-b814-37bc2ee45017",
        "text": "# 4.3 Adaptive Feature Fusion Module\n\nThe Adaptive Feature Fusion (AFF) module is designed to fuse information from different modalities of MRI scans, forming multi-scale features shared across multiple tasks. As shown in:"
    },
    {
        "id_": "dfe9aeb2-d76d-49bd-9d3f-46ec79a98809",
        "text": "# Channel-Wise Feature Enhancement\n\n|FiT1|CÃ—HÃ—W|Concatenation| |LeakRelu| |Sigmoid|\n|---|---|---|---|---|---|---|\n|FiT1Gd|C| |Pooling|Conv1|Conv2| |\n| | |4CÃ—HÃ—W| |4VCbRWÃ—1Ã—1|4VCbRWÃ—1Ã—1| |\n|FiT2|CÃ—HÃ—W| |Sigmoid| |Feature Fusion| |\n|FiFLuIR| |E| |Pooling|Conv3| |\n| | |CÃ—HÃ—W|1Ã—HÃ—W|CÃ—HÃ—W| | |\n| | |4CÃ—HÃ—W| | | | |\n\nElement-Wise Feature Enhancement\n\nFig. 5. Adaptive Feature Fusion (AFF) Module. This module integrates complementary information from different modalities. It combines channel-wise and element-wise attention to enhance feature representation."
    },
    {
        "id_": "675aa2e8-0b0e-43a5-91a2-2c439e669d79",
        "text": "# Fig. 5\n\nThe AFF module consists of three main submodules: channel-wise feature enhancement, feature fusion, and element-wise feature enhancement. The channel-wise feature enhancement uses Squeeze-and-Excitation (SE) Networks, the feature fusion is performed using convolutional networks, and the element-wise feature enhancement employs feature recalibration."
    },
    {
        "id_": "a4d9a759-7336-4158-865b-be9d78c8bb9e",
        "text": "# I. Channel-wise Feature Enhancement\n\nChannel-wise feature enhancement is performed by the standard SE block. In this block, the input feature maps from different modalities are concatenated to combine the information from multiple modalities. Given the feature maps from T1, T1Gd, T2, and FLAIR generated by the OSRSuO module at the i-th stage, represented as ğ…T1, ğ…ğ‘–T1Gd, ğ…ğ‘–T2, ğ…ğ‘–FLAIR, see the blue blocks in Fig. 3. This operation can be formulated as:\n\nğ›—ğ‘– = Concat(ğ…ğ‘–T1, ğ…ğ‘–T1Gd, ğ…ğ‘–T2, ğ…ğ‘–FLAIR) (ğ‘– = 1,2,3). (10)\n\nNext, a global average pooling operation is applied to reduce the spatial dimensions from HÃ—W to 1Ã—1, summarizing the global information of each feature map. The pooled result is then passed through two convolutional layers. The first convolution reduces the channel dimensionality by a factor of R, followed by a non-linearity operation VLeaky ReLU. The second convolution layer restores the original dimensionality. The output is then passed through a Sigmoid function to generate channel-wise attention weights:\n\nğ›‰ğ‘– = Sigmoid(Conv2(LeakyReLU(Conv1(Pooling(ğ›—)))) ğ‘– (ğ‘– = 1,2,3). (11)\n\nHere, Conv() denotes a convolutional operation, and Pooling() represents the pooling operation. LeakyReLU() and Sigmoid() are activation functions, where the former introduces non-linearity and the latter clamps the output between 0 and 1.\n\nThe generated weights ğ›‰ğ‘– are used to recalibrate the original input feature maps, emphasizing the important features. After performing elementwise multiplication, we get:\n\nğ›ğ‘– = ğ›‰â¨‚ğ›—ğ‘–ğ‘– (ğ‘– = 1,2,3). (12)\n\nHere, â¨‚ denotes elementwise multiplication."
    },
    {
        "id_": "7cb020c1-8594-4799-bb2d-2a415a5eb950",
        "text": "# II. Feature Fusion\n\nFeature fusion is performed using a convolution operation to combine the multiple features. This can be mathematically expressed as:\n\nğ›Ÿğ‘– = Conv3(ğ›ğ‘–) (ğ‘– = 1,2,3). (13)"
    },
    {
        "id_": "f5368776-7efd-44eb-85aa-a3d34ad30f73",
        "text": "# III. Element-wise Feature Enhancement\n\nThe element-wise feature enhancement recalibrates the input by multiplying it with weights generated through an average pooling operation, rather than the more complex SE operation. This process can be expressed as:\n\nğ…ğ‘– = ğ›Ÿâ¨‚Sigmoid(Pooling(ğ›Ÿğ‘–)) ğ‘– (ğ‘– = 1,2,3). (14)"
    },
    {
        "id_": "542d3746-e698-44c1-b897-b884ed1af500",
        "text": "# 4.4 Multi-feature decoder\n\nThe decoder in multiPI-TransBTS is responsible for fusing information from different scales and combining both common and individual features to produce the final BraTS results. The decoder consists of three decoding stages, with each stage including three fundamental operations: elementwise multiplication, concatenation, and convolution. These operations are combined in different ways depending on the specific segmentation task. The inputs to the modules also vary across hierarchical levels. This approach is referred to as the Personalized Feature Introduction (PCI) strategy. Fig. 6(a) illustrates the decoder for WT and TC segmentation, while Fig. 6(b) shows the decoder for ET segmentation.\n\n|fi-1Y| | |fY| |Concat|Conv| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| |Concat|Conv5|i|Elementwise Multiplication| | | | | | | | |\n| | | | | |Concat|Conv4|fi-1ET|Concat|Conv2|fETi| | |\n|F4-iT2|Concat|Conv2|Conv3| | | | | | | | | |\n| | | | |X4-i|F4-i|F4-iX|Concat|Conv1|Concat|X4-iT1Gd|Concat|Conv1|\n\nFig. 6. Multi-feature decoder. (a) Decoder for WT and TC segmentation. (b) Decoder for ET segmentation."
    },
    {
        "id_": "7368c1a8-8864-4b8f-958a-fc6a70e03921",
        "text": "# I. Decoders for WT and TC segmentation\n\nIn multiPI-TransBTS, a multi-level decoder is used to achieve multi-scale information fusion. The decoders for WT and TC segmentation, see the \"Dec\" module in Fig. 3, have similar internal structures, as depicted in Fig. 6(a). Based on the correlation analysis between region segmentation and the MRI modalities (see Fig. 2), the WT and TC segmentation decoders only use signals from the two most relevant modalities to prevent noise from overwhelming the signal.\n\nWithout loss of generality, consider the current module to be the decoder at level i (where i=1,2,3). The input data to the decoders for WT and TC segmentation includes four feature vectors:\n\n1. The common feature at level 4âˆ’i, denoted as ğ…4âˆ’ğ‘–,\n2. The fused feature from the previous level, ğŸY 1,ğ‘–âˆ’\n3. The T2-channel feature at level 4âˆ’i, ğ…T2 ,4âˆ’ğ‘–\n4. The other-channel feature at level 4âˆ’i, ğ…X âˆ’ğ‘–.4\n\nLet the output of the decoder be ğŸY. The ğ‘– decoders for the WT and TC segmentation can be represented by the following series of operations:\n\nğ›“ğ‘–1 = Conv1 (Concat ((ğ…4X âˆ’ğ‘–â¨‚ğ…4âˆ’ğ‘–), ğ…4âˆ’ğ‘–)) (ğ‘– = 1,2,3). (15)\n\nğ›“ğ‘–2 = Conv2 (Concat ((ğ…4âˆ’ğ‘–T2 â¨‚ğ…4âˆ’ğ‘–), ğ…4âˆ’ğ‘–)) (ğ‘– = 1,2,3). (16)\n\nğ›“ğ‘–3 = Concat (Conv3 (Concat(ğ›“ğ‘–1, ğ›“ğ‘–2)) , ğ…4âˆ’ğ‘–â¨‚(ğ…4âˆ’ğ‘–X T2 â¨‚ğ…4âˆ’ğ‘–)) (ğ‘– = 1,2,3). (17)"
    },
    {
        "id_": "aefb632a-24f0-46fb-8f48-d4929b6b0b33",
        "text": "# II. Decoder for ET segmentation\n\nThe decoder for ET segmentation (see the \"DecA\" module in Fig. 3), has a structure similar to that of the WT and TC decoders, as depicted in Fig. 6(b). Based on the correlation analysis between ET segmentation and MRI modalities (see Fig. 2), the ET region is highly correlated with the T1Gd channel signal, while less correlated with other channels. Therefore, the decoder for ET segmentation only takes as input the common features and the T1Gd channel features to avoid interference from other channels.\n\nWithout loss of generality, we denote the current module as the decoder at level i (where i=1,2,3). The input to the ET decoder includes three feature vectors:\n\n1. The common feature at level 4âˆ’i, denoted as ğ—4âˆ’ğ‘–,\n2. The fused feature from the previous level, ğŸET,ğ‘–âˆ’1,\n3. The T1Gd-channel feature at level 4âˆ’i, ğ—T1Gd.4âˆ’ğ‘–.\n\nLet the output of the decoder at level i be denoted as ğŸY. The series of operations performed by the ET decoder can be expressed as follows:\n\nğ›‡ğ‘–1 = Conv1 (Concat ((ğ—4âˆ’ğ‘– T1Gd â¨‚ ğ—4âˆ’ğ‘–), ğ—4âˆ’ğ‘–)) (ğ‘– = 1,2,3). (19)\n\nğŸğ‘–Y = Conv2 (Concat(ğŸğ‘–âˆ’1ET , ğ›‡ğ‘–1)) (ğ‘– = 1,2,3). (20)\n\nWhen ğ‘– = 1 or ğ‘– = 2, the inputs ğ—4âˆ’ğ‘– = ğ…4âˆ’ğ‘–, ğ—4âˆ’ğ‘–T1Gd = ğ…4âˆ’ğ‘–T1Gd. However, when ğ‘– = 3, the input features ğ—4âˆ’ğ‘– and ğ—T1Gd 4âˆ’ğ‘– correspond to the edge-enhanced versions of features ğ…4âˆ’ğ‘– and ğ…4âˆ’ğ‘–T1Gd, respectively. If ğ‘– > 1, the fused ET feature ğŸET 1ğ‘–âˆ’1 from the previous level is the output of the previous decoder stage. For ğ‘– = 1, ğŸET 1ğ‘–âˆ’1 is set to ğ…ET 4 (as shown in Fig. 3).\n\nAmong the three segmentation tasks, the performance for ET segmentation is the poorest. To improve ET segmentation accuracy, multiPI-TransBTS introduces a feature enhancement (FE) module into the ET encoder. This module is based on principles from information theory, where greater variability in feature values indicates a higher amount of information. While information entropy is a common measure of information, its computation involves logarithmic operations, which can be time-consuming. Therefore, we choose curvature as a computationally efficient."
    },
    {
        "id_": "ad169aba-f98e-48ee-ad7a-2257620ddc5d",
        "text": "# 4.5 Loss function\n\nSegmentation tasks can be framed as binary classification problems. The most commonly used loss function for classification is cross-entropy [57], which is rooted in the concept of information entropy introduced by Claude Shannon [58]. In the context of image segmentation, the target region is labeled as 1, and the background is labeled as 0. For each of the three tumor regions, we obtain a binary map comparing the multiPI-TransBTS predictions with the ground truth. To handle the issue where log(0) is undefined, the Binary Cross-Entropy (BCE) Loss with clipping for segmentation is expressed as [59]:\n\nğ¿BCE = âˆ‘ğ‘–=1ğ‘ âˆ’[ğ‘¦ğ‘– âˆ™ ğ‘šğ‘ğ‘¥(log(ğ‘¦), âˆ’100) + (1 âˆ’ ğ‘¦) âˆ™ ğ‘šğ‘ğ‘¥(log(1 âˆ’ ğ‘¦), âˆ’100)]\n\nwhere ğ‘¦ğ‘– represents the segmentation result (0 or 1), ğ‘¦ğ‘– is the ground truth label (0 or 1), i denotes the pixel index, and ğ‘ is the total number of pixels.\n\nAnother classical loss function used for image segmentation model training is Dice Loss, which is based on the Dice coefficient [60]. For Boolean data, Dice Loss is mathematically defined as [61]:\n\nğ¿Dice = 1 âˆ’ âˆ‘ğ‘–=1ğ‘ ğ‘¦ğ‘– ğ‘¦Ì‚ğ‘– / (2 âˆ‘ğ‘–=1ğ‘ ğ‘¦ğ‘– + âˆ‘ğ‘–=1ğ‘ ğ‘¦ğ‘–Â²)\n\nwhere ğ‘¦Ì‚ğ‘– represents the segmentation result (0 or 1), ğ‘¦ğ‘– is the ground truth label (0 or 1), i denotes the pixel index, and ğ‘ is the total number of pixels."
    },
    {
        "id_": "28d9460d-20fa-40a7-810a-f644d15d9664",
        "text": "# 5 EXPERIMENT SETUPS"
    },
    {
        "id_": "9a030115-2f3c-47d8-ac36-272675682d5f",
        "text": "# 5.1 Datasets\n\nSimilar to many previous studies [41, 62, 63], the training and testing datasets used in our experiments were obtained from the BraTS2019 and BraTS2020 datasets, which are part of the BraTS Challenge. These datasets consist of multi-institutional pre-operative MRI scans, primarily focused on the segmentation of intrinsically heterogeneous brain tumors in terms of appearance, shape, and histology.\n\nAll BraTS multimodal scans are provided in NIfTI format (.nii.gz) and include a) native T1-weighted (T1), b) post-contrast T1-weighted (T1Gd), c) T2-weighted (T2), and d) T2 Fluid Attenuated Inversion Recovery (FLAIR) volumes. Each dataset has been manually segmented by one to four raters following a standardized annotation protocol, and the annotations were subsequently validated by experienced neuro-radiologists. These annotations encompass the GD-enhancing tumor (ET â€” label 4), the peritumoral edema (ED â€” label 2), and the necrotic and non-enhancing tumor core (NCR/NET â€” label 1). The relationships between these labels and the final segmentation regions are outlined in Table 1.\n\nBraTS2019 and BraTS2020 include 335 and 369 annotated subject samples, respectively. Consistent with prior research [62, 63], we randomly divided the provided brain tumor segmentation dataset into training, validation, and test sets using an 8:1:1 split ratio."
    },
    {
        "id_": "ddd93bff-e0d4-4a9b-9818-828c68343475",
        "text": "# 5.2 Evaluation metrics\n\nConsistent with previous research [62-64], our evaluation employs three widely recognized metrics: Dice coefficient, Hausdorff distance (HD95), and Sensitivity.\n\nThe Dice coefficient evaluates the overall accuracy of the segmentation by measuring the overlap between the predicted segmentation and the ground truth. Mathematically, it is defined as:\n\nDice =\nğ‘€\n1âˆ‘ âˆ‘ ğ‘¦ğ‘–2 + âˆ‘ ğ‘¦ğ‘–2\nğ‘–=1\nğ‘–=1\n2âˆ‘ğ‘ğ‘¦ğ‘–\nğ‘–=ğ‘–Ì‚\n\nwhere ğ‘¦Ì‚ represents the predicted segmentation result (0 or 1), and y is the ground truth label (0 or 1). Here, i refers to the pixel index, N is the total number of pixels per sample, and M is the total number of samples.\n\nThe HD95 is the 95th percentile of the Hausdorff distance, which assesses how well the..."
    },
    {
        "id_": "08780603-b8db-4775-9018-0059f186ff73",
        "text": "# 6 EXPERIMENTAL RESULTS\n\nThe experiments were conducted on a server equipped with two NVIDIA Quadro RTX8000 (48GB) graphics cards. The code was implemented using PyTorch. The initial learning rate was set to 0.01, and the SGD optimizer was employed for learning rate optimization. A batch size of 12 was used during model training, and the total number of training epochs was set to 100."
    },
    {
        "id_": "75268661-e155-40fc-a972-ad946571fa26",
        "text": "# 6.1 Performance analysis\n\nThe performance comparison between multiPI-TransBTS and the baseline methods was conducted on two datasets. The baseline methods consist of 10 models, including both classical approaches, such as IVD-Net [65], UNet++ [66], and AttentionU [67], and recent representative models, like UCTransNet [68] and F2Net [69]. The results for the BraTS2019 and BraTS2020 datasets are presented in Table 2 and Table 3, respectively. In the tables, bold red numbers indicate the best-performing model, while bold black numbers denote the second-best model. Note that"
    },
    {
        "id_": "549658f3-59eb-4264-8c12-9c4353f69354",
        "text": "# Table 2. Performance Comparison on the BraTS2019 Dataset\n\n|Models|Dice|Dice|Dice|Dice|HD95|HD95|HD95|HD95|Sensitivity|Sensitivity|Sensitivity|Sensitivity|\n|---|---|---|---|\n| |WT|TC|ET|Mean|WT|TC|ET|Mean|WT|TC|ET|Mean|\n|UNetV15W [51]|0.944|0.922|0.190|0.919|2.211|1.120|1.434|1.162|0.941|0.939|0.905|0.921|\n|IVD-Net V1lW [15]|0.921|0.902|0.160|0.900|2.340|1.160|1.430|1.110|0.914|0.923|0.112|0.90k|\n|UNetYYV19W [11]|0.945|0.924|0.194|0.921|2.251|1.030|1.331|1.342|0.940|0.930|0.904|0.925|\n|AttentionUV19W [ik]|0.94k|0.921|0.191|0.923|2.220|1.510|1.333|1.211|0.940|0.931|0.191|0.923|\n|TransUnetV21W [39]|0.94k|0.932|0.191|0.925|2.211|1.510|1.312|1.341|0.951|0.939|0.909|0.933|\n|UCTransNet V23W [il]|0.949|0.934|0.903|0.929|2.209|1.558|1.329|1.699|0.944|0.931|0.909|0.930|\n|F2 NetV23W [i9]|0.952|0.941|0.900|0.931|2.222|1.152|1.341|1.341|0.954|0.949|0.905|0.936|\n|SPu-NetV24W [i4]|0.191|0.133|0.061|0.133|5.950|1.120|3.530|5.200|-|-|-|-|\n|Eu-DFFTUV24W [i2]|0.902|0.115|0.3k|0.111|4.900|4.100|5.400|4.9k|0.955|0.114|0.k13|0.l2k|\n|S2 Cu-NetV24W [i3]|0.910|0.144|0.101|0.152|4.001|5.110|3.1k|4.341|-|-|-|-|\n|Ours|0.953|0.944|0.904|0.934|2.177|1.538|1.334|1.683|0.962|0.954|0.199|0.938|\n\nNote that higher values indicate better performance for Dice and Sensitivity, whereas lower values are better for HD95."
    },
    {
        "id_": "a3d435cc-5b24-49c6-b58d-b3a164457bd8",
        "text": "# Table 3. Performance Comparison on the BraTS2020 Dataset\n\n|Models|Dice|Dice|Dice|Dice|HD95|HD95|HD95|HD95|Sensitivity|Sensitivity|Sensitivity|Sensitivity|\n|---|---|---|---|\n| |WT|TC|ET|Mean|WT|TC|ET|Mean|WT|TC|ET|Mean|\n|UNetV15W [51]|0.943|0.922|0.191|0.920|2.235|1.900|2.30|2.341|0.941|0.932|0.901|0.921|\n|IVD-Net V11W [15]|0.931|0.901|0.164|0.904|2.661|2.520|2.153|2.414|0.920|0.921|0.115|0.910|\n|UNetYYV19W [11]|0.945|0.923|0.194|0.922|2.411|2.115|2.216|2.301|0.943|0.931|0.904|0.921|\n|uttentionUV19W [ik]|0.942|0.914|0.191|0.911|2.200|1.534|1.332|1.116|0.931|0.932|0.905|0.925|\n|TransUnetV21W [39]|0.94k|0.932|0.199|0.921|2.222|1.551|1.310|1.411|0.945|0.940|0.902|0.929|\n|UCTransNet V23W [11]|0.94k|0.934|0.904|0.921|2.211|1.550|1.343|1.401|0.940|0.931|0.913|0.930|\n|F2 NetV23W [19]|0.951|0.940|0.902|0.931|2.195|1.544|1.313|1.684|0.953|0.951|0.916|0.940|\n|SPu-NetV24W [14]|0.900|0.132|0.111|0.13k|4.100|1.550|32.20|14.45|-|-|-|-|\n|Eu-DFFTUV24W [12]|0.93k|0.155|0.101|0.111|3.000|3.200|2.500|3.133|0.943|0.921|0.90k|0.921|\n|S2 Cu-NetV24W [13]|0.925|0.119|0.124|0.169|3.043|3.1k|2.k12|3.211|-|-|-|-|\n|Ours|0.954|0.947|0.909|0.937|2.167|1.476|1.271|1.638|0.961|0.951|0.909|0.940|\n\nNote that higher values indicate better performance for Dice and Sensitivity, whereas lower values are better for HD95.\n\nFrom Table 2, it can be observed that multiPI-TransBTS achieves the best Dice, Hausdorff Distance (HD95), and Sensitivity scores for the segmentation of WT and TC. For ET, it achieves the highest Dice score.\n\nSimilarly, from Table 3, multiPI-TransBTS demonstrates superior performance, achieving the best Dice, HD95, and Sensitivity scores for the WT and TC segmentation. For ET segmentation, it again attains the highest Dice score and the best HD95.\n\nOverall, multiPI-TransBTS consistently outperforms the baseline methods across the segmentation of three tumor regions. Although its performance varies slightly between the BraTS2019 and BraTS2020 datasets, the conclusions remain the same: multiPI-TransBTS achieves the best Dice, HD95, and Sensitivity scores for the WT and TC regions. For the ET region, it achieves the best Dice score but does not attain the highest Sensitivity, suggesting a strong ability to accurately segment the WT and TC regions but a limitation in fully capturing all relevant positive ET areas."
    },
    {
        "id_": "217ecbb8-34af-4abf-b8f5-f5f9f37f6981",
        "text": "# 6.2 Visualization of Segmentation Results\n\nTo provide an intuitive comparison of the performance of multiPI-TransBTS with other methods, we visualize the segmentation results on the BraTS2019 and BraTS2020 datasets. Due to space constraints, we randomly selected three samples from each dataset. In the figures, the blue, brown, and yellow regions represent necrotic tumor cores, enhancing tumors, and edematous regions, respectively. Red arrows indicate areas where our results outperform the baselines, while white arrows highlight some false positive results from the baseline methods.\n\nThe visualization of segmentation results on the BraTS2019 dataset is presented in Fig. 8, with the randomly selected samples being TCIA09_255, TCIA03_375, and TCIA08_406. As shown in Fig. 8, the segmentation results from multiPI-TransBTS are notably closer to the ground truth."
    },
    {
        "id_": "db0cfa26-1a0a-401e-b2d8-87a211b16f4a",
        "text": "# 6.3 Ablation study\n\nTo evaluate the contribution of each component within the multiPI-TransBTS framework, we conducted an ablation study. For this purpose, we developed three derived versions of the multiPI-TransBTS model: mT-AFF, mT-PCI, and mT-FE. The specific modifications for each version are as follows:\n\n- (1) mT-AFF omits the Adaptive Feature Fusion (AFF) module;\n- (2) mT-PCI excludes the Task-Specific Feature Introduction (TSFI) strategy;\n- (3) mT-FE removes the Feature Enhancement (FE) module.\n\nThe results of these models on the BraTS2019 and BraTS2020 datasets are shown in Tables 4 and 5, respectively. Tables 4 and 5 provide a detailed performance comparison between the original multiPI-TransBTS model and its derived versions. It is clear that the original multiPI-TransBTS generally..."
    },
    {
        "id_": "315084c2-252e-44b9-9a0d-7351b73c4545",
        "text": "# Fig. 8\n\nVisualization of segmentation results on the BraTS2019 dataset. 'GT' in the figure refers to the ground truth."
    },
    {
        "id_": "6ec0c166-a3b1-4034-855b-a7ebf8adbe1c",
        "text": "# Fig. 9\n\nVisualization of segmentation results on the BraTS2020 dataset. 'GT' in the figure refers to the ground truth."
    },
    {
        "id_": "7d5efc0d-a190-496e-a429-3f7b96ccd6a0",
        "text": "# Table 4. Ablation study results on BraTS2019 dataset\n\n|Models|Dice|Dice|Dice|Dice|HD95|HD95|HD95|HD95|Sensitivity|Sensitivity|Sensitivity|Sensitivity|\n|---|---|---|---|\n| |WT|TC|ET|Mean|WT|TC|ET|Mean|WT|TC|ET|Mean|\n|mT-AFF|0.953|0.943|0.902|0.933|2.19k|1.5k2|1.3i1|1.k10|0.954|0.94k|0.911|0.937|\n|mT-TSFI|0.kk0|0.ll5|0.lkl|0.l44|3.24k|2.129|1.524|2.300|0.i32|0.951|0.900|0.l2l|\n|mT-FE|0.954|0.944|0.902|0.933|2.182|1.521|1.325|1.676|0.952|0.945|0.l9k|0.931|\n|multiPI-TransBTS|0.953|0.944|0.904|0.934|2.177|1.538|1.334|1.683|0.962|0.954|0.l99|0.938|\n\nNote that higher values indicate better performance for Dice and Sensitivity, whereas lower values are better for HD95."
    },
    {
        "id_": "e8faa0de-05be-4e31-aa30-fe788fa71b9b",
        "text": "# Table 5. Ablation study results on BraTS2020 dataset\n\n|Models|Dice|Dice|Dice|Dice|HD95|HD95|HD95|HD95|Sensitivity|Sensitivity|Sensitivity|Sensitivity|\n|---|---|---|---|\n| |WT|TC|ET|Mean|WT|TC|ET|Mean|WT|TC|ET|Mean|\n|mT-AFF|0.950|0.941|0.904|0.933|2.1kk|1.538|1.315|1.iki|0.951|0.944|0.90k|0.934|\n|mT-TSFI|0.li1|0.l05|0.lk5|0.l4k|2.lk9|2.l15|1.i40|2.444|0.kk3|0.965|0.913|0.ll4|\n|mT-FE|0.953|0.946|0.907|0.935|2.169|1.539|1.311|1.673|0.960|0.952|0.l9i|0.936|\n|multiPI-TransBTS|0.954|0.947|0.909|0.937|2.167|1.476|1.271|1.638|0.961|0.951|0.909|0.940|\n\nNote that higher values indicate better performance for Dice and Sensitivity, whereas lower values are better for HD95."
    },
    {
        "id_": "5f3f19b3-857d-4867-b43b-f3d76bfeb5a3",
        "text": "# CONCLUSION\n\nIn this study, we introduced a novel framework for the BraTS challenge, multiPI-TransBTS: a Transformer-based architecture that integrates multi-physical information. This information includes general image attributes such as spatial and semantic information, along with specific information relevant to BraTS, particularly multimodal imaging knowledge.\n\nTo incorporate multi-physical information, multiPI-TransBTS leverages spatial-reduction attention modules, which enable the encoder to integrate multimodal information across different input sequences efficiently. Additionally, an adaptive feature fusion module uses both channel-wise and element-wise attention mechanisms to fuse complementary information from multiple modalities into a unified representation. The decoder employs a personality feature introduction strategy, which blends common features shared across tasks with individual features specific to each task. This process ensures that the abstracted features are optimally reconstructed into segmentation outputs.\n\nThrough rigorous experimental evaluations on two publicly available datasets, multiPI-TransBTS demonstrates superior performance compared to the state-of-the-art models. It achieves the best Dice, HD95, and sensitivity scores for WT and TC segmentation. For ET segmentation, it attains the best Dice and HD95 scores but does not achieve the highest sensitivity, indicating a trade-off between precision and recall. This trade-off presents a future research direction in balancing."
    },
    {
        "id_": "fcb74741-13c2-4369-8bc2-661929b187a8",
        "text": "# Segmentation Performance\n\nOverall, multiPI-TransBTS significantly outperforms existing models, validating the effectiveness of introducing multi-physical information into a Transformer-based framework for BraTS segmentation tasks. These advancements hold the potential to substantially enhance clinical outcomes for patients with brain tumors."
    }
]