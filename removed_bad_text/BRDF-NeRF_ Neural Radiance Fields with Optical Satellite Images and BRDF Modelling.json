[
  {
    "id_": "ecc4af9b-02a6-45f3-9ad6-2c2586fecd3f",
    "text": "# BRDF-NeRF: Neural Radiance Fields with Optical Satellite Images and BRDF Modelling"
  },
  {
    "id_": "c4f45b19-b15a-43e6-bbc9-9f2560343278",
    "text": "# 1. Introduction\n\nOver the past two decades, significant progress has been made in image processing algorithms. In particular, 3D surface reconstruction has bene-\n\nPreprint submitted to ISPRS Journal of Photogrammetry and Remote Sensing September 19, 2024"
  },
  {
    "id_": "6842f3fb-e819-4342-8f5b-0f54a0d90c77",
    "text": "# Modeling Surface BRDF with NeRF\n\nFited from high-resolution spatial data and algorithms such as semi-global stereo matching (SGM), which can generate detailed surface maps of urban and natural environments (Hirschmüller, 2008; Rosu et al., 2015; Pierrot-Deseilligny and Paparoditis, 2006). However, state-of-the-art approaches still face challenges, particularly with radiometrically heterogeneous surfaces, complex reflectance functions, or diachronic acquisitions. Recent research has attempted to address these challenges by leveraging learning algorithms capable of modelling complex resemblance functions, given appropriate architectures and sufficient training datasets (Chang and Chen, 2018; Chebbi et al., 2023; Wu et al., 2024).\n\nAt the same time, a new approach to surface reconstruction has emerged with Neural Radiance Fields (NeRF), which differs from other learning-based methods by operating in a self-supervised manner. It works on single pixels rather than patches, treats non-Lambertian surfaces and generates new synthetic views (Mildenhall et al., 2020). Besides, NeRF is capable of estimating the Bidirectional Reflectance Distribution Function (BRDF) of the surface at the same time. Understanding the BRDF of continental surfaces is crucial for a variety of applications, including land cover mapping, assessment of Earth’s radiation budget, climate change studies, vegetation density analysis, and intercalibration of spaceborne sensors (Dumont et al., 2010).\n\nHowever, due to the anisotropic nature of the Earth’s reflectance, estimation of the BRDF generally requires numerous angular measurements, which is challenging with a few satellite images.\n\nThe aim of this paper is to model the surface’s BRDF explicitly from sparse satellite views, and improve surface reconstruction, particularly over landscapes with anisotropic reflectance characteristics (e.g., bare soil, vegetation). We select NeRF as the algorithmic framework for its capacity to model angle dependent surface reflectance. Our BRDF-NeRF workflow (Figure 1) is designed for satellite acquisitions with only three synchronous views and incorporates the semi-empirical Rahman-Pinty-Verstraete (RPV) BRDF model, widely used in the remote sensing community to represent the BRDF of natural surfaces (Rahman et al., 1993).\n\nTo the best of our knowledge, this work is the first to integrate a BRDF model into neural radiance fields for land surfaces. It extends our previous work on neural radiance fields with sparse optical satellite images (Zhang and Rupnik, 2023). The open-source code is available at github.com/LulinZhang/BRDF-NeRF. The terms DSM and surface, as well as SGM and stereo matching are used interchangeably throughout this article."
  },
  {
    "id_": "3263a45e-8d41-47e6-ad04-4c7ddac7dc03",
    "text": "# Figure 1: BRDF-NeRF Workflow\n\nA few satellite RGB images, and the corresponding low-resolution depth maps calculated using a classical stereo matching algorithm are fed into BRDF-NeRF to predict the normals n, and the RPV parameters ρ0, k, Θ and ρc, describing the surface reflectance. ρ0 represents the amplitude component, k controls the overall shape of the anisotropic behaviour, Θ establishes the degree of forward or backward scattering, and ρc allows to model the hotspot effect. The five parameters are integrated into a RPV renderer to generate the synthetic image. In the meantime, high resolution depths are obtained by accumulating the weights in the volume estimated by BRDF-NeRF."
  },
  {
    "id_": "acb93815-2f49-4a40-b32a-5530fb888cb2",
    "text": "# 2. Related works"
  },
  {
    "id_": "c7a84c5f-a74f-4842-ba6c-9df70525de88",
    "text": "# 2.1. Neural Radiance Fields\n\nThe vanilla NeRF (Mildenhall et al., 2020) leverages a large number of images captured with a pinhole camera to represent small-size scenes as particles that emit light, instead of reflecting light. Subsequent NeRF variants proposed to relax some of those defining constraints, without compromising the quality of the outputs, i.e., the synthesised images and the 3D model. In the following paragraphs we briefly discuss the state-of-the-art approaches relevant to our work."
  },
  {
    "id_": "137ccd87-bbf6-409f-aacf-2082441775d1",
    "text": "# NeRF from few views\n\nSince NeRF relies solely on pixel values for network training, a large number of input images is essential for generating photo-realistic novel views. Attempts to train NeRF with sparse input images often result in overfitting and inaccurate estimation of scene depth, leading to artifacts in the rendered novel views. This limitation restricts the applicability of NeRF and prolongs training time. To address this challenge, efforts have been made to adapt NeRF to sparse input images by introducing various regularization priors. A common approach is to incorporate depth supervision, including sparse depth (Deng et al., 2022; Wang et al., 2023; Somraj and Soundararajan, 2023; Guo et al., 2024) and dense depth (Wei et al., 2021; Roessle et al., 2022; Zhang and Rupnik, 2023). In addition, methods such as image features (Yu et al., 2021) or semantic regularization (Xu et al., 2022) have also been explored."
  },
  {
    "id_": "be8173c7-b878-4dd6-b378-94af489f0593",
    "text": "# NeRF and BRDF\n\nWhile vanilla NeRF excels in view synthesis, it cannot relight or edit materials, due to its inability to decompose outgoing radiance into incoming radiance and surface material reflectance. Some researchers proposed to extend NeRF to incorporate information on the Bidirectional Reflectance Distribution Function (BRDF), which characterises how materials reflect light under different viewing and lighting conditions. The majority of BRDF-compatible NeRF variants, such as those proposed by (Bi et al., 2020; Srinivasan et al., 2021; Boss et al., 2021; Yang et al., 2022a; Verbin et al., 2022; Mai et al., 2023), adopt some version of the microfacet BRDF model (Walter et al., 2007). This model represents reflectance as the superposition of diffuse and specular components and typically includes a surface roughness parameter, influencing the appearance of the surface through a distribution of microfacet orientations. However, while microfacet BRDF models offer effective parameterization, they poorly model BRDF of natural surfaces (e.g., soil, vegetation) which exhibit a more or less marked backscattering behavior (hotspot effect). Additionally, data-driven BRDFs pre-trained on BRDF databases have been explored in NeRF (Zhang et al., 2021). However, the databases consist of artificial materials and have been built in controlled environments. The spectral and directional optical properties of natural materials are often very different."
  },
  {
    "id_": "c6c529b1-6953-4a16-a1a2-8104099eb5e1",
    "text": "# Figure 2\n\nshows the scattering patterns of the Lambertian, Microfacet and RPV models. The latter is an anisotropic BRDF model widely used in remote sensing and which we will use in this work."
  },
  {
    "id_": "f5567219-36bb-4620-af74-078ee06fa9ff",
    "text": "# Figure 2: Scattering Patterns\n\n|(a) Lambertian|(b) RPV: back scattering|(c) RPV: forward scattering|\n|---|---|---|\n|(d) Microfacet|(e) RPV: bell shape scattering|(f) RPV: bowl shape scattering|\n\nLambertian (a), Microfacet (d) and RPV (b, c, e, f) BRDF models. RPV can be used to simulate complex scattering indices."
  },
  {
    "id_": "59eb0197-a088-41cd-acf7-b0a348248eff",
    "text": "# NeRF in Earth Observations\n\nThe Earth observation community has mainly focused on adapting the initial NeRF’s design to meet the specificities of space imagery: changing shadows, dynamic scene due to asynchronous acquisitions, as well as sparse views. Shadow-NeRF (Derksen and Izzo, 2021) pioneered the application of NeRFs to satellite images, where the authors have explicitly modelled the shadows of the scene by leveraging the Sun’s direction. Sat-NeRF (Marí et al., 2022) extends Shadow-NeRF by replacing the pinhole camera model with an empirical push broom model (i.e., Rational Polynomial Coefficients) and modelling transient objects in the scene such as moving cars. EO-NeRF (Marí et al., 2023) employs a novel geometry-based shadow rendering, resulting in more accurate digital surface models (DSMs). SpS-NeRF (Zhang and Rupnik, 2023) further adapted NeRF for scenarios with few satellite views by introducing spatial guidance within NeRF sampling, conditioned on low-resolution input depths. Sat-Mesh (Qu and Deng, 2023) used a latent vector to deal with inconsistent appearances in satellite imagery, while SUNDIAL (Behari et al., 2024) proposed a secondary shadow ray casting technique to jointly learn satellite scene geometry, illumination components and Sun direction. SatensoRF (Zhang et al., 2024) decomposed"
  },
  {
    "id_": "efa854e6-a6b7-4d35-8309-3e1dc30acab5",
    "text": "# 2.2. BRDF in remote sensing"
  },
  {
    "id_": "f836d6e4-b59e-43ee-9c44-f5c1461d003e",
    "text": "# Existing BRDF models\n\nNumerous BRDF models have been developed to describe the spectral and directional reflectance of natural and artificial surfaces. They can be classified into physical, empirical and semi-empirical models.\n\nPhysical models (Pinty and Verstraete, 1991) are based on rigorously defined physical parameters and offer the most accurate descriptions of observed scenes. However, a large number of multiangular observations are required to retrieve these parameters by model inversion, making them impractical for optically complex surfaces.\n\nEmpirical models (Walthall et al., 1985; Minnaert, 1941; Shibayama and Wiegand, 1985) are derived as simple statistical fits to observed data, and provide no additional insights into the surface type or structure.\n\nSemi-empirical models (Rahman et al., 1993; Wanner et al., 1995; Hapke, 1981; Roujean et al., 1992; Lucht et al., 2000) employ specific mathematical functions to best represent the physical interactions between the radiation field and the surface. They accept a reduced number of parameters, which facilitate their inversion.\n\nThe semi-empirical RPV model (Rahman et al., 1993) is among the most commonly used. It is capable of representing the reflectance of various natural surfaces with just four parameters (Figure 2), and has been used to address atmospheric radiation transfer problems (Martonchik et al., 1998a,b); classify forest types (Koukal et al., 2014); simulate plant leaves reflectance (Biliouris et al., 2009); estimate BRF values under unmeasured illumination and viewing angles (Lattanzio et al., 2007); estimate surface albedo (Martonchik et al., 1998a,b; Privette and Roy, 2002); and identify surface properties (Widlowski et al., 2001; Gao et al., 2003)."
  },
  {
    "id_": "4ebc33b8-85bd-4824-abfa-75736551bef2",
    "text": "# Deriving BRDF\n\nMost of the parameters controlling BRDF cannot be measured in the field, but are obtained by inversion of surface reflectance models on observations. To guarantee reliable estimates, the surface must be observed over a wide range of illumination/viewing angles. Laboratory and field measurements using goniophotometers have traditionally been used to measure reflectance (Lv and Sun, 2016; Sandmeier and Strahler, 2000; Combes et al., 2007). Over the last few decades, several spaceborne instruments have been designed to carry out multiangular observations, such as MISR, POLDER, MODIS, CHRIS/Proba, and VIIRS. These instruments have limited spatial resolutions, ranging from a few tens to a few hundreds of meters. (Labarre et al., 2019) have inverted the Hapke model on a set of 21 multiangular Pleiades images, acquired at a spatial resolution of 2m. However, such acquisitions are rarely available and inversion with three or four images is ill-posed.\n\nIn this paper, we explore the potential for estimating the BRDF of natural surfaces using as few as three high-resolution multispectral optical images. Our approach offers new possibilities for studying solar radiation reflected from the Earth’s surface, taking advantage of the multiplicity, temporal coverage, and high spatial resolution of optical satellite imagery."
  },
  {
    "id_": "a26c3c3c-52e0-4c60-b1c6-9a7655668aa7",
    "text": "# 3. Radiance Fields with RPV Reflectance\n\nWe briefly introduce the vanilla NeRF architecture and discuss two key ingredients of our BRDF-NeRF: geometry modelling (depths and normals) as well as the radiometric rendering (RPV BRDF model). The BRDF-NeRF workflow is described in Figure 1."
  },
  {
    "id_": "708ab9b9-6286-40cd-9440-ab157e0ff4ad",
    "text": "# Preliminaries\n\nNeRF (Mildenhall et al., 2020) represents a continuous volumetric field of a static scene that emits light, optimized with a fully connected deep network. Given a 3D point x = (x, y, z) accompanied with a viewing angle wr = (dx, dy, dz), NeRF predicts a volume density σ and a colour c = (r, g, b). NeRF renders images by sampling N query points along each camera ray and accumulating the colours with weights defined by density, and imposes the rendered images to be close to the training images. Each camera ray r is defined by an origin point o and a viewing direction vector wr such that r(t) = o + t * wr. Each query point xi in r is defined as xi = o + t * wr,i where ti lies between the near and far bounds of the scene, tn and tf."
  },
  {
    "id_": "51131af2-56dd-41dc-a4a5-5fb2da58e53c",
    "text": "rendered pixel value C(r) of ray r is calculated as follows:\n\nC(r) = &Sigma; i=1N αi ci (1)\n\nwhith αi = 1 − e−σδi, Ti = Qi−1(1 − αj) and δi = ti+1 − t. αi represents the opacity of the current query point xi and Ti is the transmittance. The contribution of colour ci to the accumulated colour C(r) increases with opacity and transmittance."
  },
  {
    "id_": "8e0a666d-f7cc-4475-a00d-f9abf508d90f",
    "text": "# 3.1. Geometric modelling\n\nWe incorporate geometric information to extend the applicability of BRDF-NeRF to sparse view acquisitions and to predict surface normals that are essential for accurate estimates of BRDF, as detailed in Section 3.2."
  },
  {
    "id_": "db7faff6-ef58-4dd6-8487-cd8998d84ed3",
    "text": "# Depth supervision.\n\nInstead of querying ray points crossing the entire volume of the scene, as is the case in the vanilla NeRF, we narrow it down to a buffer space defined around the location of an approximately known surface. This tactic reduces ambiguity and enables reliable volume densities to be estimated with fewer images. We further encourage the depths predicted by NeRF to remain close to the input surface by using the following loss term introduced in (Zhang and Rupnik, 2023):\n\nLdepth(r) = &Sigma; r∈Rsub (corr(r)(D(r) − D(r))2 , (2)\n\nwhere D(r) are the predicted depths calculated as D(r) = &Sigma; i=1N Tαt,i\n\nwhile the D(r) are the input depths obtained from stereo matching on low-resolution images. We have observed that the performance of stereo matching on low resolution images is marginally affected by a change in surface BRDF and can therefore provide sufficiently good depth initialisations for our radiance fields. The parameter corr(r), which corresponds to the similarity score obtained by stereo matching, acts as a weight or confidence. It adjusts the level of supervision, having a strong impact where confidence is high and a minimal impact where input depths are uncertain. Rsub is a subset of rays that satisfy at least one of the following two conditions: (1) S(r) > Σ(r); (2) (D(r) − D(r)) > Σ(r), where S(r)2 = &Sigma; i=1N Tα(ti − D(r))2 represents the uncertainty of the predicted depth, and Σ(r) = 1 − corr(r) represents"
  },
  {
    "id_": "7607e614-8537-40c6-abcb-000ea1f4f2df",
    "text": "# 3.2. Radiometric rendering\n\nThe geometric approach presented above guarantees decent 3D reconstructions of Lambertian scenes. Next, we adapt this approach to handle non-Lambertian natural surfaces by estimating a BRDF and incorporating it into the rendering Equation (1)."
  },
  {
    "id_": "0d35cc4f-52ee-40c3-81b3-976ef5dc97bd",
    "text": "# RPV equation\n\nWe estimate the reflectance of natural surfaces using the Rahman-Pinty-Verstraete (RPV) model (Rahman et al., 1993), a semi-empirical model well suited to satellite images (see Equation (3)). We chose this model for its simplicity, its physics-based parameters and its ability to represent asymmetric BRDF, including the hotspot effect. The latter corresponds to a sharp increase in reflectance, which becomes maximum when the illumination and viewing directions are coincident.\n\nIn this model, the colour c of a surface point, defined by the normal vector n, the illumination direction wir and the viewing direction wr (Figure 3), is calculated as the product of the incoming light Lir, the cosine of the incident angle |wir · n|, and the bidirectional reflectance factor simulated by RPV:\n\nc(n, wir, wr) = Lir · |wir · n| · RPV(n, wir, wr),           (3)"
  },
  {
    "id_": "8f45bb7a-82ea-42d9-bb4b-c2cfd7ba2f15",
    "text": "# norma"
  },
  {
    "id_": "78743d57-196c-468f-9058-778b9d6294b7",
    "text": "# incident"
  },
  {
    "id_": "8fd99a8f-a473-46ec-b8ff-a1d2d3020918",
    "text": "# irradiance"
  },
  {
    "id_": "c2b3ec76-a6d2-4428-b1ea-ceae040d5df4",
    "text": "# reflected"
  },
  {
    "id_": "071c48b4-3f97-4b22-b4a3-59e262171e3d",
    "text": "# radiance"
  },
  {
    "id_": "0e3af075-94ed-46f2-bbc0-432f6578c75c",
    "text": "# Wir\n\nLir is set to a unit vector, and |wir·n| is approximated by |wir·[0, 0, 1]| because the analytical normal n is not sufficiently smooth. The RP V term can be broken down into an amplitude parameter ρ0 and three angle-dependent functions: modified Minnaert function M, Henyey-Greenstein function FHG, and backscatter function H:\n\nRP V (n, wir, wr) = ρ0 · M (θir, θr, k) · FHG(g, Θ) · H(ρc, G),\n\nwith M (θir, θr, k) = (cosθircosθr(cosθir + cosθr))k−1, FHG(g, Θ) = (1 − Θ2) · (1 + 2Θcosg + Θ2θir + tan2θr, G) = 1 + (1 − ρc)/(1 + G), and the geometric factor G = (tan−2tanθirtanθrcosΦ)1/2. The illumination wir and viewing wr directions are decomposed into zenith angles θir and θr, azimuth angles Φir and Φr, relative azimuth angle Φ and phase angle g, all defined in a spherical coordinate system determined by the surface normal n."
  },
  {
    "id_": "7880312c-d332-4c21-866b-e7a2bde81ad4",
    "text": "# Detailed Description of RPV Model Input Parameters\n\nThe RPV parameter ρ0 in Equation (4) plays the role of pseudo-albedo. The modified Minnaert function controls the anisotropic behaviour of the surface using the parameter k. If k ≈ 1 the surface is quasi-Lambertian; if k<1 a bowl-shaped pattern dominates (reflectance values increase with the viewing zenith angle); and if k>1 a bell-shaped pattern dominates (reflectance values decrease with the viewing zenith angle) (Widlowski et al., 2004). The parameter Θ of the Henyey-Greenstein function controls the amount of radiation scattered in the"
  },
  {
    "id_": "b651ca3e-f6c9-4a0d-afb5-e2693429657a",
    "text": "forward (0 ≤ Θ ≤ 1) or backward (-1 ≤ Θ ≤ 0) directions. The backscatter function H is written as a function of a geometric factor G and a parameter ρc, which represents the sharp increase in reflectance in the hotspot direction. When θir = θr and Φir = Φr, the geometric factor disappears and H reaches its maximum value, contributing to increase total reflectance. When estimating the RPV model, the ranges of variation of the parameters ρ0, k, Θ and ρc are fixed to [0, 1], [0, 2], [-1, 1] and [0, 1] (Koukal et al., 2014).\n\nTo give the reader an intuition about how RPV parameters affect reflectance, we analyse the bidirectional reflectance function (BRF) of selected points in one of our datasets (Figure 4). The BRFs are plotted by varying the viewing directions (zenith and azimuth angles between [0◦, 90◦] and [0◦, 360◦]) and fixing the Sun’s direction to θir = 52.1◦ and Φir = 142.5◦. The pseudo-albedo of the selected surface point estimated by BRDF-NeRF is ρ0 = [0.122, 0.105, 0.091], the normal n=[0, 0, 1]. Among the six combinations of (Θ, k, ρc) given in Table 1, backward scattering (Figure 4 (b)) was predicted by BRDF-NeRF. Note that this BRF is consistent with a result generated independently over the same area with 21 Pleiades views (Labarre et al., 2019)."
  },
  {
    "id_": "95a597cb-03d3-454d-bbd4-997bfcc48d7e",
    "text": "# 1. RPV Model Parameter Sets\n\n|Param.|Backward|Forward|Bowl shape|Bell shape| |Hotspot effect|\n|---|---|---|---|---|---|---|\n|k|0.996|0.996|0.500|1.500|0.996| |\n|Θ|-0.174|0.174|-0.174|-0.174|-0.174| |\n|ρc|0.979|0.979|0.979|0.979|0.500|0|"
  },
  {
    "id_": "97daea37-1b1c-4e5c-af0c-0586429326f1",
    "text": "# 3.3. Network architecture\n\nThe network architecture presented in Figure 5 consists of two progressively trained components: the geometric part and the RPV part. Initially, the geometric part is pre-trained on the assumption of a Lambertian surface. After this pre-training phase (i.e., ∼20% of the total duration of the training time), the RPV part is introduced. Three MLPs predicting k, Θ and ρc as well as the analytical normal n engage in training, while the albedo ρ0 is finetuned to match the pseudo-albedo of our new rendering equation (see Equation (3)). The separation of the geometry part from the RPV part, which handles the case of non-Lambertian surfaces, ensures that the final training stage works on well-initialised normal vectors.\n\nThe input spatial locations x are transformed by positional encoding, the activations for ρ0, k, Θ and ρc are sigmoid function, and the k and Θ are scaled to [0, 2] and [-1, 1] to match their real value ranges (Koukal et al.,\n\n**Figure 4: RPV Model Parameters – BRF**\n|(a) Point location|(b) Backward|(c) Forward|\n|---|---|---|\n|(d) Bowl shape|(e) Bell shape|(f) Hotspot effect|\n\nReflectances displayed in polar coordinates correspond to point A shown in (a). They are generated by evaluating the RPV model over a range of viewing directions, where ρ0 and n are predicted by our BRDF-NeRF and fixed, while the Sun position is fixed and displayed as a white symbol. (b) backward scattering corresponding to the RPV parameters (Θ, k, ρc) found by BRDF-NeRF; (c) forward scattering obtained by modifying Θ; (d-e) transition between bowl shaped and bell shaped scattering obtained by changing the k parameter; (f) hotspot effect with modified ρc. The combinations of the six parameters are provided in Table 1."
  },
  {
    "id_": "38cba2cf-3f62-417e-b31b-de7d216c328f",
    "text": "# Spatial MLP"
  },
  {
    "id_": "5feb45ee-6e45-44f0-8e1b-7b0afde80fbe",
    "text": "# (512 channels)"
  },
  {
    "id_": "2981582b-de04-45bf-ad47-ab0ec87153c1",
    "text": "# Feature MLP"
  },
  {
    "id_": "0f2f2fd2-145e-4453-b1a2-2076935cd17f",
    "text": "# (512 channels)"
  },
  {
    "id_": "54819add-9df2-4d89-ad46-1a1934bfb137",
    "text": "# RPV MLP"
  },
  {
    "id_": "a6da09de-6bb2-4f1f-b507-563dda208059",
    "text": "# (256 channels)\n\n8 layers\n\nRenderer\n\nPretrained on Lambertian sin\n\nWnir Wr\n\nsigmoid\n\nsoftplus\n\nFigure 5: BRDF-NeRF Architecture.\nThe input 3D locations x are sampled along the camera rays and fed into BRDF-NeRF to query density and RPV parameters. BRDF-NeRF consists of shared 8-layer spatial MLPs, 1-layer feature MLP and four 1-layer RPV MLPs. The 8-layer MLPs are followed by a softplus function to predict the density σ, which is further used to calculate the analytical normal n. The RPV MLPs are concatenated with a sigmoid function to predict ρ0, k, Θ and ρc. The elements within the dashed rectangle are pre-trained on the assumption of a Lambertian surface for the first 20% of the total training steps. The colour c is predicted with the RPV rendering equation in Equation (3), where Wir and Wr represent the Sun and camera directions, respectively.\n\n2014). Ultimately, all the parameters of the BRDF-NeRF are optimized to minimise the combination of (1) the colour loss between the ground truth pixel colour C(r) and the predicted pixel colours C(r), and (2) the depth loss Ldepth(r) (Equation (2)):\n\nL = X C(r) − C(r) 2 2 + λLdepth(r), (5)\n\nwhere λ is a weight balancing the contribution of colour and depth. See our experiment on finding optimal λ in Section 4.6."
  },
  {
    "id_": "7190f012-ec5c-423b-b279-64f57d1dbb26",
    "text": "# 4. Numerical experiments\n\nWe conduct a series of experiments to evaluate our BRDF-NeRF on novel view synthesis (Section 4.4) and altitude estimation (Section 4.5) tasks. In addition, we examine the influence of atmospheric correction from TOA (Top Of Atmosphere) to TOC (Top Of Canopy) (Section 4.3) and carry out ablation studies to determine the best training strategy (Section 4.6), and the most optimal way of rendering (Section 4.6).\n\n13"
  },
  {
    "id_": "6898b98d-ab35-4ac0-820f-2b82ab4c0ff6",
    "text": "# Evaluation Metrics\n\nPrecision metrics are Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index measure (SSIM) (Wang et al., 2004) for view synthesis, and Mean Altitude Error (MAE) for altitude extraction. Ground truth (GT) images are true images not seen during training, while the GT surface is a DSM generated with high-resolution Pleiades panchromatic images, with a ground sampling distance (GSD) of 0.5 m, and stereo image matching (Pierrot-Deseilligny and Paparoditis, 2006). BRDF-NeRF is also compared to competitions Sat-NeRF (Mar´ı et al., 2022), SpS-NeRF (Zhang and Rupnik, 2023) as well as DSMs generated with SGM using full-resolution images (i.e., SGMZ1 with GSD = 2m). The source code for EO-NeRF (Mar´ı et al., 2023) was not available for comparison. The RPV parameters are indirectly validated by examining quantitative metrics for novel view synthesis. Additionally, one can compare the BRF in Figure 4 (b) with an independent result derived from 21 Pl´eiades images in (Labarre et al., 2019)."
  },
  {
    "id_": "a264a99c-2707-4a41-989a-b2e8e774b6a5",
    "text": "# 4.1. Implementation Details"
  },
  {
    "id_": "b891b6e4-8e46-46fa-803a-15538dba76e1",
    "text": "# Training\n\nOur network is trained with the Adam Optimizer (lr=5e-4, decay=0.9, batch size=1024). We use SpS-NeRF’s ray sampling strategy to sample 64 stratified points along each ray, accompanied with 64 guided points following a Gaussian distribution. We optimize BRDF-NeRF for 100k iterations, which takes ∼10 hours on NVIDIA GPU with 40GB RAM. For fair comparison, the competitive methods (i.e., Sat-NeRF and SpS-NeRF) are also trained for 100k iterations with 64 + 64 points along each ray, which takes ∼6 hours. BRDF-NeRF is less efficient in training than competitive methods, mainly due to the normal analytical computation. Computational efficiency was not the aim of this work and could be improved in the future with techniques such as tensor decomposition."
  },
  {
    "id_": "eed0ec45-bdb5-46b4-8ff7-2fe331f06cf6",
    "text": "# Light visibility\n\nIt is important to take into account the visibility of samples when scenes contain occlusions, for example when acquiring images in mountainous or urban areas. Visibility describes the transmission of a sample between the light source and the query point. Brute-force computation of the light visibility is computationally expensive, as it requires marching rays from all the query points along the camera ray to the light source. Previous work treats the light visibility with different strategies: (1) assuming that light visibility is the same everywhere (Mai et al., 2023; Boss et al., 2021); in this scenario, a shadow is embedded into the albedo colour; (2)"
  },
  {
    "id_": "dfdbc096-fb69-454e-ac0d-af8711aa87df",
    "text": "# 4.2. Dataset\n\nEvaluations are carried out on two sites (Djibouti, Lanzhou). We extract two regions of interest (∼ 1.5 km × 1.5 km) in each site, which we refer to as A and B (e.g., Dji-A and Dji-B). In each dataset, we distinguish three test scenarios ranging from easy (novel view interpolation) to very hard (novel view extrapolation). The input images are RGB with a GSD of 2m and undergo atmospheric correction prior to processing (see Section 4.3). Sadly, we could not perform experiments on the open DFC dataset (Bosch et al., 2019) representing urban scenes since RPV is designed for natural surfaces."
  },
  {
    "id_": "284aa773-efb0-41e4-a5a2-93271eb76376",
    "text": "# Djibouti Dataset\n\nIt is located in the Asal-Ghoubbet rift (Republic of Djibouti) (Labarre et al., 2019) and consists of 21 multiangular Pleiades 1B images collected in a single flyby on January 26, 2013. Three quasi-nadir images are chosen for training, and three other images for testing, including interpolation and extrapolation scenarios (Figure 6 (a))."
  },
  {
    "id_": "62cdac3c-2c86-4871-b258-af8ab8c7c834",
    "text": "# Lanzhou Dataset\n\nIt is located in Lanzhou (China) and consists of 3 Pleiades 1B images acquired on April 23, 2013 and 3 Pleiades 1A images acquired on June 29, 2013. This is a multi-date dataset where the position of the Sun changes between acquisitions (Figure 6 (b))."
  },
  {
    "id_": "821d43e8-07af-4f0f-b0f1-891fd13b98e8",
    "text": "# Ep 26 0LZUIJ"
  },
  {
    "id_": "266515dc-64f0-4eee-86c5-ad1ca711df78",
    "text": "# Ep 2404422013"
  },
  {
    "id_": "6e79bf69-99b5-4a6e-8860-3a7244636f5c",
    "text": "# Ep 29,0622013"
  },
  {
    "id_": "d7b88fff-f358-45d4-b869-fde2e9ba3d45",
    "text": "# Imaqe_train"
  },
  {
    "id_": "edfdc34b-cc37-4258-98ad-b008772bf31e",
    "text": "# nane trin"
  },
  {
    "id_": "6c0f3c8a-62a7-4ffc-bc45-b06d3a9dad4e",
    "text": "# Imaqe"
  },
  {
    "id_": "d56c157b-8211-4f1b-9547-968755b9f217",
    "text": "# mage_train"
  },
  {
    "id_": "aa473ef1-dce0-4e28-b037-1dbdc900c68b",
    "text": "# nuge"
  },
  {
    "id_": "0c4ec5ec-14e5-4497-b4d3-58df7b1fc24e",
    "text": "# Very hard"
  },
  {
    "id_": "d212d256-4365-4453-961e-4cdb29887d77",
    "text": "# Hard"
  },
  {
    "id_": "09ccbf2e-a6be-4fea-9639-383f0e0c9dcb",
    "text": "# L Eis"
  },
  {
    "id_": "87037823-c149-4a4e-8358-7b13ce0ea85b",
    "text": "# Hard"
  },
  {
    "id_": "e77233e1-4dd8-4262-8b9e-dd7b18cad107",
    "text": "# 2T0\""
  },
  {
    "id_": "7852bd12-e85b-4d0e-9d30-fb611cc580ec",
    "text": "# (a) Djibouti"
  },
  {
    "id_": "7f260ab9-becf-40e4-8709-28c7fa2f0b6e",
    "text": "# (b) Lanzhou\n\nFigure 6: Testing Scenarios – Sun and Viewing Angles. The angles are displayed in polar coordinate systems, where the radius and angular coordinate represent zenith and azimuth, respectively. We design up to three test scenarios, which differ in the viewing angle of the test images. Easy corresponds to a case where the test images are interpolated from the training images, while hard and very hard correspond to extrapolation. Three training images are used in Djibouti, four in Lanzhou (two of each epoch)."
  },
  {
    "id_": "c4c1bd18-895d-4cb1-964f-2b16d70b3358",
    "text": "# 4.3. The effect of atmospheric correction\n\nAtmospheric correction is important for two reasons. Firstly, the atmosphere affects the signal received by the satellite sensor. Secondly, the RPV model, like all BRDF models, describes TOC reflectances whereas Pleiades images are typically supplied as calibrated at TOA. We apply the atmospheric correction using an Orfeo ToolBox (OTB) software library (national d’études spatiales, 2002) which is based on the 6S radiative transfer code (Vermote et al., 1997). The correction model requires four atmospheric parameters: ozone content UO3 (cm-atm), water vapor content UH2O (g/cm2), aerosol optical thickness τA (unitless) and atmospheric pressure Pa (hPa). The first three parameters are estimated from ancillary datasets corresponding to the day of image acquisition, available on NASA’s Earth observation website (https://neo.gsfc.nasa.gov/). The atmospheric pressure is approximated using the formula Pa = 1013.25·(1−0.0065·Z/288.15)5.31, where Z is the surface altitude expressed in meter. The four parameters, along with the adjacency radius for both epochs in the Lanzhou dataset, are shown in Table 2. The Djibouti dataset had been corrected for atmosphere by the satellite image.\n\n16"
  },
  {
    "id_": "90aeb4f4-5986-4315-9c87-43c91beec8ab",
    "text": "# Table 2: Atmospheric Correction Parameters\n\n|Epoch|UO3 (cm-atm)|UH2O2 (g/cm)|τA (unitless)|Pa (hPa)|adjacency radius (-)|\n|---|---|---|---|---|---|\n|23/04/2013|0.3220|1.7333|0.4665|783|1.0|\n|29/06/2013|0.2969|2.5625|0.0980|783|1.0|\n\nValues for ozone content UO3, water vapor content UH2O, aerosol optical thickness τA and atmospheric pressure Pa, as well as the adjacency radius used for atmospheric correction in the Lanzhou dataset."
  },
  {
    "id_": "06d9f660-410f-463f-bce0-bb3b4935ea1f",
    "text": "# Figure 7: Atmospheric Correction – Visualisations\n\nComparison between input images without and with atmospheric correction for the Lzh-A multi-date dataset. The similarity of images tones between different epochs is improved after atmospheric correction.\n\n(a) Without atmospheric correction\n\n(b) With atmospheric correction"
  },
  {
    "id_": "af16584a-4a2b-4a78-ad1e-d672af2e0823",
    "text": "# Figure 8 and Table 3\n\nCompared results based on images with and without atmospheric correction on novel view synthesis and altitude estimation. Results without atmospheric correction gained generally worse metrics and displayed artifacts in synthetic images."
  },
  {
    "id_": "0f6a6c45-b833-41ef-add7-6062fdd46532",
    "text": "# Table 3: Ablation of Atmospheric Correction – Quantitative Evaluation on Lzh-\n\n|Method|AC|MAE ↓|PSNR ↑|PSNR ↑|SSIM ↑|SSIM ↑|\n|---|---|---|---|---|\n| | | |Easy|Hard|Easy|Hard|\n|SpS-NeRF|✘|3.697|28.282|26.131|0.880|0.858|\n|SpS-NeRF|✔|3.558|29.548|24.755|0.965|0.900|\n|BRDF-NeRF|✘|3.439|33.315|29.857|0.946|0.923|\n|BRDF-NeRF|✔|3.420|32.196|28.420|0.979|0.940|\n\nExperiments with ✔and without ✘atmospheric correction (AC). BRDF-NeRF ✔performs best, with the smallest MAE and biggest SSIM. Although BRDF-NeRF ✘shows slightly higher PSNR, qualitative visualisation in Figure 8(b) reveal stripe artefacts, which are not present in BRDF-NeRF ✔in Figure 10(e)."
  },
  {
    "id_": "c4cee2eb-9794-4deb-a0b4-94235afcfbbd",
    "text": "# 4.4. Novel view synthesis\n\nQuantitative metrics are presented in Table 4 while qualitative visualisations are provided in Figures 9 and 10. The visualisations here are limited to the most challenging scenario (very hard for Djibouti and hard for Lanzhou) due to space limitation. Our BRDF-NeRF outperforms Sat-NeRF and SpS-NeRF. Among competitive methods, SpS-NeRF produces less blurry renderings than Sat-NeRF (compare Figure 9 (a) and (c) as well as Figure 10 (b) and (d)). Nevertheless, both methods produce minimal hallucination effects (Figure 9 (a), Figure 10 (b,d)). The two competitive methods remain less sharp and far from the colour tone of the NeRF which includes our realistic RPV-based BRDF (compare Figure 9 (c) and (e)). PSNR and SSIM metrics are best for BRDF-NeRF, followed by SpS-NeRF in second place. The margins between BRDF-NeRF and competitive approaches increase from easy to very hard mode, indicating greater robustness of BRDF-NeRF. From single to multiple epoch datasets (i.e., from Djibouti to Lanzhou), the image quality rendered by Sat-NeRF and SpS-NeRF decreased significantly, while BRDF-NeRF recovered photorealistic images in both cases."
  },
  {
    "id_": "afad67b2-1b54-4c50-ac29-0e10083bb873",
    "text": "# 4.5. Altitude estimation\n\nQuantitative metrics are presented in Table 4, whereas qualitative visualisations are provided in Figures 11 and 12. Sat-NeRF, which was not designed for scenarios with few images, estimates surface altitudes that are tens of meters away from the ground truth surface. SpS-NeRF performs better, thanks to the dense depth supervision, as opposed to supervision with sparse points in Sat-NeRF. Visual assessment reveals that altitudes predicted"
  },
  {
    "id_": "8653e5f1-6cc2-4970-9467-d3786081c444",
    "text": "# Table 4: Quantitative Evaluation\n\n|Metric|Method|Dji-A|Dji-B|Lzh-A|Lzh-B|\n|---|---|---|---|---|---|\n|PSNR|Sat-NeRF|32.747|31.818|29.979|25.470|\n| |SpS-NeRF|38.832|38.174|29.548|30.904|\n| |BRDF-NeRF|41.844|40.823|32.196|32.165|\n|PSNR|Sat-NeRF|25.542|23.699|25.963|20.811|\n| |SpS-NeRF|28.348|27.468|24.755|24.148|\n| |BRDF-NeRF|36.232|35.448|28.420|27.814|\n|PSNR|Sat-NeRF|23.581|21.288|/|/|\n| |SpS-NeRF|23.144|22.31|/|/|\n| |BRDF-NeRF|33.35|32.376|/|/|\n|SSIM|Sat-NeRF|0.927|0.950|0.962|0.925|\n| |SpS-NeRF|0.975|0.979|0.965|0.970|\n| |BRDF-NeRF|0.985|0.988|0.979|0.975|\n|SSIM|Sat-NeRF|0.766|0.825|0.909|0.760|\n| |SpS-NeRF|0.840|0.887|0.900|0.928|\n| |BRDF-NeRF|0.957|0.965|0.94|0.953|\n|SSIM|Sat-NeRF|0.676|0.768|/|/|\n| |SpS-NeRF|0.614|0.72|/|/|\n| |BRDF-NeRF|0.918|0.942|/|/|\n|MAE|Sat-NeRF|12.85|18.059|61.299|27.489|\n| |SpS-NeRF|1.438|1.761|3.558|3.235|\n| |BRDF-NeRF|1.378|1.614|3.42|2.941|\n| |SGMZ1|1.061|1.052|1.409|1.220|\n\nThe best and second best performing metrics are in blue and magenta. For each dataset, we train a BRDF-NeRF to render three images in easy, hard (and very hard when existing) modes at the same time. BRDF-NeRF achieves better PSNR, SSIM and MAE than Sat-NeRF and SpS-NeRF. However, BRDF-NeRF has higher MAEs than SGMZ1, which we attribute to NeRF’s design that handles pixels individually without taking context into account."
  },
  {
    "id_": "283bb033-2d37-4d5a-a015-6e76cf3090b5",
    "text": "# Figure 8: Ablation of Atmospheric Correction – View Synthesis and Altitude Estimation Visualisation\n\nResults on the Lzh-A site using images without atmospheric correction (✘AC). The view synthesis is displayed in the first line, and the altitude estimation in the second. BRDF-NeRF synthesises images with fewer artifacts than SpS-NeRF and estimates surface closer to GT. Performance is further improved on images with AC (Figure 10 (e) and Figure 12 (e)).\n\nby Sat-NeRF are either flat (Figure 11 (a-b)), or contain a made up pattern (Figure 12 (a)). SpS-NeRF altitudes are more faithful to ground truth but remain noisy. Our BRDF-NeRF outperforms both versions of NeRF, producing less noisy surfaces while retaining detail.\n\nCompared with surfaces obtained with the classical stereo matching (Pierrot-Deseilligny and Paparoditis, 2006), BRDF-NeRF appears smoother and less detailed (compare Figure 12 (f) and (h)) in areas with good texture. However, on poorly textured areas where stereo matching is challenging, our BRDF-NeRF predicts coherent altitudes (compare Figure 11 (e) and (g)). Note also that our ground truth surface was generated with stereo matching algorithms, thus the comparison is possibly slightly biasing the MAE metric in favour of the stereo matching surface.\n\n20"
  },
  {
    "id_": "591f14a1-f525-4bea-b325-649c9d746bc2",
    "text": "# Figure 9: Novel View Synthesis – Djibouti Dataset\n\n|(a) Sat-NeRF Dji-A|(b) Sat-NeRF Dji-B|\n|---|---|\n|(c) SpS-NeRF Dji-A|(d) SpS-NeRF Dji-B|\n|(e) BRDF-NeRF Dji-A|(f) BRDF-NeRF Dji-B|\n|(g) GT Dji-A|(h) GT Dji-B|\n\nRenderings of sites A and B of the Djibouti dataset in the very hard scenario (Figure 6). Sat-NeRF and SpS-NeRF renderings are less sharp and detailed than BRDF-NeRF. Note that the colour tone of BRDF-NeRF is closest to that of ground truth (GT).\n\n21"
  },
  {
    "id_": "4d462e63-763f-43ed-abd2-e85f7d2e72c8",
    "text": "# Figure 10: Novel View Synthesis – Lanzhou Dataset\n\nRenderings of sites A and B of the multi-date Lanzhou dataset in the hard scenario. Hallucination artefacts are revealed in both Sat-NeRF and SpS-NeRF but are more pronounced in the latter. Despite this, SpS-NeRF renders sharper surface details than Sat-NeRF. BRDF-NeRF generates images with fine details and much less artifacts.\n\n|(a) Sat-NeRF Lzh-A|(b) Sat-NeRF Lzh-B|\n|---|---|\n|(c) SpS-NeRF Lzh-A|(d) SpS-NeRF Lzh-B|\n|(e) BRDF-NeRF Lzh-A|(f) BRDF-NeRF Lzh-B|\n|(g) GT Lzh-A|(h) GT Lzh-B|\n\n22"
  },
  {
    "id_": "6edf9725-d1ed-42a1-b06b-676d5148f8a4",
    "text": "# Figure 11: Altitude Estimation – Djibouti Dataset\n\n|(a) Sat-NeRF Dji-A|(b) Sat-NeRF Dji-B|\n|---|---|\n|(c) SpS-NeRF Dji-A|(d) SpS-NeRF Dji-B|\n|(e) BRDF-NeRF Dji-A|(f) BRDF-NeRF Dji-B|\n|(g) SGMZ1 Dji-A|(h) SGMZ1 Dji-B|\n|(i) GT Dji-A|(j) GT Dji-B|\n\nSat-NeRF fails to recover the altitudes. SpS-NeRF recovers noisy altitudes. The noise is suppressed to some extent in BRDF-NeRF. SGMZ1 generates generally more detailed surfaces, but underperforms in weakly textured areas (see green rectangle)."
  },
  {
    "id_": "bd71e814-9dfe-4607-92ca-96233fb108a9",
    "text": "# Figure 12: Altitude Estimation – Lanzhou Dataset\n\n|(a) Sat-NeRF Lzh-A|(b) Sat-NeRF Lzh-B| |\n|---|---|---|\n|(c) SpS-NeRF Lzh-A|(d) SpS-NeRF Lzh-B| |\n|(e) BRDF-NeRF Lzh-A|(f) BRDF-NeRF Lzh-B| |\n|(g) SGMZ1 Lzh-A|(h) SGMZ1 Lzh-B| |\n|(i) GT Lzh-A|24|(j) GT Lzh-B|\n\nSat-NeRF fails to recover the altitudes. Surfaces obtained by SpS-NeRF and BRDF-NeRF are similar, with SpS-NeRF manifesting slightly noisier altitude predictions and a hallucination artefact. SGMZ1 recovers surfaces with more detailed topography but more noise than SpS-NeRF and BRDF-NeRF."
  },
  {
    "id_": "dad2a8b6-682d-47da-bb48-d3d78a6d4ba7",
    "text": "# 4.6. Ablations"
  },
  {
    "id_": "b040037d-42d9-40a9-8ea6-2359aef612bd",
    "text": "# Training strategy\n\nOur BRDF-NeRF model is trained progressively to ensure proper initialisation of the geometry (i.e., density weights in NeRF) before learning the RPV parameters. We perform an ablation with different pre-training strategies to determine the optimal point at which the transition from Lambertian to RPV model should occur. In the Preno approach, the entire network is trained without pre-training. In the Presho, Premed and Prelon approaches, we start from the Lambertian assumption and switch to the RPV model at different training steps, as shown in Figure 13. The initial learning rate is set to 5e4 and decreases to 3.65e5, 2.15e5 and 1.27e5, respectively.\n\nQualitative results are presented in Figures 14 and 15, while quantitative metrics are provided in Table 5. The absence of pre-training leads to blurry synthetic images and noisy altitude estimations. Performance differences between Presho, Premed, and Prelon are minor, with Premed emerging as the most optimal choice."
  },
  {
    "id_": "fe1741ce-2b6e-4b7f-927f-5d4cd52d36ad",
    "text": "# Depth Loss Weighting\n\nWe perform an ablation experiment to evaluate the contribution of the depth loss term. Removing the term entirely results in very poor altitude predictions, confirming that a simple NeRF architecture is unable to learn from just three views. By increasing the weight in the range [1, 3 50], altitude metrics improve consistently (see Figure 15). The PSNR and SSIM metrics corresponding to the synthetic image quality reach a maximum at λ = 3 10, suggesting that assigning greater importance to depths compromises the rendering quality. We set the λ = 3 10 in all our experiments."
  },
  {
    "id_": "37274c03-23fb-44be-ac93-0ab32b285daf",
    "text": "# Surface or volume rendering\n\nThe rendering equation in Equation (3) can be applied as surface rendering (Rensur) or volume rendering (Renvol). In surface rendering, the RPV parameters n, ρ0, k, Θ and ρc are estimated at the surface by accumulating N points along the ray and applying the rendering once for each ray. In volume rendering, rendering is applied to each sample individually, associating it with a color c, and accumulation is done on the sample colours instead of RPV parameters. Rensur is more rigorous than Renvol, as the latter assumes every point along the ray follows the RPV reflectance, while Rensur assumes the same only for points on the surface which is concordant with the RPV model definition. We demonstrate in Table 6 and Figure 16 that Rensur outperforms Renvol and adopt this rendering method in our experiments."
  },
  {
    "id_": "e01d81f8-91f7-49db-a007-931b67a27401",
    "text": "# Figure 13: Training Strategies\n\nFours training strategies of BRDF-NeRF from no pre-training to long pre-training.\n\n|Pre|λ|MAE ↓| |PSNR ↑| | |SSIM ↑| |\n|---|---|---|---|---|---|---|---|---|\n| | | |Easy|Hard|VHard|Easy|Hard|VHard|\n|no| |1.632|38.057|33.446|31.186|0.966|0.93|0.884|\n|sho|10|1.449|41.166|35.36|32.999|0.983|0.951|0.913|\n|med|3|1.378|41.844|36.232|33.35|0.985|0.957|0.918|\n|lon| |1.39|41.415|35.645|31.663|0.984|0.947|0.88|\n| |λ = 0|9.432|39.408|34.755|31.870|0.973|0.941|0.900|\n|med|λ = 31|1.877|40.894|35.822|33.318|0.982|0.951|0.914|\n| |λ = 350|1.353|41.109|34.915|32.107|0.983|0.949|0.908|\n\nTable 5: Training Strategies – Quantitative Evaluation. Pre refers to the various training settings shown in Figure 13, while λ is the parameter that balances the contribution of colour and depth losses in Equation (5). The best and second best performing metrics are in blue and magenta. Premed achieved the best PSNR and SSIM and the second best MAE. λ = 350 ranks the best for MAE, but has worse PSNR and SSIM than Premed. Tests correspond to Dji-A dataset.\n\n|MAE↓| |PSNR↑| | |SSIM↑| | |\n|---|---|---|---|---|---|---|---|\n| |Easy|Hard|VHard|Easy|Hard|VHard| |\n|Renvol|1.399|41.743|35.867|31.770|0.985|0.955|0.903|\n|Rensur|1.378|41.844|36.232|33.350|0.985|0.957|0.918|\n\nTable 6: Rendering – Quantitative Evaluation of Renvol and Rensur. Renvol produces new views and surfaces close to Rensur with slightly poorer metrics overall. Tests correspond to Dji-A dataset."
  },
  {
    "id_": "c1afaae5-53ed-4bc3-a074-9b1df7f5a059",
    "text": "# Figure 14: Training Strategies – View Synthesis Visualisation\n\nDifferent pre-training strategies are tested on the very hard scenario of Dji-A. Preno generates a blurry image; Presho, Premed and Prelon synthesised images that are close to GT, while Premed shows advantages in restoring better details, particularly in the zoom-in view labeled in the orange rectangle. λ = 0 recovers blurry image, λ = 31 and λ = 350 synthesised novel views with unified image tone, but zoom-in view shows blurrier detail than λ = 3.\n\n|(a) Preno, λ = 310|(b) Presho, λ = 310|\n|---|---|\n|(c) Premed, λ = 310|(d) PreLon, λ = 310|\n|(e) Premed, λ = 0|(f) Premed, λ = 31|\n|(g) Premed, λ = 350|(h) GT|"
  },
  {
    "id_": "f3c4a35e-d840-4452-8425-087e36ccb2b6",
    "text": "# Figure 15: Training Strategies – Altitude Estimation Visualisation on Dji-A sites\n\nUsing different training strategy for BRDF-NeRF. Preno’s surface is very noisy; Presho, Premed and Prelon generate similar surfaces, among which Presho is the noisiest. λ = 0 fails to recover topographic reliefs due to a lack of depth supervision. Performance is improved in λ = 31, with noisy details. λ = 31 estimates surface smoother than λ = 350, thanks to the good quality of input depth.\n\n|(a) Preno, λ = 310|(b) Presho, λ = 310|\n|---|---|\n|(c) Premed, λ = 310|(d) Prelon, λ = 310|\n|(e) Premed, λ = 0|(f) Premed, λ = 31|\n|(g) Premed, λ = 350|(h) GT|"
  },
  {
    "id_": "346af381-260d-4366-b3b5-31c833dc5f24",
    "text": "# Figure 16: Rendering – Visualisations\n\n|(a) Renvol novel view|(b) Renvol surface|\n|---|---|\n|(c) Rensur novel view|(d) Rensur surface|\n|(e) GT novel view|(f) GT surface|\n\nRenvol rendered novel view with similar quality to Rensur, while zoom-in shows blurrier details. The surface qualities of Renvol and Rensur are similar with no obvious differences.\n\n29"
  },
  {
    "id_": "ca45b19c-ba74-4c2c-9d50-4ea7ecb1dbc4",
    "text": "# 5. Conclusion\n\nWe presented BRDF-NeRF, an extension of NeRF adapted to sparse satellite imagery, capable of estimating realistic BRDFs for natural surfaces. By incorporating the semi-empirical Rahman-Pinty-Verstraete (RPV) model, BRDF-NeRF enhances the rendering of anisotropic surface reflectance, leading to improved quality of both synthetic images and recovered surface altitudes. Although our experiments show promising results, certain limitations remain. At present, our method does not explicitly model shading effects, and although our surface reconstructions outperform other NeRF-based approaches, they still lack the regularity achieved by SGM. Future work will address these challenges."
  }
]
